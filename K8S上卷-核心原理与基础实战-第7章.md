# 第7章：调度与资源管理

> 在第6章中，我们系统学习了Kubernetes的配置与安全管理，包括ResourceQuota资源配额、Pod安全策略、RBAC权限控制和网络策略。本章将深入探讨Kubernetes的调度机制和高级资源管理，揭示Pod如何被智能分配到合适的节点，以及如何优化资源利用率。

**本章学习目标：**
- 深入理解Kubernetes调度器（kube-scheduler）工作原理
- 掌握节点选择器（NodeSelector）和节点亲和性（Node Affinity）
- 学习Pod亲和性与反亲和性（Pod Affinity/Anti-Affinity）
- 掌握污点（Taints）和容忍（Tolerations）机制
- 理解优先级（Priority）和抢占（Preemption）
- 实战：构建高可用的多层应用架构

---

## 7.1 Kubernetes调度器原理

Kubernetes调度器（kube-scheduler）是集群的"交通指挥官"，负责将新创建的Pod分配到合适的节点上运行。理解调度器的工作原理，是优化应用性能和资源利用率的关键。

### 7.1.1 为什么需要调度器

#### 7.1.1.1 资源分配的挑战

在一个包含数百个节点和数千个Pod的Kubernetes集群中，如何高效地分配资源是一个复杂的问题：

```
典型的生产集群场景：
┌──────────────────────────────────────────────────┐
│  集群规模：                                       │
│  - 节点数量：200个                                │
│  - Pod总数：5000+                                │
│  - 每天新建Pod：1000+                            │
│                                                   │
│  节点异构性：                                     │
│  - CPU型节点：64核CPU，128GB内存                 │
│  - 内存型节点：32核CPU，512GB内存                │
│  - GPU节点：16核CPU，256GB内存，8卡V100          │
│  - 边缘节点：8核CPU，16GB内存（低延迟）          │
│                                                   │
│  应用多样性：                                     │
│  - Web应用：CPU中等，内存中等，需要高可用        │
│  - 数据库：CPU低，内存高，需要持久化存储         │
│  - 机器学习：CPU高/GPU，内存极高，计算密集       │
│  - 消息队列：CPU中等，内存高，网络IO密集         │
│                                                   │
│  调度约束：                                       │
│  - 亲和性：前端Pod需要靠近后端Pod（降低延迟）   │
│  - 反亲和性：同一应用的多个副本分散到不同节点    │
│  - 污点容忍：只有特定Pod能运行在GPU节点          │
│  - 资源预留：保证关键应用的资源需求              │
└──────────────────────────────────────────────────┘
```

**没有智能调度的后果：**

| 问题 | 场景 | 影响 |
|-----|------|------|
| **资源浪费** | 小Pod调度到大节点 | 资源碎片化，浪费严重 |
| **性能下降** | CPU密集Pod和内存密集Pod挤在同一节点 | 资源竞争，性能下降 |
| **高可用性差** | 同一应用的多个副本都在一个节点 | 节点故障导致服务中断 |
| **负载不均** | 部分节点超载，部分节点空闲 | 集群利用率低 |
| **延迟增加** | 前后端应用跨可用区部署 | 网络延迟增加 |

#### 7.1.1.2 调度器的价值

**Kubernetes调度器提供的核心价值：**

```
┌─────────────────────────────────────────────────┐
│          调度器核心价值                          │
├─────────────────────────────────────────────────┤
│  ✅ 智能资源分配                                 │
│     根据节点资源和Pod需求，找到最佳匹配         │
│                                                  │
│  ✅ 高可用保障                                   │
│     通过反亲和性将副本分散到不同节点/可用区     │
│                                                  │
│  ✅ 性能优化                                     │
│     将相互依赖的Pod调度到同一节点/可用区        │
│                                                  │
│  ✅ 资源利用率最大化                             │
│     Bin Packing算法，尽量减少资源碎片           │
│                                                  │
│  ✅ 灵活的调度策略                               │
│     支持自定义调度规则和优先级                  │
│                                                  │
│  ✅ 故障自愈                                     │
│     节点故障时自动将Pod重新调度到健康节点       │
└─────────────────────────────────────────────────┘
```

**真实案例对比：**

```yaml
# ❌ 场景1：没有调度约束
# 结果：3个Nginx副本都调度到了Node1
kubectl get pods -o wide
NAME                     NODE
nginx-7c5ddbdf54-abc12   node1
nginx-7c5ddbdf54-def34   node1  # ❌ 高可用性差！
nginx-7c5ddbdf54-ghi56   node1  # ❌ Node1故障则全部宕机！

# ✅ 场景2：使用反亲和性
# 结果：3个副本分散到不同节点
kubectl get pods -o wide
NAME                     NODE
nginx-7c5ddbdf54-abc12   node1
nginx-7c5ddbdf54-def34   node2  # ✅ 高可用！
nginx-7c5ddbdf54-ghi56   node3  # ✅ 单节点故障不影响服务！
```

#### 7.1.1.3 调度的类型

Kubernetes支持多种调度方式：

```
┌────────────────┬───────────────────┬─────────────────────┐
│  调度类型      │    触发条件       │    使用场景         │
├────────────────┼───────────────────┼─────────────────────┤
│ 默认调度       │ Pod创建时         │ 大部分应用          │
│ (Default)      │ nodeName字段为空  │ (90%+场景)          │
├────────────────┼───────────────────┼─────────────────────┤
│ 绑定调度       │ Pod创建时         │ 特定节点部署        │
│ (NodeName)     │ nodeName字段指定  │ (DaemonSet等)       │
├────────────────┼───────────────────┼─────────────────────┤
│ 重新调度       │ 节点故障          │ 故障恢复            │
│ (Rescheduling) │ Pod被驱逐         │                     │
├────────────────┼───────────────────┼─────────────────────┤
│ 抢占调度       │ 资源不足          │ 高优先级Pod         │
│ (Preemption)   │ 高优先级Pod到达   │                     │
├────────────────┼───────────────────┼─────────────────────┤
│ 自定义调度     │ 配置自定义调度器  │ 特殊调度需求        │
│ (Custom)       │ schedulerName字段 │ (AI/大数据场景)     │
└────────────────┴───────────────────┴─────────────────────┘
```

### 7.1.2 调度器工作流程

#### 7.1.2.1 调度流程概览

Kubernetes调度器采用两阶段调度策略：**预选（Predicates）→ 优选（Priorities）**

```
┌─────────────────────────────────────────────────────────────┐
│              Kubernetes调度器工作流程                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  1. 监听Pod创建                                              │
│     kube-scheduler通过Watch机制监听API Server              │
│     发现新创建的、nodeName为空的Pod                         │
│          ↓                                                   │
│  2. 预选阶段（Predicates/Filtering）                        │
│     过滤掉不符合条件的节点                                  │
│     ┌─────────────────────────────────────────┐            │
│     │ 所有节点（200个）                       │            │
│     └─────────────────────────────────────────┘            │
│          ↓                                                   │
│     [ 预选算法 ]                                             │
│     - PodFitsResources：资源是否足够？                      │
│     - PodFitsHostPorts：端口是否冲突？                      │
│     - PodMatchNodeSelector：是否匹配NodeSelector？         │
│     - PodToleratesNodeTaints：是否容忍污点？               │
│     - CheckNodeMemoryPressure：内存压力是否过大？          │
│     - CheckNodeDiskPressure：磁盘压力是否过大？            │
│          ↓                                                   │
│     ┌─────────────────────────────────────────┐            │
│     │ 可行节点（50个）                        │            │
│     └─────────────────────────────────────────┘            │
│          ↓                                                   │
│  3. 优选阶段（Priorities/Scoring）                          │
│     对可行节点打分，选择最优节点                            │
│     ┌───────────┬───────────┬───────────┐                  │
│     │ Node1: 85 │ Node2: 92 │ Node3: 78 │  ...             │
│     └───────────┴───────────┴───────────┘                  │
│          ↓                                                   │
│     [ 优选算法 ]                                             │
│     - LeastRequestedPriority：资源请求最少                 │
│     - BalancedResourceAllocation：CPU/内存均衡             │
│     - SelectorSpreadPriority：副本分散                     │
│     - NodeAffinityPriority：节点亲和性得分                 │
│     - InterPodAffinityPriority：Pod亲和性得分              │
│          ↓                                                   │
│     ┌─────────────────────────────────────────┐            │
│     │ 最优节点：Node2（得分92）              │            │
│     └─────────────────────────────────────────┘            │
│          ↓                                                   │
│  4. 绑定（Binding）                                          │
│     将Pod绑定到选定的节点                                   │
│     - 更新Pod.spec.nodeName = "node2"                       │
│     - 通知kubelet启动Pod                                    │
│          ↓                                                   │
│  5. 假设调度（Assume）                                       │
│     乐观假设Pod已调度，允许后续Pod调度                     │
│     （避免多个Pod同时调度导致资源超售）                    │
└─────────────────────────────────────────────────────────────┘
```

**详细调度流程：**

```go
// 调度器伪代码
func scheduleOne(pod *v1.Pod) {
    // 1. 预选：过滤不可行节点
    feasibleNodes := findNodesThatFit(pod, allNodes)

    if len(feasibleNodes) == 0 {
        // 没有可行节点，触发抢占或失败
        return preemptOrFail(pod)
    }

    // 2. 优选：对可行节点打分
    prioritizedNodes := prioritizeNodes(pod, feasibleNodes)

    // 3. 选择得分最高的节点
    bestNode := selectHost(prioritizedNodes)

    // 4. 假设调度（乐观锁）
    assumedPod := pod.DeepCopy()
    assumedPod.Spec.NodeName = bestNode
    scheduler.Cache.AssumePod(assumedPod)

    // 5. 异步绑定
    go func() {
        err := bind(pod, bestNode)
        if err != nil {
            scheduler.Cache.ForgetPod(assumedPod)
        }
    }()
}
```

#### 7.1.2.2 预选（Predicates）详解

预选阶段会运行一系列预选函数（Predicate Functions），过滤掉不满足条件的节点：

**核心预选算法：**

| 预选算法 | 检查内容 | 示例 |
|---------|---------|------|
| **PodFitsResources** | 节点资源是否满足Pod需求 | Pod请求2核4GB，节点剩余1核2GB → ❌ |
| **PodFitsHostPorts** | 节点端口是否被占用 | Pod使用hostPort 8080，节点已有Pod占用 → ❌ |
| **PodMatchNodeSelector** | Pod的nodeSelector是否匹配节点标签 | nodeSelector: {disk: ssd}，节点无此标签 → ❌ |
| **PodToleratesNodeTaints** | Pod是否容忍节点污点 | 节点有gpu=true:NoSchedule，Pod无容忍 → ❌ |
| **CheckNodeMemoryPressure** | 节点内存压力是否过大 | 节点内存使用>85% → ❌ |
| **CheckNodeDiskPressure** | 节点磁盘压力是否过大 | 节点磁盘使用>85% → ❌ |
| **CheckNodePIDPressure** | 节点PID资源是否耗尽 | PID数量接近上限 → ❌ |
| **CheckVolumeBinding** | 存储卷是否可绑定 | PVC要求local-storage，节点无此类型 → ❌ |
| **NoDiskConflict** | 存储卷是否冲突 | Pod使用同一GCE PD的多个Pod在同一节点 → ❌ |
| **NoVolumeZoneConflict** | 存储卷可用区是否匹配 | Pod使用us-east-1a的EBS，节点在us-east-1b → ❌ |

**预选示例：**

```yaml
# Pod资源请求
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "4Gi"
        cpu: "2"
      limits:
        memory: "8Gi"
        cpu: "4"
```

```
预选过程：

节点1：16核64GB，已使用12核58GB
  ├─ 剩余资源：4核6GB
  ├─ Pod请求：2核4GB
  └─ ✅ 通过 PodFitsResources

节点2：8核32GB，已使用7核30GB
  ├─ 剩余资源：1核2GB
  ├─ Pod请求：2核4GB
  └─ ❌ 失败 PodFitsResources（资源不足）

节点3：32核128GB，有污点gpu=true:NoSchedule
  ├─ 剩余资源：30核120GB（充足）
  ├─ Pod无toleration
  └─ ❌ 失败 PodToleratesNodeTaints

节点4：8核16GB，已有Pod使用hostPort 80
  ├─ 剩余资源：6核12GB
  ├─ Pod使用hostPort 80
  └─ ❌ 失败 PodFitsHostPorts

结果：只有节点1通过预选
```

#### 7.1.2.3 优选（Priorities）详解

优选阶段对通过预选的节点进行打分（0-100分），得分最高的节点被选中：

**核心优选算法：**

| 优选算法 | 打分逻辑 | 权重 |
|---------|---------|------|
| **LeastRequestedPriority** | 资源请求越少得分越高（Bin Packing） | 1 |
| **BalancedResourceAllocation** | CPU和内存使用率越均衡得分越高 | 1 |
| **SelectorSpreadPriority** | 同一Service/RC的Pod分散得分越高 | 1 |
| **NodeAffinityPriority** | 满足NodeAffinity偏好得分越高 | 1 |
| **InterPodAffinityPriority** | 满足PodAffinity偏好得分越高 | 1 |
| **TaintTolerationPriority** | 匹配Toleration越多得分越高 | 1 |
| **ImageLocalityPriority** | 节点已有镜像得分越高 | 1 |
| **NodePreferAvoidPodsPriority** | 避免调度到特定节点 | 10000 |

**1. LeastRequestedPriority（最少资源请求）**

鼓励将Pod调度到资源利用率低的节点，实现Bin Packing：

```
得分公式：
score = (capacity - requestedResources - podRequest) * 100 / capacity

示例：
节点1：16核64GB，已使用4核16GB
  Pod请求：2核4GB
  CPU得分 = (16 - 4 - 2) * 100 / 16 = 62.5
  内存得分 = (64 - 16 - 4) * 100 / 64 = 68.75
  平均得分 = (62.5 + 68.75) / 2 = 65.6

节点2：16核64GB，已使用8核32GB
  Pod请求：2核4GB
  CPU得分 = (16 - 8 - 2) * 100 / 16 = 37.5
  内存得分 = (64 - 32 - 4) * 100 / 64 = 43.75
  平均得分 = (37.5 + 43.75) / 2 = 40.6

结果：节点1得分更高（资源利用率低）
```

**2. BalancedResourceAllocation（资源均衡分配）**

鼓励CPU和内存使用率接近，避免某一维度资源过度使用：

```
得分公式：
cpuFraction = (podRequest.cpu + node.allocatedCPU) / node.capacity.cpu
memFraction = (podRequest.mem + node.allocatedMem) / node.capacity.mem
score = 100 - abs(cpuFraction - memFraction) * 10

示例：
节点1：16核64GB，已使用8核16GB
  Pod请求：2核8GB
  CPU使用率 = (2 + 8) / 16 = 62.5%
  内存使用率 = (8 + 16) / 64 = 37.5%
  得分 = 100 - |62.5 - 37.5| * 10 = 75

节点2：16核64GB，已使用8核32GB
  Pod请求：2核8GB
  CPU使用率 = (2 + 8) / 16 = 62.5%
  内存使用率 = (8 + 32) / 64 = 62.5%
  得分 = 100 - |62.5 - 62.5| * 10 = 100

结果：节点2得分更高（CPU/内存使用率均衡）
```

**3. SelectorSpreadPriority（副本分散）**

鼓励将同一Service/ReplicaSet的Pod分散到不同节点：

```yaml
# 假设有一个Deployment，3个副本
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
```

```
打分逻辑：

节点1：已有0个web Pod
  得分 = 100

节点2：已有1个web Pod
  得分 = 50

节点3：已有2个web Pod
  得分 = 0

结果：节点1得分最高（实现高可用）
```

**综合打分示例：**

```
假设有3个候选节点，Pod请求2核4GB：

节点1：16核64GB，已使用4核16GB，无web Pod
  LeastRequested: 65.6
  Balanced: 85
  Spread: 100
  综合得分 = (65.6 + 85 + 100) / 3 = 83.5

节点2：16核64GB，已使用8核32GB，有1个web Pod
  LeastRequested: 40.6
  Balanced: 100
  Spread: 50
  综合得分 = (40.6 + 100 + 50) / 3 = 63.5

节点3：16核64GB，已使用12核48GB，有2个web Pod
  LeastRequested: 25
  Balanced: 90
  Spread: 0
  综合得分 = (25 + 90 + 0) / 3 = 38.3

最终选择：节点1（得分83.5最高）
```

### 7.1.3 调度性能优化

#### 7.1.3.1 调度性能指标

Kubernetes调度器的性能直接影响集群的整体效率：

```
┌────────────────┬───────────────┬──────────────────┐
│  性能指标      │   目标值      │   影响            │
├────────────────┼───────────────┼──────────────────┤
│ 调度延迟       │ <100ms        │ Pod启动速度      │
│ (Scheduling    │ (P99 <500ms)  │                  │
│  Latency)      │               │                  │
├────────────────┼───────────────┼──────────────────┤
│ 调度吞吐量     │ >100 pods/s   │ 大规模扩容速度   │
│ (Throughput)   │               │                  │
├────────────────┼───────────────┼──────────────────┤
│ 调度成功率     │ >99%          │ Pod Pending时间  │
│ (Success Rate) │               │                  │
├────────────────┼───────────────┼──────────────────┤
│ 抢占次数       │ 尽量少        │ 应用稳定性       │
│ (Preemptions)  │               │                  │
└────────────────┴───────────────┴──────────────────┘
```

**查看调度器性能指标：**

```bash
# 查看调度器Metrics
kubectl get --raw /metrics | grep scheduler

# 关键指标：
# scheduler_scheduling_duration_seconds - 调度延迟
# scheduler_pod_scheduling_attempts - 调度尝试次数
# scheduler_pending_pods - 等待调度的Pod数量
# scheduler_schedule_attempts_total - 总调度次数
# scheduler_preemption_attempts_total - 抢占次数
```

#### 7.1.3.2 调度器配置优化

**1. 调整调度器并发度**

```yaml
# kube-scheduler配置文件
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: default-scheduler

# 并发度配置
parallelism: 16  # 并发处理的Pod数量（默认16）
percentageOfNodesToScore: 50  # 预选阶段评估节点百分比（默认50%）
```

**2. 启用调度缓存**

```yaml
# 缓存配置
clientConnection:
  qps: 100      # API Server请求速率
  burst: 200    # 突发请求数量

# 缓存大小
# 默认情况下，调度器会缓存所有节点和Pod信息
# 对于超大集群（>5000节点），可以考虑：
# - 使用多调度器
# - 按namespace分片
```

**3. 禁用不必要的预选/优选算法**

```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- pluginConfig:
  # 禁用某些插件
  - name: InterPodAffinity
    args:
      hardPodAffinityWeight: 0  # 禁用硬亲和性
```

#### 7.1.3.3 大规模集群调度优化

**场景：10000节点集群**

```yaml
# 优化策略1：限制预选节点数量
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: high-performance-scheduler

# 只评估50%的节点
percentageOfNodesToScore: 50

# 优化策略2：使用节点分组
# 通过NodeSelector预先筛选节点池
spec:
  nodeSelector:
    node-pool: compute-intensive  # 只在特定节点池调度
```

**调度性能对比：**

```
默认配置（100%节点评估）：
- 10000节点 × 每节点1ms = 10秒
- P99延迟：15秒 ❌

优化配置（50%节点评估）：
- 5000节点 × 每节点1ms = 5秒
- P99延迟：7秒 ✅（提升53%）

进一步优化（NodeSelector + 50%评估）：
- 1000节点池 × 50% × 1ms = 500ms
- P99延迟：1秒 ✅（提升93%）
```

### 7.1.4 调度失败处理

#### 7.1.4.1 常见调度失败原因

```
┌────────────────────┬─────────────────────────────┐
│  失败原因          │   解决方案                  │
├────────────────────┼─────────────────────────────┤
│ Insufficient CPU   │ 增加节点或降低资源请求      │
│ Insufficient Memory│                             │
├────────────────────┼─────────────────────────────┤
│ No nodes available │ 检查节点是否Ready           │
│                    │ 检查是否有污点              │
├────────────────────┼─────────────────────────────┤
│ Pod affinity       │ 放宽亲和性要求              │
│ not satisfied      │ 或增加匹配的节点            │
├────────────────────┼─────────────────────────────┤
│ Node doesn't match │ 修改nodeSelector            │
│ node selector      │ 或给节点添加标签            │
├────────────────────┼─────────────────────────────┤
│ Didn't tolerate    │ 添加toleration              │
│ node taints        │ 或移除节点污点              │
├────────────────────┼─────────────────────────────┤
│ PVC not bound      │ 检查StorageClass            │
│                    │ 或创建PV                    │
└────────────────────┴─────────────────────────────┘
```

**查看调度失败事件：**

```bash
# 查看Pod事件
kubectl describe pod <pod-name>

# 示例输出
Events:
  Type     Reason            Message
  ----     ------            -------
  Warning  FailedScheduling  0/3 nodes are available: 1 Insufficient cpu,
                             2 node(s) didn't match Pod's node affinity/selector.

# 查看所有Pending的Pod
kubectl get pods --field-selector=status.phase=Pending -A

# 查看调度器日志
kubectl logs -n kube-system kube-scheduler-<node>
```

#### 7.1.4.2 调度失败示例与排查

**示例1：资源不足**

```yaml
# Pod定义
apiVersion: v1
kind: Pod
metadata:
  name: big-app
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        cpu: "32"        # 请求32核
        memory: "128Gi"  # 请求128GB内存
```

```bash
# 调度失败
kubectl describe pod big-app

Events:
  Warning  FailedScheduling  0/5 nodes are available:
           5 Insufficient cpu (available: 16, requested: 32).

# 解决方案1：降低资源请求
resources:
  requests:
    cpu: "8"
    memory: "32Gi"

# 解决方案2：添加更大的节点
kubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu,MEMORY:.status.capacity.memory
```

**示例2：节点选择器不匹配**

```yaml
# Pod定义
apiVersion: v1
kind: Pod
metadata:
  name: gpu-app
spec:
  nodeSelector:
    gpu: "true"  # 要求节点有gpu=true标签
  containers:
  - name: app
    image: tensorflow
```

```bash
# 调度失败
kubectl describe pod gpu-app

Events:
  Warning  FailedScheduling  0/5 nodes are available:
           5 node(s) didn't match Pod's node selector.

# 排查：检查节点标签
kubectl get nodes --show-labels | grep gpu
# 发现所有节点都没有gpu=true标签

# 解决方案：给GPU节点添加标签
kubectl label node node-gpu-1 gpu=true
```

**示例3：污点容忍不匹配**

```yaml
# 节点有污点
kubectl describe node node-gpu-1
Taints: nvidia.com/gpu=present:NoSchedule

# Pod没有容忍
apiVersion: v1
kind: Pod
metadata:
  name: gpu-app
spec:
  containers:
  - name: app
    image: tensorflow
  # ❌ 缺少tolerations
```

```bash
# 调度失败
Events:
  Warning  FailedScheduling  0/1 nodes are available:
           1 node(s) had untolerated taint {nvidia.com/gpu: present}.

# 解决方案：添加容忍
spec:
  tolerations:
  - key: nvidia.com/gpu
    operator: Equal
    value: present
    effect: NoSchedule
```

### 7.1.5 调度器监控与调试

#### 7.1.5.1 调度器监控指标

```yaml
# 部署Prometheus监控调度器
apiVersion: v1
kind: Service
metadata:
  name: kube-scheduler
  namespace: kube-system
  labels:
    component: kube-scheduler
spec:
  ports:
  - name: metrics
    port: 10259
    protocol: TCP
  selector:
    component: kube-scheduler
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kube-scheduler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: kube-scheduler
  endpoints:
  - port: metrics
    interval: 30s
```

**关键监控指标：**

```promql
# 调度延迟（P99）
histogram_quantile(0.99,
  sum(rate(scheduler_scheduling_duration_seconds_bucket[5m])) by (le)
)

# 调度失败率
sum(rate(scheduler_schedule_attempts_total{result="error"}[5m]))
/
sum(rate(scheduler_schedule_attempts_total[5m]))

# Pending Pod数量
sum(kube_pod_status_phase{phase="Pending"}) by (namespace)

# 调度吞吐量
sum(rate(scheduler_schedule_attempts_total{result="scheduled"}[5m]))
```

**Grafana仪表板示例：**

```json
{
  "dashboard": {
    "title": "Kubernetes Scheduler监控",
    "panels": [
      {
        "title": "调度延迟 (P50/P95/P99)",
        "targets": [{
          "expr": "histogram_quantile(0.99, sum(rate(scheduler_scheduling_duration_seconds_bucket[5m])) by (le))"
        }]
      },
      {
        "title": "调度成功率",
        "targets": [{
          "expr": "sum(rate(scheduler_schedule_attempts_total{result=\"scheduled\"}[5m])) / sum(rate(scheduler_schedule_attempts_total[5m]))"
        }]
      },
      {
        "title": "Pending Pod趋势",
        "targets": [{
          "expr": "sum(kube_pod_status_phase{phase=\"Pending\"}) by (namespace)"
        }]
      },
      {
        "title": "抢占事件",
        "targets": [{
          "expr": "sum(rate(scheduler_preemption_attempts_total[5m]))"
        }]
      }
    ]
  }
}
```

#### 7.1.5.2 调度器调试技巧

**1. 启用调度器详细日志**

```bash
# 修改kube-scheduler启动参数
--v=4  # 日志级别（0-10，越大越详细）

# 查看详细调度日志
kubectl logs -n kube-system kube-scheduler-master-1 --tail=100 -f | grep -i "schedule"
```

**2. 使用调度模拟器**

```bash
# 安装scheduler simulator
go install sigs.k8s.io/kube-scheduler-simulator@latest

# 启动模拟器
kube-scheduler-simulator

# 导入集群状态
kubectl get nodes -o json > nodes.json
kubectl get pods -A -o json > pods.json

# 模拟调度
./simulator --nodes=nodes.json --pods=pods.json --simulate=new-pod.yaml
```

**3. 手动模拟调度过程**

```bash
# 查看Pod的调度约束
kubectl get pod <pod-name> -o yaml | grep -A 10 "nodeSelector\|affinity\|tolerations"

# 查看节点资源
kubectl describe nodes | grep -A 5 "Allocated resources"

# 检查节点是否有污点
kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints

# 检查节点是否Ready
kubectl get nodes
```

### 7.1.6 调度器最佳实践

#### 7.1.6.1 资源请求最佳实践

**✅ 推荐做法：**

```yaml
# 1. 始终设置资源请求和限制
apiVersion: v1
kind: Pod
metadata:
  name: best-practice-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:        # ✅ 调度依据
        cpu: "500m"
        memory: "512Mi"
      limits:          # ✅ 资源上限
        cpu: "1"
        memory: "1Gi"
```

**❌ 避免的陷阱：**

```yaml
# ❌ 陷阱1：不设置requests
# 后果：调度器无法准确评估，可能导致节点过载
resources:
  limits:
    cpu: "1"
    memory: "1Gi"

# ❌ 陷阱2：requests == limits (QoS: Guaranteed)
# 后果：资源利用率低，浪费严重
resources:
  requests:
    cpu: "2"
    memory: "4Gi"
  limits:
    cpu: "2"        # requests == limits
    memory: "4Gi"   # 即使应用只用了500m/1Gi，也预留了2核/4GB

# ✅ 推荐：requests < limits (QoS: Burstable)
resources:
  requests:
    cpu: "500m"     # 平时使用
    memory: "1Gi"
  limits:
    cpu: "2"        # 高峰时可用
    memory: "4Gi"
```

**资源请求建议值：**

| 应用类型 | CPU Requests | Memory Requests | CPU Limits | Memory Limits |
|---------|--------------|-----------------|------------|---------------|
| **Web前端** | 100-500m | 128-512Mi | 1-2 | 512Mi-1Gi |
| **API服务** | 500m-1 | 512Mi-2Gi | 2-4 | 2-4Gi |
| **数据库** | 1-2 | 2-8Gi | 2-4 | 8-16Gi |
| **消息队列** | 500m-1 | 1-4Gi | 2-4 | 4-8Gi |
| **批处理** | 1-2 | 1-2Gi | 不限制 | 4-8Gi |

#### 7.1.6.2 调度策略最佳实践

**1. 高可用部署**

```yaml
# ✅ 使用Pod反亲和性确保副本分散
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web
              topologyKey: kubernetes.io/hostname  # 分散到不同节点
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web
              topologyKey: topology.kubernetes.io/zone  # 分散到不同可用区
```

**2. 性能优化部署**

```yaml
# ✅ 使用Pod亲和性将相关服务部署在一起
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
spec:
  template:
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - database  # 靠近数据库部署，降低延迟
              topologyKey: kubernetes.io/hostname
```

**3. 专用节点池**

```yaml
# ✅ 使用污点和容忍将特定工作负载调度到专用节点
# 1. 给GPU节点打污点
kubectl taint nodes node-gpu-1 gpu=true:NoSchedule

# 2. GPU应用添加容忍
apiVersion: v1
kind: Pod
metadata:
  name: ml-training
spec:
  tolerations:
  - key: gpu
    operator: Equal
    value: "true"
    effect: NoSchedule
  nodeSelector:
    gpu: "true"  # 确保调度到GPU节点
  containers:
  - name: trainer
    image: tensorflow/tensorflow:latest-gpu
    resources:
      limits:
        nvidia.com/gpu: 1
```

---

本节我们深入学习了Kubernetes调度器的工作原理，包括两阶段调度策略（预选和优选）、性能优化技巧、调度失败处理、监控调试方法以及最佳实践。理解调度器原理是优化应用性能和资源利用率的关键。在下一节中，我们将学习如何使用节点选择器和节点亲和性来精确控制Pod的调度位置。

---

**本节知识点回顾：**
- ✅ 调度器的核心价值和工作流程
- ✅ 预选（Predicates）和优选（Priorities）算法
- ✅ 调度性能优化和大规模集群调度
- ✅ 调度失败原因排查和解决方案
- ✅ 调度器监控指标和调试技巧
- ✅ 资源请求和调度策略最佳实践
