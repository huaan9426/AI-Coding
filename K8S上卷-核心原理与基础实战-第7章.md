# 第7章：调度与资源管理

> 在第6章中，我们系统学习了Kubernetes的配置与安全管理，包括ResourceQuota资源配额、Pod安全策略、RBAC权限控制和网络策略。本章将深入探讨Kubernetes的调度机制和高级资源管理，揭示Pod如何被智能分配到合适的节点，以及如何优化资源利用率。

**本章学习目标：**
- 深入理解Kubernetes调度器（kube-scheduler）工作原理
- 掌握节点选择器（NodeSelector）和节点亲和性（Node Affinity）
- 学习Pod亲和性与反亲和性（Pod Affinity/Anti-Affinity）
- 掌握污点（Taints）和容忍（Tolerations）机制
- 理解优先级（Priority）和抢占（Preemption）
- 实战：构建高可用的多层应用架构

---

## 7.1 Kubernetes调度器原理

Kubernetes调度器（kube-scheduler）是集群的"交通指挥官"，负责将新创建的Pod分配到合适的节点上运行。理解调度器的工作原理，是优化应用性能和资源利用率的关键。

### 7.1.1 为什么需要调度器

#### 7.1.1.1 资源分配的挑战

在一个包含数百个节点和数千个Pod的Kubernetes集群中，如何高效地分配资源是一个复杂的问题：

```
典型的生产集群场景：
┌──────────────────────────────────────────────────┐
│  集群规模：                                       │
│  - 节点数量：200个                                │
│  - Pod总数：5000+                                │
│  - 每天新建Pod：1000+                            │
│                                                   │
│  节点异构性：                                     │
│  - CPU型节点：64核CPU，128GB内存                 │
│  - 内存型节点：32核CPU，512GB内存                │
│  - GPU节点：16核CPU，256GB内存，8卡V100          │
│  - 边缘节点：8核CPU，16GB内存（低延迟）          │
│                                                   │
│  应用多样性：                                     │
│  - Web应用：CPU中等，内存中等，需要高可用        │
│  - 数据库：CPU低，内存高，需要持久化存储         │
│  - 机器学习：CPU高/GPU，内存极高，计算密集       │
│  - 消息队列：CPU中等，内存高，网络IO密集         │
│                                                   │
│  调度约束：                                       │
│  - 亲和性：前端Pod需要靠近后端Pod（降低延迟）   │
│  - 反亲和性：同一应用的多个副本分散到不同节点    │
│  - 污点容忍：只有特定Pod能运行在GPU节点          │
│  - 资源预留：保证关键应用的资源需求              │
└──────────────────────────────────────────────────┘
```

**没有智能调度的后果：**

| 问题 | 场景 | 影响 |
|-----|------|------|
| **资源浪费** | 小Pod调度到大节点 | 资源碎片化，浪费严重 |
| **性能下降** | CPU密集Pod和内存密集Pod挤在同一节点 | 资源竞争，性能下降 |
| **高可用性差** | 同一应用的多个副本都在一个节点 | 节点故障导致服务中断 |
| **负载不均** | 部分节点超载，部分节点空闲 | 集群利用率低 |
| **延迟增加** | 前后端应用跨可用区部署 | 网络延迟增加 |

#### 7.1.1.2 调度器的价值

**Kubernetes调度器提供的核心价值：**

```
┌─────────────────────────────────────────────────┐
│          调度器核心价值                          │
├─────────────────────────────────────────────────┤
│  ✅ 智能资源分配                                 │
│     根据节点资源和Pod需求，找到最佳匹配         │
│                                                  │
│  ✅ 高可用保障                                   │
│     通过反亲和性将副本分散到不同节点/可用区     │
│                                                  │
│  ✅ 性能优化                                     │
│     将相互依赖的Pod调度到同一节点/可用区        │
│                                                  │
│  ✅ 资源利用率最大化                             │
│     Bin Packing算法，尽量减少资源碎片           │
│                                                  │
│  ✅ 灵活的调度策略                               │
│     支持自定义调度规则和优先级                  │
│                                                  │
│  ✅ 故障自愈                                     │
│     节点故障时自动将Pod重新调度到健康节点       │
└─────────────────────────────────────────────────┘
```

**真实案例对比：**

```yaml
# ❌ 场景1：没有调度约束
# 结果：3个Nginx副本都调度到了Node1
kubectl get pods -o wide
NAME                     NODE
nginx-7c5ddbdf54-abc12   node1
nginx-7c5ddbdf54-def34   node1  # ❌ 高可用性差！
nginx-7c5ddbdf54-ghi56   node1  # ❌ Node1故障则全部宕机！

# ✅ 场景2：使用反亲和性
# 结果：3个副本分散到不同节点
kubectl get pods -o wide
NAME                     NODE
nginx-7c5ddbdf54-abc12   node1
nginx-7c5ddbdf54-def34   node2  # ✅ 高可用！
nginx-7c5ddbdf54-ghi56   node3  # ✅ 单节点故障不影响服务！
```

#### 7.1.1.3 调度的类型

Kubernetes支持多种调度方式：

```
┌────────────────┬───────────────────┬─────────────────────┐
│  调度类型      │    触发条件       │    使用场景         │
├────────────────┼───────────────────┼─────────────────────┤
│ 默认调度       │ Pod创建时         │ 大部分应用          │
│ (Default)      │ nodeName字段为空  │ (90%+场景)          │
├────────────────┼───────────────────┼─────────────────────┤
│ 绑定调度       │ Pod创建时         │ 特定节点部署        │
│ (NodeName)     │ nodeName字段指定  │ (DaemonSet等)       │
├────────────────┼───────────────────┼─────────────────────┤
│ 重新调度       │ 节点故障          │ 故障恢复            │
│ (Rescheduling) │ Pod被驱逐         │                     │
├────────────────┼───────────────────┼─────────────────────┤
│ 抢占调度       │ 资源不足          │ 高优先级Pod         │
│ (Preemption)   │ 高优先级Pod到达   │                     │
├────────────────┼───────────────────┼─────────────────────┤
│ 自定义调度     │ 配置自定义调度器  │ 特殊调度需求        │
│ (Custom)       │ schedulerName字段 │ (AI/大数据场景)     │
└────────────────┴───────────────────┴─────────────────────┘
```

### 7.1.2 调度器工作流程

#### 7.1.2.1 调度流程概览

Kubernetes调度器采用两阶段调度策略：**预选（Predicates）→ 优选（Priorities）**

```
┌─────────────────────────────────────────────────────────────┐
│              Kubernetes调度器工作流程                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  1. 监听Pod创建                                              │
│     kube-scheduler通过Watch机制监听API Server              │
│     发现新创建的、nodeName为空的Pod                         │
│          ↓                                                   │
│  2. 预选阶段（Predicates/Filtering）                        │
│     过滤掉不符合条件的节点                                  │
│     ┌─────────────────────────────────────────┐            │
│     │ 所有节点（200个）                       │            │
│     └─────────────────────────────────────────┘            │
│          ↓                                                   │
│     [ 预选算法 ]                                             │
│     - PodFitsResources：资源是否足够？                      │
│     - PodFitsHostPorts：端口是否冲突？                      │
│     - PodMatchNodeSelector：是否匹配NodeSelector？         │
│     - PodToleratesNodeTaints：是否容忍污点？               │
│     - CheckNodeMemoryPressure：内存压力是否过大？          │
│     - CheckNodeDiskPressure：磁盘压力是否过大？            │
│          ↓                                                   │
│     ┌─────────────────────────────────────────┐            │
│     │ 可行节点（50个）                        │            │
│     └─────────────────────────────────────────┘            │
│          ↓                                                   │
│  3. 优选阶段（Priorities/Scoring）                          │
│     对可行节点打分，选择最优节点                            │
│     ┌───────────┬───────────┬───────────┐                  │
│     │ Node1: 85 │ Node2: 92 │ Node3: 78 │  ...             │
│     └───────────┴───────────┴───────────┘                  │
│          ↓                                                   │
│     [ 优选算法 ]                                             │
│     - LeastRequestedPriority：资源请求最少                 │
│     - BalancedResourceAllocation：CPU/内存均衡             │
│     - SelectorSpreadPriority：副本分散                     │
│     - NodeAffinityPriority：节点亲和性得分                 │
│     - InterPodAffinityPriority：Pod亲和性得分              │
│          ↓                                                   │
│     ┌─────────────────────────────────────────┐            │
│     │ 最优节点：Node2（得分92）              │            │
│     └─────────────────────────────────────────┘            │
│          ↓                                                   │
│  4. 绑定（Binding）                                          │
│     将Pod绑定到选定的节点                                   │
│     - 更新Pod.spec.nodeName = "node2"                       │
│     - 通知kubelet启动Pod                                    │
│          ↓                                                   │
│  5. 假设调度（Assume）                                       │
│     乐观假设Pod已调度，允许后续Pod调度                     │
│     （避免多个Pod同时调度导致资源超售）                    │
└─────────────────────────────────────────────────────────────┘
```

**详细调度流程：**

```go
// 调度器伪代码
func scheduleOne(pod *v1.Pod) {
    // 1. 预选：过滤不可行节点
    feasibleNodes := findNodesThatFit(pod, allNodes)

    if len(feasibleNodes) == 0 {
        // 没有可行节点，触发抢占或失败
        return preemptOrFail(pod)
    }

    // 2. 优选：对可行节点打分
    prioritizedNodes := prioritizeNodes(pod, feasibleNodes)

    // 3. 选择得分最高的节点
    bestNode := selectHost(prioritizedNodes)

    // 4. 假设调度（乐观锁）
    assumedPod := pod.DeepCopy()
    assumedPod.Spec.NodeName = bestNode
    scheduler.Cache.AssumePod(assumedPod)

    // 5. 异步绑定
    go func() {
        err := bind(pod, bestNode)
        if err != nil {
            scheduler.Cache.ForgetPod(assumedPod)
        }
    }()
}
```

#### 7.1.2.2 预选（Predicates）详解

预选阶段会运行一系列预选函数（Predicate Functions），过滤掉不满足条件的节点：

**核心预选算法：**

| 预选算法 | 检查内容 | 示例 |
|---------|---------|------|
| **PodFitsResources** | 节点资源是否满足Pod需求 | Pod请求2核4GB，节点剩余1核2GB → ❌ |
| **PodFitsHostPorts** | 节点端口是否被占用 | Pod使用hostPort 8080，节点已有Pod占用 → ❌ |
| **PodMatchNodeSelector** | Pod的nodeSelector是否匹配节点标签 | nodeSelector: {disk: ssd}，节点无此标签 → ❌ |
| **PodToleratesNodeTaints** | Pod是否容忍节点污点 | 节点有gpu=true:NoSchedule，Pod无容忍 → ❌ |
| **CheckNodeMemoryPressure** | 节点内存压力是否过大 | 节点内存使用>85% → ❌ |
| **CheckNodeDiskPressure** | 节点磁盘压力是否过大 | 节点磁盘使用>85% → ❌ |
| **CheckNodePIDPressure** | 节点PID资源是否耗尽 | PID数量接近上限 → ❌ |
| **CheckVolumeBinding** | 存储卷是否可绑定 | PVC要求local-storage，节点无此类型 → ❌ |
| **NoDiskConflict** | 存储卷是否冲突 | Pod使用同一GCE PD的多个Pod在同一节点 → ❌ |
| **NoVolumeZoneConflict** | 存储卷可用区是否匹配 | Pod使用us-east-1a的EBS，节点在us-east-1b → ❌ |

**预选示例：**

```yaml
# Pod资源请求
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "4Gi"
        cpu: "2"
      limits:
        memory: "8Gi"
        cpu: "4"
```

```
预选过程：

节点1：16核64GB，已使用12核58GB
  ├─ 剩余资源：4核6GB
  ├─ Pod请求：2核4GB
  └─ ✅ 通过 PodFitsResources

节点2：8核32GB，已使用7核30GB
  ├─ 剩余资源：1核2GB
  ├─ Pod请求：2核4GB
  └─ ❌ 失败 PodFitsResources（资源不足）

节点3：32核128GB，有污点gpu=true:NoSchedule
  ├─ 剩余资源：30核120GB（充足）
  ├─ Pod无toleration
  └─ ❌ 失败 PodToleratesNodeTaints

节点4：8核16GB，已有Pod使用hostPort 80
  ├─ 剩余资源：6核12GB
  ├─ Pod使用hostPort 80
  └─ ❌ 失败 PodFitsHostPorts

结果：只有节点1通过预选
```

#### 7.1.2.3 优选（Priorities）详解

优选阶段对通过预选的节点进行打分（0-100分），得分最高的节点被选中：

**核心优选算法：**

| 优选算法 | 打分逻辑 | 权重 |
|---------|---------|------|
| **LeastRequestedPriority** | 资源请求越少得分越高（Bin Packing） | 1 |
| **BalancedResourceAllocation** | CPU和内存使用率越均衡得分越高 | 1 |
| **SelectorSpreadPriority** | 同一Service/RC的Pod分散得分越高 | 1 |
| **NodeAffinityPriority** | 满足NodeAffinity偏好得分越高 | 1 |
| **InterPodAffinityPriority** | 满足PodAffinity偏好得分越高 | 1 |
| **TaintTolerationPriority** | 匹配Toleration越多得分越高 | 1 |
| **ImageLocalityPriority** | 节点已有镜像得分越高 | 1 |
| **NodePreferAvoidPodsPriority** | 避免调度到特定节点 | 10000 |

**1. LeastRequestedPriority（最少资源请求）**

鼓励将Pod调度到资源利用率低的节点，实现Bin Packing：

```
得分公式：
score = (capacity - requestedResources - podRequest) * 100 / capacity

示例：
节点1：16核64GB，已使用4核16GB
  Pod请求：2核4GB
  CPU得分 = (16 - 4 - 2) * 100 / 16 = 62.5
  内存得分 = (64 - 16 - 4) * 100 / 64 = 68.75
  平均得分 = (62.5 + 68.75) / 2 = 65.6

节点2：16核64GB，已使用8核32GB
  Pod请求：2核4GB
  CPU得分 = (16 - 8 - 2) * 100 / 16 = 37.5
  内存得分 = (64 - 32 - 4) * 100 / 64 = 43.75
  平均得分 = (37.5 + 43.75) / 2 = 40.6

结果：节点1得分更高（资源利用率低）
```

**2. BalancedResourceAllocation（资源均衡分配）**

鼓励CPU和内存使用率接近，避免某一维度资源过度使用：

```
得分公式：
cpuFraction = (podRequest.cpu + node.allocatedCPU) / node.capacity.cpu
memFraction = (podRequest.mem + node.allocatedMem) / node.capacity.mem
score = 100 - abs(cpuFraction - memFraction) * 10

示例：
节点1：16核64GB，已使用8核16GB
  Pod请求：2核8GB
  CPU使用率 = (2 + 8) / 16 = 62.5%
  内存使用率 = (8 + 16) / 64 = 37.5%
  得分 = 100 - |62.5 - 37.5| * 10 = 75

节点2：16核64GB，已使用8核32GB
  Pod请求：2核8GB
  CPU使用率 = (2 + 8) / 16 = 62.5%
  内存使用率 = (8 + 32) / 64 = 62.5%
  得分 = 100 - |62.5 - 62.5| * 10 = 100

结果：节点2得分更高（CPU/内存使用率均衡）
```

**3. SelectorSpreadPriority（副本分散）**

鼓励将同一Service/ReplicaSet的Pod分散到不同节点：

```yaml
# 假设有一个Deployment，3个副本
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
```

```
打分逻辑：

节点1：已有0个web Pod
  得分 = 100

节点2：已有1个web Pod
  得分 = 50

节点3：已有2个web Pod
  得分 = 0

结果：节点1得分最高（实现高可用）
```

**综合打分示例：**

```
假设有3个候选节点，Pod请求2核4GB：

节点1：16核64GB，已使用4核16GB，无web Pod
  LeastRequested: 65.6
  Balanced: 85
  Spread: 100
  综合得分 = (65.6 + 85 + 100) / 3 = 83.5

节点2：16核64GB，已使用8核32GB，有1个web Pod
  LeastRequested: 40.6
  Balanced: 100
  Spread: 50
  综合得分 = (40.6 + 100 + 50) / 3 = 63.5

节点3：16核64GB，已使用12核48GB，有2个web Pod
  LeastRequested: 25
  Balanced: 90
  Spread: 0
  综合得分 = (25 + 90 + 0) / 3 = 38.3

最终选择：节点1（得分83.5最高）
```

### 7.1.3 调度性能优化

#### 7.1.3.1 调度性能指标

Kubernetes调度器的性能直接影响集群的整体效率：

```
┌────────────────┬───────────────┬──────────────────┐
│  性能指标      │   目标值      │   影响            │
├────────────────┼───────────────┼──────────────────┤
│ 调度延迟       │ <100ms        │ Pod启动速度      │
│ (Scheduling    │ (P99 <500ms)  │                  │
│  Latency)      │               │                  │
├────────────────┼───────────────┼──────────────────┤
│ 调度吞吐量     │ >100 pods/s   │ 大规模扩容速度   │
│ (Throughput)   │               │                  │
├────────────────┼───────────────┼──────────────────┤
│ 调度成功率     │ >99%          │ Pod Pending时间  │
│ (Success Rate) │               │                  │
├────────────────┼───────────────┼──────────────────┤
│ 抢占次数       │ 尽量少        │ 应用稳定性       │
│ (Preemptions)  │               │                  │
└────────────────┴───────────────┴──────────────────┘
```

**查看调度器性能指标：**

```bash
# 查看调度器Metrics
kubectl get --raw /metrics | grep scheduler

# 关键指标：
# scheduler_scheduling_duration_seconds - 调度延迟
# scheduler_pod_scheduling_attempts - 调度尝试次数
# scheduler_pending_pods - 等待调度的Pod数量
# scheduler_schedule_attempts_total - 总调度次数
# scheduler_preemption_attempts_total - 抢占次数
```

#### 7.1.3.2 调度器配置优化

**1. 调整调度器并发度**

```yaml
# kube-scheduler配置文件
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: default-scheduler

# 并发度配置
parallelism: 16  # 并发处理的Pod数量（默认16）
percentageOfNodesToScore: 50  # 预选阶段评估节点百分比（默认50%）
```

**2. 启用调度缓存**

```yaml
# 缓存配置
clientConnection:
  qps: 100      # API Server请求速率
  burst: 200    # 突发请求数量

# 缓存大小
# 默认情况下，调度器会缓存所有节点和Pod信息
# 对于超大集群（>5000节点），可以考虑：
# - 使用多调度器
# - 按namespace分片
```

**3. 禁用不必要的预选/优选算法**

```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- pluginConfig:
  # 禁用某些插件
  - name: InterPodAffinity
    args:
      hardPodAffinityWeight: 0  # 禁用硬亲和性
```

#### 7.1.3.3 大规模集群调度优化

**场景：10000节点集群**

```yaml
# 优化策略1：限制预选节点数量
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: high-performance-scheduler

# 只评估50%的节点
percentageOfNodesToScore: 50

# 优化策略2：使用节点分组
# 通过NodeSelector预先筛选节点池
spec:
  nodeSelector:
    node-pool: compute-intensive  # 只在特定节点池调度
```

**调度性能对比：**

```
默认配置（100%节点评估）：
- 10000节点 × 每节点1ms = 10秒
- P99延迟：15秒 ❌

优化配置（50%节点评估）：
- 5000节点 × 每节点1ms = 5秒
- P99延迟：7秒 ✅（提升53%）

进一步优化（NodeSelector + 50%评估）：
- 1000节点池 × 50% × 1ms = 500ms
- P99延迟：1秒 ✅（提升93%）
```

### 7.1.4 调度失败处理

#### 7.1.4.1 常见调度失败原因

```
┌────────────────────┬─────────────────────────────┐
│  失败原因          │   解决方案                  │
├────────────────────┼─────────────────────────────┤
│ Insufficient CPU   │ 增加节点或降低资源请求      │
│ Insufficient Memory│                             │
├────────────────────┼─────────────────────────────┤
│ No nodes available │ 检查节点是否Ready           │
│                    │ 检查是否有污点              │
├────────────────────┼─────────────────────────────┤
│ Pod affinity       │ 放宽亲和性要求              │
│ not satisfied      │ 或增加匹配的节点            │
├────────────────────┼─────────────────────────────┤
│ Node doesn't match │ 修改nodeSelector            │
│ node selector      │ 或给节点添加标签            │
├────────────────────┼─────────────────────────────┤
│ Didn't tolerate    │ 添加toleration              │
│ node taints        │ 或移除节点污点              │
├────────────────────┼─────────────────────────────┤
│ PVC not bound      │ 检查StorageClass            │
│                    │ 或创建PV                    │
└────────────────────┴─────────────────────────────┘
```

**查看调度失败事件：**

```bash
# 查看Pod事件
kubectl describe pod <pod-name>

# 示例输出
Events:
  Type     Reason            Message
  ----     ------            -------
  Warning  FailedScheduling  0/3 nodes are available: 1 Insufficient cpu,
                             2 node(s) didn't match Pod's node affinity/selector.

# 查看所有Pending的Pod
kubectl get pods --field-selector=status.phase=Pending -A

# 查看调度器日志
kubectl logs -n kube-system kube-scheduler-<node>
```

#### 7.1.4.2 调度失败示例与排查

**示例1：资源不足**

```yaml
# Pod定义
apiVersion: v1
kind: Pod
metadata:
  name: big-app
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        cpu: "32"        # 请求32核
        memory: "128Gi"  # 请求128GB内存
```

```bash
# 调度失败
kubectl describe pod big-app

Events:
  Warning  FailedScheduling  0/5 nodes are available:
           5 Insufficient cpu (available: 16, requested: 32).

# 解决方案1：降低资源请求
resources:
  requests:
    cpu: "8"
    memory: "32Gi"

# 解决方案2：添加更大的节点
kubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu,MEMORY:.status.capacity.memory
```

**示例2：节点选择器不匹配**

```yaml
# Pod定义
apiVersion: v1
kind: Pod
metadata:
  name: gpu-app
spec:
  nodeSelector:
    gpu: "true"  # 要求节点有gpu=true标签
  containers:
  - name: app
    image: tensorflow
```

```bash
# 调度失败
kubectl describe pod gpu-app

Events:
  Warning  FailedScheduling  0/5 nodes are available:
           5 node(s) didn't match Pod's node selector.

# 排查：检查节点标签
kubectl get nodes --show-labels | grep gpu
# 发现所有节点都没有gpu=true标签

# 解决方案：给GPU节点添加标签
kubectl label node node-gpu-1 gpu=true
```

**示例3：污点容忍不匹配**

```yaml
# 节点有污点
kubectl describe node node-gpu-1
Taints: nvidia.com/gpu=present:NoSchedule

# Pod没有容忍
apiVersion: v1
kind: Pod
metadata:
  name: gpu-app
spec:
  containers:
  - name: app
    image: tensorflow
  # ❌ 缺少tolerations
```

```bash
# 调度失败
Events:
  Warning  FailedScheduling  0/1 nodes are available:
           1 node(s) had untolerated taint {nvidia.com/gpu: present}.

# 解决方案：添加容忍
spec:
  tolerations:
  - key: nvidia.com/gpu
    operator: Equal
    value: present
    effect: NoSchedule
```

### 7.1.5 调度器监控与调试

#### 7.1.5.1 调度器监控指标

```yaml
# 部署Prometheus监控调度器
apiVersion: v1
kind: Service
metadata:
  name: kube-scheduler
  namespace: kube-system
  labels:
    component: kube-scheduler
spec:
  ports:
  - name: metrics
    port: 10259
    protocol: TCP
  selector:
    component: kube-scheduler
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kube-scheduler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: kube-scheduler
  endpoints:
  - port: metrics
    interval: 30s
```

**关键监控指标：**

```promql
# 调度延迟（P99）
histogram_quantile(0.99,
  sum(rate(scheduler_scheduling_duration_seconds_bucket[5m])) by (le)
)

# 调度失败率
sum(rate(scheduler_schedule_attempts_total{result="error"}[5m]))
/
sum(rate(scheduler_schedule_attempts_total[5m]))

# Pending Pod数量
sum(kube_pod_status_phase{phase="Pending"}) by (namespace)

# 调度吞吐量
sum(rate(scheduler_schedule_attempts_total{result="scheduled"}[5m]))
```

**Grafana仪表板示例：**

```json
{
  "dashboard": {
    "title": "Kubernetes Scheduler监控",
    "panels": [
      {
        "title": "调度延迟 (P50/P95/P99)",
        "targets": [{
          "expr": "histogram_quantile(0.99, sum(rate(scheduler_scheduling_duration_seconds_bucket[5m])) by (le))"
        }]
      },
      {
        "title": "调度成功率",
        "targets": [{
          "expr": "sum(rate(scheduler_schedule_attempts_total{result=\"scheduled\"}[5m])) / sum(rate(scheduler_schedule_attempts_total[5m]))"
        }]
      },
      {
        "title": "Pending Pod趋势",
        "targets": [{
          "expr": "sum(kube_pod_status_phase{phase=\"Pending\"}) by (namespace)"
        }]
      },
      {
        "title": "抢占事件",
        "targets": [{
          "expr": "sum(rate(scheduler_preemption_attempts_total[5m]))"
        }]
      }
    ]
  }
}
```

#### 7.1.5.2 调度器调试技巧

**1. 启用调度器详细日志**

```bash
# 修改kube-scheduler启动参数
--v=4  # 日志级别（0-10，越大越详细）

# 查看详细调度日志
kubectl logs -n kube-system kube-scheduler-master-1 --tail=100 -f | grep -i "schedule"
```

**2. 使用调度模拟器**

```bash
# 安装scheduler simulator
go install sigs.k8s.io/kube-scheduler-simulator@latest

# 启动模拟器
kube-scheduler-simulator

# 导入集群状态
kubectl get nodes -o json > nodes.json
kubectl get pods -A -o json > pods.json

# 模拟调度
./simulator --nodes=nodes.json --pods=pods.json --simulate=new-pod.yaml
```

**3. 手动模拟调度过程**

```bash
# 查看Pod的调度约束
kubectl get pod <pod-name> -o yaml | grep -A 10 "nodeSelector\|affinity\|tolerations"

# 查看节点资源
kubectl describe nodes | grep -A 5 "Allocated resources"

# 检查节点是否有污点
kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints

# 检查节点是否Ready
kubectl get nodes
```

### 7.1.6 调度器最佳实践

#### 7.1.6.1 资源请求最佳实践

**✅ 推荐做法：**

```yaml
# 1. 始终设置资源请求和限制
apiVersion: v1
kind: Pod
metadata:
  name: best-practice-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:        # ✅ 调度依据
        cpu: "500m"
        memory: "512Mi"
      limits:          # ✅ 资源上限
        cpu: "1"
        memory: "1Gi"
```

**❌ 避免的陷阱：**

```yaml
# ❌ 陷阱1：不设置requests
# 后果：调度器无法准确评估，可能导致节点过载
resources:
  limits:
    cpu: "1"
    memory: "1Gi"

# ❌ 陷阱2：requests == limits (QoS: Guaranteed)
# 后果：资源利用率低，浪费严重
resources:
  requests:
    cpu: "2"
    memory: "4Gi"
  limits:
    cpu: "2"        # requests == limits
    memory: "4Gi"   # 即使应用只用了500m/1Gi，也预留了2核/4GB

# ✅ 推荐：requests < limits (QoS: Burstable)
resources:
  requests:
    cpu: "500m"     # 平时使用
    memory: "1Gi"
  limits:
    cpu: "2"        # 高峰时可用
    memory: "4Gi"
```

**资源请求建议值：**

| 应用类型 | CPU Requests | Memory Requests | CPU Limits | Memory Limits |
|---------|--------------|-----------------|------------|---------------|
| **Web前端** | 100-500m | 128-512Mi | 1-2 | 512Mi-1Gi |
| **API服务** | 500m-1 | 512Mi-2Gi | 2-4 | 2-4Gi |
| **数据库** | 1-2 | 2-8Gi | 2-4 | 8-16Gi |
| **消息队列** | 500m-1 | 1-4Gi | 2-4 | 4-8Gi |
| **批处理** | 1-2 | 1-2Gi | 不限制 | 4-8Gi |

#### 7.1.6.2 调度策略最佳实践

**1. 高可用部署**

```yaml
# ✅ 使用Pod反亲和性确保副本分散
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web
              topologyKey: kubernetes.io/hostname  # 分散到不同节点
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web
              topologyKey: topology.kubernetes.io/zone  # 分散到不同可用区
```

**2. 性能优化部署**

```yaml
# ✅ 使用Pod亲和性将相关服务部署在一起
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
spec:
  template:
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - database  # 靠近数据库部署，降低延迟
              topologyKey: kubernetes.io/hostname
```

**3. 专用节点池**

```yaml
# ✅ 使用污点和容忍将特定工作负载调度到专用节点
# 1. 给GPU节点打污点
kubectl taint nodes node-gpu-1 gpu=true:NoSchedule

# 2. GPU应用添加容忍
apiVersion: v1
kind: Pod
metadata:
  name: ml-training
spec:
  tolerations:
  - key: gpu
    operator: Equal
    value: "true"
    effect: NoSchedule
  nodeSelector:
    gpu: "true"  # 确保调度到GPU节点
  containers:
  - name: trainer
    image: tensorflow/tensorflow:latest-gpu
    resources:
      limits:
        nvidia.com/gpu: 1
```

---

本节我们深入学习了Kubernetes调度器的工作原理，包括两阶段调度策略（预选和优选）、性能优化技巧、调度失败处理、监控调试方法以及最佳实践。理解调度器原理是优化应用性能和资源利用率的关键。在下一节中，我们将学习如何使用节点选择器和节点亲和性来精确控制Pod的调度位置。

---

**本节知识点回顾：**
- ✅ 调度器的核心价值和工作流程
- ✅ 预选（Predicates）和优选（Priorities）算法
- ✅ 调度性能优化和大规模集群调度
- ✅ 调度失败原因排查和解决方案
- ✅ 调度器监控指标和调试技巧
- ✅ 资源请求和调度策略最佳实践
## 7.2 节点选择器与节点亲和性

在上一节中，我们学习了Kubernetes调度器的工作原理。本节将深入探讨如何通过节点选择器（NodeSelector）和节点亲和性（Node Affinity）来精确控制Pod的调度位置，实现更灵活的调度策略。

### 7.2.1 节点选择器（NodeSelector）

#### 7.2.1.1 NodeSelector基础概念

NodeSelector是Kubernetes最简单的节点选择机制，通过标签（Label）匹配将Pod调度到特定节点。

**工作原理：**

```
┌─────────────────────────────────────────────────────┐
│          NodeSelector工作流程                        │
├─────────────────────────────────────────────────────┤
│                                                      │
│  1. 给节点打标签                                     │
│     kubectl label nodes node1 disktype=ssd          │
│                                                      │
│  2. Pod中指定nodeSelector                           │
│     spec:                                            │
│       nodeSelector:                                  │
│         disktype: ssd                                │
│                                                      │
│  3. 调度器过滤                                       │
│     只考虑标签匹配的节点                            │
│     ┌──────┐  ┌──────┐  ┌──────┐                  │
│     │Node1 │  │Node2 │  │Node3 │                  │
│     │ssd   │  │hdd   │  │ssd   │                  │
│     └──┬───┘  └──────┘  └──┬───┘                  │
│        │                    │                       │
│        └──────── ✅ ────────┘                       │
│        候选节点：Node1、Node3                       │
│                                                      │
│  4. 在候选节点中应用优选算法                        │
│     选择最优节点                                     │
└─────────────────────────────────────────────────────┘
```

**基本示例：**

```yaml
# 1. 给节点打标签
apiVersion: v1
kind: Node
metadata:
  name: node1
  labels:
    disktype: ssd
    region: us-west-1
    gpu: "true"
---
# 2. Pod使用nodeSelector
apiVersion: v1
kind: Pod
metadata:
  name: nginx-ssd
spec:
  nodeSelector:
    disktype: ssd    # 要求节点有disktype=ssd标签
  containers:
  - name: nginx
    image: nginx:1.25
```

#### 7.2.1.2 常用节点标签

**系统内置标签（自动添加）：**

```bash
# 查看节点标签
kubectl get nodes --show-labels

# 常见内置标签
kubernetes.io/hostname: node1                    # 节点主机名
kubernetes.io/os: linux                          # 操作系统
kubernetes.io/arch: amd64                        # CPU架构
node.kubernetes.io/instance-type: m5.2xlarge    # 实例类型（云环境）
topology.kubernetes.io/zone: us-west-1a         # 可用区
topology.kubernetes.io/region: us-west-1        # 区域
```

**自定义标签最佳实践：**

| 标签用途 | 标签键 | 示例值 |
|---------|--------|--------|
| **硬件类型** | disktype | ssd, hdd, nvme |
| **GPU** | gpu | true, false |
| **网络** | network | 10g, 1g |
| **环境** | environment | production, staging, dev |
| **业务线** | business-unit | payment, order, user |
| **节点池** | node-pool | compute, memory, gpu |
| **专用节点** | dedicated | database, cache, ml |

**给节点添加标签：**

```bash
# 添加单个标签
kubectl label nodes node1 disktype=ssd

# 添加多个标签
kubectl label nodes node1 \
  disktype=ssd \
  gpu=true \
  environment=production

# 修改标签
kubectl label nodes node1 disktype=nvme --overwrite

# 删除标签
kubectl label nodes node1 disktype-

# 批量打标签
kubectl label nodes node{1..3} node-pool=compute
```

#### 7.2.1.3 NodeSelector使用场景

**场景1：SSD存储节点**

```yaml
# 数据库Pod需要高性能存储
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  template:
    spec:
      nodeSelector:
        disktype: ssd      # 调度到SSD节点
      containers:
      - name: mysql
        image: mysql:8.0
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: local-ssd
      resources:
        requests:
          storage: 100Gi
```

**场景2：GPU节点**

```yaml
# 机器学习训练任务
apiVersion: v1
kind: Pod
metadata:
  name: tensorflow-training
spec:
  nodeSelector:
    gpu: "true"              # 调度到GPU节点
    gpu-type: nvidia-v100    # 指定GPU型号
  containers:
  - name: trainer
    image: tensorflow/tensorflow:latest-gpu
    resources:
      limits:
        nvidia.com/gpu: 2    # 请求2块GPU
```

**场景3：区域/可用区调度**

```yaml
# 指定可用区部署（多可用区高可用）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-us-west-1a
spec:
  replicas: 3
  template:
    spec:
      nodeSelector:
        topology.kubernetes.io/zone: us-west-1a  # 部署在us-west-1a
      containers:
      - name: web
        image: nginx:1.25
```

**场景4：专用节点池**

```yaml
# 支付服务部署在专用节点池
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-service
spec:
  replicas: 5
  template:
    spec:
      nodeSelector:
        node-pool: payment    # 专用支付节点池
        environment: production
      containers:
      - name: payment
        image: payment:v2.0
```

#### 7.2.1.4 NodeSelector的局限性

```
┌────────────────────┬──────────────────────────────┐
│  局限性            │   说明                        │
├────────────────────┼──────────────────────────────┤
│ ❌ 只支持AND逻辑   │ 多个标签必须全部匹配          │
│                    │ 无法实现OR逻辑                │
├────────────────────┼──────────────────────────────┤
│ ❌ 不支持软约束     │ 标签不匹配则调度失败          │
│                    │ 无法实现"尽量满足"            │
├────────────────────┼──────────────────────────────┤
│ ❌ 表达能力有限     │ 无法使用In、NotIn等操作符     │
│                    │ 无法基于标签值范围选择        │
├────────────────────┼──────────────────────────────┤
│ ❌ 无法指定权重     │ 无法表达偏好程度              │
└────────────────────┴──────────────────────────────┘

解决方案：使用Node Affinity（节点亲和性）
```

**示例：NodeSelector的限制**

```yaml
# ❌ 无法实现：调度到ssd=true OR nvme=true的节点
spec:
  nodeSelector:
    disktype: ssd    # 只能是AND关系
    disktype: nvme   # ❌ 这样写会冲突

# ✅ 解决方案：使用Node Affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
            - nvme    # ✅ 支持OR逻辑
```

### 7.2.2 节点亲和性（Node Affinity）

#### 7.2.2.1 Node Affinity概述

Node Affinity是NodeSelector的增强版，提供更强大和灵活的节点选择能力。

**Node Affinity vs NodeSelector：**

```
┌───────────────────┬────────────────┬────────────────┐
│      特性         │  NodeSelector  │ Node Affinity  │
├───────────────────┼────────────────┼────────────────┤
│ 表达能力          │ 基础（仅等于） │ 强大（In/NotIn等）│
│ 逻辑运算          │ 仅AND          │ AND + OR       │
│ 软约束/硬约束     │ 仅硬约束       │ 都支持         │
│ 权重              │ 不支持         │ 支持           │
│ 反亲和性          │ 不支持         │ 支持           │
│ 配置复杂度        │ 简单           │ 中等           │
│ 推荐使用场景      │ 简单场景       │ 复杂调度需求   │
└───────────────────┴────────────────┴────────────────┘
```

**Node Affinity两种类型：**

```yaml
spec:
  affinity:
    nodeAffinity:
      # 1. 硬约束（必须满足）
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms: [...]
      
      # 2. 软约束（尽量满足）
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions: [...]
```

**命名规则解析：**

```
requiredDuringSchedulingIgnoredDuringExecution
    ↓             ↓              ↓
  必须满足      调度阶段      执行阶段忽略
  
解释：
- requiredDuringScheduling：调度时必须满足
- IgnoredDuringExecution：Pod运行时节点标签变化不影响

preferredDuringSchedulingIgnoredDuringExecution
    ↓             ↓              ↓
  尽量满足      调度阶段      执行阶段忽略
```

#### 7.2.2.2 硬亲和性（Required）

硬亲和性是强制性约束，如果没有满足条件的节点，Pod将无法调度（Pending状态）。

**基本语法：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: affinity-required
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:    # OR关系（任一term满足即可）
        - matchExpressions:   # AND关系（同一term内所有表达式必须满足）
          - key: disktype
            operator: In      # 支持In、NotIn、Exists、DoesNotExist、Gt、Lt
            values:
            - ssd
            - nvme
  containers:
  - name: app
    image: nginx
```

**支持的操作符：**

| 操作符 | 说明 | 示例 |
|-------|------|------|
| **In** | 标签值在列表中 | disktype In [ssd, nvme] |
| **NotIn** | 标签值不在列表中 | disktype NotIn [hdd] |
| **Exists** | 标签键存在（不关心值） | gpu Exists |
| **DoesNotExist** | 标签键不存在 | legacy DoesNotExist |
| **Gt** | 标签值大于（数值） | cpu-count Gt 16 |
| **Lt** | 标签值小于（数值） | cpu-count Lt 32 |

**示例1：In操作符（OR逻辑）**

```yaml
# 调度到SSD或NVMe节点
apiVersion: v1
kind: Pod
metadata:
  name: database
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd      # disktype=ssd 或
            - nvme     # disktype=nvme
  containers:
  - name: mysql
    image: mysql:8.0
```

**示例2：NotIn操作符（排除）**

```yaml
# 不调度到HDD节点
apiVersion: v1
kind: Pod
metadata:
  name: high-performance-app
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: NotIn
            values:
            - hdd      # 排除hdd节点
  containers:
  - name: app
    image: myapp:latest
```

**示例3：Exists操作符（标签存在性）**

```yaml
# 调度到有GPU的节点（不关心GPU具体型号）
apiVersion: v1
kind: Pod
metadata:
  name: ml-training
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu
            operator: Exists    # 只要有gpu标签即可
  containers:
  - name: trainer
    image: tensorflow/tensorflow:latest-gpu
```

**示例4：Gt/Lt操作符（数值比较）**

```yaml
# 调度到CPU核心数大于16的节点
apiVersion: v1
kind: Pod
metadata:
  name: compute-intensive
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu-count
            operator: Gt
            values:
            - "16"     # 必须是字符串
  containers:
  - name: app
    image: compute-app:latest
```

**示例5：多条件AND逻辑**

```yaml
# 调度到SSD+GPU+生产环境节点
apiVersion: v1
kind: Pod
metadata:
  name: production-ml-app
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
          - key: gpu           # AND 关系
            operator: Exists
          - key: environment   # AND 关系
            operator: In
            values:
            - production
  containers:
  - name: app
    image: ml-app:v2.0
```

**示例6：多个nodeSelectorTerms（OR逻辑）**

```yaml
# 调度到（SSD节点）OR（GPU节点）
apiVersion: v1
kind: Pod
metadata:
  name: flexible-scheduling
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:   # Term 1：SSD节点
          - key: disktype
            operator: In
            values:
            - ssd
        - matchExpressions:   # Term 2：GPU节点（OR关系）
          - key: gpu
            operator: Exists
  containers:
  - name: app
    image: myapp:latest
```

#### 7.2.2.3 软亲和性（Preferred）

软亲和性是偏好性约束，调度器会尽量满足，但不满足也可以调度到其他节点。

**基本语法：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: affinity-preferred
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100          # 权重（1-100）
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
      - weight: 50           # 第二优先级
        preference:
          matchExpressions:
          - key: zone
            operator: In
            values:
            - us-west-1a
  containers:
  - name: app
    image: nginx
```

**权重计算机制：**

```
调度器打分流程：

1. 基础得分（LeastRequested、Balanced等）
2. + Node Affinity权重得分
3. = 最终得分

示例：
节点1：SSD=true, zone=us-west-1a
  基础得分：70
  匹配disktype=ssd：+100（权重100）
  匹配zone=us-west-1a：+50（权重50）
  最终得分：70 + 100 + 50 = 220

节点2：HDD=true, zone=us-west-1a
  基础得分：80
  不匹配disktype=ssd：+0
  匹配zone=us-west-1a：+50（权重50）
  最终得分：80 + 0 + 50 = 130

结果：选择节点1（得分更高）
```

**示例1：单个软约束**

```yaml
# 优先调度到SSD节点，但HDD节点也可以
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 5
  template:
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
      containers:
      - name: nginx
        image: nginx:1.25
```

**示例2：多级优先级**

```yaml
# 优先级：SSD(100) > us-west-1a(80) > 16核以上(50)
apiVersion: v1
kind: Pod
metadata:
  name: multi-preference
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100        # 第一优先级：SSD
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
      - weight: 80         # 第二优先级：可用区
        preference:
          matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: In
            values:
            - us-west-1a
      - weight: 50         # 第三优先级：CPU核心数
        preference:
          matchExpressions:
          - key: cpu-count
            operator: Gt
            values:
            - "16"
  containers:
  - name: app
    image: myapp:latest
```

**示例3：硬约束+软约束组合**

```yaml
# 硬约束：必须有GPU
# 软约束：优先V100，其次A100
apiVersion: v1
kind: Pod
metadata:
  name: gpu-training
spec:
  affinity:
    nodeAffinity:
      # 硬约束：必须有GPU
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu
            operator: Exists
      
      # 软约束：优先V100
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: gpu-type
            operator: In
            values:
            - nvidia-v100
      - weight: 80
        preference:
          matchExpressions:
          - key: gpu-type
            operator: In
            values:
            - nvidia-a100
  containers:
  - name: trainer
    image: tensorflow/tensorflow:latest-gpu
    resources:
      limits:
        nvidia.com/gpu: 1
```

#### 7.2.2.4 区域与可用区调度

**多可用区高可用部署：**

```yaml
# Deployment：跨可用区部署（高可用）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-ha
spec:
  replicas: 6
  template:
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：必须在us-west-1区域
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - us-west-1
          
          # 软约束：优先级 a > b > c
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-west-1a
          - weight: 80
            preference:
              matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-west-1b
          - weight: 60
            preference:
              matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-west-1c
      
      # Pod反亲和性：确保副本分散
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-ha
            topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: nginx
        image: nginx:1.25
```

**结果分布示例：**

```
期望结果（6个副本跨3个可用区）：

us-west-1a: 2个Pod  ✅ 高可用
us-west-1b: 2个Pod  ✅ 高可用
us-west-1c: 2个Pod  ✅ 高可用

如果某个可用区故障：
us-west-1a: ❌ 故障
us-west-1b: 2个Pod  ✅ 正常
us-west-1c: 2个Pod  ✅ 正常
服务可用性：66.7%（而非0%）
```

### 7.2.3 实战案例

#### 7.2.3.1 案例1：数据库高可用部署

**需求：**
- MySQL主从复制（1主2从）
- 主库部署在SSD节点
- 从库分散到不同可用区
- 优先使用高性能节点

```yaml
# MySQL主库：必须SSD，优先高配节点
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-master
spec:
  serviceName: mysql-master
  replicas: 1
  template:
    metadata:
      labels:
        app: mysql
        role: master
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：必须SSD
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
                - nvme
          
          # 软约束：优先高配节点
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node-tier
                operator: In
                values:
                - high-performance
          - weight: 80
            preference:
              matchExpressions:
              - key: cpu-count
                operator: Gt
                values:
                - "32"
      
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        resources:
          requests:
            cpu: "4"
            memory: "16Gi"
          limits:
            cpu: "8"
            memory: "32Gi"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: local-ssd
      resources:
        requests:
          storage: 500Gi
---
# MySQL从库：跨可用区部署
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-slave
spec:
  serviceName: mysql-slave
  replicas: 2
  template:
    metadata:
      labels:
        app: mysql
        role: slave
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：必须SSD
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
        
        # Pod反亲和性：分散到不同可用区
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - mysql
              - key: role
                operator: In
                values:
                - slave
            topologyKey: topology.kubernetes.io/zone  # 不同可用区
      
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        - name: MYSQL_MASTER_HOST
          value: mysql-master-0.mysql-master
        resources:
          requests:
            cpu: "2"
            memory: "8Gi"
          limits:
            cpu: "4"
            memory: "16Gi"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: local-ssd
      resources:
        requests:
          storage: 500Gi
```

**部署结果：**

```
Master节点选择：
✅ Node-SSD-1（高性能节点，SSD，32核64GB）

Slave节点选择：
✅ Node-SSD-2（us-west-1a，SSD，16核32GB）
✅ Node-SSD-3（us-west-1b，SSD，16核32GB）

高可用性分析：
- 主从分离 ✅
- 从库跨可用区 ✅
- 单可用区故障，主库+1个从库仍可用 ✅
```

#### 7.2.3.2 案例2：机器学习训练集群

**需求：**
- GPU训练节点（必须有GPU）
- 优先使用V100，其次A100
- 数据预处理任务使用CPU节点
- 训练和预处理Pod不能在同一节点（避免资源竞争）

```yaml
# GPU训练任务
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-training
spec:
  parallelism: 4    # 4个并行训练任务
  template:
    metadata:
      labels:
        app: ml-training
        component: trainer
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：必须有GPU
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu
                operator: Exists
              - key: gpu-count
                operator: Gt
                values:
                - "0"
          
          # 软约束：优先V100
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: gpu-type
                operator: In
                values:
                - nvidia-v100
          - weight: 80
            preference:
              matchExpressions:
              - key: gpu-type
                operator: In
                values:
                - nvidia-a100
        
        # Pod反亲和性：避免与数据预处理任务共存
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: component
                  operator: In
                  values:
                  - preprocessor
              topologyKey: kubernetes.io/hostname
      
      restartPolicy: OnFailure
      containers:
      - name: trainer
        image: tensorflow/tensorflow:latest-gpu
        command: ["python", "train.py"]
        resources:
          limits:
            nvidia.com/gpu: 2    # 每个任务使用2块GPU
            memory: "64Gi"
            cpu: "16"
        volumeMounts:
        - name: data
          mountPath: /data
        - name: model
          mountPath: /model
      
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: training-data
      - name: model
        persistentVolumeClaim:
          claimName: model-output
---
# 数据预处理任务（CPU密集型）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-preprocessor
spec:
  replicas: 10
  template:
    metadata:
      labels:
        app: ml-training
        component: preprocessor
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：CPU节点（无GPU）
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu
                operator: DoesNotExist
          
          # 软约束：优先高核心数节点
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: cpu-count
                operator: Gt
                values:
                - "16"
        
        # Pod反亲和性：避免与训练任务共存
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: component
                  operator: In
                  values:
                  - trainer
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: preprocessor
        image: python:3.9
        command: ["python", "preprocess.py"]
        resources:
          requests:
            cpu: "8"
            memory: "16Gi"
          limits:
            cpu: "16"
            memory: "32Gi"
```

**部署结果：**

```
训练任务分布（4个Pod）：
✅ GPU-Node-1（2×V100）- 2个训练Pod
✅ GPU-Node-2（2×V100）- 2个训练Pod

预处理任务分布（10个Pod）：
✅ CPU-Node-1（32核） - 3个预处理Pod
✅ CPU-Node-2（32核） - 3个预处理Pod
✅ CPU-Node-3（32核） - 4个预处理Pod

资源隔离验证：
- 训练任务仅在GPU节点 ✅
- 预处理任务仅在CPU节点 ✅
- 训练和预处理不共存节点 ✅
```

### 7.2.4 常见问题与排查

#### 7.2.4.1 Pod一直Pending

**问题现象：**

```bash
kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
myapp-7c5ddbdf54-abc   0/1     Pending   0          5m

kubectl describe pod myapp-7c5ddbdf54-abc
Events:
  Warning  FailedScheduling  0/5 nodes are available:
           5 node(s) didn't match Pod's node affinity/selector.
```

**排查步骤：**

```bash
# 1. 查看Pod的节点亲和性配置
kubectl get pod myapp-7c5ddbdf54-abc -o yaml | grep -A 20 "affinity"

# 2. 查看所有节点的标签
kubectl get nodes --show-labels

# 3. 检查是否有节点匹配要求
kubectl get nodes -l disktype=ssd
# 如果为空，说明没有节点有该标签

# 4. 解决方案1：给节点添加标签
kubectl label nodes node1 disktype=ssd

# 5. 解决方案2：修改Pod的亲和性配置
# 将required改为preferred，或放宽条件
```

**常见错误示例：**

```yaml
# ❌ 错误1：标签键拼写错误
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktpye    # ❌ 拼写错误（disktype）
            operator: In
            values:
            - ssd

# ❌ 错误2：值大小写错误
spec:
  nodeSelector:
    disktype: SSD    # ❌ 节点标签是小写ssd

# ✅ 正确
spec:
  nodeSelector:
    disktype: ssd    # ✅ 匹配节点标签
```

#### 7.2.4.2 调度不均衡

**问题现象：**

```bash
# 所有Pod都调度到了同一个节点
kubectl get pods -o wide
NAME                   NODE
web-7c5ddbdf54-abc     node1
web-7c5ddbdf54-def     node1
web-7c5ddbdf54-ghi     node1
```

**原因分析：**

```yaml
# 原因：软亲和性权重过高，导致其他优选算法失效
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

# 如果只有node1是SSD，所有Pod都会调度到node1
```

**解决方案：**

```yaml
# 方案1：降低权重，让其他优选算法生效
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 30    # ✅ 降低权重
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

# 方案2：添加Pod反亲和性，强制分散
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - web
          topologyKey: kubernetes.io/hostname
```

### 7.2.5 最佳实践

#### 7.2.5.1 标签管理最佳实践

**✅ 推荐的标签命名规范：**

```yaml
# 使用分层命名空间
labels:
  # 基础设施层
  topology.kubernetes.io/region: us-west-1
  topology.kubernetes.io/zone: us-west-1a
  
  # 硬件层
  hardware.example.com/disktype: ssd
  hardware.example.com/gpu: "true"
  hardware.example.com/gpu-type: nvidia-v100
  hardware.example.com/cpu-count: "32"
  
  # 业务层
  business.example.com/environment: production
  business.example.com/team: payment
  business.example.com/cost-center: engineering
  
  # 应用层
  app.kubernetes.io/name: mysql
  app.kubernetes.io/component: database
  app.kubernetes.io/version: "8.0"
```

**❌ 避免的陷阱：**

```yaml
# ❌ 1. 使用空格或特殊字符
labels:
  disk type: ssd         # ❌ 空格
  gpu-type!: v100        # ❌ 感叹号

# ❌ 2. 过长的标签值
labels:
  description: "This is a very very very long description..."  # ❌ >63字符

# ❌ 3. 使用敏感信息
labels:
  password: secret123    # ❌ 安全风险

# ✅ 正确做法
labels:
  disktype: ssd
  gpu-type: v100
  description: high-performance-node
```

#### 7.2.5.2 亲和性配置最佳实践

**1. 优先使用软约束（Preferred）**

```yaml
# ✅ 推荐：使用软约束提高灵活性
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

# ❌ 避免：过度使用硬约束
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
# 后果：如果所有SSD节点资源不足，Pod将无法调度
```

**2. 硬约束+软约束组合**

```yaml
# ✅ 最佳实践：关键要求用硬约束，偏好用软约束
spec:
  affinity:
    nodeAffinity:
      # 硬约束：必须在生产环境
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: environment
            operator: In
            values:
            - production
      
      # 软约束：优先SSD
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
```

**3. 合理设置权重**

```yaml
# ✅ 权重设置建议
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100    # 最高优先级：关键要求
        preference:
          matchExpressions:
          - key: environment
            operator: In
            values:
            - production
      
      - weight: 50     # 中等优先级：性能优化
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
      
      - weight: 20     # 低优先级：成本优化
        preference:
          matchExpressions:
          - key: instance-type
            operator: In
            values:
            - spot    # Spot实例（便宜但可能被回收）
```

**权重设置原则：**

| 优先级 | 权重范围 | 使用场景 |
|-------|---------|---------|
| **最高** | 80-100 | 关键业务要求（环境、合规性） |
| **高** | 60-79 | 重要性能要求（GPU、SSD） |
| **中** | 40-59 | 一般性能优化（CPU核心数） |
| **低** | 20-39 | 成本优化（可用区、实例类型） |
| **极低** | 1-19 | 次要偏好（便利性） |

#### 7.2.5.3 生产环境配置模板

**模板1：无状态应用（高可用）**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stateless-app
spec:
  replicas: 6
  template:
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：生产环境
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: environment
                operator: In
                values:
                - production
          
          # 软约束：性能优化
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 50
            preference:
              matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
        
        # Pod反亲和性：跨节点+跨可用区
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - stateless-app
              topologyKey: kubernetes.io/hostname
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - stateless-app
              topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: app
        image: myapp:v2.0
```

**模板2：有状态应用（数据库）**

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
spec:
  replicas: 3
  template:
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：SSD+生产环境
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
                - nvme
              - key: environment
                operator: In
                values:
                - production
          
          # 软约束：高配节点
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: node-tier
                operator: In
                values:
                - high-performance
        
        # Pod反亲和性：强制跨可用区
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - database
            topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: mysql
        image: mysql:8.0
```

---

本节我们深入学习了节点选择器（NodeSelector）和节点亲和性（Node Affinity），包括基本概念、使用场景、硬约束与软约束的区别、实战案例以及最佳实践。通过合理使用这些调度约束，可以实现精确的Pod调度控制，优化应用性能和高可用性。在下一节中，我们将学习Pod亲和性与反亲和性，探讨如何基于Pod之间的关系来调度。

---

**本节知识点回顾：**
- ✅ NodeSelector的基本用法和局限性
- ✅ Node Affinity的硬约束（Required）和软约束（Preferred）
- ✅ 6种操作符（In、NotIn、Exists、DoesNotExist、Gt、Lt）
- ✅ 多条件AND/OR逻辑组合
- ✅ 区域与可用区调度策略
- ✅ 实战案例（数据库高可用、机器学习集群）
- ✅ 常见问题排查和最佳实践
