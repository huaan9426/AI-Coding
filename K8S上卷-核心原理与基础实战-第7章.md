# 第7章：调度与资源管理

> 在第6章中，我们系统学习了Kubernetes的配置与安全管理，包括ResourceQuota资源配额、Pod安全策略、RBAC权限控制和网络策略。本章将深入探讨Kubernetes的调度机制和高级资源管理，揭示Pod如何被智能分配到合适的节点，以及如何优化资源利用率。

**本章学习目标：**
- 深入理解Kubernetes调度器（kube-scheduler）工作原理
- 掌握节点选择器（NodeSelector）和节点亲和性（Node Affinity）
- 学习Pod亲和性与反亲和性（Pod Affinity/Anti-Affinity）
- 掌握污点（Taints）和容忍（Tolerations）机制
- 理解优先级（Priority）和抢占（Preemption）
- 实战：构建高可用的多层应用架构

---

## 7.1 Kubernetes调度器原理

Kubernetes调度器（kube-scheduler）是集群的"交通指挥官"，负责将新创建的Pod分配到合适的节点上运行。理解调度器的工作原理，是优化应用性能和资源利用率的关键。

### 7.1.1 为什么需要调度器

#### 7.1.1.1 资源分配的挑战

在一个包含数百个节点和数千个Pod的Kubernetes集群中，如何高效地分配资源是一个复杂的问题：

```
典型的生产集群场景：
┌──────────────────────────────────────────────────┐
│  集群规模：                                       │
│  - 节点数量：200个                                │
│  - Pod总数：5000+                                │
│  - 每天新建Pod：1000+                            │
│                                                   │
│  节点异构性：                                     │
│  - CPU型节点：64核CPU，128GB内存                 │
│  - 内存型节点：32核CPU，512GB内存                │
│  - GPU节点：16核CPU，256GB内存，8卡V100          │
│  - 边缘节点：8核CPU，16GB内存（低延迟）          │
│                                                   │
│  应用多样性：                                     │
│  - Web应用：CPU中等，内存中等，需要高可用        │
│  - 数据库：CPU低，内存高，需要持久化存储         │
│  - 机器学习：CPU高/GPU，内存极高，计算密集       │
│  - 消息队列：CPU中等，内存高，网络IO密集         │
│                                                   │
│  调度约束：                                       │
│  - 亲和性：前端Pod需要靠近后端Pod（降低延迟）   │
│  - 反亲和性：同一应用的多个副本分散到不同节点    │
│  - 污点容忍：只有特定Pod能运行在GPU节点          │
│  - 资源预留：保证关键应用的资源需求              │
└──────────────────────────────────────────────────┘
```

**没有智能调度的后果：**

| 问题 | 场景 | 影响 |
|-----|------|------|
| **资源浪费** | 小Pod调度到大节点 | 资源碎片化，浪费严重 |
| **性能下降** | CPU密集Pod和内存密集Pod挤在同一节点 | 资源竞争，性能下降 |
| **高可用性差** | 同一应用的多个副本都在一个节点 | 节点故障导致服务中断 |
| **负载不均** | 部分节点超载，部分节点空闲 | 集群利用率低 |
| **延迟增加** | 前后端应用跨可用区部署 | 网络延迟增加 |

#### 7.1.1.2 调度器的价值

**Kubernetes调度器提供的核心价值：**

```
┌─────────────────────────────────────────────────┐
│          调度器核心价值                          │
├─────────────────────────────────────────────────┤
│  ✅ 智能资源分配                                 │
│     根据节点资源和Pod需求，找到最佳匹配         │
│                                                  │
│  ✅ 高可用保障                                   │
│     通过反亲和性将副本分散到不同节点/可用区     │
│                                                  │
│  ✅ 性能优化                                     │
│     将相互依赖的Pod调度到同一节点/可用区        │
│                                                  │
│  ✅ 资源利用率最大化                             │
│     Bin Packing算法，尽量减少资源碎片           │
│                                                  │
│  ✅ 灵活的调度策略                               │
│     支持自定义调度规则和优先级                  │
│                                                  │
│  ✅ 故障自愈                                     │
│     节点故障时自动将Pod重新调度到健康节点       │
└─────────────────────────────────────────────────┘
```

**真实案例对比：**

```yaml
# ❌ 场景1：没有调度约束
# 结果：3个Nginx副本都调度到了Node1
kubectl get pods -o wide
NAME                     NODE
nginx-7c5ddbdf54-abc12   node1
nginx-7c5ddbdf54-def34   node1  # ❌ 高可用性差！
nginx-7c5ddbdf54-ghi56   node1  # ❌ Node1故障则全部宕机！

# ✅ 场景2：使用反亲和性
# 结果：3个副本分散到不同节点
kubectl get pods -o wide
NAME                     NODE
nginx-7c5ddbdf54-abc12   node1
nginx-7c5ddbdf54-def34   node2  # ✅ 高可用！
nginx-7c5ddbdf54-ghi56   node3  # ✅ 单节点故障不影响服务！
```

#### 7.1.1.3 调度的类型

Kubernetes支持多种调度方式：

```
┌────────────────┬───────────────────┬─────────────────────┐
│  调度类型      │    触发条件       │    使用场景         │
├────────────────┼───────────────────┼─────────────────────┤
│ 默认调度       │ Pod创建时         │ 大部分应用          │
│ (Default)      │ nodeName字段为空  │ (90%+场景)          │
├────────────────┼───────────────────┼─────────────────────┤
│ 绑定调度       │ Pod创建时         │ 特定节点部署        │
│ (NodeName)     │ nodeName字段指定  │ (DaemonSet等)       │
├────────────────┼───────────────────┼─────────────────────┤
│ 重新调度       │ 节点故障          │ 故障恢复            │
│ (Rescheduling) │ Pod被驱逐         │                     │
├────────────────┼───────────────────┼─────────────────────┤
│ 抢占调度       │ 资源不足          │ 高优先级Pod         │
│ (Preemption)   │ 高优先级Pod到达   │                     │
├────────────────┼───────────────────┼─────────────────────┤
│ 自定义调度     │ 配置自定义调度器  │ 特殊调度需求        │
│ (Custom)       │ schedulerName字段 │ (AI/大数据场景)     │
└────────────────┴───────────────────┴─────────────────────┘
```

### 7.1.2 调度器工作流程

#### 7.1.2.1 调度流程概览

Kubernetes调度器采用两阶段调度策略：**预选（Predicates）→ 优选（Priorities）**

```
┌─────────────────────────────────────────────────────────────┐
│              Kubernetes调度器工作流程                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  1. 监听Pod创建                                              │
│     kube-scheduler通过Watch机制监听API Server              │
│     发现新创建的、nodeName为空的Pod                         │
│          ↓                                                   │
│  2. 预选阶段（Predicates/Filtering）                        │
│     过滤掉不符合条件的节点                                  │
│     ┌─────────────────────────────────────────┐            │
│     │ 所有节点（200个）                       │            │
│     └─────────────────────────────────────────┘            │
│          ↓                                                   │
│     [ 预选算法 ]                                             │
│     - PodFitsResources：资源是否足够？                      │
│     - PodFitsHostPorts：端口是否冲突？                      │
│     - PodMatchNodeSelector：是否匹配NodeSelector？         │
│     - PodToleratesNodeTaints：是否容忍污点？               │
│     - CheckNodeMemoryPressure：内存压力是否过大？          │
│     - CheckNodeDiskPressure：磁盘压力是否过大？            │
│          ↓                                                   │
│     ┌─────────────────────────────────────────┐            │
│     │ 可行节点（50个）                        │            │
│     └─────────────────────────────────────────┘            │
│          ↓                                                   │
│  3. 优选阶段（Priorities/Scoring）                          │
│     对可行节点打分，选择最优节点                            │
│     ┌───────────┬───────────┬───────────┐                  │
│     │ Node1: 85 │ Node2: 92 │ Node3: 78 │  ...             │
│     └───────────┴───────────┴───────────┘                  │
│          ↓                                                   │
│     [ 优选算法 ]                                             │
│     - LeastRequestedPriority：资源请求最少                 │
│     - BalancedResourceAllocation：CPU/内存均衡             │
│     - SelectorSpreadPriority：副本分散                     │
│     - NodeAffinityPriority：节点亲和性得分                 │
│     - InterPodAffinityPriority：Pod亲和性得分              │
│          ↓                                                   │
│     ┌─────────────────────────────────────────┐            │
│     │ 最优节点：Node2（得分92）              │            │
│     └─────────────────────────────────────────┘            │
│          ↓                                                   │
│  4. 绑定（Binding）                                          │
│     将Pod绑定到选定的节点                                   │
│     - 更新Pod.spec.nodeName = "node2"                       │
│     - 通知kubelet启动Pod                                    │
│          ↓                                                   │
│  5. 假设调度（Assume）                                       │
│     乐观假设Pod已调度，允许后续Pod调度                     │
│     （避免多个Pod同时调度导致资源超售）                    │
└─────────────────────────────────────────────────────────────┘
```

**详细调度流程：**

```go
// 调度器伪代码
func scheduleOne(pod *v1.Pod) {
    // 1. 预选：过滤不可行节点
    feasibleNodes := findNodesThatFit(pod, allNodes)

    if len(feasibleNodes) == 0 {
        // 没有可行节点，触发抢占或失败
        return preemptOrFail(pod)
    }

    // 2. 优选：对可行节点打分
    prioritizedNodes := prioritizeNodes(pod, feasibleNodes)

    // 3. 选择得分最高的节点
    bestNode := selectHost(prioritizedNodes)

    // 4. 假设调度（乐观锁）
    assumedPod := pod.DeepCopy()
    assumedPod.Spec.NodeName = bestNode
    scheduler.Cache.AssumePod(assumedPod)

    // 5. 异步绑定
    go func() {
        err := bind(pod, bestNode)
        if err != nil {
            scheduler.Cache.ForgetPod(assumedPod)
        }
    }()
}
```

#### 7.1.2.2 预选（Predicates）详解

预选阶段会运行一系列预选函数（Predicate Functions），过滤掉不满足条件的节点：

**核心预选算法：**

| 预选算法 | 检查内容 | 示例 |
|---------|---------|------|
| **PodFitsResources** | 节点资源是否满足Pod需求 | Pod请求2核4GB，节点剩余1核2GB → ❌ |
| **PodFitsHostPorts** | 节点端口是否被占用 | Pod使用hostPort 8080，节点已有Pod占用 → ❌ |
| **PodMatchNodeSelector** | Pod的nodeSelector是否匹配节点标签 | nodeSelector: {disk: ssd}，节点无此标签 → ❌ |
| **PodToleratesNodeTaints** | Pod是否容忍节点污点 | 节点有gpu=true:NoSchedule，Pod无容忍 → ❌ |
| **CheckNodeMemoryPressure** | 节点内存压力是否过大 | 节点内存使用>85% → ❌ |
| **CheckNodeDiskPressure** | 节点磁盘压力是否过大 | 节点磁盘使用>85% → ❌ |
| **CheckNodePIDPressure** | 节点PID资源是否耗尽 | PID数量接近上限 → ❌ |
| **CheckVolumeBinding** | 存储卷是否可绑定 | PVC要求local-storage，节点无此类型 → ❌ |
| **NoDiskConflict** | 存储卷是否冲突 | Pod使用同一GCE PD的多个Pod在同一节点 → ❌ |
| **NoVolumeZoneConflict** | 存储卷可用区是否匹配 | Pod使用us-east-1a的EBS，节点在us-east-1b → ❌ |

**预选示例：**

```yaml
# Pod资源请求
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "4Gi"
        cpu: "2"
      limits:
        memory: "8Gi"
        cpu: "4"
```

```
预选过程：

节点1：16核64GB，已使用12核58GB
  ├─ 剩余资源：4核6GB
  ├─ Pod请求：2核4GB
  └─ ✅ 通过 PodFitsResources

节点2：8核32GB，已使用7核30GB
  ├─ 剩余资源：1核2GB
  ├─ Pod请求：2核4GB
  └─ ❌ 失败 PodFitsResources（资源不足）

节点3：32核128GB，有污点gpu=true:NoSchedule
  ├─ 剩余资源：30核120GB（充足）
  ├─ Pod无toleration
  └─ ❌ 失败 PodToleratesNodeTaints

节点4：8核16GB，已有Pod使用hostPort 80
  ├─ 剩余资源：6核12GB
  ├─ Pod使用hostPort 80
  └─ ❌ 失败 PodFitsHostPorts

结果：只有节点1通过预选
```

#### 7.1.2.3 优选（Priorities）详解

优选阶段对通过预选的节点进行打分（0-100分），得分最高的节点被选中：

**核心优选算法：**

| 优选算法 | 打分逻辑 | 权重 |
|---------|---------|------|
| **LeastRequestedPriority** | 资源请求越少得分越高（Bin Packing） | 1 |
| **BalancedResourceAllocation** | CPU和内存使用率越均衡得分越高 | 1 |
| **SelectorSpreadPriority** | 同一Service/RC的Pod分散得分越高 | 1 |
| **NodeAffinityPriority** | 满足NodeAffinity偏好得分越高 | 1 |
| **InterPodAffinityPriority** | 满足PodAffinity偏好得分越高 | 1 |
| **TaintTolerationPriority** | 匹配Toleration越多得分越高 | 1 |
| **ImageLocalityPriority** | 节点已有镜像得分越高 | 1 |
| **NodePreferAvoidPodsPriority** | 避免调度到特定节点 | 10000 |

**1. LeastRequestedPriority（最少资源请求）**

鼓励将Pod调度到资源利用率低的节点，实现Bin Packing：

```
得分公式：
score = (capacity - requestedResources - podRequest) * 100 / capacity

示例：
节点1：16核64GB，已使用4核16GB
  Pod请求：2核4GB
  CPU得分 = (16 - 4 - 2) * 100 / 16 = 62.5
  内存得分 = (64 - 16 - 4) * 100 / 64 = 68.75
  平均得分 = (62.5 + 68.75) / 2 = 65.6

节点2：16核64GB，已使用8核32GB
  Pod请求：2核4GB
  CPU得分 = (16 - 8 - 2) * 100 / 16 = 37.5
  内存得分 = (64 - 32 - 4) * 100 / 64 = 43.75
  平均得分 = (37.5 + 43.75) / 2 = 40.6

结果：节点1得分更高（资源利用率低）
```

**2. BalancedResourceAllocation（资源均衡分配）**

鼓励CPU和内存使用率接近，避免某一维度资源过度使用：

```
得分公式：
cpuFraction = (podRequest.cpu + node.allocatedCPU) / node.capacity.cpu
memFraction = (podRequest.mem + node.allocatedMem) / node.capacity.mem
score = 100 - abs(cpuFraction - memFraction) * 10

示例：
节点1：16核64GB，已使用8核16GB
  Pod请求：2核8GB
  CPU使用率 = (2 + 8) / 16 = 62.5%
  内存使用率 = (8 + 16) / 64 = 37.5%
  得分 = 100 - |62.5 - 37.5| * 10 = 75

节点2：16核64GB，已使用8核32GB
  Pod请求：2核8GB
  CPU使用率 = (2 + 8) / 16 = 62.5%
  内存使用率 = (8 + 32) / 64 = 62.5%
  得分 = 100 - |62.5 - 62.5| * 10 = 100

结果：节点2得分更高（CPU/内存使用率均衡）
```

**3. SelectorSpreadPriority（副本分散）**

鼓励将同一Service/ReplicaSet的Pod分散到不同节点：

```yaml
# 假设有一个Deployment，3个副本
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
```

```
打分逻辑：

节点1：已有0个web Pod
  得分 = 100

节点2：已有1个web Pod
  得分 = 50

节点3：已有2个web Pod
  得分 = 0

结果：节点1得分最高（实现高可用）
```

**综合打分示例：**

```
假设有3个候选节点，Pod请求2核4GB：

节点1：16核64GB，已使用4核16GB，无web Pod
  LeastRequested: 65.6
  Balanced: 85
  Spread: 100
  综合得分 = (65.6 + 85 + 100) / 3 = 83.5

节点2：16核64GB，已使用8核32GB，有1个web Pod
  LeastRequested: 40.6
  Balanced: 100
  Spread: 50
  综合得分 = (40.6 + 100 + 50) / 3 = 63.5

节点3：16核64GB，已使用12核48GB，有2个web Pod
  LeastRequested: 25
  Balanced: 90
  Spread: 0
  综合得分 = (25 + 90 + 0) / 3 = 38.3

最终选择：节点1（得分83.5最高）
```

### 7.1.3 调度性能优化

#### 7.1.3.1 调度性能指标

Kubernetes调度器的性能直接影响集群的整体效率：

```
┌────────────────┬───────────────┬──────────────────┐
│  性能指标      │   目标值      │   影响            │
├────────────────┼───────────────┼──────────────────┤
│ 调度延迟       │ <100ms        │ Pod启动速度      │
│ (Scheduling    │ (P99 <500ms)  │                  │
│  Latency)      │               │                  │
├────────────────┼───────────────┼──────────────────┤
│ 调度吞吐量     │ >100 pods/s   │ 大规模扩容速度   │
│ (Throughput)   │               │                  │
├────────────────┼───────────────┼──────────────────┤
│ 调度成功率     │ >99%          │ Pod Pending时间  │
│ (Success Rate) │               │                  │
├────────────────┼───────────────┼──────────────────┤
│ 抢占次数       │ 尽量少        │ 应用稳定性       │
│ (Preemptions)  │               │                  │
└────────────────┴───────────────┴──────────────────┘
```

**查看调度器性能指标：**

```bash
# 查看调度器Metrics
kubectl get --raw /metrics | grep scheduler

# 关键指标：
# scheduler_scheduling_duration_seconds - 调度延迟
# scheduler_pod_scheduling_attempts - 调度尝试次数
# scheduler_pending_pods - 等待调度的Pod数量
# scheduler_schedule_attempts_total - 总调度次数
# scheduler_preemption_attempts_total - 抢占次数
```

#### 7.1.3.2 调度器配置优化

**1. 调整调度器并发度**

```yaml
# kube-scheduler配置文件
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: default-scheduler

# 并发度配置
parallelism: 16  # 并发处理的Pod数量（默认16）
percentageOfNodesToScore: 50  # 预选阶段评估节点百分比（默认50%）
```

**2. 启用调度缓存**

```yaml
# 缓存配置
clientConnection:
  qps: 100      # API Server请求速率
  burst: 200    # 突发请求数量

# 缓存大小
# 默认情况下，调度器会缓存所有节点和Pod信息
# 对于超大集群（>5000节点），可以考虑：
# - 使用多调度器
# - 按namespace分片
```

**3. 禁用不必要的预选/优选算法**

```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- pluginConfig:
  # 禁用某些插件
  - name: InterPodAffinity
    args:
      hardPodAffinityWeight: 0  # 禁用硬亲和性
```

#### 7.1.3.3 大规模集群调度优化

**场景：10000节点集群**

```yaml
# 优化策略1：限制预选节点数量
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: high-performance-scheduler

# 只评估50%的节点
percentageOfNodesToScore: 50

# 优化策略2：使用节点分组
# 通过NodeSelector预先筛选节点池
spec:
  nodeSelector:
    node-pool: compute-intensive  # 只在特定节点池调度
```

**调度性能对比：**

```
默认配置（100%节点评估）：
- 10000节点 × 每节点1ms = 10秒
- P99延迟：15秒 ❌

优化配置（50%节点评估）：
- 5000节点 × 每节点1ms = 5秒
- P99延迟：7秒 ✅（提升53%）

进一步优化（NodeSelector + 50%评估）：
- 1000节点池 × 50% × 1ms = 500ms
- P99延迟：1秒 ✅（提升93%）
```

### 7.1.4 调度失败处理

#### 7.1.4.1 常见调度失败原因

```
┌────────────────────┬─────────────────────────────┐
│  失败原因          │   解决方案                  │
├────────────────────┼─────────────────────────────┤
│ Insufficient CPU   │ 增加节点或降低资源请求      │
│ Insufficient Memory│                             │
├────────────────────┼─────────────────────────────┤
│ No nodes available │ 检查节点是否Ready           │
│                    │ 检查是否有污点              │
├────────────────────┼─────────────────────────────┤
│ Pod affinity       │ 放宽亲和性要求              │
│ not satisfied      │ 或增加匹配的节点            │
├────────────────────┼─────────────────────────────┤
│ Node doesn't match │ 修改nodeSelector            │
│ node selector      │ 或给节点添加标签            │
├────────────────────┼─────────────────────────────┤
│ Didn't tolerate    │ 添加toleration              │
│ node taints        │ 或移除节点污点              │
├────────────────────┼─────────────────────────────┤
│ PVC not bound      │ 检查StorageClass            │
│                    │ 或创建PV                    │
└────────────────────┴─────────────────────────────┘
```

**查看调度失败事件：**

```bash
# 查看Pod事件
kubectl describe pod <pod-name>

# 示例输出
Events:
  Type     Reason            Message
  ----     ------            -------
  Warning  FailedScheduling  0/3 nodes are available: 1 Insufficient cpu,
                             2 node(s) didn't match Pod's node affinity/selector.

# 查看所有Pending的Pod
kubectl get pods --field-selector=status.phase=Pending -A

# 查看调度器日志
kubectl logs -n kube-system kube-scheduler-<node>
```

#### 7.1.4.2 调度失败示例与排查

**示例1：资源不足**

```yaml
# Pod定义
apiVersion: v1
kind: Pod
metadata:
  name: big-app
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        cpu: "32"        # 请求32核
        memory: "128Gi"  # 请求128GB内存
```

```bash
# 调度失败
kubectl describe pod big-app

Events:
  Warning  FailedScheduling  0/5 nodes are available:
           5 Insufficient cpu (available: 16, requested: 32).

# 解决方案1：降低资源请求
resources:
  requests:
    cpu: "8"
    memory: "32Gi"

# 解决方案2：添加更大的节点
kubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu,MEMORY:.status.capacity.memory
```

**示例2：节点选择器不匹配**

```yaml
# Pod定义
apiVersion: v1
kind: Pod
metadata:
  name: gpu-app
spec:
  nodeSelector:
    gpu: "true"  # 要求节点有gpu=true标签
  containers:
  - name: app
    image: tensorflow
```

```bash
# 调度失败
kubectl describe pod gpu-app

Events:
  Warning  FailedScheduling  0/5 nodes are available:
           5 node(s) didn't match Pod's node selector.

# 排查：检查节点标签
kubectl get nodes --show-labels | grep gpu
# 发现所有节点都没有gpu=true标签

# 解决方案：给GPU节点添加标签
kubectl label node node-gpu-1 gpu=true
```

**示例3：污点容忍不匹配**

```yaml
# 节点有污点
kubectl describe node node-gpu-1
Taints: nvidia.com/gpu=present:NoSchedule

# Pod没有容忍
apiVersion: v1
kind: Pod
metadata:
  name: gpu-app
spec:
  containers:
  - name: app
    image: tensorflow
  # ❌ 缺少tolerations
```

```bash
# 调度失败
Events:
  Warning  FailedScheduling  0/1 nodes are available:
           1 node(s) had untolerated taint {nvidia.com/gpu: present}.

# 解决方案：添加容忍
spec:
  tolerations:
  - key: nvidia.com/gpu
    operator: Equal
    value: present
    effect: NoSchedule
```

### 7.1.5 调度器监控与调试

#### 7.1.5.1 调度器监控指标

```yaml
# 部署Prometheus监控调度器
apiVersion: v1
kind: Service
metadata:
  name: kube-scheduler
  namespace: kube-system
  labels:
    component: kube-scheduler
spec:
  ports:
  - name: metrics
    port: 10259
    protocol: TCP
  selector:
    component: kube-scheduler
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kube-scheduler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: kube-scheduler
  endpoints:
  - port: metrics
    interval: 30s
```

**关键监控指标：**

```promql
# 调度延迟（P99）
histogram_quantile(0.99,
  sum(rate(scheduler_scheduling_duration_seconds_bucket[5m])) by (le)
)

# 调度失败率
sum(rate(scheduler_schedule_attempts_total{result="error"}[5m]))
/
sum(rate(scheduler_schedule_attempts_total[5m]))

# Pending Pod数量
sum(kube_pod_status_phase{phase="Pending"}) by (namespace)

# 调度吞吐量
sum(rate(scheduler_schedule_attempts_total{result="scheduled"}[5m]))
```

**Grafana仪表板示例：**

```json
{
  "dashboard": {
    "title": "Kubernetes Scheduler监控",
    "panels": [
      {
        "title": "调度延迟 (P50/P95/P99)",
        "targets": [{
          "expr": "histogram_quantile(0.99, sum(rate(scheduler_scheduling_duration_seconds_bucket[5m])) by (le))"
        }]
      },
      {
        "title": "调度成功率",
        "targets": [{
          "expr": "sum(rate(scheduler_schedule_attempts_total{result=\"scheduled\"}[5m])) / sum(rate(scheduler_schedule_attempts_total[5m]))"
        }]
      },
      {
        "title": "Pending Pod趋势",
        "targets": [{
          "expr": "sum(kube_pod_status_phase{phase=\"Pending\"}) by (namespace)"
        }]
      },
      {
        "title": "抢占事件",
        "targets": [{
          "expr": "sum(rate(scheduler_preemption_attempts_total[5m]))"
        }]
      }
    ]
  }
}
```

#### 7.1.5.2 调度器调试技巧

**1. 启用调度器详细日志**

```bash
# 修改kube-scheduler启动参数
--v=4  # 日志级别（0-10，越大越详细）

# 查看详细调度日志
kubectl logs -n kube-system kube-scheduler-master-1 --tail=100 -f | grep -i "schedule"
```

**2. 使用调度模拟器**

```bash
# 安装scheduler simulator
go install sigs.k8s.io/kube-scheduler-simulator@latest

# 启动模拟器
kube-scheduler-simulator

# 导入集群状态
kubectl get nodes -o json > nodes.json
kubectl get pods -A -o json > pods.json

# 模拟调度
./simulator --nodes=nodes.json --pods=pods.json --simulate=new-pod.yaml
```

**3. 手动模拟调度过程**

```bash
# 查看Pod的调度约束
kubectl get pod <pod-name> -o yaml | grep -A 10 "nodeSelector\|affinity\|tolerations"

# 查看节点资源
kubectl describe nodes | grep -A 5 "Allocated resources"

# 检查节点是否有污点
kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints

# 检查节点是否Ready
kubectl get nodes
```

### 7.1.6 调度器最佳实践

#### 7.1.6.1 资源请求最佳实践

**✅ 推荐做法：**

```yaml
# 1. 始终设置资源请求和限制
apiVersion: v1
kind: Pod
metadata:
  name: best-practice-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:        # ✅ 调度依据
        cpu: "500m"
        memory: "512Mi"
      limits:          # ✅ 资源上限
        cpu: "1"
        memory: "1Gi"
```

**❌ 避免的陷阱：**

```yaml
# ❌ 陷阱1：不设置requests
# 后果：调度器无法准确评估，可能导致节点过载
resources:
  limits:
    cpu: "1"
    memory: "1Gi"

# ❌ 陷阱2：requests == limits (QoS: Guaranteed)
# 后果：资源利用率低，浪费严重
resources:
  requests:
    cpu: "2"
    memory: "4Gi"
  limits:
    cpu: "2"        # requests == limits
    memory: "4Gi"   # 即使应用只用了500m/1Gi，也预留了2核/4GB

# ✅ 推荐：requests < limits (QoS: Burstable)
resources:
  requests:
    cpu: "500m"     # 平时使用
    memory: "1Gi"
  limits:
    cpu: "2"        # 高峰时可用
    memory: "4Gi"
```

**资源请求建议值：**

| 应用类型 | CPU Requests | Memory Requests | CPU Limits | Memory Limits |
|---------|--------------|-----------------|------------|---------------|
| **Web前端** | 100-500m | 128-512Mi | 1-2 | 512Mi-1Gi |
| **API服务** | 500m-1 | 512Mi-2Gi | 2-4 | 2-4Gi |
| **数据库** | 1-2 | 2-8Gi | 2-4 | 8-16Gi |
| **消息队列** | 500m-1 | 1-4Gi | 2-4 | 4-8Gi |
| **批处理** | 1-2 | 1-2Gi | 不限制 | 4-8Gi |

#### 7.1.6.2 调度策略最佳实践

**1. 高可用部署**

```yaml
# ✅ 使用Pod反亲和性确保副本分散
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web
              topologyKey: kubernetes.io/hostname  # 分散到不同节点
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web
              topologyKey: topology.kubernetes.io/zone  # 分散到不同可用区
```

**2. 性能优化部署**

```yaml
# ✅ 使用Pod亲和性将相关服务部署在一起
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
spec:
  template:
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - database  # 靠近数据库部署，降低延迟
              topologyKey: kubernetes.io/hostname
```

**3. 专用节点池**

```yaml
# ✅ 使用污点和容忍将特定工作负载调度到专用节点
# 1. 给GPU节点打污点
kubectl taint nodes node-gpu-1 gpu=true:NoSchedule

# 2. GPU应用添加容忍
apiVersion: v1
kind: Pod
metadata:
  name: ml-training
spec:
  tolerations:
  - key: gpu
    operator: Equal
    value: "true"
    effect: NoSchedule
  nodeSelector:
    gpu: "true"  # 确保调度到GPU节点
  containers:
  - name: trainer
    image: tensorflow/tensorflow:latest-gpu
    resources:
      limits:
        nvidia.com/gpu: 1
```

---

本节我们深入学习了Kubernetes调度器的工作原理，包括两阶段调度策略（预选和优选）、性能优化技巧、调度失败处理、监控调试方法以及最佳实践。理解调度器原理是优化应用性能和资源利用率的关键。在下一节中，我们将学习如何使用节点选择器和节点亲和性来精确控制Pod的调度位置。

---

**本节知识点回顾：**
- ✅ 调度器的核心价值和工作流程
- ✅ 预选（Predicates）和优选（Priorities）算法
- ✅ 调度性能优化和大规模集群调度
- ✅ 调度失败原因排查和解决方案
- ✅ 调度器监控指标和调试技巧
- ✅ 资源请求和调度策略最佳实践
## 7.2 节点选择器与节点亲和性

在上一节中，我们学习了Kubernetes调度器的工作原理。本节将深入探讨如何通过节点选择器（NodeSelector）和节点亲和性（Node Affinity）来精确控制Pod的调度位置，实现更灵活的调度策略。

### 7.2.1 节点选择器（NodeSelector）

#### 7.2.1.1 NodeSelector基础概念

NodeSelector是Kubernetes最简单的节点选择机制，通过标签（Label）匹配将Pod调度到特定节点。

**工作原理：**

```
┌─────────────────────────────────────────────────────┐
│          NodeSelector工作流程                        │
├─────────────────────────────────────────────────────┤
│                                                      │
│  1. 给节点打标签                                     │
│     kubectl label nodes node1 disktype=ssd          │
│                                                      │
│  2. Pod中指定nodeSelector                           │
│     spec:                                            │
│       nodeSelector:                                  │
│         disktype: ssd                                │
│                                                      │
│  3. 调度器过滤                                       │
│     只考虑标签匹配的节点                            │
│     ┌──────┐  ┌──────┐  ┌──────┐                  │
│     │Node1 │  │Node2 │  │Node3 │                  │
│     │ssd   │  │hdd   │  │ssd   │                  │
│     └──┬───┘  └──────┘  └──┬───┘                  │
│        │                    │                       │
│        └──────── ✅ ────────┘                       │
│        候选节点：Node1、Node3                       │
│                                                      │
│  4. 在候选节点中应用优选算法                        │
│     选择最优节点                                     │
└─────────────────────────────────────────────────────┘
```

**基本示例：**

```yaml
# 1. 给节点打标签
apiVersion: v1
kind: Node
metadata:
  name: node1
  labels:
    disktype: ssd
    region: us-west-1
    gpu: "true"
---
# 2. Pod使用nodeSelector
apiVersion: v1
kind: Pod
metadata:
  name: nginx-ssd
spec:
  nodeSelector:
    disktype: ssd    # 要求节点有disktype=ssd标签
  containers:
  - name: nginx
    image: nginx:1.25
```

#### 7.2.1.2 常用节点标签

**系统内置标签（自动添加）：**

```bash
# 查看节点标签
kubectl get nodes --show-labels

# 常见内置标签
kubernetes.io/hostname: node1                    # 节点主机名
kubernetes.io/os: linux                          # 操作系统
kubernetes.io/arch: amd64                        # CPU架构
node.kubernetes.io/instance-type: m5.2xlarge    # 实例类型（云环境）
topology.kubernetes.io/zone: us-west-1a         # 可用区
topology.kubernetes.io/region: us-west-1        # 区域
```

**自定义标签最佳实践：**

| 标签用途 | 标签键 | 示例值 |
|---------|--------|--------|
| **硬件类型** | disktype | ssd, hdd, nvme |
| **GPU** | gpu | true, false |
| **网络** | network | 10g, 1g |
| **环境** | environment | production, staging, dev |
| **业务线** | business-unit | payment, order, user |
| **节点池** | node-pool | compute, memory, gpu |
| **专用节点** | dedicated | database, cache, ml |

**给节点添加标签：**

```bash
# 添加单个标签
kubectl label nodes node1 disktype=ssd

# 添加多个标签
kubectl label nodes node1 \
  disktype=ssd \
  gpu=true \
  environment=production

# 修改标签
kubectl label nodes node1 disktype=nvme --overwrite

# 删除标签
kubectl label nodes node1 disktype-

# 批量打标签
kubectl label nodes node{1..3} node-pool=compute
```

#### 7.2.1.3 NodeSelector使用场景

**场景1：SSD存储节点**

```yaml
# 数据库Pod需要高性能存储
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  template:
    spec:
      nodeSelector:
        disktype: ssd      # 调度到SSD节点
      containers:
      - name: mysql
        image: mysql:8.0
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: local-ssd
      resources:
        requests:
          storage: 100Gi
```

**场景2：GPU节点**

```yaml
# 机器学习训练任务
apiVersion: v1
kind: Pod
metadata:
  name: tensorflow-training
spec:
  nodeSelector:
    gpu: "true"              # 调度到GPU节点
    gpu-type: nvidia-v100    # 指定GPU型号
  containers:
  - name: trainer
    image: tensorflow/tensorflow:latest-gpu
    resources:
      limits:
        nvidia.com/gpu: 2    # 请求2块GPU
```

**场景3：区域/可用区调度**

```yaml
# 指定可用区部署（多可用区高可用）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-us-west-1a
spec:
  replicas: 3
  template:
    spec:
      nodeSelector:
        topology.kubernetes.io/zone: us-west-1a  # 部署在us-west-1a
      containers:
      - name: web
        image: nginx:1.25
```

**场景4：专用节点池**

```yaml
# 支付服务部署在专用节点池
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-service
spec:
  replicas: 5
  template:
    spec:
      nodeSelector:
        node-pool: payment    # 专用支付节点池
        environment: production
      containers:
      - name: payment
        image: payment:v2.0
```

#### 7.2.1.4 NodeSelector的局限性

```
┌────────────────────┬──────────────────────────────┐
│  局限性            │   说明                        │
├────────────────────┼──────────────────────────────┤
│ ❌ 只支持AND逻辑   │ 多个标签必须全部匹配          │
│                    │ 无法实现OR逻辑                │
├────────────────────┼──────────────────────────────┤
│ ❌ 不支持软约束     │ 标签不匹配则调度失败          │
│                    │ 无法实现"尽量满足"            │
├────────────────────┼──────────────────────────────┤
│ ❌ 表达能力有限     │ 无法使用In、NotIn等操作符     │
│                    │ 无法基于标签值范围选择        │
├────────────────────┼──────────────────────────────┤
│ ❌ 无法指定权重     │ 无法表达偏好程度              │
└────────────────────┴──────────────────────────────┘

解决方案：使用Node Affinity（节点亲和性）
```

**示例：NodeSelector的限制**

```yaml
# ❌ 无法实现：调度到ssd=true OR nvme=true的节点
spec:
  nodeSelector:
    disktype: ssd    # 只能是AND关系
    disktype: nvme   # ❌ 这样写会冲突

# ✅ 解决方案：使用Node Affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
            - nvme    # ✅ 支持OR逻辑
```

### 7.2.2 节点亲和性（Node Affinity）

#### 7.2.2.1 Node Affinity概述

Node Affinity是NodeSelector的增强版，提供更强大和灵活的节点选择能力。

**Node Affinity vs NodeSelector：**

```
┌───────────────────┬────────────────┬────────────────┐
│      特性         │  NodeSelector  │ Node Affinity  │
├───────────────────┼────────────────┼────────────────┤
│ 表达能力          │ 基础（仅等于） │ 强大（In/NotIn等）│
│ 逻辑运算          │ 仅AND          │ AND + OR       │
│ 软约束/硬约束     │ 仅硬约束       │ 都支持         │
│ 权重              │ 不支持         │ 支持           │
│ 反亲和性          │ 不支持         │ 支持           │
│ 配置复杂度        │ 简单           │ 中等           │
│ 推荐使用场景      │ 简单场景       │ 复杂调度需求   │
└───────────────────┴────────────────┴────────────────┘
```

**Node Affinity两种类型：**

```yaml
spec:
  affinity:
    nodeAffinity:
      # 1. 硬约束（必须满足）
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms: [...]
      
      # 2. 软约束（尽量满足）
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions: [...]
```

**命名规则解析：**

```
requiredDuringSchedulingIgnoredDuringExecution
    ↓             ↓              ↓
  必须满足      调度阶段      执行阶段忽略
  
解释：
- requiredDuringScheduling：调度时必须满足
- IgnoredDuringExecution：Pod运行时节点标签变化不影响

preferredDuringSchedulingIgnoredDuringExecution
    ↓             ↓              ↓
  尽量满足      调度阶段      执行阶段忽略
```

#### 7.2.2.2 硬亲和性（Required）

硬亲和性是强制性约束，如果没有满足条件的节点，Pod将无法调度（Pending状态）。

**基本语法：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: affinity-required
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:    # OR关系（任一term满足即可）
        - matchExpressions:   # AND关系（同一term内所有表达式必须满足）
          - key: disktype
            operator: In      # 支持In、NotIn、Exists、DoesNotExist、Gt、Lt
            values:
            - ssd
            - nvme
  containers:
  - name: app
    image: nginx
```

**支持的操作符：**

| 操作符 | 说明 | 示例 |
|-------|------|------|
| **In** | 标签值在列表中 | disktype In [ssd, nvme] |
| **NotIn** | 标签值不在列表中 | disktype NotIn [hdd] |
| **Exists** | 标签键存在（不关心值） | gpu Exists |
| **DoesNotExist** | 标签键不存在 | legacy DoesNotExist |
| **Gt** | 标签值大于（数值） | cpu-count Gt 16 |
| **Lt** | 标签值小于（数值） | cpu-count Lt 32 |

**示例1：In操作符（OR逻辑）**

```yaml
# 调度到SSD或NVMe节点
apiVersion: v1
kind: Pod
metadata:
  name: database
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd      # disktype=ssd 或
            - nvme     # disktype=nvme
  containers:
  - name: mysql
    image: mysql:8.0
```

**示例2：NotIn操作符（排除）**

```yaml
# 不调度到HDD节点
apiVersion: v1
kind: Pod
metadata:
  name: high-performance-app
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: NotIn
            values:
            - hdd      # 排除hdd节点
  containers:
  - name: app
    image: myapp:latest
```

**示例3：Exists操作符（标签存在性）**

```yaml
# 调度到有GPU的节点（不关心GPU具体型号）
apiVersion: v1
kind: Pod
metadata:
  name: ml-training
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu
            operator: Exists    # 只要有gpu标签即可
  containers:
  - name: trainer
    image: tensorflow/tensorflow:latest-gpu
```

**示例4：Gt/Lt操作符（数值比较）**

```yaml
# 调度到CPU核心数大于16的节点
apiVersion: v1
kind: Pod
metadata:
  name: compute-intensive
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu-count
            operator: Gt
            values:
            - "16"     # 必须是字符串
  containers:
  - name: app
    image: compute-app:latest
```

**示例5：多条件AND逻辑**

```yaml
# 调度到SSD+GPU+生产环境节点
apiVersion: v1
kind: Pod
metadata:
  name: production-ml-app
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
          - key: gpu           # AND 关系
            operator: Exists
          - key: environment   # AND 关系
            operator: In
            values:
            - production
  containers:
  - name: app
    image: ml-app:v2.0
```

**示例6：多个nodeSelectorTerms（OR逻辑）**

```yaml
# 调度到（SSD节点）OR（GPU节点）
apiVersion: v1
kind: Pod
metadata:
  name: flexible-scheduling
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:   # Term 1：SSD节点
          - key: disktype
            operator: In
            values:
            - ssd
        - matchExpressions:   # Term 2：GPU节点（OR关系）
          - key: gpu
            operator: Exists
  containers:
  - name: app
    image: myapp:latest
```

#### 7.2.2.3 软亲和性（Preferred）

软亲和性是偏好性约束，调度器会尽量满足，但不满足也可以调度到其他节点。

**基本语法：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: affinity-preferred
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100          # 权重（1-100）
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
      - weight: 50           # 第二优先级
        preference:
          matchExpressions:
          - key: zone
            operator: In
            values:
            - us-west-1a
  containers:
  - name: app
    image: nginx
```

**权重计算机制：**

```
调度器打分流程：

1. 基础得分（LeastRequested、Balanced等）
2. + Node Affinity权重得分
3. = 最终得分

示例：
节点1：SSD=true, zone=us-west-1a
  基础得分：70
  匹配disktype=ssd：+100（权重100）
  匹配zone=us-west-1a：+50（权重50）
  最终得分：70 + 100 + 50 = 220

节点2：HDD=true, zone=us-west-1a
  基础得分：80
  不匹配disktype=ssd：+0
  匹配zone=us-west-1a：+50（权重50）
  最终得分：80 + 0 + 50 = 130

结果：选择节点1（得分更高）
```

**示例1：单个软约束**

```yaml
# 优先调度到SSD节点，但HDD节点也可以
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 5
  template:
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
      containers:
      - name: nginx
        image: nginx:1.25
```

**示例2：多级优先级**

```yaml
# 优先级：SSD(100) > us-west-1a(80) > 16核以上(50)
apiVersion: v1
kind: Pod
metadata:
  name: multi-preference
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100        # 第一优先级：SSD
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
      - weight: 80         # 第二优先级：可用区
        preference:
          matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: In
            values:
            - us-west-1a
      - weight: 50         # 第三优先级：CPU核心数
        preference:
          matchExpressions:
          - key: cpu-count
            operator: Gt
            values:
            - "16"
  containers:
  - name: app
    image: myapp:latest
```

**示例3：硬约束+软约束组合**

```yaml
# 硬约束：必须有GPU
# 软约束：优先V100，其次A100
apiVersion: v1
kind: Pod
metadata:
  name: gpu-training
spec:
  affinity:
    nodeAffinity:
      # 硬约束：必须有GPU
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu
            operator: Exists
      
      # 软约束：优先V100
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: gpu-type
            operator: In
            values:
            - nvidia-v100
      - weight: 80
        preference:
          matchExpressions:
          - key: gpu-type
            operator: In
            values:
            - nvidia-a100
  containers:
  - name: trainer
    image: tensorflow/tensorflow:latest-gpu
    resources:
      limits:
        nvidia.com/gpu: 1
```

#### 7.2.2.4 区域与可用区调度

**多可用区高可用部署：**

```yaml
# Deployment：跨可用区部署（高可用）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-ha
spec:
  replicas: 6
  template:
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：必须在us-west-1区域
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - us-west-1
          
          # 软约束：优先级 a > b > c
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-west-1a
          - weight: 80
            preference:
              matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-west-1b
          - weight: 60
            preference:
              matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-west-1c
      
      # Pod反亲和性：确保副本分散
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-ha
            topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: nginx
        image: nginx:1.25
```

**结果分布示例：**

```
期望结果（6个副本跨3个可用区）：

us-west-1a: 2个Pod  ✅ 高可用
us-west-1b: 2个Pod  ✅ 高可用
us-west-1c: 2个Pod  ✅ 高可用

如果某个可用区故障：
us-west-1a: ❌ 故障
us-west-1b: 2个Pod  ✅ 正常
us-west-1c: 2个Pod  ✅ 正常
服务可用性：66.7%（而非0%）
```

### 7.2.3 实战案例

#### 7.2.3.1 案例1：数据库高可用部署

**需求：**
- MySQL主从复制（1主2从）
- 主库部署在SSD节点
- 从库分散到不同可用区
- 优先使用高性能节点

```yaml
# MySQL主库：必须SSD，优先高配节点
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-master
spec:
  serviceName: mysql-master
  replicas: 1
  template:
    metadata:
      labels:
        app: mysql
        role: master
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：必须SSD
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
                - nvme
          
          # 软约束：优先高配节点
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node-tier
                operator: In
                values:
                - high-performance
          - weight: 80
            preference:
              matchExpressions:
              - key: cpu-count
                operator: Gt
                values:
                - "32"
      
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        resources:
          requests:
            cpu: "4"
            memory: "16Gi"
          limits:
            cpu: "8"
            memory: "32Gi"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: local-ssd
      resources:
        requests:
          storage: 500Gi
---
# MySQL从库：跨可用区部署
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-slave
spec:
  serviceName: mysql-slave
  replicas: 2
  template:
    metadata:
      labels:
        app: mysql
        role: slave
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：必须SSD
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
        
        # Pod反亲和性：分散到不同可用区
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - mysql
              - key: role
                operator: In
                values:
                - slave
            topologyKey: topology.kubernetes.io/zone  # 不同可用区
      
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        - name: MYSQL_MASTER_HOST
          value: mysql-master-0.mysql-master
        resources:
          requests:
            cpu: "2"
            memory: "8Gi"
          limits:
            cpu: "4"
            memory: "16Gi"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: local-ssd
      resources:
        requests:
          storage: 500Gi
```

**部署结果：**

```
Master节点选择：
✅ Node-SSD-1（高性能节点，SSD，32核64GB）

Slave节点选择：
✅ Node-SSD-2（us-west-1a，SSD，16核32GB）
✅ Node-SSD-3（us-west-1b，SSD，16核32GB）

高可用性分析：
- 主从分离 ✅
- 从库跨可用区 ✅
- 单可用区故障，主库+1个从库仍可用 ✅
```

#### 7.2.3.2 案例2：机器学习训练集群

**需求：**
- GPU训练节点（必须有GPU）
- 优先使用V100，其次A100
- 数据预处理任务使用CPU节点
- 训练和预处理Pod不能在同一节点（避免资源竞争）

```yaml
# GPU训练任务
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-training
spec:
  parallelism: 4    # 4个并行训练任务
  template:
    metadata:
      labels:
        app: ml-training
        component: trainer
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：必须有GPU
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu
                operator: Exists
              - key: gpu-count
                operator: Gt
                values:
                - "0"
          
          # 软约束：优先V100
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: gpu-type
                operator: In
                values:
                - nvidia-v100
          - weight: 80
            preference:
              matchExpressions:
              - key: gpu-type
                operator: In
                values:
                - nvidia-a100
        
        # Pod反亲和性：避免与数据预处理任务共存
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: component
                  operator: In
                  values:
                  - preprocessor
              topologyKey: kubernetes.io/hostname
      
      restartPolicy: OnFailure
      containers:
      - name: trainer
        image: tensorflow/tensorflow:latest-gpu
        command: ["python", "train.py"]
        resources:
          limits:
            nvidia.com/gpu: 2    # 每个任务使用2块GPU
            memory: "64Gi"
            cpu: "16"
        volumeMounts:
        - name: data
          mountPath: /data
        - name: model
          mountPath: /model
      
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: training-data
      - name: model
        persistentVolumeClaim:
          claimName: model-output
---
# 数据预处理任务（CPU密集型）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-preprocessor
spec:
  replicas: 10
  template:
    metadata:
      labels:
        app: ml-training
        component: preprocessor
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：CPU节点（无GPU）
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu
                operator: DoesNotExist
          
          # 软约束：优先高核心数节点
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: cpu-count
                operator: Gt
                values:
                - "16"
        
        # Pod反亲和性：避免与训练任务共存
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: component
                  operator: In
                  values:
                  - trainer
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: preprocessor
        image: python:3.9
        command: ["python", "preprocess.py"]
        resources:
          requests:
            cpu: "8"
            memory: "16Gi"
          limits:
            cpu: "16"
            memory: "32Gi"
```

**部署结果：**

```
训练任务分布（4个Pod）：
✅ GPU-Node-1（2×V100）- 2个训练Pod
✅ GPU-Node-2（2×V100）- 2个训练Pod

预处理任务分布（10个Pod）：
✅ CPU-Node-1（32核） - 3个预处理Pod
✅ CPU-Node-2（32核） - 3个预处理Pod
✅ CPU-Node-3（32核） - 4个预处理Pod

资源隔离验证：
- 训练任务仅在GPU节点 ✅
- 预处理任务仅在CPU节点 ✅
- 训练和预处理不共存节点 ✅
```

### 7.2.4 常见问题与排查

#### 7.2.4.1 Pod一直Pending

**问题现象：**

```bash
kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
myapp-7c5ddbdf54-abc   0/1     Pending   0          5m

kubectl describe pod myapp-7c5ddbdf54-abc
Events:
  Warning  FailedScheduling  0/5 nodes are available:
           5 node(s) didn't match Pod's node affinity/selector.
```

**排查步骤：**

```bash
# 1. 查看Pod的节点亲和性配置
kubectl get pod myapp-7c5ddbdf54-abc -o yaml | grep -A 20 "affinity"

# 2. 查看所有节点的标签
kubectl get nodes --show-labels

# 3. 检查是否有节点匹配要求
kubectl get nodes -l disktype=ssd
# 如果为空，说明没有节点有该标签

# 4. 解决方案1：给节点添加标签
kubectl label nodes node1 disktype=ssd

# 5. 解决方案2：修改Pod的亲和性配置
# 将required改为preferred，或放宽条件
```

**常见错误示例：**

```yaml
# ❌ 错误1：标签键拼写错误
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktpye    # ❌ 拼写错误（disktype）
            operator: In
            values:
            - ssd

# ❌ 错误2：值大小写错误
spec:
  nodeSelector:
    disktype: SSD    # ❌ 节点标签是小写ssd

# ✅ 正确
spec:
  nodeSelector:
    disktype: ssd    # ✅ 匹配节点标签
```

#### 7.2.4.2 调度不均衡

**问题现象：**

```bash
# 所有Pod都调度到了同一个节点
kubectl get pods -o wide
NAME                   NODE
web-7c5ddbdf54-abc     node1
web-7c5ddbdf54-def     node1
web-7c5ddbdf54-ghi     node1
```

**原因分析：**

```yaml
# 原因：软亲和性权重过高，导致其他优选算法失效
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

# 如果只有node1是SSD，所有Pod都会调度到node1
```

**解决方案：**

```yaml
# 方案1：降低权重，让其他优选算法生效
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 30    # ✅ 降低权重
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

# 方案2：添加Pod反亲和性，强制分散
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - web
          topologyKey: kubernetes.io/hostname
```

### 7.2.5 最佳实践

#### 7.2.5.1 标签管理最佳实践

**✅ 推荐的标签命名规范：**

```yaml
# 使用分层命名空间
labels:
  # 基础设施层
  topology.kubernetes.io/region: us-west-1
  topology.kubernetes.io/zone: us-west-1a
  
  # 硬件层
  hardware.example.com/disktype: ssd
  hardware.example.com/gpu: "true"
  hardware.example.com/gpu-type: nvidia-v100
  hardware.example.com/cpu-count: "32"
  
  # 业务层
  business.example.com/environment: production
  business.example.com/team: payment
  business.example.com/cost-center: engineering
  
  # 应用层
  app.kubernetes.io/name: mysql
  app.kubernetes.io/component: database
  app.kubernetes.io/version: "8.0"
```

**❌ 避免的陷阱：**

```yaml
# ❌ 1. 使用空格或特殊字符
labels:
  disk type: ssd         # ❌ 空格
  gpu-type!: v100        # ❌ 感叹号

# ❌ 2. 过长的标签值
labels:
  description: "This is a very very very long description..."  # ❌ >63字符

# ❌ 3. 使用敏感信息
labels:
  password: secret123    # ❌ 安全风险

# ✅ 正确做法
labels:
  disktype: ssd
  gpu-type: v100
  description: high-performance-node
```

#### 7.2.5.2 亲和性配置最佳实践

**1. 优先使用软约束（Preferred）**

```yaml
# ✅ 推荐：使用软约束提高灵活性
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

# ❌ 避免：过度使用硬约束
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
# 后果：如果所有SSD节点资源不足，Pod将无法调度
```

**2. 硬约束+软约束组合**

```yaml
# ✅ 最佳实践：关键要求用硬约束，偏好用软约束
spec:
  affinity:
    nodeAffinity:
      # 硬约束：必须在生产环境
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: environment
            operator: In
            values:
            - production
      
      # 软约束：优先SSD
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
```

**3. 合理设置权重**

```yaml
# ✅ 权重设置建议
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100    # 最高优先级：关键要求
        preference:
          matchExpressions:
          - key: environment
            operator: In
            values:
            - production
      
      - weight: 50     # 中等优先级：性能优化
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
      
      - weight: 20     # 低优先级：成本优化
        preference:
          matchExpressions:
          - key: instance-type
            operator: In
            values:
            - spot    # Spot实例（便宜但可能被回收）
```

**权重设置原则：**

| 优先级 | 权重范围 | 使用场景 |
|-------|---------|---------|
| **最高** | 80-100 | 关键业务要求（环境、合规性） |
| **高** | 60-79 | 重要性能要求（GPU、SSD） |
| **中** | 40-59 | 一般性能优化（CPU核心数） |
| **低** | 20-39 | 成本优化（可用区、实例类型） |
| **极低** | 1-19 | 次要偏好（便利性） |

#### 7.2.5.3 生产环境配置模板

**模板1：无状态应用（高可用）**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stateless-app
spec:
  replicas: 6
  template:
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：生产环境
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: environment
                operator: In
                values:
                - production
          
          # 软约束：性能优化
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 50
            preference:
              matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
        
        # Pod反亲和性：跨节点+跨可用区
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - stateless-app
              topologyKey: kubernetes.io/hostname
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - stateless-app
              topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: app
        image: myapp:v2.0
```

**模板2：有状态应用（数据库）**

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
spec:
  replicas: 3
  template:
    spec:
      affinity:
        nodeAffinity:
          # 硬约束：SSD+生产环境
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
                - nvme
              - key: environment
                operator: In
                values:
                - production
          
          # 软约束：高配节点
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: node-tier
                operator: In
                values:
                - high-performance
        
        # Pod反亲和性：强制跨可用区
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - database
            topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: mysql
        image: mysql:8.0
```

---

本节我们深入学习了节点选择器（NodeSelector）和节点亲和性（Node Affinity），包括基本概念、使用场景、硬约束与软约束的区别、实战案例以及最佳实践。通过合理使用这些调度约束，可以实现精确的Pod调度控制，优化应用性能和高可用性。在下一节中，我们将学习Pod亲和性与反亲和性，探讨如何基于Pod之间的关系来调度。

---

**本节知识点回顾：**
- ✅ NodeSelector的基本用法和局限性
- ✅ Node Affinity的硬约束（Required）和软约束（Preferred）
- ✅ 6种操作符（In、NotIn、Exists、DoesNotExist、Gt、Lt）
- ✅ 多条件AND/OR逻辑组合
- ✅ 区域与可用区调度策略
- ✅ 实战案例（数据库高可用、机器学习集群）
- ✅ 常见问题排查和最佳实践
## 7.3 Pod亲和性与反亲和性

在上一节中，我们学习了如何使用节点选择器和节点亲和性来控制Pod调度到特定节点。本节将深入探讨Pod亲和性（Pod Affinity）和Pod反亲和性（Pod Anti-Affinity），学习如何基于Pod之间的关系来调度，实现更复杂的调度策略。

### 7.3.1 Pod亲和性与反亲和性概述

#### 7.3.1.1 核心概念

Pod亲和性/反亲和性允许根据**已经运行在节点上的Pod的标签**来约束新Pod的调度位置，而不是基于节点标签。

```
┌─────────────────────────────────────────────────────────────┐
│        Node Affinity vs Pod Affinity 对比                   │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Node Affinity (节点亲和性):                                 │
│  ┌──────────────────────────────────────┐                  │
│  │  基于节点标签调度                    │                  │
│  │                                      │                  │
│  │  Pod ──检查──> Node Labels          │                  │
│  │         (disktype=ssd?)             │                  │
│  └──────────────────────────────────────┘                  │
│                                                              │
│  Pod Affinity (Pod亲和性):                                  │
│  ┌──────────────────────────────────────┐                  │
│  │  基于已运行的Pod标签调度              │                  │
│  │                                      │                  │
│  │  新Pod ──检查──> 已有Pod的Labels     │                  │
│  │         (app=database?)             │                  │
│  │  └─> 调度到同一拓扑域                │                  │
│  └──────────────────────────────────────┘                  │
│                                                              │
│  Pod Anti-Affinity (Pod反亲和性):                           │
│  ┌──────────────────────────────────────┐                  │
│  │  避免与特定Pod在同一拓扑域            │                  │
│  │                                      │                  │
│  │  新Pod ──检查──> 已有Pod的Labels     │                  │
│  │         (app=web?)                  │                  │
│  │  └─> 调度到不同拓扑域                │                  │
│  └──────────────────────────────────────┘                  │
└─────────────────────────────────────────────────────────────┘
```

**典型使用场景：**

| 场景 | 使用类型 | 目的 |
|-----|---------|------|
| **前后端就近部署** | Pod Affinity | 降低网络延迟 |
| **缓存和应用部署** | Pod Affinity | 数据本地性 |
| **高可用部署** | Pod Anti-Affinity | 副本分散，容错 |
| **资源隔离** | Pod Anti-Affinity | 避免资源竞争 |

#### 7.3.1.2 拓扑域（Topology Key）

拓扑域是Pod亲和性/反亲和性的核心概念，定义了"同一位置"的范围。

```
拓扑域示例：

topologyKey: kubernetes.io/hostname
└─> 同一主机（节点）
    ┌────────────────┐
    │    Node-1      │
    │  ┌──────────┐  │
    │  │  Pod A   │  │  <- 同一拓扑域
    │  │  Pod B   │  │  <- 同一拓扑域
    │  └──────────┘  │
    └────────────────┘

topologyKey: topology.kubernetes.io/zone
└─> 同一可用区
    ┌─────────────────────────────────┐
    │      us-west-1a (可用区)         │
    │  ┌────────────┐  ┌────────────┐ │
    │  │   Node-1   │  │   Node-2   │ │
    │  │  ┌──────┐  │  │  ┌──────┐  │ │
    │  │  │Pod A │  │  │  │Pod B │  │ │ <- 同一拓扑域
    │  │  └──────┘  │  │  └──────┘  │ │
    │  └────────────┘  └────────────┘ │
    └─────────────────────────────────┘

topologyKey: topology.kubernetes.io/region
└─> 同一区域
    ┌──────────────────────────────────────────────┐
    │           us-west-1 (区域)                    │
    │  ┌──────────────┐      ┌──────────────┐     │
    │  │ us-west-1a   │      │ us-west-1b   │     │
    │  │  ┌────────┐  │      │  ┌────────┐  │     │
    │  │  │ Pod A  │  │      │  │ Pod B  │  │     │ <- 同一拓扑域
    │  │  └────────┘  │      │  └────────┘  │     │
    │  └──────────────┘      └──────────────┘     │
    └──────────────────────────────────────────────┘
```

**常用拓扑键：**

| 拓扑键 | 作用域 | 使用场景 |
|-------|--------|---------|
| **kubernetes.io/hostname** | 节点级别 | 同节点/避免同节点 |
| **topology.kubernetes.io/zone** | 可用区级别 | 跨可用区高可用 |
| **topology.kubernetes.io/region** | 区域级别 | 跨区域容灾 |
| **自定义标签** | 自定义 | 机架、数据中心等 |

### 7.3.2 Pod亲和性（Pod Affinity）

#### 7.3.2.1 Pod亲和性基本语法

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      # 硬亲和性（必须满足）
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - cache
        topologyKey: kubernetes.io/hostname
      
      # 软亲和性（尽量满足）
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - database
          topologyKey: topology.kubernetes.io/zone
  
  containers:
  - name: app
    image: myapp:latest
```

**语法结构：**

```
podAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:  # 硬亲和性
  - labelSelector:                  # 匹配哪些Pod
      matchExpressions:             # 标签选择器
      - key: app
        operator: In
        values:
        - cache
    topologyKey: kubernetes.io/hostname  # 在什么范围内
  
  preferredDuringSchedulingIgnoredDuringExecution:  # 软亲和性
  - weight: 100                     # 权重
    podAffinityTerm:
      labelSelector:                # 匹配哪些Pod
        matchExpressions:
        - key: app
          operator: In
          values:
          - database
      topologyKey: topology.kubernetes.io/zone  # 在什么范围内
```

#### 7.3.2.2 硬亲和性示例

**示例1：应用与Redis缓存部署在同一节点**

```yaml
# Redis缓存
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
spec:
  replicas: 3
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis        # ← 重要：这个标签会被应用Pod引用
        component: cache
    spec:
      containers:
      - name: redis
        image: redis:7.0
        ports:
        - containerPort: 6379
---
# 应用Pod：要求与Redis在同一节点
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      affinity:
        podAffinity:
          # 硬亲和性：必须与Redis在同一节点
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - redis    # 匹配Redis Pod
            topologyKey: kubernetes.io/hostname  # 同一节点
      
      containers:
      - name: web
        image: web-app:v2.0
        env:
        - name: REDIS_HOST
          value: localhost  # 因为在同一节点，可以用localhost
```

**部署结果：**

```
Node-1:
  ├─ redis-abc123    (Redis Pod)
  └─ web-xyz789      (Web Pod)  ✅ 调度到同一节点

Node-2:
  ├─ redis-def456    (Redis Pod)
  └─ web-uvw012      (Web Pod)  ✅ 调度到同一节点

Node-3:
  ├─ redis-ghi789    (Redis Pod)
  └─ web-rst345      (Web Pod)  ✅ 调度到同一节点

优势：
- 应用和缓存在同一节点，访问延迟极低
- 减少网络开销
```

**示例2：前端和后端部署在同一可用区**

```yaml
# 后端API服务
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-api
spec:
  replicas: 6
  selector:
    matchLabels:
      app: backend
      tier: api
  template:
    metadata:
      labels:
        app: backend
        tier: api
    spec:
      containers:
      - name: api
        image: backend-api:v3.0
---
# 前端服务：部署在后端所在的可用区
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 6
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      affinity:
        podAffinity:
          # 硬亲和性：必须与后端在同一可用区
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - backend
              - key: tier
                operator: In
                values:
                - api
            topologyKey: topology.kubernetes.io/zone  # 同一可用区
      
      containers:
      - name: frontend
        image: frontend:v2.0
```

**部署结果：**

```
us-west-1a (可用区):
  ├─ backend-api Pod × 2
  └─ frontend Pod × 2     ✅ 同一可用区

us-west-1b (可用区):
  ├─ backend-api Pod × 2
  └─ frontend Pod × 2     ✅ 同一可用区

us-west-1c (可用区):
  ├─ backend-api Pod × 2
  └─ frontend Pod × 2     ✅ 同一可用区

优势：
- 前后端跨可用区延迟低（同区域1-2ms）
- 跨可用区高可用
```

#### 7.3.2.3 软亲和性示例

**示例：应用优先与数据库在同一可用区**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
spec:
  replicas: 5
  selector:
    matchLabels:
      app: order
  template:
    metadata:
      labels:
        app: order
    spec:
      affinity:
        podAffinity:
          # 软亲和性：优先与MySQL在同一可用区
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - mysql
                - key: role
                  operator: In
                  values:
                  - master
              topologyKey: topology.kubernetes.io/zone
          
          # 次优：与Redis在同一节点
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - redis
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: order
        image: order-service:v1.5
```

**调度决策：**

```
假设集群状态：
- us-west-1a: MySQL主库，3个节点
- us-west-1b: 无MySQL，5个节点
- us-west-1c: 无MySQL，4个节点

调度结果（5个order-service Pod）：
- us-west-1a: 3个Pod  ✅ 权重100，优先与MySQL同区
- us-west-1b: 1个Pod  ⚠️  资源不足时溢出
- us-west-1c: 1个Pod  ⚠️  资源不足时溢出

如果设置为硬亲和性：
- 只能调度3个Pod到us-west-1a
- 另外2个Pod会Pending（无满足条件的节点）❌
```

### 7.3.3 Pod反亲和性（Pod Anti-Affinity）

#### 7.3.3.1 Pod反亲和性基本概念

Pod反亲和性用于确保Pod**不会**调度到运行特定Pod的拓扑域，主要用于：
- **高可用部署**：将副本分散到不同节点/可用区
- **资源隔离**：避免资源密集型Pod在同一节点

```yaml
spec:
  affinity:
    podAntiAffinity:
      # 硬反亲和性：绝不允许
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - web
        topologyKey: kubernetes.io/hostname  # 不同节点
      
      # 软反亲和性：尽量避免
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - web
          topologyKey: topology.kubernetes.io/zone  # 尽量不同可用区
```

#### 7.3.3.2 高可用部署示例

**示例1：Web应用副本分散到不同节点**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-ha
spec:
  replicas: 5
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      affinity:
        podAntiAffinity:
          # 硬反亲和性：强制分散到不同节点
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web
            topologyKey: kubernetes.io/hostname
      
      containers:
      - name: nginx
        image: nginx:1.25
```

**部署结果：**

```
假设集群有3个节点：

Node-1: web-pod-1  ✅
Node-2: web-pod-2  ✅
Node-3: web-pod-3  ✅
Pending: web-pod-4, web-pod-5  ❌ 无可用节点

问题：硬反亲和性导致无法调度所有副本

改进方案：使用软反亲和性
```

**改进：使用软反亲和性**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-ha-improved
spec:
  replicas: 5
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      affinity:
        podAntiAffinity:
          # 软反亲和性：尽量分散，但允许共存
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: nginx
        image: nginx:1.25
```

**部署结果（软反亲和性）：**

```
Node-1: web-pod-1            ✅
Node-2: web-pod-2            ✅
Node-3: web-pod-3            ✅
Node-1: web-pod-4 (共存)     ✅ 软约束允许
Node-2: web-pod-5 (共存)     ✅ 软约束允许

优势：
- 优先分散到不同节点
- 节点不足时允许共存
- 所有副本都能调度
```

**示例2：跨可用区高可用部署**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-service
spec:
  replicas: 6
  selector:
    matchLabels:
      app: critical
  template:
    metadata:
      labels:
        app: critical
    spec:
      affinity:
        podAntiAffinity:
          # 硬反亲和性：强制跨节点
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - critical
            topologyKey: kubernetes.io/hostname
          
          # 软反亲和性：尽量跨可用区
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - critical
              topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: app
        image: critical-service:v2.0
```

**部署结果：**

```
假设集群：3个可用区，每个可用区2个节点

us-west-1a:
  Node-1: critical-pod-1  ✅
  Node-2: critical-pod-2  ✅

us-west-1b:
  Node-3: critical-pod-3  ✅
  Node-4: critical-pod-4  ✅

us-west-1c:
  Node-5: critical-pod-5  ✅
  Node-6: critical-pod-6  ✅

高可用分析：
- 单节点故障：影响1/6 (16.7%)
- 单可用区故障：影响2/6 (33.3%)
- 剩余4个Pod仍可服务 ✅
```

#### 7.3.3.3 资源隔离示例

**示例：GPU训练任务与CPU任务隔离**

```yaml
# GPU训练任务
apiVersion: batch/v1
kind: Job
metadata:
  name: gpu-training
spec:
  parallelism: 4
  template:
    metadata:
      labels:
        workload: gpu-training
        resource-intensive: "true"
    spec:
      affinity:
        podAntiAffinity:
          # 避免与CPU密集型任务共存
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: workload
                  operator: In
                  values:
                  - cpu-intensive
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: trainer
        image: tensorflow/tensorflow:latest-gpu
        resources:
          limits:
            nvidia.com/gpu: 2
---
# CPU密集型任务
apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing
spec:
  parallelism: 10
  template:
    metadata:
      labels:
        workload: cpu-intensive
        resource-intensive: "true"
    spec:
      affinity:
        podAntiAffinity:
          # 避免与GPU任务共存
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: workload
                  operator: In
                  values:
                  - gpu-training
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: processor
        image: data-processor:v1.0
        resources:
          limits:
            cpu: "16"
            memory: "32Gi"
```

**部署结果：**

```
GPU节点：
  Node-GPU-1: gpu-training Pod × 2  ✅
  Node-GPU-2: gpu-training Pod × 2  ✅

CPU节点：
  Node-CPU-1: data-processing Pod × 3  ✅
  Node-CPU-2: data-processing Pod × 3  ✅
  Node-CPU-3: data-processing Pod × 4  ✅

隔离效果：
- GPU和CPU任务不会共存
- 避免资源竞争
- 各自性能最优
```

### 7.3.4 组合使用亲和性策略

#### 7.3.4.1 亲和性+反亲和性组合

**场景：Web应用+数据库高可用部署**

```yaml
# MySQL数据库（StatefulSet）
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
        component: database
    spec:
      affinity:
        # Pod反亲和性：MySQL副本分散到不同可用区
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - mysql
            topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: mysql
        image: mysql:8.0
---
# Web应用
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 9
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
        component: frontend
    spec:
      affinity:
        # Pod亲和性：Web应用靠近MySQL
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - mysql
              topologyKey: topology.kubernetes.io/zone
        
        # Pod反亲和性：Web副本分散到不同节点
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: web
        image: web-app:v2.0
```

**部署结果：**

```
us-west-1a (可用区):
  Node-1: mysql-0, web-1, web-2
  Node-2: web-3

us-west-1b (可用区):
  Node-3: mysql-1, web-4, web-5
  Node-4: web-6

us-west-1c (可用区):
  Node-5: mysql-2, web-7, web-8
  Node-6: web-9

优势分析：
✅ MySQL跨可用区高可用（3个可用区）
✅ Web应用靠近MySQL（低延迟）
✅ Web副本分散到不同节点（单节点故障影响小）
✅ 可用区级别容灾（单可用区故障，服务仍可用）
```

#### 7.3.4.2 节点亲和性+Pod亲和性组合

**场景：SSD节点上的高性能应用集群**

```yaml
# 数据库：必须在SSD节点
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: database
        tier: data
    spec:
      affinity:
        # 节点亲和性：必须SSD节点
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
                - nvme
        
        # Pod反亲和性：跨节点部署
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - database
            topologyKey: kubernetes.io/hostname
      
      containers:
      - name: db
        image: postgresql:14
---
# 应用服务：靠近数据库，优先SSD
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-service
spec:
  replicas: 6
  template:
    metadata:
      labels:
        app: app-service
        tier: application
    spec:
      affinity:
        # 节点亲和性：优先SSD节点
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
        
        # Pod亲和性：靠近数据库
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - database
              topologyKey: kubernetes.io/hostname
        
        # Pod反亲和性：尽量分散
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - app-service
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: app
        image: app-service:v3.0
```

**调度优先级分析：**

```
调度决策权重：
1. 数据库硬约束：必须SSD节点 (权重∞)
2. 数据库反亲和性：必须不同节点 (权重∞)
3. 应用Pod亲和性：靠近数据库 (权重100)
4. 应用节点亲和性：优先SSD (权重80)
5. 应用反亲和性：尽量分散 (权重50)

部署结果（假设3个SSD节点，2个HDD节点）：

SSD-Node-1: database-0, app-service-1, app-service-2
SSD-Node-2: database-1, app-service-3, app-service-4
SSD-Node-3: database-2, app-service-5, app-service-6

优势：
✅ 数据库在SSD节点（高性能）
✅ 应用靠近数据库（低延迟）
✅ 数据库副本分散（高可用）
✅ 应用副本分散（容错）
```

### 7.3.5 实战案例

#### 7.3.5.1 案例1：电商平台多层架构

**架构设计：**
- 前端（Nginx）：3副本，跨可用区
- API网关：6副本，靠近前端
- 业务服务：9副本，靠近API网关
- 数据库（MySQL）：3副本，跨可用区

```yaml
# 1. MySQL数据库（最底层）
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: ecommerce
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
      tier: database
  template:
    metadata:
      labels:
        app: mysql
        tier: database
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
        
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - mysql
            topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: mysql
        image: mysql:8.0
        resources:
          requests:
            cpu: "4"
            memory: "16Gi"
          limits:
            cpu: "8"
            memory: "32Gi"
---
# 2. 业务服务（中间层）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: business-service
  namespace: ecommerce
spec:
  replicas: 9
  selector:
    matchLabels:
      app: business
      tier: service
  template:
    metadata:
      labels:
        app: business
        tier: service
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - mysql
              topologyKey: topology.kubernetes.io/zone
        
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - business
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: business
        image: business-service:v2.0
        resources:
          requests:
            cpu: "1"
            memory: "2Gi"
          limits:
            cpu: "2"
            memory: "4Gi"
---
# 3. API网关
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
  namespace: ecommerce
spec:
  replicas: 6
  selector:
    matchLabels:
      app: gateway
      tier: api
  template:
    metadata:
      labels:
        app: gateway
        tier: api
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - business
              topologyKey: topology.kubernetes.io/zone
        
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - gateway
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: gateway
        image: api-gateway:v1.5
---
# 4. 前端Nginx（最上层）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: ecommerce
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
      tier: web
  template:
    metadata:
      labels:
        app: frontend
        tier: web
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - gateway
              topologyKey: topology.kubernetes.io/zone
        
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - frontend
            topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: nginx
        image: nginx:1.25
```

**部署拓扑：**

```
us-west-1a (可用区):
  ├─ MySQL-0 (主库)
  ├─ Business-Service × 3
  ├─ API-Gateway × 2
  └─ Frontend-1

us-west-1b (可用区):
  ├─ MySQL-1 (从库)
  ├─ Business-Service × 3
  ├─ API-Gateway × 2
  └─ Frontend-2

us-west-1c (可用区):
  ├─ MySQL-2 (从库)
  ├─ Business-Service × 3
  ├─ API-Gateway × 2
  └─ Frontend-3

流量路径：
用户 → Frontend (us-west-1a)
     → API-Gateway (us-west-1a, 低延迟)
     → Business-Service (us-west-1a, 低延迟)
     → MySQL (us-west-1a, 低延迟)

容灾能力：
- 单可用区故障：影响1/3流量
- 其他2个可用区继续服务 ✅
```

#### 7.3.5.2 案例2：大数据处理集群

**需求：**
- Spark Master：3副本，跨可用区
- Spark Worker：12副本，靠近Master
- HDFS DataNode：6副本，专用节点
- 隔离计算和存储任务

```yaml
# Spark Master
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: spark-master
spec:
  serviceName: spark-master
  replicas: 3
  template:
    metadata:
      labels:
        app: spark
        component: master
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: component
                operator: In
                values:
                - master
            topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: spark-master
        image: spark:3.5.0
---
# Spark Worker
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-worker
spec:
  replicas: 12
  template:
    metadata:
      labels:
        app: spark
        component: worker
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: component
                  operator: In
                  values:
                  - master
              topologyKey: topology.kubernetes.io/zone
        
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: component
                  operator: In
                  values:
                  - worker
              topologyKey: kubernetes.io/hostname
          
          # 避免与HDFS DataNode共存
          - weight: 60
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: component
                  operator: In
                  values:
                  - datanode
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: spark-worker
        image: spark:3.5.0
---
# HDFS DataNode
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: hdfs-datanode
spec:
  selector:
    matchLabels:
      app: hdfs
      component: datanode
  template:
    metadata:
      labels:
        app: hdfs
        component: datanode
    spec:
      nodeSelector:
        storage-node: "true"  # 专用存储节点
      
      containers:
      - name: datanode
        image: hadoop:3.3.0
```

### 7.3.6 常见问题与排查

#### 7.3.6.1 Pod无法调度（Pending）

**问题1：硬亲和性导致无可用节点**

```yaml
# 问题配置
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - nonexistent-app  # ❌ 不存在这个app
        topologyKey: kubernetes.io/hostname
```

```bash
# 排查
kubectl describe pod <pod-name>

Events:
  Warning  FailedScheduling  0/10 nodes are available:
           10 node(s) didn't match pod affinity rules.

# 解决方案1：检查目标Pod是否存在
kubectl get pods -l app=nonexistent-app
# 如果为空，说明没有匹配的Pod

# 解决方案2：改为软亲和性
preferredDuringSchedulingIgnoredDuringExecution:

# 解决方案3：修改标签选择器
- key: app
  operator: In
  values:
  - existing-app  # ✅ 存在的app
```

**问题2：反亲和性过于严格**

```yaml
# 问题配置：5个副本，硬反亲和性，只有3个节点
spec:
  replicas: 5
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - web
        topologyKey: kubernetes.io/hostname  # ❌ 节点不够
```

```bash
# 现象
kubectl get pods
NAME        READY   STATUS    AGE
web-1       1/1     Running   5m
web-2       1/1     Running   5m
web-3       1/1     Running   5m
web-4       0/1     Pending   5m  # ❌ 无可用节点
web-5       0/1     Pending   5m  # ❌ 无可用节点

# 解决方案：改为软反亲和性
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:  # ✅
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - web
          topologyKey: kubernetes.io/hostname
```

#### 7.3.6.2 topologyKey配置错误

```yaml
# ❌ 错误1：topologyKey对应的标签不存在
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - web
        topologyKey: nonexistent-key  # ❌ 节点没有这个标签
```

```bash
# 排查
kubectl get nodes --show-labels | grep nonexistent-key
# 如果为空，说明没有这个标签

# 解决方案：使用正确的标签
topologyKey: kubernetes.io/hostname  # ✅ 节点主机名
topologyKey: topology.kubernetes.io/zone  # ✅ 可用区
```

### 7.3.7 最佳实践

#### 7.3.7.1 亲和性配置原则

**1. 优先使用软亲和性**

```yaml
# ✅ 推荐：软约束提供灵活性
spec:
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - database
          topologyKey: topology.kubernetes.io/zone

# ❌ 避免：过度使用硬约束
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:  # 可能导致无法调度
      - labelSelector:
          ...
```

**2. 反亲和性分层策略**

```yaml
# ✅ 推荐：硬约束跨节点，软约束跨可用区
spec:
  affinity:
    podAntiAffinity:
      # 硬约束：必须不同节点
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - web
        topologyKey: kubernetes.io/hostname
      
      # 软约束：尽量不同可用区
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - web
          topologyKey: topology.kubernetes.io/zone
```

**3. 权重设置指南**

| 优先级 | 权重 | 使用场景 |
|-------|------|---------|
| **极高** | 100 | 关键性能要求（数据本地性） |
| **高** | 80 | 重要性能优化（低延迟） |
| **中** | 50-60 | 一般优化（负载均衡） |
| **低** | 20-40 | 次要偏好（成本优化） |

#### 7.3.7.2 常用配置模板

**模板1：高可用Web服务**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-ha-template
spec:
  replicas: 6
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      affinity:
        podAntiAffinity:
          # 硬约束：不同节点
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web
            topologyKey: kubernetes.io/hostname
          
          # 软约束：不同可用区
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web
              topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: web
        image: nginx:1.25
```

**模板2：应用+缓存亲和部署**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-with-cache-template
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: application
    spec:
      affinity:
        # Pod亲和性：靠近Redis
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - redis
              topologyKey: kubernetes.io/hostname
        
        # Pod反亲和性：应用副本分散
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - application
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: app
        image: myapp:latest
```

---

本节我们深入学习了Pod亲和性和Pod反亲和性，包括基本概念、拓扑域机制、硬约束与软约束的使用、组合策略、实战案例以及最佳实践。通过合理使用Pod亲和性/反亲和性，可以实现基于Pod关系的精确调度控制，优化应用性能、提升高可用性并实现资源隔离。在下一节中，我们将学习污点（Taints）和容忍（Tolerations）机制，探讨如何实现节点的专用化和隔离。

---

**本节知识点回顾：**
- ✅ Pod Affinity和Pod Anti-Affinity核心概念
- ✅ 拓扑域（topologyKey）机制详解
- ✅ 硬亲和性和软亲和性的使用场景
- ✅ 高可用部署和资源隔离策略
- ✅ 亲和性策略组合使用（节点+Pod）
- ✅ 实战案例（电商平台、大数据集群）
- ✅ 常见问题排查和最佳实践
## 7.4 污点与容忍（Taints and Tolerations）

在上一节中，我们学习了Pod亲和性和反亲和性，掌握了如何基于Pod之间的关系进行调度。本节将深入探讨污点（Taints）和容忍（Tolerations）机制，学习如何通过"排斥"策略实现节点的专用化、资源隔离和故障自动处理。

### 7.4.1 污点与容忍概述

#### 7.4.1.1 核心概念

**污点（Taint）** 是应用在节点上的"排斥标记"，用于拒绝Pod调度到该节点，除非Pod明确声明可以"容忍"（Tolerate）这个污点。

```
┌─────────────────────────────────────────────────────────────┐
│           Taint与Affinity的对比（互补机制）                  │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Affinity（亲和性）- 吸引策略：                               │
│  ┌────────────────────────────────────┐                     │
│  │  Pod主动选择节点                    │                     │
│  │                                    │                     │
│  │  Pod ──"我想去"──> Node            │                     │
│  │        (选择加入)                  │                     │
│  └────────────────────────────────────┘                     │
│                                                              │
│  Taint（污点）- 排斥策略：                                    │
│  ┌────────────────────────────────────┐                     │
│  │  Node主动拒绝Pod                    │                     │
│  │                                    │                     │
│  │  Node ──"禁止进入"──X Pod          │                     │
│  │         (默认拒绝)                 │                     │
│  │                                    │                     │
│  │  Node ──"除非你有通行证"──> Pod    │                     │
│  │         (Toleration容忍)           │                     │
│  └────────────────────────────────────┘                     │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

**关键特性：**
- **默认排斥**：有污点的节点默认拒绝所有Pod
- **容忍通行**：只有带有匹配Toleration的Pod才能调度
- **主动控制**：由节点管理员控制节点可接受的工作负载类型

#### 7.4.1.2 Taint与Toleration工作流程

```
┌─────────────────────────────────────────────────────────────┐
│              Taint与Toleration调度流程                       │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Step 1: 节点打上污点                                         │
│  ┌──────────────────────────────────────┐                   │
│  │ Node: gpu-node-1                     │                   │
│  │ Taint: gpu=true:NoSchedule           │                   │
│  │ (只允许GPU任务)                       │                   │
│  └──────────────────────────────────────┘                   │
│                   │                                          │
│                   ▼                                          │
│  Step 2: Pod尝试调度                                          │
│  ┌──────────────────────────────────────┐                   │
│  │ Pod A (普通应用，无Toleration)        │                   │
│  │  → 调度器检查 → 发现Taint             │                   │
│  │  → 匹配失败 → ❌ 拒绝调度             │                   │
│  └──────────────────────────────────────┘                   │
│                                                              │
│  ┌──────────────────────────────────────┐                   │
│  │ Pod B (GPU应用，有Toleration)         │                   │
│  │  Toleration:                         │                   │
│  │    key: gpu                          │                   │
│  │    value: true                       │                   │
│  │    effect: NoSchedule                │                   │
│  │  → 调度器检查 → 容忍匹配              │                   │
│  │  → ✅ 允许调度到gpu-node-1            │                   │
│  └──────────────────────────────────────┘                   │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

#### 7.4.1.3 使用场景

**1. 节点专用化（Dedicated Nodes）**
- GPU节点专用于机器学习任务
- 高性能节点专用于数据库
- 特殊硬件节点（FPGA、高内存）

**2. 资源隔离**
- 生产环境与测试环境隔离
- 多租户资源隔离
- 不同团队/项目隔离

**3. 故障处理**
- 节点维护时驱逐Pod
- 节点故障时自动迁移
- 磁盘压力、内存压力自动驱逐

### 7.4.2 Taint详解

#### 7.4.2.1 Taint语法

Taint由三部分组成：`key=value:effect`

```yaml
# Taint结构
key=value:effect

# 示例
gpu=true:NoSchedule           # 带值的污点
special-hardware:NoSchedule   # 不带值的污点（value为空）
memory-pressure:NoExecute     # 系统自动添加的污点
```

**各部分说明：**

| 组成部分 | 说明 | 示例 |
|---------|------|------|
| **key** | 污点的键名（必填） | `gpu`, `env`, `node-role` |
| **value** | 污点的值（可选） | `true`, `prod`, `master` |
| **effect** | 污点效果（必填） | `NoSchedule`, `PreferNoSchedule`, `NoExecute` |

#### 7.4.2.2 三种Taint效果

**1. NoSchedule（硬性约束-调度阶段）**

```
效果：新Pod如果不容忍该污点，绝对不会被调度到该节点
影响：仅影响调度决策，不影响已运行的Pod
时机：调度阶段生效
```

```bash
# 为节点添加NoSchedule污点
kubectl taint nodes node1 env=prod:NoSchedule

# 效果示例
# ✅ 已在node1上运行的Pod → 继续运行（不受影响）
# ❌ 新Pod（无Toleration）→ 无法调度到node1
# ✅ 新Pod（有Toleration）→ 可以调度到node1
```

**2. PreferNoSchedule（软性约束-尽量避免）**

```
效果：调度器尽量避免将Pod调度到该节点，但不是硬性要求
影响：软性建议，集群资源紧张时可能仍会调度
时机：调度阶段生效，优先级较低
```

```bash
# 为节点添加PreferNoSchedule污点
kubectl taint nodes node2 disktype=hdd:PreferNoSchedule

# 效果示例
# 场景1：集群有其他可用节点
#   → 调度器优先选择其他节点
# 场景2：集群资源紧张，无其他节点可用
#   → 即使Pod无Toleration，也可能调度到node2
```

**3. NoExecute（最严格-立即驱逐）**

```
效果：不仅拒绝新Pod调度，还会驱逐已运行的不容忍Pod
影响：立即生效，影响新旧Pod
时机：添加污点后立即执行驱逐
驱逐宽限期：可通过tolerationSeconds控制
```

```bash
# 为节点添加NoExecute污点
kubectl taint nodes node3 maintenance=true:NoExecute

# 效果示例
# ✅ 有Toleration的Pod → 继续运行
# ❌ 无Toleration的Pod → 立即被驱逐（默认30秒宽限期）
# ❌ 新Pod（无Toleration）→ 无法调度
```

**三种效果对比表：**

| 效果 | 调度新Pod | 影响已运行Pod | 严格程度 | 使用场景 |
|------|----------|--------------|---------|---------|
| **NoSchedule** | ❌ 拒绝 | ✅ 不影响 | 硬约束 | 节点专用化 |
| **PreferNoSchedule** | ⚠️ 尽量避免 | ✅ 不影响 | 软约束 | 优化调度 |
| **NoExecute** | ❌ 拒绝 | ❌ 驱逐 | 最严格 | 维护/故障处理 |

#### 7.4.2.3 管理Taint

```bash
# 1. 添加Taint
kubectl taint nodes <node-name> <key>=<value>:<effect>

# 示例：标记GPU节点
kubectl taint nodes gpu-node-1 gpu=true:NoSchedule

# 示例：标记生产节点（无value）
kubectl taint nodes prod-node-1 env:NoSchedule

# 2. 查看节点的Taint
kubectl describe node <node-name> | grep Taints

# 输出示例：
# Taints:  gpu=true:NoSchedule
#          env=prod:NoExecute

# 3. 删除Taint（在key后加"-"）
kubectl taint nodes <node-name> <key>:<effect>-

# 示例：删除gpu污点
kubectl taint nodes gpu-node-1 gpu:NoSchedule-

# 4. 删除所有同key的Taint
kubectl taint nodes <node-name> <key>-

# 5. 更新Taint（先删后加）
kubectl taint nodes node1 env:NoSchedule- && \
kubectl taint nodes node1 env=prod:NoSchedule

# 6. 批量为多个节点添加Taint
kubectl taint nodes node1 node2 node3 dedicated=special:NoSchedule
```

### 7.4.3 Toleration详解

#### 7.4.3.1 Toleration语法

Toleration是Pod的属性，声明在`spec.tolerations`中。

```yaml
# 完整Toleration示例
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  tolerations:
  - key: "gpu"              # 容忍的污点key
    operator: "Equal"       # 匹配操作符：Equal或Exists
    value: "true"           # 容忍的污点value（operator为Equal时必填）
    effect: "NoSchedule"    # 容忍的污点effect（可选，为空表示容忍所有effect）
  
  containers:
  - name: app
    image: tensorflow/tensorflow:latest-gpu
```

**Toleration字段说明：**

| 字段 | 必填 | 说明 | 可选值 |
|------|------|------|--------|
| **key** | 否* | 污点的key | 任意字符串（为空表示匹配所有key） |
| **operator** | 否 | 匹配操作符 | `Equal`（默认）、`Exists` |
| **value** | 否* | 污点的value | 任意字符串（operator为Exists时忽略） |
| **effect** | 否 | 污点的effect | `NoSchedule`、`PreferNoSchedule`、`NoExecute`（为空表示匹配所有） |
| **tolerationSeconds** | 否 | NoExecute驱逐宽限期 | 秒数（仅NoExecute生效） |

#### 7.4.3.2 两种匹配操作符

**1. Equal（精确匹配）- 默认**

```yaml
# Taint: gpu=true:NoSchedule
# Toleration需要精确匹配key、value、effect

tolerations:
- key: "gpu"
  operator: "Equal"   # 可省略，默认为Equal
  value: "true"       # 必须与Taint的value完全一致
  effect: "NoSchedule"
```

**2. Exists（存在性匹配）**

```yaml
# 场景1：容忍特定key的所有值
# Taint: gpu=true:NoSchedule 或 gpu=v100:NoSchedule 都匹配
tolerations:
- key: "gpu"
  operator: "Exists"  # 不检查value
  effect: "NoSchedule"

# 场景2：容忍特定key的所有effect
# Taint: env=prod:NoSchedule 或 env=prod:NoExecute 都匹配
tolerations:
- key: "env"
  operator: "Exists"
  # effect为空，匹配所有effect

# 场景3：容忍节点上的所有污点（危险！）
tolerations:
- operator: "Exists"  # key、effect都为空
  # 匹配所有污点，慎用！
```

**匹配规则对比：**

| Toleration配置 | 匹配的Taint示例 | 说明 |
|---------------|----------------|------|
| `key=gpu, op=Equal, value=true, effect=NoSchedule` | `gpu=true:NoSchedule` | 精确匹配 |
| `key=gpu, op=Exists, effect=NoSchedule` | `gpu=true:NoSchedule`<br>`gpu=v100:NoSchedule` | 匹配key和effect，忽略value |
| `key=gpu, op=Exists` | `gpu=true:NoSchedule`<br>`gpu=true:NoExecute` | 匹配key，忽略value和effect |
| `op=Exists` | 所有污点 | ⚠️ 容忍所有污点（慎用） |

#### 7.4.3.3 tolerationSeconds驱逐宽限期

仅对`effect=NoExecute`生效，控制Pod被驱逐前的存活时间。

```yaml
# 场景：节点进入维护模式，给Pod 300秒时间完成任务
apiVersion: v1
kind: Pod
metadata:
  name: batch-job
spec:
  tolerations:
  - key: "maintenance"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
    tolerationSeconds: 300  # 5分钟宽限期
  
  containers:
  - name: job
    image: batch-processor:latest
```

**驱逐时间线：**

```
T+0s:    节点添加Taint: maintenance=true:NoExecute
T+0s:    Pod检测到Taint，开始倒计时300秒
T+300s:  宽限期结束，Pod被驱逐
T+330s:  Pod在其他节点重新调度（如果是Deployment管理）

特殊值：
- tolerationSeconds未设置：永久容忍，不会被驱逐
- tolerationSeconds: 0：立即驱逐
```

### 7.4.4 实战案例

#### 7.4.4.1 案例1：GPU节点专用化

**需求：** 集群中有3个GPU节点，仅允许机器学习任务使用，防止普通应用占用宝贵的GPU资源。

```bash
# Step 1: 为GPU节点打上污点
kubectl taint nodes gpu-node-1 gpu=true:NoSchedule
kubectl taint nodes gpu-node-2 gpu=true:NoSchedule
kubectl taint nodes gpu-node-3 gpu=true:NoSchedule

# Step 2: 验证污点
kubectl get nodes -l gpu=true -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints
```

```yaml
# Step 3: 机器学习任务Pod（可以调度到GPU节点）
apiVersion: v1
kind: Pod
metadata:
  name: ml-training
  labels:
    app: ml-training
spec:
  tolerations:
  - key: "gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  
  containers:
  - name: trainer
    image: tensorflow/tensorflow:2.13.0-gpu
    resources:
      limits:
        nvidia.com/gpu: 1  # 请求1个GPU
    command:
    - python
    - train.py
---
# 普通Web应用（无Toleration，无法调度到GPU节点）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 5
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      # 无tolerations字段，无法调度到GPU节点
      containers:
      - name: nginx
        image: nginx:1.25
```

**效果验证：**

```bash
# 检查Pod调度情况
kubectl get pods -o wide

# 输出示例：
# NAME                       NODE          STATUS
# ml-training                gpu-node-1    Running   # ✅ GPU任务在GPU节点
# web-app-7d8f9c-abc12       worker-1      Running   # ✅ Web应用在普通节点
# web-app-7d8f9c-def34       worker-2      Running
```

#### 7.4.4.2 案例2：生产与测试环境隔离

**需求：** 生产节点仅运行生产应用，测试应用只能在测试节点运行，避免资源争抢。

```bash
# Step 1: 标记节点环境
# 生产节点
kubectl label nodes prod-node-1 prod-node-2 env=prod
kubectl taint nodes prod-node-1 prod-node-2 env=prod:NoSchedule

# 测试节点
kubectl label nodes test-node-1 test-node-2 env=test
kubectl taint nodes test-node-1 test-node-2 env=test:NoSchedule
```

```yaml
# Step 2: 生产应用配置
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-service
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: payment
  template:
    metadata:
      labels:
        app: payment
        env: prod
    spec:
      # 容忍生产环境污点
      tolerations:
      - key: "env"
        operator: "Equal"
        value: "prod"
        effect: "NoSchedule"
      
      # 节点亲和性：确保只调度到生产节点
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: env
                operator: In
                values:
                - prod
      
      containers:
      - name: payment
        image: payment-service:v2.1.0
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
---
# Step 3: 测试应用配置
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-service-test
  namespace: testing
spec:
  replicas: 1
  selector:
    matchLabels:
      app: payment-test
  template:
    metadata:
      labels:
        app: payment-test
        env: test
    spec:
      # 容忍测试环境污点
      tolerations:
      - key: "env"
        operator: "Equal"
        value: "test"
        effect: "NoSchedule"
      
      # 节点亲和性：确保只调度到测试节点
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: env
                operator: In
                values:
                - test
      
      containers:
      - name: payment
        image: payment-service:v2.2.0-beta
```

**最佳实践：Toleration + Node Affinity组合**

```
┌──────────────────────────────────────────────────────────┐
│         双重保障机制（推荐生产使用）                      │
├──────────────────────────────────────────────────────────┤
│                                                           │
│  仅Toleration（不推荐）：                                  │
│  ┌────────────────────────────────────────┐              │
│  │ Pod有生产环境Toleration                 │              │
│  │ ✅ 可以调度到生产节点                   │              │
│  │ ⚠️ 也可能调度到测试节点（如果测试节点   │              │
│  │    没有Taint或Pod也容忍测试Taint）     │              │
│  └────────────────────────────────────────┘              │
│                                                           │
│  Toleration + Node Affinity（推荐）：                     │
│  ┌────────────────────────────────────────┐              │
│  │ Step 1: Toleration让Pod可以进入生产节点 │              │
│  │         (通过污点检查)                  │              │
│  │ Step 2: Node Affinity确保只选择生产节点 │              │
│  │         (主动选择)                      │              │
│  │ ✅ 结果：Pod只会调度到生产节点           │              │
│  └────────────────────────────────────────┘              │
│                                                           │
└──────────────────────────────────────────────────────────┘
```

#### 7.4.4.3 案例3：节点维护与自动驱逐

**需求：** 对节点进行硬件升级，需要在维护前自动驱逐所有Pod，关键任务Pod给予5分钟宽限期完成任务。

```bash
# Step 1: 标记节点进入维护模式
kubectl taint nodes worker-3 maintenance=true:NoExecute

# 立即效果：
# - 无Toleration的Pod立即被驱逐（30秒宽限期）
# - 有Toleration的Pod根据tolerationSeconds决定
```

```yaml
# Step 2: 关键任务Pod配置（带宽限期）
apiVersion: batch/v1
kind: Job
metadata:
  name: data-backup
spec:
  template:
    spec:
      tolerations:
      # 容忍维护污点，给予5分钟完成备份
      - key: "maintenance"
        operator: "Equal"
        value: "true"
        effect: "NoExecute"
        tolerationSeconds: 300  # 5分钟
      
      restartPolicy: Never
      containers:
      - name: backup
        image: backup-tool:latest
        command:
        - /bin/sh
        - -c
        - |
          echo "开始数据备份..."
          # 执行备份任务（需在5分钟内完成）
          /usr/local/bin/backup.sh
          echo "备份完成"
---
# Step 3: 普通应用（立即驱逐，在其他节点重建）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      # 无tolerations，立即被驱逐
      containers:
      - name: nginx
        image: nginx:1.25
```

**驱逐过程时间线：**

```
T+0s:     管理员执行: kubectl taint nodes worker-3 maintenance=true:NoExecute
T+0s:     Kubernetes开始处理：
          - web-frontend Pod → 无Toleration → 标记为删除（30秒宽限期）
          - data-backup Job → 有Toleration(300s) → 开始倒计时

T+30s:    web-frontend Pod被驱逐
T+31s:    Deployment控制器在worker-1上重建web-frontend Pod

T+120s:   data-backup Job正常完成备份，自动退出（Job Completed）

T+300s:   如果data-backup Job仍在运行，被强制驱逐

维护完成后：
kubectl taint nodes worker-3 maintenance:NoExecute-  # 移除污点
节点恢复正常调度
```

#### 7.4.4.4 案例4：Master节点保护

**背景：** Kubernetes默认为Master节点添加污点，防止用户工作负载调度到控制平面。

```bash
# 查看Master节点污点
kubectl describe node master-1 | grep Taints

# 输出示例：
# Taints: node-role.kubernetes.io/master:NoSchedule
#         node-role.kubernetes.io/control-plane:NoSchedule
```

```yaml
# 特殊场景：允许监控DaemonSet运行在Master节点
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      # 容忍Master节点污点，确保每个节点都有监控
      tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
      
      hostNetwork: true  # 使用主机网络
      hostPID: true
      
      containers:
      - name: node-exporter
        image: quay.io/prometheus/node-exporter:v1.6.1
        ports:
        - containerPort: 9100
          protocol: TCP
```

### 7.4.5 系统自动添加的Taint

Kubernetes会在检测到节点问题时自动添加Taint，实现故障自动转移。

#### 7.4.5.1 节点状态Taint

| Taint Key | Effect | 触发条件 | 说明 |
|-----------|--------|---------|------|
| `node.kubernetes.io/not-ready` | NoExecute | Node状态为NotReady | 节点不健康 |
| `node.kubernetes.io/unreachable` | NoExecute | Node无法访问 | 网络分区 |
| `node.kubernetes.io/memory-pressure` | NoSchedule | 内存压力 | 可用内存不足 |
| `node.kubernetes.io/disk-pressure` | NoSchedule | 磁盘压力 | 磁盘空间不足 |
| `node.kubernetes.io/pid-pressure` | NoSchedule | PID压力 | 进程数过多 |
| `node.kubernetes.io/network-unavailable` | NoSchedule | 网络不可用 | 网络插件未就绪 |
| `node.kubernetes.io/unschedulable` | NoSchedule | 节点标记为不可调度 | `kubectl cordon` |

#### 7.4.5.2 自动驱逐行为

```yaml
# Pod默认自动容忍节点故障（给予恢复时间）
# Kubernetes自动为所有Pod添加以下Toleration：

apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  # 以下Toleration由Kubernetes自动添加，无需手动配置
  tolerations:
  - key: "node.kubernetes.io/not-ready"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 300  # 5分钟后驱逐
  
  - key: "node.kubernetes.io/unreachable"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 300  # 5分钟后驱逐
```

**故障转移时间线（默认行为）：**

```
T+0s:     节点宕机，kubelet停止心跳
T+40s:    Node Controller标记节点为NotReady
T+40s:    自动添加Taint: node.kubernetes.io/not-ready:NoExecute
T+40s:    Pod开始tolerationSeconds倒计时（300秒）
T+340s:   Pod被标记为Terminating，在其他节点重建

总故障转移时间：约6分钟（40s检测 + 300s宽限期）
```

#### 7.4.5.3 自定义故障转移时间

```yaml
# 关键应用：缩短故障转移时间到30秒
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: critical
  template:
    metadata:
      labels:
        app: critical
    spec:
      # 覆盖默认的300秒，改为30秒快速转移
      tolerations:
      - key: "node.kubernetes.io/not-ready"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 30
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 30
      
      containers:
      - name: app
        image: critical-service:v1.0
---
# 有状态应用：延长宽限期到10分钟，避免频繁重建
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
spec:
  serviceName: db
  replicas: 3
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      # 延长到10分钟，给节点足够恢复时间
      tolerations:
      - key: "node.kubernetes.io/not-ready"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 600
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 600
      
      containers:
      - name: mysql
        image: mysql:8.0
```

### 7.4.6 常见问题与排查

#### 7.4.6.1 Pod一直处于Pending状态

**问题现象：**

```bash
kubectl get pods
# NAME           READY   STATUS    RESTARTS   AGE
# my-app-abc12   0/1     Pending   0          5m
```

**排查步骤：**

```bash
# Step 1: 查看Pod事件
kubectl describe pod my-app-abc12

# 关键输出示例：
# Events:
#   Warning  FailedScheduling  1m (x12 over 5m)  default-scheduler  
#   0/5 nodes are available: 3 node(s) had taints that the pod didn't tolerate, 
#   2 Insufficient cpu.

# Step 2: 检查节点Taint
kubectl get nodes -o custom-columns=\
NAME:.metadata.name,\
TAINTS:.spec.taints

# 输出示例：
# NAME         TAINTS
# node1        [map[effect:NoSchedule key:gpu value:true]]
# node2        [map[effect:NoSchedule key:env value:prod]]
# node3        [map[effect:NoExecute key:maintenance value:true]]

# Step 3: 查看Pod的Toleration配置
kubectl get pod my-app-abc12 -o jsonpath='{.spec.tolerations}'
```

**解决方案：**

```yaml
# 方案1：为Pod添加匹配的Toleration
apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  tolerations:
  - key: "gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  containers:
  - name: app
    image: myapp:latest
```

```bash
# 方案2：删除不必要的Taint
kubectl taint nodes node1 gpu:NoSchedule-

# 方案3：临时允许调度到特定节点
kubectl taint nodes node2 env:NoSchedule-
```

#### 7.4.6.2 Pod被意外驱逐

**问题现象：**

```bash
kubectl get pods
# NAME           READY   STATUS        RESTARTS   AGE
# my-app-def34   1/1     Terminating   0          10m

kubectl get events --sort-by='.lastTimestamp'
# LAST SEEN   TYPE      REASON     MESSAGE
# 1m          Normal    Killing    Stopping container app
```

**排查步骤：**

```bash
# Step 1: 检查节点是否添加了NoExecute污点
kubectl describe node worker-2 | grep Taints

# 输出示例：
# Taints: node.kubernetes.io/unreachable:NoExecute

# Step 2: 检查Pod的Toleration
kubectl get pod my-app-def34 -o yaml | grep -A 10 tolerations

# Step 3: 查看节点事件
kubectl describe node worker-2

# Events:
#   Normal  NodeNotReady  5m  Node worker-2 status is now: NotReady
```

**解决方案：**

```yaml
# 方案1：为关键Pod添加永久容忍（慎用）
apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  tolerations:
  - key: "node.kubernetes.io/unreachable"
    operator: "Exists"
    effect: "NoExecute"
    # 不设置tolerationSeconds，永久容忍

# 方案2: 修复节点问题
# - 检查节点网络
# - 重启kubelet
# - 检查系统资源

# 方案3：移除手动添加的维护污点
kubectl taint nodes worker-2 maintenance:NoExecute-
```

#### 7.4.6.3 DaemonSet无法在所有节点运行

**问题现象：**

```bash
kubectl get daemonset -n kube-system
# NAME              DESIRED   CURRENT   READY   NODE SELECTOR
# kube-proxy        5         3         3       <none>

# 期望5个节点都运行，实际只有3个
```

**排查步骤：**

```bash
# Step 1: 检查DaemonSet Pod状态
kubectl get pods -n kube-system -l app=kube-proxy -o wide

# Step 2: 检查未运行节点的Taint
kubectl get nodes -o json | jq -r '.items[] | 
  select(.spec.taints != null) | 
  {name: .metadata.name, taints: .spec.taints}'

# 输出示例：
# {
#   "name": "master-1",
#   "taints": [
#     {"key": "node-role.kubernetes.io/master", "effect": "NoSchedule"}
#   ]
# }
```

**解决方案：**

```yaml
# 为DaemonSet添加容忍Master节点污点
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-proxy
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: kube-proxy
  template:
    metadata:
      labels:
        app: kube-proxy
    spec:
      tolerations:
      # 容忍Master节点污点
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
      
      # 容忍所有NoExecute污点（确保即使节点故障也运行）
      - operator: "Exists"
        effect: "NoExecute"
      
      containers:
      - name: kube-proxy
        image: k8s.gcr.io/kube-proxy:v1.28.0
```

### 7.4.7 最佳实践

#### 7.4.7.1 设计原则

**1. Taint使用原则**

```yaml
# ✅ 推荐：使用语义化的key
gpu=true:NoSchedule                    # 清晰表达节点特性
env=prod:NoSchedule                    # 明确环境类型
disktype=ssd:PreferNoSchedule          # 描述硬件属性

# ❌ 不推荐：使用模糊的key
special:NoSchedule                     # 不明确
node1:NoSchedule                       # 缺乏语义
test:NoSchedule                        # 过于简单
```

**2. 效果选择原则**

| 场景 | 推荐效果 | 原因 |
|------|---------|------|
| 节点专用化（GPU、高性能） | NoSchedule | 仅阻止新调度，不影响已运行Pod |
| 资源优化建议 | PreferNoSchedule | 软性约束，提供调度灵活性 |
| 节点维护 | NoExecute | 需要清空节点 |
| 紧急故障 | NoExecute | 立即驱逐，快速转移 |
| 环境隔离 | NoSchedule | 防止误调度 |

**3. 组合使用原则**

```yaml
# 最佳实践：Toleration + Node Affinity + Resource Limits
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-training
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ml-training
  template:
    metadata:
      labels:
        app: ml-training
    spec:
      # 第1层：Toleration允许进入GPU节点
      tolerations:
      - key: "gpu"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      
      # 第2层：Node Affinity确保选择GPU节点
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: accelerator
                operator: In
                values:
                - nvidia-tesla-v100
                - nvidia-a100
        
        # 第3层：Pod反亲和性确保分散部署
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - ml-training
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: trainer
        image: tensorflow/tensorflow:2.13.0-gpu
        # 第4层：Resource Limits明确资源需求
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
          limits:
            memory: "32Gi"
            cpu: "8"
            nvidia.com/gpu: "1"
```

#### 7.4.7.2 常用配置模板

**模板1：生产环境节点保护**

```bash
# 节点配置
kubectl label nodes prod-node-{1..5} env=prod tier=production
kubectl taint nodes prod-node-{1..5} env=prod:NoSchedule
```

```yaml
# Pod配置模板
apiVersion: v1
kind: Pod
metadata:
  name: production-app
  labels:
    env: prod
spec:
  tolerations:
  - key: "env"
    operator: "Equal"
    value: "prod"
    effect: "NoSchedule"
  
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: env
            operator: In
            values:
            - prod
  
  containers:
  - name: app
    image: myapp:v1.0
```

**模板2：节点维护脚本**

```bash
#!/bin/bash
# 节点维护标准流程

NODE_NAME=$1

echo "开始维护节点: $NODE_NAME"

# Step 1: 标记节点不可调度
kubectl cordon $NODE_NAME

# Step 2: 添加NoExecute污点，给予5分钟宽限期
kubectl taint nodes $NODE_NAME maintenance=true:NoExecute

# Step 3: 等待Pod驱逐完成
echo "等待Pod驱逐（最多6分钟）..."
timeout 360 bash -c "
  while kubectl get pods --all-namespaces --field-selector spec.nodeName=$NODE_NAME | grep -v Terminating > /dev/null; do
    sleep 10
  done
"

# Step 4: 验证节点已清空
POD_COUNT=$(kubectl get pods --all-namespaces --field-selector spec.nodeName=$NODE_NAME --no-headers 2>/dev/null | wc -l)

if [ "$POD_COUNT" -eq 0 ]; then
  echo "✅ 节点已清空，可以进行维护"
else
  echo "⚠️  仍有 $POD_COUNT 个Pod运行，请检查"
  kubectl get pods --all-namespaces --field-selector spec.nodeName=$NODE_NAME
fi

# 维护完成后执行：
# kubectl uncordon $NODE_NAME
# kubectl taint nodes $NODE_NAME maintenance:NoExecute-
```

**模板3：关键应用快速故障转移**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-api
spec:
  replicas: 5
  selector:
    matchLabels:
      app: critical-api
  template:
    metadata:
      labels:
        app: critical-api
    spec:
      # 快速故障转移：30秒宽限期
      tolerations:
      - key: "node.kubernetes.io/not-ready"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 30
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 30
      
      # 跨可用区高可用
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - critical-api
            topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: api
        image: critical-api:v1.0
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 3
```

#### 7.4.7.3 监控与告警

**Prometheus监控指标：**

```yaml
# 监控节点Taint数量
sum(kube_node_spec_taint) by (node, key, effect)

# 监控Pending Pod（因Taint无法调度）
count(kube_pod_status_phase{phase="Pending"})

# 监控Pod驱逐事件
rate(kube_pod_deletion_timestamp[5m])
```

**告警规则示例：**

```yaml
groups:
- name: taint-alerts
  rules:
  # 节点污点异常增加
  - alert: NodeTaintIncreased
    expr: |
      increase(kube_node_spec_taint[10m]) > 5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "节点污点异常增加"
      description: "过去10分钟内有{{ $value }}个节点被添加污点"
  
  # 大量Pod因Taint无法调度
  - alert: PodsUnschedulableDueToTaint
    expr: |
      count(kube_pod_status_phase{phase="Pending"}) > 10
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: "大量Pod无法调度"
      description: "当前有{{ $value }}个Pod处于Pending状态，可能是Taint配置问题"
```

---

本节我们深入学习了污点（Taints）和容忍（Tolerations）机制，包括核心概念、三种污点效果（NoSchedule、PreferNoSchedule、NoExecute）、Toleration匹配规则、节点专用化、环境隔离、故障自动转移等实战案例，以及常见问题排查和最佳实践。通过合理使用Taint和Toleration，可以实现节点的专用化管理、资源隔离和自动故障处理。在下一节中，我们将学习优先级（Priority）和抢占（Preemption）机制，探讨如何管理Pod的调度优先级和资源抢占策略。

---

**本节知识点回顾：**
- ✅ Taint与Toleration核心概念和工作流程
- ✅ 三种Taint效果（NoSchedule、PreferNoSchedule、NoExecute）
- ✅ Toleration两种匹配操作符（Equal、Exists）
- ✅ tolerationSeconds驱逐宽限期机制
- ✅ GPU节点专用化、环境隔离实战案例
- ✅ 系统自动添加的Taint和故障转移机制
- ✅ Taint + Node Affinity组合使用最佳实践
- ✅ 常见问题排查和监控告警配置
