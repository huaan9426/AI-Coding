# 向量数据库底层原理

## 1. 相似度搜索算法

### 1.1 精确搜索（Brute Force）

```python
import numpy as np

def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:
    """余弦相似度计算"""
    dot_product = np.dot(v1, v2)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)
    return dot_product / (norm_v1 * norm_v2)

def euclidean_distance(v1: np.ndarray, v2: np.ndarray) -> float:
    """欧氏距离计算"""
    return np.sqrt(np.sum((v1 - v2) ** 2))

def brute_force_search(query: np.ndarray, vectors: np.ndarray, k: int = 5) -> list:
    """
    暴力搜索最近邻
    时间复杂度: O(n*d) 其中 n=向量数量, d=维度
    """
    similarities = []
    for i, vec in enumerate(vectors):
        sim = cosine_similarity(query, vec)
        similarities.append((i, sim))

    # 排序返回top-k
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:k]
```

**问题：** 当向量库有百万、千万条数据时，暴力搜索每次查询都要计算 n 次相似度，延迟不可接受。

**解决方案：** 近似最近邻（ANN）算法，牺牲少量准确度换取几个数量级的速度提升。

---

## 2. HNSW 算法详解

### 2.1 核心思想

HNSW（Hierarchical Navigable Small World）基于跳表思想构建多层图：
- **底层**：包含所有向量的完整图
- **上层**：稀疏采样的"快速通道"
- **搜索过程**：从顶层快速定位大致区域，逐层下降精确搜索

### 2.2 数据结构

```python
from typing import List, Set, Dict
import heapq
import random

class HNSWNode:
    def __init__(self, vector: np.ndarray, level: int, idx: int):
        self.vector = vector      # 向量数据
        self.level = level        # 节点最高层级
        self.idx = idx            # 节点ID
        # neighbors[layer] = {邻居节点ID集合}
        self.neighbors: Dict[int, Set[int]] = {i: set() for i in range(level + 1)}

class HNSW:
    def __init__(self, M: int = 16, M_max: int = 16, ef_construction: int = 200, ml: float = 1.0):
        """
        M: 每层最大边数（除底层外）
        M_max: 底层(layer=0)最大边数，通常是 M*2
        ef_construction: 构建时的动态候选列表大小
        ml: 层级选择参数（通常1/ln(2) ≈ 1.44）
        """
        self.M = M
        self.M_max = M_max
        self.ef_construction = ef_construction
        self.ml = ml
        self.nodes: Dict[int, HNSWNode] = {}
        self.entry_point = None  # 入口节点ID
        self.max_level = -1      # 当前最大层级
        self.node_count = 0

    def _random_level(self) -> int:
        """
        指数衰减概率选择层级
        P(level=l) = (1/M)^l
        """
        level = 0
        while random.random() < (1 / self.ml) and level < 16:
            level += 1
        return level

    def insert(self, vector: np.ndarray):
        """插入新向量"""
        idx = self.node_count
        self.node_count += 1

        # 随机选择层级
        level = self._random_level()
        node = HNSWNode(vector, level, idx)
        self.nodes[idx] = node

        if self.entry_point is None:
            # 第一个节点
            self.entry_point = idx
            self.max_level = level
            return

        # 从入口点开始搜索
        nearest = self._search_layer(vector, [self.entry_point], 1, self.max_level, level + 1)

        # 从 level 层向下插入连接
        for lc in range(level, -1, -1):
            candidates = self._search_layer(vector, nearest, self.ef_construction, lc, lc)

            # 选择M个最近邻建立双向连接
            M = self.M if lc > 0 else self.M_max
            neighbors = self._select_neighbors(vector, candidates, M)

            # 建立双向边
            for neighbor_idx in neighbors:
                node.neighbors[lc].add(neighbor_idx)
                self.nodes[neighbor_idx].neighbors[lc].add(idx)

                # 修剪邻居的连接（保持度数限制）
                max_conn = self.M if lc > 0 else self.M_max
                if len(self.nodes[neighbor_idx].neighbors[lc]) > max_conn:
                    self._prune_connections(neighbor_idx, lc, max_conn)

            nearest = neighbors

        # 更新入口点
        if level > self.max_level:
            self.entry_point = idx
            self.max_level = level

    def _search_layer(self, query: np.ndarray, entry_points: List[int],
                     num_closest: int, layer: int, stop_layer: int) -> List[int]:
        """
        在指定层搜索
        返回 num_closest 个最近邻
        """
        visited = set()
        candidates = []  # 小顶堆 (distance, node_id)
        w = []          # 大顶堆 (-distance, node_id)

        for ep in entry_points:
            dist = euclidean_distance(query, self.nodes[ep].vector)
            heapq.heappush(candidates, (dist, ep))
            heapq.heappush(w, (-dist, ep))
            visited.add(ep)

        while candidates:
            current_dist, current = heapq.heappop(candidates)

            # 如果当前点比结果集中最远的点还远，停止
            if current_dist > -w[0][0]:
                break

            # 检查当前层的所有邻居
            for neighbor in self.nodes[current].neighbors.get(layer, set()):
                if neighbor not in visited:
                    visited.add(neighbor)
                    dist = euclidean_distance(query, self.nodes[neighbor].vector)

                    if dist < -w[0][0] or len(w) < num_closest:
                        heapq.heappush(candidates, (dist, neighbor))
                        heapq.heappush(w, (-dist, neighbor))

                        # 保持w的大小为num_closest
                        if len(w) > num_closest:
                            heapq.heappop(w)

        return [idx for _, idx in w]

    def _select_neighbors(self, query: np.ndarray, candidates: List[int], M: int) -> Set[int]:
        """
        启发式选择邻居（简化版）
        实际HNSW使用更复杂的启发式算法（考虑角度分布）
        """
        # 按距离排序
        distances = [(euclidean_distance(query, self.nodes[idx].vector), idx)
                    for idx in candidates]
        distances.sort()

        # 返回最近的M个
        return {idx for _, idx in distances[:M]}

    def _prune_connections(self, node_idx: int, layer: int, max_conn: int):
        """修剪连接以维持度数限制"""
        node = self.nodes[node_idx]
        neighbors = list(node.neighbors[layer])

        # 按距离排序
        distances = [(euclidean_distance(node.vector, self.nodes[n].vector), n)
                    for n in neighbors]
        distances.sort()

        # 保留最近的max_conn个
        new_neighbors = {n for _, n in distances[:max_conn]}

        # 删除多余的双向边
        for old_neighbor in node.neighbors[layer] - new_neighbors:
            self.nodes[old_neighbor].neighbors[layer].discard(node_idx)

        node.neighbors[layer] = new_neighbors

    def search(self, query: np.ndarray, k: int = 10, ef: int = 50) -> List[tuple]:
        """
        搜索最近邻
        ef: 搜索时的动态列表大小（越大越准确但越慢）
        """
        if self.entry_point is None:
            return []

        # 从顶层贪心搜索到layer 0上方
        nearest = [self.entry_point]
        for layer in range(self.max_level, 0, -1):
            nearest = self._search_layer(query, nearest, 1, layer, layer)

        # 在底层(layer 0)搜索ef个候选
        candidates = self._search_layer(query, nearest, ef, 0, 0)

        # 返回top-k
        results = []
        for idx in candidates:
            dist = euclidean_distance(query, self.nodes[idx].vector)
            results.append((idx, dist))

        results.sort(key=lambda x: x[1])
        return results[:k]
```

### 2.3 复杂度分析

- **插入复杂度**：O(M * log(n)) - M是最大边数，log(n)是层数
- **搜索复杂度**：O(ef * log(n)) - ef是搜索时动态列表大小
- **空间复杂度**：O(n * M * log(n)) - 每个节点在各层的边

**关键参数调优：**
- `M` 增大 → 召回率↑、内存↑、插入慢
- `ef_construction` 增大 → 索引质量↑、构建慢
- `ef` (搜索时) 增大 → 召回率↑、查询慢

---

## 3. IVF (Inverted File Index) 算法

### 3.1 核心思想

将向量空间分割为 Voronoi 单元（聚类）：
1. 使用 K-Means 将 n 个向量聚类为 nlist 个簇
2. 查询时只搜索 nprobe 个最近的簇中心
3. 在这些簇内进行精确搜索

### 3.2 实现

```python
from sklearn.cluster import KMeans

class IVFIndex:
    def __init__(self, nlist: int = 100):
        """
        nlist: 聚类中心数量
        """
        self.nlist = nlist
        self.centroids = None      # 聚类中心 (nlist, d)
        self.inverted_lists = {}   # {centroid_id: [向量ID列表]}
        self.vectors = {}          # {向量ID: 向量}
        self.trained = False

    def train(self, vectors: np.ndarray):
        """
        训练阶段：对向量进行聚类
        vectors: (n, d)
        """
        kmeans = KMeans(n_clusters=self.nlist, random_state=42)
        kmeans.fit(vectors)
        self.centroids = kmeans.cluster_centers_
        self.trained = True

    def add(self, vectors: np.ndarray, ids: List[int] = None):
        """添加向量到索引"""
        if not self.trained:
            raise ValueError("Index not trained!")

        if ids is None:
            ids = list(range(len(self.vectors), len(self.vectors) + len(vectors)))

        # 分配每个向量到最近的聚类中心
        for vec, vec_id in zip(vectors, ids):
            # 找到最近的聚类中心
            distances = [euclidean_distance(vec, centroid) for centroid in self.centroids]
            centroid_id = np.argmin(distances)

            # 添加到倒排列表
            if centroid_id not in self.inverted_lists:
                self.inverted_lists[centroid_id] = []
            self.inverted_lists[centroid_id].append(vec_id)

            # 存储向量
            self.vectors[vec_id] = vec

    def search(self, query: np.ndarray, k: int = 10, nprobe: int = 10) -> List[tuple]:
        """
        搜索
        nprobe: 搜索的聚类中心数量（越大越准确但越慢）
        """
        # 找到nprobe个最近的聚类中心
        centroid_distances = [(i, euclidean_distance(query, centroid))
                             for i, centroid in enumerate(self.centroids)]
        centroid_distances.sort(key=lambda x: x[1])
        nearest_centroids = [cid for cid, _ in centroid_distances[:nprobe]]

        # 在这些聚类中心的倒排列表中搜索
        candidates = []
        for centroid_id in nearest_centroids:
            if centroid_id in self.inverted_lists:
                for vec_id in self.inverted_lists[centroid_id]:
                    dist = euclidean_distance(query, self.vectors[vec_id])
                    candidates.append((vec_id, dist))

        # 排序返回top-k
        candidates.sort(key=lambda x: x[1])
        return candidates[:k]
```

### 3.3 复杂度分析

- **训练复杂度**：O(n * d * nlist * iter) - K-Means 迭代次数
- **搜索复杂度**：O(nprobe * n/nlist * d) ≈ O(n/nlist * d) 当 nprobe << nlist
- **加速比**：约 nlist/nprobe 倍（例如 nlist=1000, nprobe=10 → 100倍加速）

**关键参数：**
- `nlist` 增大 → 搜索更快、训练更慢、召回率可能降低
- `nprobe` 增大 → 召回率↑、搜索变慢

---

## 4. Product Quantization (乘积量化)

### 4.1 核心思想

降低内存占用：将高维向量分段量化压缩
- 将 d 维向量分为 m 个子向量，每个 d/m 维
- 对每个子空间独立做 K-Means 聚类（通常 k=256 用1字节表示）
- 原向量用 m 个 codebook ID 表示（压缩率 = d*4字节 / m*1字节）

### 4.2 实现

```python
class ProductQuantizer:
    def __init__(self, d: int, M: int = 8, nbits: int = 8):
        """
        d: 向量维度（必须能被M整除）
        M: 子空间数量
        nbits: 每个子空间的码本大小（2^nbits 个聚类中心）
        """
        assert d % M == 0, "d must be divisible by M"
        self.d = d
        self.M = M
        self.dsub = d // M  # 每个子空间的维度
        self.ksub = 2 ** nbits  # 每个码本的大小
        self.codebooks = []  # M个码本，每个是 (ksub, dsub) 的聚类中心
        self.trained = False

    def train(self, vectors: np.ndarray):
        """
        训练：对每个子空间独立聚类
        vectors: (n, d)
        """
        n, d = vectors.shape
        assert d == self.d

        for m in range(self.M):
            # 提取第m个子向量
            start = m * self.dsub
            end = (m + 1) * self.dsub
            sub_vectors = vectors[:, start:end]  # (n, dsub)

            # K-Means 聚类
            kmeans = KMeans(n_clusters=self.ksub, random_state=42)
            kmeans.fit(sub_vectors)
            self.codebooks.append(kmeans.cluster_centers_)  # (ksub, dsub)

        self.trained = True

    def encode(self, vectors: np.ndarray) -> np.ndarray:
        """
        编码：将向量量化为码本ID
        返回: (n, M) 的整数数组，每个元素是0~ksub-1
        """
        if not self.trained:
            raise ValueError("PQ not trained!")

        n = vectors.shape[0]
        codes = np.zeros((n, self.M), dtype=np.uint8)

        for m in range(self.M):
            start = m * self.dsub
            end = (m + 1) * self.dsub
            sub_vectors = vectors[:, start:end]  # (n, dsub)

            # 找到每个子向量最近的码字
            codebook = self.codebooks[m]  # (ksub, dsub)
            for i, sub_vec in enumerate(sub_vectors):
                distances = np.linalg.norm(codebook - sub_vec, axis=1)
                codes[i, m] = np.argmin(distances)

        return codes

    def decode(self, codes: np.ndarray) -> np.ndarray:
        """
        解码：从码本ID重建近似向量
        codes: (n, M)
        返回: (n, d)
        """
        n = codes.shape[0]
        reconstructed = np.zeros((n, self.d), dtype=np.float32)

        for m in range(self.M):
            start = m * self.dsub
            end = (m + 1) * self.dsub
            codebook = self.codebooks[m]
            reconstructed[:, start:end] = codebook[codes[:, m]]

        return reconstructed

    def compute_distance_table(self, query: np.ndarray) -> np.ndarray:
        """
        计算查询向量到所有码字的距离表
        query: (d,)
        返回: (M, ksub) 每个子空间、每个码字的距离
        """
        distance_table = np.zeros((self.M, self.ksub))

        for m in range(self.M):
            start = m * self.dsub
            end = (m + 1) * self.dsub
            sub_query = query[start:end]  # (dsub,)
            codebook = self.codebooks[m]  # (ksub, dsub)

            # 计算子查询到该码本所有码字的距离
            for k in range(self.ksub):
                distance_table[m, k] = euclidean_distance(sub_query, codebook[k])

        return distance_table

    def search(self, query: np.ndarray, codes: np.ndarray, k: int = 10) -> List[tuple]:
        """
        使用ADC (Asymmetric Distance Computation) 搜索
        query: (d,) 查询向量
        codes: (n, M) 数据库中所有向量的量化码
        """
        # 预计算距离表
        distance_table = self.compute_distance_table(query)  # (M, ksub)

        # 计算每个数据库向量到查询的近似距离
        n = codes.shape[0]
        distances = np.zeros(n)

        for i in range(n):
            # 使用查表法快速计算距离
            dist = 0
            for m in range(self.M):
                code = codes[i, m]
                dist += distance_table[m, code] ** 2
            distances[i] = np.sqrt(dist)

        # 返回top-k
        indices = np.argsort(distances)[:k]
        return [(idx, distances[idx]) for idx in indices]
```

### 4.3 压缩效果

**示例：** d=768 (BERT embedding), M=8, nbits=8
- **原始**：768 * 4字节 = 3072字节
- **PQ后**：8 * 1字节 = 8字节
- **压缩比**：384倍

**精度损失：**
- 量化误差（MSE）取决于子空间内的方差
- 通常召回率@10 约降低 5-10%

---

## 5. IVFPQ 混合索引

结合 IVF 和 PQ 的优势：

```python
class IVFPQIndex:
    def __init__(self, nlist: int = 100, M: int = 8, nbits: int = 8):
        self.ivf = IVFIndex(nlist=nlist)
        self.pq = ProductQuantizer(d=768, M=M, nbits=nbits)  # 假设d=768
        self.codes_storage = {}  # {centroid_id: {vec_id: PQ codes}}
        self.trained = False

    def train(self, vectors: np.ndarray):
        # 训练IVF聚类
        self.ivf.train(vectors)
        # 训练PQ码本
        self.pq.train(vectors)
        self.trained = True

    def add(self, vectors: np.ndarray, ids: List[int] = None):
        if ids is None:
            ids = list(range(len(vectors)))

        # PQ编码
        codes = self.pq.encode(vectors)

        # 分配到IVF聚类
        for vec, vec_id, code in zip(vectors, ids, codes):
            distances = [euclidean_distance(vec, centroid) for centroid in self.ivf.centroids]
            centroid_id = np.argmin(distances)

            if centroid_id not in self.codes_storage:
                self.codes_storage[centroid_id] = {}
            self.codes_storage[centroid_id][vec_id] = code

    def search(self, query: np.ndarray, k: int = 10, nprobe: int = 10) -> List[tuple]:
        # 找到nprobe个最近的聚类中心
        centroid_distances = [(i, euclidean_distance(query, centroid))
                             for i, centroid in enumerate(self.ivf.centroids)]
        centroid_distances.sort(key=lambda x: x[1])
        nearest_centroids = [cid for cid, _ in centroid_distances[:nprobe]]

        # 收集所有候选的PQ codes
        all_codes = []
        all_ids = []
        for centroid_id in nearest_centroids:
            if centroid_id in self.codes_storage:
                for vec_id, code in self.codes_storage[centroid_id].items():
                    all_codes.append(code)
                    all_ids.append(vec_id)

        # 使用PQ距离表快速计算
        distance_table = self.pq.compute_distance_table(query)
        distances = []
        for vec_id, code in zip(all_ids, all_codes):
            dist = 0
            for m in range(self.pq.M):
                dist += distance_table[m, code[m]] ** 2
            distances.append((vec_id, np.sqrt(dist)))

        # 返回top-k
        distances.sort(key=lambda x: x[1])
        return distances[:k]
```

**性能对比：**
- **Brute Force**：100% 召回率，100ms 查询（1M向量）
- **IVF (nlist=1000, nprobe=10)**：95% 召回率，10ms 查询
- **IVFPQ (M=8)**：90% 召回率，2ms 查询，内存节省 384倍

---

## 6. Chroma 内部架构

### 6.1 核心组件

```
Chroma 架构
├── Client API (Python/JS)
├── FastAPI Server
│   ├── Collection Manager
│   ├── Embedding Function Registry
│   └── Query Engine
├── Metadata Store (SQLite)
│   ├── Collections 表
│   ├── Documents 表
│   └── Metadata 表
├── Vector Store (hnswlib)
│   ├── HNSW Index (per collection)
│   └── Binary persistence
└── DuckDB (SQL on vectors)
    └── Metadata 过滤查询
```

### 6.2 数据流程

```python
# Chroma 内部伪代码

class Collection:
    def __init__(self, name: str):
        self.name = name
        self.metadata_db = SQLiteDB(f"{name}_metadata.db")
        self.hnsw_index = hnswlib.Index(space='cosine', dim=768)
        self.embedding_function = None

    def add(self, documents: List[str], metadatas: List[dict], ids: List[str]):
        # 1. 调用 embedding function
        embeddings = self.embedding_function(documents)  # 调用 OpenAI API

        # 2. 存储元数据到 SQLite
        for doc, meta, doc_id in zip(documents, metadatas, ids):
            self.metadata_db.execute(
                "INSERT INTO documents (id, document, metadata) VALUES (?, ?, ?)",
                (doc_id, doc, json.dumps(meta))
            )

        # 3. 添加向量到 HNSW 索引
        internal_ids = self._get_internal_ids(ids)
        self.hnsw_index.add_items(embeddings, internal_ids)

        # 4. 持久化
        self.hnsw_index.save_index(f"{self.name}_hnsw.bin")
        self.metadata_db.commit()

    def query(self, query_texts: List[str], n_results: int = 10, where: dict = None):
        # 1. Embedding 查询文本
        query_embeddings = self.embedding_function(query_texts)

        # 2. 如果有 metadata 过滤
        if where:
            # 使用 DuckDB 执行 SQL 过滤
            filtered_ids = self._filter_by_metadata(where)
            # HNSW 不支持原生过滤，需要后处理
            labels, distances = self.hnsw_index.knn_query(query_embeddings, k=n_results*10)
            # 过滤不在 filtered_ids 中的结果
            final_results = self._post_filter(labels, distances, filtered_ids, n_results)
        else:
            # 3. 直接 HNSW 搜索
            labels, distances = self.hnsw_index.knn_query(query_embeddings, k=n_results)
            final_results = labels, distances

        # 4. 从 SQLite 加载文档和元数据
        results = []
        for label, distance in zip(*final_results):
            doc_id = self._get_document_id(label)
            row = self.metadata_db.execute(
                "SELECT document, metadata FROM documents WHERE id = ?",
                (doc_id,)
            ).fetchone()
            results.append({
                'id': doc_id,
                'distance': distance,
                'document': row[0],
                'metadata': json.loads(row[1])
            })

        return results
```

### 6.3 Chroma 的存储结构

```bash
# Chroma 持久化目录
./chroma_data/
├── chroma.sqlite3              # 元数据：collections, segments
├── <collection_id>/
│   ├── data_level0.bin         # HNSW 层级0数据
│   ├── length.bin              # 向量数量
│   ├── link_lists.bin          # HNSW 邻接表
│   └── header.bin              # 索引元数据
└── <collection_id>.parquet     # DuckDB 列存（可选）
```

**SQLite Schema:**
```sql
-- Collections 表
CREATE TABLE collections (
    id TEXT PRIMARY KEY,
    name TEXT UNIQUE,
    metadata TEXT,
    dimension INTEGER,
    created_at TIMESTAMP
);

-- Embeddings 表（简化版）
CREATE TABLE embeddings (
    id TEXT PRIMARY KEY,
    collection_id TEXT,
    embedding BLOB,           -- 序列化的向量（np.ndarray → bytes）
    document TEXT,
    metadata TEXT,
    FOREIGN KEY (collection_id) REFERENCES collections(id)
);
```

---

## 7. HNSW vs IVF vs IVFPQ 对比

| 算法 | 索引构建时间 | 查询延迟 | 召回率 | 内存占用 | 适用场景 |
|------|------------|---------|--------|---------|---------|
| **Brute Force** | O(1) | O(n*d) | 100% | n*d*4B | n < 10K |
| **HNSW** | O(n*M*log n) | O(ef*log n) | 95-99% | n*M*log(n)*4B | 高召回率、内存充足 |
| **IVF** | O(n*d*nlist) | O(n/nlist*d) | 80-95% | n*d*4B | 平衡性能 |
| **IVFPQ** | O(n*d*nlist) | O(n/nlist) | 70-90% | n*M*1B | 大规模、内存受限 |

**实测数据**（1M 向量，768维，RTX 3090）：
```
Brute Force:  查询 120ms,  内存 3GB,   召回@10: 100%
HNSW:         查询 0.8ms,  内存 8GB,   召回@10: 98%
IVF:          查询 5ms,    内存 3GB,   召回@10: 92%
IVFPQ:        查询 1.2ms,  内存 8MB,   召回@10: 85%
```

---

## 8. 高级优化技术

### 8.1 GPU 加速搜索

```python
# 使用 FAISS GPU 实现
import faiss

# CPU 索引
index_cpu = faiss.IndexFlatL2(768)
index_cpu.add(vectors)  # (n, 768)

# 转移到 GPU
res = faiss.StandardGpuResources()
index_gpu = faiss.index_cpu_to_gpu(res, 0, index_cpu)  # GPU 0

# 搜索（快100倍）
D, I = index_gpu.search(query, k=10)  # D: distances, I: indices
```

**加速比：**
- 小数据集（<100K）：3-5倍
- 大数据集（>10M）：50-100倍

### 8.2 混合精度索引

```python
# Float16 向量（减半内存，轻微损失精度）
vectors_fp16 = vectors.astype(np.float16)
index = faiss.IndexFlatL2(768)
index.add(vectors_fp16)

# Int8 量化（1/4内存，5-10% 召回率损失）
quantizer = faiss.IndexFlatL2(768)
index_ivfpq = faiss.IndexIVFPQ(quantizer, 768, nlist=1000, M=8, nbits=8)
index_ivfpq.train(vectors)
index_ivfpq.add(vectors)
```

### 8.3 两阶段检索

```python
def two_stage_retrieval(query, coarse_index, fine_index, k=10, coarse_k=100):
    """
    第一阶段：粗排（快速过滤）
    第二阶段：精排（精确计算）
    """
    # 粗排：IVFPQ 检索 100 个候选
    coarse_ids, _ = coarse_index.search(query, k=coarse_k)

    # 精排：对100个候选用原始向量计算精确距离
    candidates = fine_index[coarse_ids]  # 原始向量
    distances = [euclidean_distance(query, vec) for vec in candidates]

    # 返回最终 top-k
    sorted_indices = np.argsort(distances)[:k]
    return coarse_ids[sorted_indices]
```

---

## 9. 分布式向量数据库

### 9.1 分片策略

```python
class DistributedVectorDB:
    def __init__(self, num_shards: int = 4):
        self.num_shards = num_shards
        self.shards = [HNSW() for _ in range(num_shards)]

    def _get_shard(self, vec_id: int) -> int:
        """一致性哈希分片"""
        return vec_id % self.num_shards

    def add(self, vectors: np.ndarray, ids: List[int]):
        # 按 ID 分配到不同 shard
        for vec, vec_id in zip(vectors, ids):
            shard_id = self._get_shard(vec_id)
            self.shards[shard_id].insert(vec)

    def search(self, query: np.ndarray, k: int = 10):
        # 并行搜索所有 shard
        from concurrent.futures import ThreadPoolExecutor

        with ThreadPoolExecutor(max_workers=self.num_shards) as executor:
            futures = [executor.submit(shard.search, query, k)
                      for shard in self.shards]
            all_results = [f.result() for f in futures]

        # 合并结果并重新排序
        merged = []
        for results in all_results:
            merged.extend(results)
        merged.sort(key=lambda x: x[1])  # 按距离排序

        return merged[:k]
```

### 9.2 Milvus 架构（生产级向量数据库）

```
Milvus 分布式架构
├── Access Layer
│   └── SDK / RESTful API
├── Coordinator Service
│   ├── Root Coord (DDL, 分配 segment)
│   ├── Data Coord (数据分片、负载均衡)
│   ├── Query Coord (查询路由)
│   └── Index Coord (索引构建调度)
├── Worker Nodes
│   ├── Query Node (向量搜索)
│   ├── Data Node (数据写入)
│   └── Index Node (索引构建)
└── Storage
    ├── MinIO (对象存储：原始向量、索引文件)
    ├── etcd (元数据：collection schema, segment info)
    └── Pulsar (消息队列：WAL, binlog)
```

**查询流程：**
1. Client → Proxy → Query Coord（路由到相关 segment）
2. Query Node 并行搜索各 segment
3. 全局 top-k 合并返回

---

## 10. 性能基准测试

### 10.1 ANN-Benchmarks 结果

```python
# 数据集: SIFT1M (1M 128维向量)
# 指标: QPS (Queries Per Second) vs 召回率@10

算法配置                  QPS     召回率@10   内存(MB)
----------------------------------------
HNSW(M=16, ef=100)      5000      0.98       512
HNSW(M=32, ef=200)      2000      0.995      1024
IVF(nlist=1000, nprobe=10)  8000  0.92       480
IVFPQ(M=8, nprobe=20)   15000     0.85       60
ScaNN(量化+剪枝)         12000     0.95       128
```

### 10.2 延迟分解

```
HNSW 查询延迟分解（1M 向量，k=10）
├── Entry point 定位: 0.05ms
├── Layer 遍历: 0.3ms
│   ├── Layer 4→3: 0.02ms
│   ├── Layer 3→2: 0.05ms
│   ├── Layer 2→1: 0.08ms
│   └── Layer 1→0: 0.15ms
└── 底层精确搜索: 0.45ms
    ├── 距离计算: 0.35ms (dominant)
    └── 堆操作: 0.1ms
TOTAL: 0.8ms
```

**优化方向：**
- SIMD 向量化距离计算 → 减少 50%
- 预计算部分距离（PQ） → 减少 70%
- GPU 并行 → 减少 90%

---

## 11. 向量索引持久化

### 11.1 HNSW 序列化格式

```python
class HNSWPersistence:
    @staticmethod
    def save(hnsw: HNSW, filepath: str):
        """保存 HNSW 索引到磁盘"""
        with open(filepath, 'wb') as f:
            # Header
            f.write(struct.pack('I', hnsw.M))
            f.write(struct.pack('I', hnsw.M_max))
            f.write(struct.pack('I', hnsw.ef_construction))
            f.write(struct.pack('f', hnsw.ml))
            f.write(struct.pack('I', hnsw.node_count))
            f.write(struct.pack('i', hnsw.max_level))
            f.write(struct.pack('I', hnsw.entry_point if hnsw.entry_point else 0))

            # Nodes
            for idx in range(hnsw.node_count):
                node = hnsw.nodes[idx]
                # 向量
                vector_bytes = node.vector.tobytes()
                f.write(struct.pack('I', len(vector_bytes)))
                f.write(vector_bytes)

                # 层级
                f.write(struct.pack('I', node.level))

                # 每层的邻居
                for layer in range(node.level + 1):
                    neighbors = list(node.neighbors[layer])
                    f.write(struct.pack('I', len(neighbors)))
                    for neighbor in neighbors:
                        f.write(struct.pack('I', neighbor))

    @staticmethod
    def load(filepath: str) -> HNSW:
        """从磁盘加载 HNSW 索引"""
        with open(filepath, 'rb') as f:
            # Header
            M = struct.unpack('I', f.read(4))[0]
            M_max = struct.unpack('I', f.read(4))[0]
            ef_construction = struct.unpack('I', f.read(4))[0]
            ml = struct.unpack('f', f.read(4))[0]
            node_count = struct.unpack('I', f.read(4))[0]
            max_level = struct.unpack('i', f.read(4))[0]
            entry_point = struct.unpack('I', f.read(4))[0]

            hnsw = HNSW(M=M, M_max=M_max, ef_construction=ef_construction, ml=ml)
            hnsw.node_count = node_count
            hnsw.max_level = max_level
            hnsw.entry_point = entry_point if entry_point != 0 else None

            # Nodes
            for idx in range(node_count):
                # 向量
                vec_len = struct.unpack('I', f.read(4))[0]
                vector = np.frombuffer(f.read(vec_len), dtype=np.float32)

                # 层级
                level = struct.unpack('I', f.read(4))[0]

                node = HNSWNode(vector, level, idx)

                # 邻居
                for layer in range(level + 1):
                    neighbor_count = struct.unpack('I', f.read(4))[0]
                    neighbors = set()
                    for _ in range(neighbor_count):
                        neighbor = struct.unpack('I', f.read(4))[0]
                        neighbors.add(neighbor)
                    node.neighbors[layer] = neighbors

                hnsw.nodes[idx] = node

            return hnsw
```

---

## 12. 向量数据库选型决策树

```
开始
│
├─ 数据量 < 10万？
│  └─ 是 → 使用 Brute Force / Chroma (HNSW)
│  └─ 否 → 继续
│
├─ 内存充足（数据量*4*维度*10倍）？
│  └─ 是 → HNSW (Chroma, Weaviate)
│  └─ 否 → 继续
│
├─ 需要强过滤（复杂 metadata 查询）？
│  └─ 是 → Weaviate / Qdrant（混合索引优化）
│  └─ 否 → 继续
│
├─ 数据量 > 1亿？
│  └─ 是 → 分布式方案
│  │  ├─ 开源：Milvus
│  │  └─ 托管：Pinecone / Zilliz Cloud
│  └─ 否 → IVFPQ (FAISS)
```

**常见向量数据库对比：**

| 数据库 | 核心算法 | 分布式 | 过滤性能 | 开源 | 适用场景 |
|-------|---------|--------|---------|------|---------|
| **Chroma** | HNSW | ❌ | 低 | ✅ | 原型开发、小规模 |
| **Weaviate** | HNSW | ✅ | 高 | ✅ | 混合搜索、知识图谱 |
| **Milvus** | HNSW/IVF/IVFPQ | ✅ | 中 | ✅ | 大规模生产、多租户 |
| **Qdrant** | HNSW | ✅ | 高 | ✅ | 高吞吐过滤查询 |
| **Pinecone** | 专有 | ✅ | 高 | ❌ | 托管服务、零运维 |
| **FAISS** | 全算法 | ❌ | 无 | ✅ | 研究、自建索引 |

---

## 13. 实际应用代码模式

### 13.1 混合搜索（向量+关键词）

```python
from typing import List, Tuple

def hybrid_search(
    query: str,
    vector_db: HNSW,
    bm25_index: BM25,
    documents: List[str],
    embedding_fn,
    alpha: float = 0.7,  # 向量权重
    k: int = 10
) -> List[Tuple[int, float]]:
    """
    混合搜索：向量检索 + BM25
    alpha: 向量相似度权重（1-alpha 为 BM25 权重）
    """
    # 向量搜索
    query_embedding = embedding_fn(query)
    vector_results = vector_db.search(query_embedding, k=k*2)
    vector_scores = {idx: 1/(1+dist) for idx, dist in vector_results}  # 转为相似度

    # BM25 搜索
    bm25_scores = bm25_index.get_scores(query.split())
    bm25_dict = {i: score for i, score in enumerate(bm25_scores)}

    # 归一化
    def normalize(scores: dict) -> dict:
        max_score = max(scores.values()) if scores else 1
        return {k: v/max_score for k, v in scores.items()}

    vector_scores = normalize(vector_scores)
    bm25_dict = normalize(bm25_dict)

    # 混合
    all_ids = set(vector_scores.keys()) | set(bm25_dict.keys())
    hybrid_scores = []
    for idx in all_ids:
        vec_score = vector_scores.get(idx, 0)
        bm25_score = bm25_dict.get(idx, 0)
        final_score = alpha * vec_score + (1 - alpha) * bm25_score
        hybrid_scores.append((idx, final_score))

    hybrid_scores.sort(key=lambda x: x[1], reverse=True)
    return hybrid_scores[:k]
```

### 13.2 动态索引更新

```python
class DynamicHNSW:
    def __init__(self, *args, **kwargs):
        self.index = HNSW(*args, **kwargs)
        self.pending_deletes = set()
        self.write_lock = threading.Lock()

    def delete(self, vec_id: int):
        """软删除（标记删除，不立即重建索引）"""
        with self.write_lock:
            self.pending_deletes.add(vec_id)

    def search(self, query: np.ndarray, k: int = 10):
        """搜索时过滤已删除的向量"""
        results = self.index.search(query, k=k*2)  # 多检索一些
        filtered = [(idx, dist) for idx, dist in results
                   if idx not in self.pending_deletes]
        return filtered[:k]

    def compact(self):
        """
        定期压缩：重建索引以移除已删除向量
        在低峰时段执行
        """
        with self.write_lock:
            # 收集未删除的向量
            active_vectors = []
            for idx, node in self.index.nodes.items():
                if idx not in self.pending_deletes:
                    active_vectors.append((idx, node.vector))

            # 重建索引
            new_index = HNSW(M=self.index.M, M_max=self.index.M_max)
            for idx, vec in active_vectors:
                new_index.insert(vec)

            self.index = new_index
            self.pending_deletes.clear()
```

### 13.3 多模态检索

```python
class MultiModalVectorDB:
    def __init__(self):
        self.text_index = HNSW()       # 文本向量索引
        self.image_index = HNSW()      # 图像向量索引
        self.modality_map = {}         # {doc_id: 'text'|'image'}

    def add_text(self, text: str, text_embedding: np.ndarray, doc_id: str):
        self.text_index.insert(text_embedding)
        self.modality_map[doc_id] = 'text'

    def add_image(self, image_embedding: np.ndarray, doc_id: str):
        self.image_index.insert(image_embedding)
        self.modality_map[doc_id] = 'image'

    def search_cross_modal(self, query_embedding: np.ndarray,
                          query_modality: str, k: int = 10):
        """
        跨模态搜索（例如：文本查图像）
        需要多模态嵌入模型（如 CLIP）
        """
        if query_modality == 'text':
            # 文本查询 → 搜索图像索引
            return self.image_index.search(query_embedding, k=k)
        else:
            # 图像查询 → 搜索文本索引
            return self.text_index.search(query_embedding, k=k)
```
