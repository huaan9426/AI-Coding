# RAG æ•…éšœæ’æŸ¥æ·±åº¦æŒ‡å—

> **é€†å‘å­¦ä¹ å“²å­¦**: ä»ç”Ÿäº§ç¯å¢ƒçš„çœŸå®é—®é¢˜å‡ºå‘ï¼Œå€’æ¨åº•å±‚åŸç†ï¼Œæ·±æŒ–æŠ€æœ¯ç»†èŠ‚
>
> **ç›®æ ‡è¯»è€…**: å·²æŒæ¡ RAG åŸºç¡€ç†è®ºï¼Œéœ€è¦è§£å†³å®é™…å·¥ç¨‹é—®é¢˜çš„å¼€å‘è€…

---

## ç›®å½•

1. [é—®é¢˜åˆ†ç±»æ ‘](#é—®é¢˜åˆ†ç±»æ ‘)
2. [æ£€ç´¢è´¨é‡é—®é¢˜](#æ£€ç´¢è´¨é‡é—®é¢˜)
3. [ç”Ÿæˆè´¨é‡é—®é¢˜](#ç”Ÿæˆè´¨é‡é—®é¢˜)
4. [æ€§èƒ½ä¸æˆæœ¬é—®é¢˜](#æ€§èƒ½ä¸æˆæœ¬é—®é¢˜)
5. [ç³»ç»Ÿç¨³å®šæ€§é—®é¢˜](#ç³»ç»Ÿç¨³å®šæ€§é—®é¢˜)
6. [è°ƒè¯•å·¥å…·ä¸æµ‹è¯•æ–¹æ³•](#è°ƒè¯•å·¥å…·ä¸æµ‹è¯•æ–¹æ³•)

---

## é—®é¢˜åˆ†ç±»æ ‘

```
RAG ç³»ç»Ÿæ•…éšœ
â”‚
â”œâ”€ æ£€ç´¢è´¨é‡å·® (Retrieval Quality)
â”‚  â”œâ”€ å¬å›ç‡ä½ (Recall)
â”‚  â”œâ”€ ç²¾ç¡®ç‡ä½ (Precision)
â”‚  â””â”€ ç›¸å…³æ€§åˆ¤æ–­é”™è¯¯ (Relevance)
â”‚
â”œâ”€ ç”Ÿæˆè´¨é‡å·® (Generation Quality)
â”‚  â”œâ”€ å¹»è§‰ (Hallucination)
â”‚  â”œâ”€ ä¸Šä¸‹æ–‡ä¸¢å¤± (Context Loss)
â”‚  â””â”€ ç­”æ¡ˆä¸å‡†ç¡® (Inaccuracy)
â”‚
â”œâ”€ æ€§èƒ½é—®é¢˜ (Performance)
â”‚  â”œâ”€ å»¶è¿Ÿé«˜ (Latency)
â”‚  â”œâ”€ ååé‡ä½ (Throughput)
â”‚  â””â”€ æˆæœ¬é«˜ (Cost)
â”‚
â””â”€ ç¨³å®šæ€§é—®é¢˜ (Stability)
   â”œâ”€ API é™æµ/è¶…æ—¶
   â”œâ”€ å†…å­˜æº¢å‡º (OOM)
   â””â”€ å¹¶å‘é”™è¯¯
```

---

# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ£€ç´¢è´¨é‡é—®é¢˜

## æ¡ˆä¾‹ 1: "ä¸ºä»€ä¹ˆæ£€ç´¢ä¸åˆ°æ˜æ˜å­˜åœ¨çš„æ–‡æ¡£ï¼Ÿ"

### ğŸ”´ æ•…éšœç°è±¡

```python
# çŸ¥è¯†åº“ä¸­æœ‰è¿™æ®µæ–‡æœ¬
document = """
GPT-4 æ‹¥æœ‰çº¦ 1.76 ä¸‡äº¿å‚æ•°ï¼Œæ˜¯ OpenAI åœ¨ 2023 å¹´ 3 æœˆå‘å¸ƒçš„å¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚
å®ƒæ”¯æŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº† GPT-3.5ã€‚
"""

# ç”¨æˆ·é—®é¢˜
query = "GPT-4 æœ‰å¤šå°‘å‚æ•°ï¼Ÿ"

# æ£€ç´¢ç»“æœ
retrieved_docs = [
    "GPT-3 æœ‰ 175B å‚æ•°...",
    "å¤§æ¨¡å‹çš„å‚æ•°é‡å†³å®šäº†æ€§èƒ½...",
    "OpenAI æ˜¯ AI é¢†åŸŸçš„é¢†å¯¼è€…..."
]

# âŒ é—®é¢˜ï¼šæ˜æ˜æ–‡æ¡£é‡Œæœ‰ç­”æ¡ˆï¼Œä¸ºä»€ä¹ˆæ£€ç´¢ä¸åˆ°ï¼Ÿ
```

### ğŸ” æ ¹å› åˆ†æï¼ˆ5 å±‚æ·±æŒ–ï¼‰

#### å±‚æ¬¡ 1: è¡¨é¢åŸå›  - åˆ†å—ç­–ç•¥å¤±æ•ˆ

```python
# æ£€æŸ¥æ–‡æ¡£åˆ†å—ç»“æœ
def debug_chunking(text, chunk_size=200, chunk_overlap=50):
    """è°ƒè¯•åˆ†å—é€»è¾‘"""
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunk = text[start:end]
        chunks.append({
            "chunk": chunk,
            "start": start,
            "end": end,
            "length": len(chunk)
        })
        start += (chunk_size - chunk_overlap)
    return chunks

# å®é™…è¾“å‡º
chunks = debug_chunking(document, chunk_size=50, chunk_overlap=10)
# [
#   {"chunk": "GPT-4 æ‹¥æœ‰çº¦ 1.76 ä¸‡äº¿å‚æ•°ï¼Œæ˜¯ OpenAI åœ¨ 2023 å¹´ 3 æœˆå‘å¸ƒ", ...},
#   {"chunk": "åœ¨ 2023 å¹´ 3 æœˆå‘å¸ƒçš„å¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚\nå®ƒæ”¯æŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥", ...},
#   {"chunk": "è¾“å…¥ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº† GPT-3.5ã€‚", ...}
# ]

# âŒ é—®é¢˜å‘ç°ï¼šå…³é”®ä¿¡æ¯ "GPT-4" å’Œ "1.76 ä¸‡äº¿å‚æ•°" åœ¨åŒä¸€ä¸ªå—
# âœ… ä½†å¦‚æœ chunk_size=30ï¼Œä¼šè¢«åˆ†å‰²ï¼š
#   å—1: "GPT-4 æ‹¥æœ‰çº¦ 1.76 ä¸‡äº¿å‚æ•°"
#   å—2: "æ˜¯ OpenAI åœ¨ 2023 å¹´ 3 æœˆå‘å¸ƒçš„å¤šæ¨¡"
```

**ä¸­é—´ç»“è®º**: åˆ†å—å‚æ•°ä¸å½“å¯¼è‡´è¯­ä¹‰å®Œæ•´æ€§è¢«ç ´åã€‚

---

#### å±‚æ¬¡ 2: Embedding è¯­ä¹‰ç¼–ç é—®é¢˜

```python
# æ£€æŸ¥ Embedding çš„è¯­ä¹‰æ•è·èƒ½åŠ›
import openai

# åŸæ–‡
text = "GPT-4 æ‹¥æœ‰çº¦ 1.76 ä¸‡äº¿å‚æ•°"

# æŸ¥è¯¢
query = "GPT-4 æœ‰å¤šå°‘å‚æ•°ï¼Ÿ"

# è·å–å‘é‡
text_emb = openai.Embedding.create(input=text, model="text-embedding-ada-002")
query_emb = openai.Embedding.create(input=query, model="text-embedding-ada-002")

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
import numpy as np
def cosine_sim(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

similarity = cosine_sim(
    text_emb['data'][0]['embedding'],
    query_emb['data'][0]['embedding']
)
# é¢„æœŸ: similarity â‰ˆ 0.85 ~ 0.95

# â“ å¦‚æœ similarity < 0.7ï¼Œè¯´æ˜ä»€ä¹ˆé—®é¢˜ï¼Ÿ
```

**æ·±å…¥åˆ†æ**:

1. **è¯æ±‡å·®å¼‚é—®é¢˜** (Vocabulary Gap)
   ```python
   # é—®é¢˜: "æœ‰å¤šå°‘" vs "æ‹¥æœ‰çº¦"
   # Embedding å¯èƒ½æ— æ³•å¾ˆå¥½åœ°å¯¹é½è¿™ç§è¡¨è¾¾å·®å¼‚

   # è§£å†³æ–¹æ¡ˆ 1: Query Rewriting
   def query_expansion(query):
       """æŸ¥è¯¢æ‰©å±•"""
       synonyms = {
           "æœ‰å¤šå°‘": ["æ‹¥æœ‰çº¦", "æ˜¯", "ç­‰äº", "è¾¾åˆ°"],
           "å‚æ•°": ["å‚æ•°é‡", "å‚æ•°è§„æ¨¡", "å‚æ•°æ•°é‡"]
       }
       expanded_queries = [query]
       for word, syns in synonyms.items():
           if word in query:
               for syn in syns:
                   expanded_queries.append(query.replace(word, syn))
       return expanded_queries

   # æ‰©å±•åçš„æŸ¥è¯¢
   queries = query_expansion("GPT-4 æœ‰å¤šå°‘å‚æ•°ï¼Ÿ")
   # ["GPT-4 æœ‰å¤šå°‘å‚æ•°ï¼Ÿ", "GPT-4 æ‹¥æœ‰çº¦å‚æ•°ï¼Ÿ", "GPT-4 æ˜¯å‚æ•°ï¼Ÿ", ...]
   ```

2. **æ•°å­—è¡¨ç¤ºé—®é¢˜** (Numeric Representation)
   ```python
   # é—®é¢˜: "1.76 ä¸‡äº¿" vs "1.76T" vs "1760B"
   # Embedding æ¨¡å‹å¯¹æ•°å­—çš„è¡¨ç¤ºèƒ½åŠ›æœ‰é™

   # è¯æ˜å®éªŒ
   texts = [
       "GPT-4 æœ‰ 1.76 ä¸‡äº¿å‚æ•°",
       "GPT-4 æœ‰ 1.76T å‚æ•°",
       "GPT-4 æœ‰ 1760B å‚æ•°",
       "GPT-4 æœ‰ 1,760,000,000,000 å‚æ•°"
   ]

   embeddings = [get_embedding(t) for t in texts]

   # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
   # é¢„æœŸ: æ‰€æœ‰å‘é‡åº”è¯¥éå¸¸æ¥è¿‘ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ > 0.95ï¼‰
   # å®é™…: å¯èƒ½åªæœ‰ 0.75 ~ 0.85

   # æ ¹å› : Embedding æ¨¡å‹åœ¨é¢„è®­ç»ƒæ—¶ï¼Œä¸åŒæ•°å­—è¡¨ç¤ºå½¢å¼çš„å…±ç°é¢‘ç‡ä¸åŒ
   ```

**ä¸­é—´ç»“è®º**: Embedding æ¨¡å‹çš„è¯­ä¹‰æ•è·èƒ½åŠ›å­˜åœ¨å±€é™æ€§ã€‚

---

#### å±‚æ¬¡ 3: å‘é‡æ•°æ®åº“ç´¢å¼•é—®é¢˜

```python
# æ£€æŸ¥å‘é‡æ•°æ®åº“çš„ç´¢å¼•é…ç½®
from chromadb.config import Settings

# å¸¸è§é…ç½®é”™è¯¯
config = Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="./chroma_db",
    anonymized_telemetry=False
)

# âŒ é—®é¢˜ï¼šæ²¡æœ‰æŒ‡å®šè·ç¦»åº¦é‡
# Chroma é»˜è®¤ä½¿ç”¨ L2 è·ç¦»ï¼Œä½†å¾ˆå¤šåœºæ™¯ä¸‹ä½™å¼¦ç›¸ä¼¼åº¦æ›´å¥½

# æ­£ç¡®é…ç½®
collection = client.create_collection(
    name="documents",
    metadata={"hnsw:space": "cosine"}  # æŒ‡å®šä½™å¼¦ç›¸ä¼¼åº¦
)

# å¯¹æ¯”å®éªŒ
def compare_distance_metrics(query_vec, doc_vecs):
    """å¯¹æ¯”ä¸åŒè·ç¦»åº¦é‡çš„æ£€ç´¢ç»“æœ"""
    import numpy as np

    # L2 è·ç¦»
    l2_distances = [np.linalg.norm(query_vec - doc) for doc in doc_vecs]
    l2_ranking = np.argsort(l2_distances)

    # ä½™å¼¦ç›¸ä¼¼åº¦
    cos_similarities = [
        np.dot(query_vec, doc) / (np.linalg.norm(query_vec) * np.linalg.norm(doc))
        for doc in doc_vecs
    ]
    cos_ranking = np.argsort(cos_similarities)[::-1]

    print("L2 æ’åº:", l2_ranking)
    print("Cosine æ’åº:", cos_ranking)

    # å¯èƒ½å‡ºç°ä¸åŒçš„æ’åºç»“æœï¼
```

**æ·±å…¥åŸç†**:

```
L2 è·ç¦» vs ä½™å¼¦ç›¸ä¼¼åº¦çš„æœ¬è´¨åŒºåˆ«ï¼š

å‡è®¾æœ‰ä¸‰ä¸ªå‘é‡ï¼š
query = [1, 0]  (æŸ¥è¯¢å‘é‡)
doc1  = [2, 0]  (æ–¹å‘ç›¸åŒï¼Œæ¨¡é•¿ä¸åŒ)
doc2  = [0.7, 0.7]  (æ–¹å‘ä¸åŒï¼Œæ¨¡é•¿ç›¸è¿‘)

L2 è·ç¦»:
d(query, doc1) = ||[1,0] - [2,0]|| = 1.0
d(query, doc2) = ||[1,0] - [0.7,0.7]|| = 0.72

ç»“è®º: doc2 æ›´è¿‘ï¼ˆâŒ é”™è¯¯ï¼Œdoc2 æ–¹å‘å®Œå…¨ä¸åŒï¼‰

ä½™å¼¦ç›¸ä¼¼åº¦:
cos(query, doc1) = 1.0  (æ–¹å‘å®Œå…¨ä¸€è‡´)
cos(query, doc2) = 0.7  (æ–¹å‘åç¦»)

ç»“è®º: doc1 æ›´ç›¸å…³ï¼ˆâœ… æ­£ç¡®ï¼‰

ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿ
- L2 è·ç¦»å—å‘é‡æ¨¡é•¿å½±å“ï¼ˆæ–‡æœ¬é•¿åº¦å½±å“ï¼‰
- ä½™å¼¦ç›¸ä¼¼åº¦åªå…³æ³¨æ–¹å‘ï¼ˆè¯­ä¹‰ç›¸ä¼¼æ€§ï¼‰

é€‚ç”¨åœºæ™¯ï¼š
- æ–‡æœ¬æ£€ç´¢ â†’ ä½™å¼¦ç›¸ä¼¼åº¦
- å›¾åƒæ£€ç´¢ï¼ˆå›ºå®šå°ºå¯¸ï¼‰â†’ L2 è·ç¦»
```

**ä¸­é—´ç»“è®º**: è·ç¦»åº¦é‡é€‰æ‹©ä¸å½“å¯¼è‡´æ’åºé”™è¯¯ã€‚

---

#### å±‚æ¬¡ 4: Top-K å‚æ•°è°ƒä¼˜é—®é¢˜

```python
# æ£€æŸ¥æ£€ç´¢çš„ Top-K è®¾ç½®
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # âŒ é—®é¢˜ï¼šK å€¼å¤ªå°
)

# å®éªŒï¼šK å€¼å¯¹å¬å›ç‡çš„å½±å“
def analyze_recall_at_k(vectorstore, test_queries, ground_truth):
    """åˆ†æä¸åŒ K å€¼ä¸‹çš„å¬å›ç‡"""
    k_values = [1, 3, 5, 10, 20, 50]
    results = {}

    for k in k_values:
        recall_scores = []
        for query, relevant_docs in zip(test_queries, ground_truth):
            # æ£€ç´¢ Top-K
            retrieved = vectorstore.similarity_search(query, k=k)
            retrieved_ids = {doc.metadata['id'] for doc in retrieved}

            # è®¡ç®—å¬å›ç‡
            relevant_ids = set(relevant_docs)
            recall = len(retrieved_ids & relevant_ids) / len(relevant_ids)
            recall_scores.append(recall)

        results[k] = np.mean(recall_scores)

    return results

# å…¸å‹è¾“å‡º
# {
#   1: 0.42,   # K=1 æ—¶ï¼Œå¬å›ç‡ä»… 42%
#   3: 0.68,   # K=3 æ—¶ï¼Œå¬å›ç‡ 68%
#   5: 0.81,   # K=5 æ—¶ï¼Œå¬å›ç‡ 81%
#   10: 0.92,  # K=10 æ—¶ï¼Œå¬å›ç‡ 92%
#   20: 0.97,  # K=20 æ—¶ï¼Œå¬å›ç‡ 97%ï¼ˆä½†ä¼šå¼•å…¥å™ªéŸ³ï¼‰
# }

# âš–ï¸ æƒè¡¡ç‚¹ï¼šå¬å›ç‡ vs ç²¾ç¡®ç‡ vs LLM æˆæœ¬
```

**æ·±åº¦åˆ†æï¼šK å€¼çš„å¤šç»´å½±å“**

```python
class KValueAnalyzer:
    """K å€¼çš„å…¨ç»´åº¦åˆ†æå™¨"""

    def __init__(self, vectorstore, llm_cost_per_token=0.0001):
        self.vectorstore = vectorstore
        self.llm_cost_per_token = llm_cost_per_token

    def analyze(self, query, ground_truth_doc_id, k_values=[1, 3, 5, 10]):
        """å¤šç»´åº¦åˆ†æ K å€¼å½±å“"""
        results = []

        for k in k_values:
            # æ£€ç´¢
            docs = self.vectorstore.similarity_search(query, k=k)

            # 1. å¬å›ç‡
            retrieved_ids = [doc.metadata['id'] for doc in docs]
            recall = 1.0 if ground_truth_doc_id in retrieved_ids else 0.0

            # 2. ç²¾ç¡®ç‡ï¼ˆäººå·¥æ ‡æ³¨å‰ k ä¸ªæ˜¯å¦éƒ½ç›¸å…³ï¼‰
            # ç®€åŒ–ï¼šå‡è®¾åªæœ‰ ground_truth ç›¸å…³
            precision = recall / k

            # 3. ä¸Šä¸‹æ–‡é•¿åº¦
            total_tokens = sum([len(doc.page_content.split()) for doc in docs])

            # 4. LLM æˆæœ¬
            llm_cost = total_tokens * self.llm_cost_per_token

            # 5. ç­”æ¡ˆè´¨é‡ï¼ˆéœ€è¦å®é™…è°ƒç”¨ LLMï¼Œè¿™é‡Œæ¨¡æ‹Ÿï¼‰
            # å‡è®¾ï¼šæ›´å¤šä¸Šä¸‹æ–‡ â†’ æ›´é«˜è´¨é‡ï¼Œä½†è¾¹é™…æ•ˆç›Šé€’å‡
            quality_score = min(recall * (1 + 0.1 * k), 1.0)

            results.append({
                "k": k,
                "recall": recall,
                "precision": precision,
                "context_tokens": total_tokens,
                "llm_cost": llm_cost,
                "estimated_quality": quality_score,
                "efficiency": quality_score / llm_cost  # è´¨é‡/æˆæœ¬æ¯”
            })

        return results

# è¿è¡Œåˆ†æ
analyzer = KValueAnalyzer(vectorstore)
analysis = analyzer.analyze(
    query="GPT-4 æœ‰å¤šå°‘å‚æ•°ï¼Ÿ",
    ground_truth_doc_id="doc_123"
)

# å…¸å‹è¾“å‡ºï¼ˆæ ¼å¼åŒ–ä¸ºè¡¨æ ¼ï¼‰
"""
| K  | Recall | Precision | Context Tokens | LLM Cost | Quality | Efficiency |
|----|--------|-----------|----------------|----------|---------|------------|
| 1  | 0.0    | 0.0       | 150            | $0.015   | 0.0     | 0.0        |
| 3  | 1.0    | 0.33      | 450            | $0.045   | 1.3     | 28.9       |
| 5  | 1.0    | 0.20      | 750            | $0.075   | 1.5     | 20.0       |
| 10 | 1.0    | 0.10      | 1500           | $0.150   | 2.0     | 13.3       |
"""

# ğŸ¯ æœ€ä¼˜é€‰æ‹©ï¼šK=3 æˆ– K=5ï¼ˆå¬å›ç‡é«˜ + æ•ˆç‡æœ€ä½³ï¼‰
```

**ä¸­é—´ç»“è®º**: K å€¼éœ€è¦æ ¹æ®ä¸šåŠ¡åœºæ™¯æƒè¡¡å¬å›ç‡ã€æˆæœ¬å’Œè´¨é‡ã€‚

---

#### å±‚æ¬¡ 5: æ··åˆæ£€ç´¢ç­–ç•¥ç¼ºå¤±

```python
# é—®é¢˜ï¼šçº¯å‘é‡æ£€ç´¢æ— æ³•å¤„ç†ç²¾ç¡®åŒ¹é…éœ€æ±‚
query = "GPT-4 æœ‰å¤šå°‘å‚æ•°ï¼Ÿ"

# çº¯å‘é‡æ£€ç´¢çš„é—®é¢˜
vector_results = vectorstore.similarity_search(query, k=5)
# å¯èƒ½è¿”å›ï¼š
# - "GPT-3 æœ‰ 175B å‚æ•°"ï¼ˆè¯­ä¹‰ç›¸ä¼¼ä½†å®ä½“é”™è¯¯ï¼‰
# - "å‚æ•°é‡æ˜¯è¡¡é‡æ¨¡å‹èƒ½åŠ›çš„å…³é”®æŒ‡æ ‡"ï¼ˆç›¸å…³ä½†æ— ç­”æ¡ˆï¼‰

# è§£å†³æ–¹æ¡ˆï¼šHybrid Searchï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
from rank_bpm25 import BM25Okapi

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨"""

    def __init__(self, vectorstore, documents):
        self.vectorstore = vectorstore
        self.documents = documents

        # æ„å»º BM25 ç´¢å¼•
        tokenized_docs = [doc.page_content.split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized_docs)

    def retrieve(self, query, k=5, alpha=0.5):
        """
        æ··åˆæ£€ç´¢

        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ•°é‡
            alpha: å‘é‡æ£€ç´¢æƒé‡ï¼ˆ0-1ï¼‰ï¼Œ1-alpha ä¸º BM25 æƒé‡
        """
        # 1. å‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰ç›¸ä¼¼ï¼‰
        vector_results = self.vectorstore.similarity_search_with_score(query, k=k*2)
        vector_scores = {
            doc.metadata['id']: 1 - score  # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦
            for doc, score in vector_results
        }

        # 2. BM25 æ£€ç´¢ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
        tokenized_query = query.split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        bm25_scores_dict = {
            self.documents[i].metadata['id']: score
            for i, score in enumerate(bm25_scores)
        }

        # 3. å½’ä¸€åŒ–åˆ†æ•°
        def normalize(scores):
            max_s = max(scores.values()) if scores else 1
            return {k: v/max_s for k, v in scores.items()}

        vector_scores_norm = normalize(vector_scores)
        bm25_scores_norm = normalize(bm25_scores_dict)

        # 4. åŠ æƒèåˆ
        all_doc_ids = set(vector_scores_norm.keys()) | set(bm25_scores_norm.keys())
        hybrid_scores = {}
        for doc_id in all_doc_ids:
            vec_score = vector_scores_norm.get(doc_id, 0)
            bm25_score = bm25_scores_norm.get(doc_id, 0)
            hybrid_scores[doc_id] = alpha * vec_score + (1 - alpha) * bm25_score

        # 5. æ’åºè¿”å› Top-K
        sorted_ids = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:k]
        return [self.get_doc_by_id(doc_id) for doc_id, _ in sorted_ids]

# å¯¹æ¯”å®éªŒ
def compare_retrieval_methods(query, ground_truth_id):
    """å¯¹æ¯”çº¯å‘é‡ vs æ··åˆæ£€ç´¢"""
    # çº¯å‘é‡
    vector_results = vectorstore.similarity_search(query, k=5)
    vector_has_answer = any(doc.metadata['id'] == ground_truth_id for doc in vector_results)

    # æ··åˆæ£€ç´¢
    hybrid_retriever = HybridRetriever(vectorstore, all_documents)
    hybrid_results = hybrid_retriever.retrieve(query, k=5, alpha=0.7)
    hybrid_has_answer = any(doc.metadata['id'] == ground_truth_id for doc in hybrid_results)

    return {
        "vector_recall": vector_has_answer,
        "hybrid_recall": hybrid_has_answer
    }

# å…¸å‹æ”¹è¿›
# Before (çº¯å‘é‡): å¬å›ç‡ 68%
# After (æ··åˆæ£€ç´¢): å¬å›ç‡ 87%
```

**æ·±åº¦åŸç†ï¼šä¸ºä»€ä¹ˆæ··åˆæ£€ç´¢æ›´å¥½ï¼Ÿ**

```
çº¯å‘é‡æ£€ç´¢çš„å±€é™æ€§ï¼š

1. å®ä½“è¯†åˆ«å¼±
   æŸ¥è¯¢: "GPT-4 æœ‰å¤šå°‘å‚æ•°ï¼Ÿ"
   å‘é‡å¯èƒ½æ··æ·†: GPT-3, GPT-3.5, GPT-4

   BM25 ä¼˜åŠ¿: ç²¾ç¡®åŒ¹é… "GPT-4" token

2. æ•°å­—è¡¨ç¤ºå¼±
   æŸ¥è¯¢: "1.76 ä¸‡äº¿"
   å‘é‡å¯èƒ½åŒ¹é…: "æ•°ä¸‡äº¿", "çº¦ 2 ä¸‡äº¿"

   BM25 ä¼˜åŠ¿: ç²¾ç¡®åŒ¹é…æ•°å­—

3. ç½•è§è¯æ±‡å¼±
   æŸ¥è¯¢: "RLHF ç®—æ³•"
   å‘é‡å¯èƒ½æ³›åŒ–ä¸º: "å¼ºåŒ–å­¦ä¹ ç®—æ³•"

   BM25 ä¼˜åŠ¿: ç²¾ç¡®åŒ¹é…ä¸“ä¸šæœ¯è¯­

æ··åˆç­–ç•¥çš„æ•°å­¦åŸºç¡€ï¼š

Score_hybrid = Î± Ã— Score_semantic + (1-Î±) Ã— Score_lexical

å…¶ä¸­ï¼š
- Score_semantic: ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ•è·è¯­ä¹‰ï¼‰
- Score_lexical: BM25 åˆ†æ•°ï¼ˆæ•è·å­—é¢åŒ¹é…ï¼‰
- Î± âˆˆ [0.5, 0.8]: é€šå¸¸å‘é‡æƒé‡æ›´é«˜

è°ƒä¼˜å»ºè®®ï¼š
- é—®ç­”åœºæ™¯: Î±=0.7 (æ›´é‡è¯­ä¹‰)
- ä»£ç æœç´¢: Î±=0.3 (æ›´é‡ç²¾ç¡®åŒ¹é…)
- å­¦æœ¯æ£€ç´¢: Î±=0.5 (å‡è¡¡)
```

---

### âœ… å®Œæ•´è§£å†³æ–¹æ¡ˆ

```python
class ProductionRetriever:
    """ç”Ÿäº§çº§æ£€ç´¢å™¨ï¼ˆé›†æˆæ‰€æœ‰ä¼˜åŒ–ï¼‰"""

    def __init__(self, documents, embedding_model, chunk_config):
        # 1. ä¼˜åŒ–åˆ†å—ç­–ç•¥
        self.chunks = self._smart_chunking(documents, chunk_config)

        # 2. åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆæŒ‡å®šä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        self.vectorstore = Chroma.from_documents(
            documents=self.chunks,
            embedding=embedding_model,
            collection_metadata={"hnsw:space": "cosine"}
        )

        # 3. åˆå§‹åŒ– BM25
        self.bm25 = BM25Okapi([doc.page_content.split() for doc in self.chunks])

    def _smart_chunking(self, documents, config):
        """æ™ºèƒ½åˆ†å—ï¼ˆä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼‰"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=config['chunk_size'],
            chunk_overlap=config['chunk_overlap'],
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""],  # è¯­ä¹‰è¾¹ç•Œ
            length_function=len
        )

        return splitter.split_documents(documents)

    def retrieve(self, query, k=5, rerank=True):
        """æ··åˆæ£€ç´¢ + é‡æ’åº"""
        # Step 1: æ··åˆæ£€ç´¢ï¼ˆå– 2*k å€™é€‰ï¼‰
        candidates = self._hybrid_search(query, k=k*2)

        # Step 2: é‡æ’åºï¼ˆå¯é€‰ï¼‰
        if rerank:
            candidates = self._rerank(query, candidates)

        # Step 3: è¿”å› Top-K
        return candidates[:k]

    def _hybrid_search(self, query, k):
        """æ··åˆæ£€ç´¢å®ç°"""
        # å‘é‡æ£€ç´¢
        vector_results = self.vectorstore.similarity_search_with_score(query, k=k)

        # BM25 æ£€ç´¢
        tokenized_query = query.split()
        bm25_scores = self.bm25.get_scores(tokenized_query)

        # èåˆåˆ†æ•°ï¼ˆå‚è€ƒä¸Šé¢çš„å®ç°ï¼‰
        # ...

        return merged_results

    def _rerank(self, query, candidates):
        """ä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ’åº"""
        from sentence_transformers import CrossEncoder

        # åŠ è½½é‡æ’åºæ¨¡å‹
        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

        # è®¡ç®—æŸ¥è¯¢ä¸æ¯ä¸ªå€™é€‰çš„ç›¸å…³æ€§åˆ†æ•°
        pairs = [(query, doc.page_content) for doc in candidates]
        scores = reranker.predict(pairs)

        # æŒ‰åˆ†æ•°æ’åº
        ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
        return [doc for doc, score in ranked]

# ä½¿ç”¨ç¤ºä¾‹
retriever = ProductionRetriever(
    documents=all_docs,
    embedding_model=OpenAIEmbeddings(),
    chunk_config={"chunk_size": 500, "chunk_overlap": 50}
)

results = retriever.retrieve("GPT-4 æœ‰å¤šå°‘å‚æ•°ï¼Ÿ", k=3, rerank=True)
```

---

## æ¡ˆä¾‹ 2: "ä¸ºä»€ä¹ˆè¿”å›çš„æ–‡æ¡£ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œä½†ç­”æ¡ˆå´é”™äº†ï¼Ÿ"

### ğŸ”´ æ•…éšœç°è±¡

```python
# æ£€ç´¢ç»“æœ
retrieved_docs = [
    {
        "content": "GPT-3 æ‹¥æœ‰ 175B å‚æ•°ï¼Œæ˜¯ 2020 å¹´æœ€å¤§çš„è¯­è¨€æ¨¡å‹ã€‚",
        "similarity": 0.91  # âœ… é«˜ç›¸ä¼¼åº¦
    },
    {
        "content": "OpenAI çš„ GPT ç³»åˆ—æ¨¡å‹ä¸æ–­çªç ´å‚æ•°è§„æ¨¡è®°å½•ã€‚",
        "similarity": 0.88
    },
    {
        "content": "GPT-4 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å‚æ•°é‡æœªå…¬å¼€ã€‚",
        "similarity": 0.86
    }
]

# ç”¨æˆ·é—®é¢˜
query = "GPT-4 æœ‰å¤šå°‘å‚æ•°ï¼Ÿ"

# LLM ç”Ÿæˆç­”æ¡ˆ
answer = "æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ŒGPT-4 æ‹¥æœ‰ 175B å‚æ•°ã€‚"
# âŒ é”™è¯¯ï¼æ··æ·†äº† GPT-3 å’Œ GPT-4
```

### ğŸ” æ ¹å› åˆ†æ

#### é—®é¢˜ 1: ç›¸ä¼¼åº¦â‰ ç›¸å…³æ€§

```python
# æ·±å…¥ç†è§£ï¼šä½™å¼¦ç›¸ä¼¼åº¦çš„å±€é™æ€§

def analyze_false_positive(query, doc1, doc2):
    """åˆ†æå‡é˜³æ€§æ¡ˆä¾‹"""

    # æŸ¥è¯¢å’Œä¸¤ä¸ªæ–‡æ¡£
    query = "GPT-4 æœ‰å¤šå°‘å‚æ•°ï¼Ÿ"
    doc1 = "GPT-3 æ‹¥æœ‰ 175B å‚æ•°"  # âŒ é”™è¯¯å®ä½“
    doc2 = "GPT-4 çš„å‚æ•°é‡æœªå…¬å¼€"  # âœ… æ­£ç¡®ç­”æ¡ˆ

    # è·å– Embedding
    query_emb = get_embedding(query)
    doc1_emb = get_embedding(doc1)
    doc2_emb = get_embedding(doc2)

    # è®¡ç®—ç›¸ä¼¼åº¦
    sim1 = cosine_similarity(query_emb, doc1_emb)  # 0.91
    sim2 = cosine_similarity(query_emb, doc2_emb)  # 0.86

    # â“ ä¸ºä»€ä¹ˆ doc1 åˆ†æ•°æ›´é«˜ï¼Ÿ

    # åŸå› åˆ†æï¼šè¯æ±‡é‡å åº¦
    def lexical_overlap(text1, text2):
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        return len(words1 & words2) / len(words1 | words2)

    overlap1 = lexical_overlap(query, doc1)
    # å…±åŒè¯: {"å‚æ•°"} â†’ overlap â‰ˆ 0.25

    overlap2 = lexical_overlap(query, doc2)
    # å…±åŒè¯: {"GPT-4", "å‚æ•°"} â†’ overlap â‰ˆ 0.40

    # ğŸ” å‘ç°ï¼šè¯æ±‡é‡å åº¦ doc2 æ›´é«˜ï¼Œä½†ç›¸ä¼¼åº¦ doc1 æ›´é«˜
    #    åŸå› ï¼šEmbedding æ•è·äº†"å…·ä½“æ•°å€¼"çš„è¯­ä¹‰ä¿¡å·
    #         "æœ‰å¤šå°‘" ä¸ "175B" çš„è¯­ä¹‰å…³è”å¼ºäº "æœªå…¬å¼€"
```

**æ·±åº¦åŸç†ï¼šEmbedding çš„ç»Ÿè®¡åè§**

```
Embedding æ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¸­ï¼š

é«˜é¢‘æ¨¡å¼ 1:
"XXX æœ‰å¤šå°‘å‚æ•°ï¼Ÿ" â†’ "XXX æœ‰ N å‚æ•°"
å‡ºç°é¢‘ç‡: å¾ˆé«˜

é«˜é¢‘æ¨¡å¼ 2:
"XXX æœ‰å¤šå°‘å‚æ•°ï¼Ÿ" â†’ "XXX çš„å‚æ•°é‡æœªå…¬å¼€"
å‡ºç°é¢‘ç‡: è¾ƒä½

ç»“æœï¼š
æ¨¡å‹å­¦ä¹ åˆ°çš„ç»Ÿè®¡å…³è”ï¼š
  "æœ‰å¤šå°‘" â†’ "å…·ä½“æ•°å€¼" (å¼ºå…³è”)
  "æœ‰å¤šå°‘" â†’ "æœªå…¬å¼€" (å¼±å…³è”)

å³ä½¿å®ä½“ä¸åŒ¹é…ï¼ˆGPT-3 vs GPT-4ï¼‰ï¼Œ
è¯­ä¹‰æ¨¡å¼åŒ¹é…ï¼ˆé—®æ•°å­— â†’ ç­”æ•°å­—ï¼‰å¯¼è‡´é«˜ç›¸ä¼¼åº¦ã€‚

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦é‡æ’åºæ¨¡å‹ï¼ˆCrossEncoderï¼‰æ¥äºŒæ¬¡éªŒè¯ï¼
```

#### é—®é¢˜ 2: ç¼ºå°‘å®ä½“è¯†åˆ«å’ŒéªŒè¯

```python
class EntityAwareRetriever:
    """å®ä½“æ„ŸçŸ¥çš„æ£€ç´¢å™¨"""

    def __init__(self, vectorstore, entity_extractor):
        self.vectorstore = vectorstore
        self.entity_extractor = entity_extractor  # NER æ¨¡å‹

    def retrieve_with_entity_verification(self, query, k=5):
        """æ£€ç´¢ + å®ä½“éªŒè¯"""
        # Step 1: ä»æŸ¥è¯¢ä¸­æå–å…³é”®å®ä½“
        query_entities = self.entity_extractor(query)
        # ä¾‹å¦‚: {"GPT-4": "MODEL", "å‚æ•°": "ATTRIBUTE"}

        # Step 2: å‘é‡æ£€ç´¢
        candidates = self.vectorstore.similarity_search_with_score(query, k=k*3)

        # Step 3: å®ä½“åŒ¹é…è¿‡æ»¤
        filtered_results = []
        for doc, score in candidates:
            doc_entities = self.entity_extractor(doc.page_content)

            # æ£€æŸ¥å…³é”®å®ä½“æ˜¯å¦åŒ¹é…
            entity_match_score = self._calculate_entity_match(
                query_entities,
                doc_entities
            )

            # è°ƒæ•´æœ€ç»ˆåˆ†æ•°
            final_score = 0.6 * score + 0.4 * entity_match_score
            filtered_results.append((doc, final_score))

        # Step 4: é‡æ–°æ’åº
        filtered_results.sort(key=lambda x: x[1], reverse=True)
        return filtered_results[:k]

    def _calculate_entity_match(self, query_entities, doc_entities):
        """è®¡ç®—å®ä½“åŒ¹é…åº¦"""
        # ç²¾ç¡®åŒ¹é…å…³é”®å®ä½“
        key_entities = {k for k, v in query_entities.items() if v in ["MODEL", "ORG"]}
        doc_entity_set = set(doc_entities.keys())

        if not key_entities:
            return 1.0  # æ— å…³é”®å®ä½“ï¼Œä¸æƒ©ç½š

        # è®¡ç®—åŒ¹é…æ¯”ä¾‹
        matched = key_entities & doc_entity_set
        return len(matched) / len(key_entities)

# ä½¿ç”¨ç¤ºä¾‹
from spacy import load
nlp = load("en_core_web_sm")

def simple_entity_extractor(text):
    """ç®€å•çš„å®ä½“æå–å™¨"""
    doc = nlp(text)
    return {ent.text: ent.label_ for ent in doc.ents}

entity_retriever = EntityAwareRetriever(vectorstore, simple_entity_extractor)
results = entity_retriever.retrieve_with_entity_verification(
    "GPT-4 æœ‰å¤šå°‘å‚æ•°ï¼Ÿ",
    k=3
)

# ç»“æœå¯¹æ¯”ï¼š
# Before: [GPT-3 (0.91), GPT ç³»åˆ— (0.88), GPT-4 æœªå…¬å¼€ (0.86)]
# After:  [GPT-4 æœªå…¬å¼€ (0.92), GPT-4 æµ‹è¯• (0.84), ...]
```

---

### âœ… å®Œæ•´è§£å†³æ–¹æ¡ˆï¼šå¤šé˜¶æ®µæ£€ç´¢ç®¡é“

```python
class MultiStageRetrievalPipeline:
    """å¤šé˜¶æ®µæ£€ç´¢ç®¡é“"""

    def __init__(self, vectorstore, config):
        self.vectorstore = vectorstore
        self.config = config

        # åŠ è½½å„ç§ç»„ä»¶
        self.bm25 = None  # BM25 ç´¢å¼•
        self.entity_extractor = None  # å®ä½“æå–å™¨
        self.reranker = None  # é‡æ’åºæ¨¡å‹

    def retrieve(self, query, k=5):
        """å®Œæ•´æ£€ç´¢æµç¨‹"""
        # é˜¶æ®µ 1: ç²—å¬å›ï¼ˆRecall Stageï¼‰
        candidates = self._stage1_recall(query, k=k*10)

        # é˜¶æ®µ 2: å®ä½“è¿‡æ»¤ï¼ˆEntity Filteringï¼‰
        filtered = self._stage2_entity_filter(query, candidates)

        # é˜¶æ®µ 3: é‡æ’åºï¼ˆReranking Stageï¼‰
        reranked = self._stage3_rerank(query, filtered, k=k*2)

        # é˜¶æ®µ 4: å¤šæ ·æ€§é€‰æ‹©ï¼ˆDiversity Selectionï¼‰
        final = self._stage4_diversity_select(reranked, k=k)

        return final

    def _stage1_recall(self, query, k):
        """é˜¶æ®µ 1: ç²—å¬å›ï¼ˆæ··åˆæ£€ç´¢ï¼‰"""
        # å‘é‡æ£€ç´¢
        vector_results = self.vectorstore.similarity_search_with_score(query, k=k)

        # BM25 æ£€ç´¢
        bm25_results = self.bm25.get_top_n(query.split(), k=k)

        # åˆå¹¶å»é‡
        all_results = self._merge_results(vector_results, bm25_results)
        return all_results[:k]

    def _stage2_entity_filter(self, query, candidates):
        """é˜¶æ®µ 2: å®ä½“åŒ¹é…è¿‡æ»¤"""
        query_entities = self.entity_extractor(query)

        scored_candidates = []
        for doc, base_score in candidates:
            doc_entities = self.entity_extractor(doc.page_content)
            entity_score = self._calculate_entity_match(query_entities, doc_entities)

            # å®ä½“ä¸åŒ¹é…ï¼Œé™ä½åˆ†æ•°
            if entity_score < 0.5:
                final_score = base_score * 0.5
            else:
                final_score = base_score

            scored_candidates.append((doc, final_score))

        return scored_candidates

    def _stage3_rerank(self, query, candidates, k):
        """é˜¶æ®µ 3: é‡æ’åºï¼ˆCrossEncoderï¼‰"""
        from sentence_transformers import CrossEncoder

        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

        # è®¡ç®—ç²¾ç¡®ç›¸å…³æ€§åˆ†æ•°
        pairs = [(query, doc.page_content) for doc, _ in candidates]
        rerank_scores = reranker.predict(pairs)

        # åˆå¹¶åˆ†æ•°ï¼ˆ0.3 åˆæ’ + 0.7 é‡æ’ï¼‰
        final_results = []
        for (doc, init_score), rerank_score in zip(candidates, rerank_scores):
            final_score = 0.3 * init_score + 0.7 * rerank_score
            final_results.append((doc, final_score))

        # æ’åº
        final_results.sort(key=lambda x: x[1], reverse=True)
        return final_results[:k]

    def _stage4_diversity_select(self, candidates, k):
        """é˜¶æ®µ 4: å¤šæ ·æ€§é€‰æ‹©ï¼ˆMMRï¼‰"""
        # Maximal Marginal Relevance
        # åœ¨ä¿è¯ç›¸å…³æ€§çš„åŒæ—¶ï¼Œå¢åŠ å¤šæ ·æ€§

        selected = []
        remaining = candidates.copy()

        # é€‰æ‹©ç¬¬ä¸€ä¸ªï¼ˆæœ€ç›¸å…³ï¼‰
        selected.append(remaining.pop(0))

        # è¿­ä»£é€‰æ‹©å‰©ä½™
        lambda_param = 0.7  # ç›¸å…³æ€§ vs å¤šæ ·æ€§æƒè¡¡

        while len(selected) < k and remaining:
            mmr_scores = []

            for doc, rel_score in remaining:
                # è®¡ç®—ä¸å·²é€‰æ–‡æ¡£çš„æœ€å¤§ç›¸ä¼¼åº¦
                max_sim = max([
                    self._doc_similarity(doc, sel_doc)
                    for sel_doc, _ in selected
                ])

                # MMR åˆ†æ•°
                mmr = lambda_param * rel_score - (1 - lambda_param) * max_sim
                mmr_scores.append(mmr)

            # é€‰æ‹© MMR æœ€é«˜çš„
            best_idx = np.argmax(mmr_scores)
            selected.append(remaining.pop(best_idx))

        return selected

    def _doc_similarity(self, doc1, doc2):
        """è®¡ç®—æ–‡æ¡£é—´ç›¸ä¼¼åº¦"""
        emb1 = self.vectorstore._embedding_function.embed_query(doc1.page_content)
        emb2 = self.vectorstore._embedding_function.embed_query(doc2.page_content)
        return cosine_similarity(emb1, emb2)
```

**æ”¹è¿›æ•ˆæœ**:

```
æµ‹è¯•é›†: 1000 ä¸ªé—®é¢˜

æŒ‡æ ‡                   | Naive RAG | å¤šé˜¶æ®µç®¡é“
-----------------------|-----------|------------
å‡†ç¡®ç‡                 | 62%       | 84%
å¬å›ç‡@5               | 71%       | 91%
å¹³å‡ç›¸å…³æ€§åˆ†æ•°         | 0.73      | 0.89
é”™è¯¯å®ä½“ç‡             | 23%       | 5%
å¹³å‡å»¶è¿Ÿ               | 0.8s      | 1.2s
```

---

## æ¡ˆä¾‹ 3: "æ–‡æ¡£è¢«æˆªæ–­ï¼Œå¯¼è‡´å…³é”®ä¿¡æ¯ä¸¢å¤±"

### ğŸ”´ æ•…éšœç°è±¡

```python
# åŸå§‹æ–‡æ¡£
original_doc = """
ç¬¬ä¸‰ç« ï¼šGPT-4 æ¶æ„è¯¦è§£

3.1 æ¨¡å‹è§„æ¨¡
GPT-4 é‡‡ç”¨ MoE (Mixture of Experts) æ¶æ„ï¼Œæ€»å‚æ•°é‡çº¦ 1.76 ä¸‡äº¿ã€‚
å…¶ä¸­ï¼Œæ¯æ¬¡æ¨ç†ä»…æ¿€æ´»çº¦ 280B å‚æ•°ï¼Œå¤§å¹…é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

3.2 è®­ç»ƒæ•°æ®
è®­ç»ƒæ•°æ®æˆªæ­¢åˆ° 2023 å¹´ 4 æœˆï¼ŒåŒ…å«äº’è”ç½‘æ–‡æœ¬ã€ä¹¦ç±ã€å­¦æœ¯è®ºæ–‡ç­‰ã€‚
æ•°æ®é‡çº¦ 13 ä¸‡äº¿ tokenï¼Œæ˜¯ GPT-3 è®­ç»ƒæ•°æ®çš„ 8 å€ã€‚

3.3 å¤šæ¨¡æ€èƒ½åŠ›
GPT-4 æ”¯æŒå›¾åƒè¾“å…¥ï¼Œèƒ½å¤Ÿç†è§£å›¾è¡¨ã€æˆªå›¾ç­‰è§†è§‰ä¿¡æ¯ã€‚
"""

# åˆ†å—å
chunks = chunk_document(original_doc, chunk_size=100, chunk_overlap=0)
# chunks[0]: "ç¬¬ä¸‰ç« ï¼šGPT-4 æ¶æ„è¯¦è§£\n\n3.1 æ¨¡å‹è§„æ¨¡\nGPT-4 é‡‡ç”¨ MoE (Mixture of Experts) æ¶æ„ï¼Œæ€»å‚æ•°"
# chunks[1]: "é‡çº¦ 1.76 ä¸‡äº¿ã€‚\nå…¶ä¸­ï¼Œæ¯æ¬¡æ¨ç†ä»…æ¿€æ´»çº¦ 280B å‚æ•°ï¼Œå¤§å¹…é™ä½äº†è®¡ç®—æˆæœ¬ã€‚\n\n3.2 è®­ç»ƒæ•°æ®"
# chunks[2]: "\nè®­ç»ƒæ•°æ®æˆªæ­¢åˆ° 2023 å¹´ 4 æœˆï¼ŒåŒ…å«äº’è”ç½‘æ–‡æœ¬ã€ä¹¦ç±ã€å­¦æœ¯è®ºæ–‡ç­‰ã€‚\næ•°æ®é‡çº¦ 13 ä¸‡äº¿ to"

# ç”¨æˆ·æŸ¥è¯¢
query = "GPT-4 çš„æ€»å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ"

# æ£€ç´¢ç»“æœ
retrieved_chunks = [chunks[1]]  # âŒ "é‡çº¦ 1.76 ä¸‡äº¿..." - ç¼ºå°‘ä¸»è¯­ "GPT-4"
```

### ğŸ” æ ¹å› åˆ†æ + è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆ 1: ä¼˜åŒ– Chunk Overlap

```python
def analyze_optimal_overlap(documents, queries, chunk_sizes=[200, 500, 1000]):
    """åˆ†ææœ€ä¼˜ overlap å‚æ•°"""
    results = []

    for chunk_size in chunk_sizes:
        for overlap_ratio in [0, 0.1, 0.2, 0.3, 0.5]:
            overlap = int(chunk_size * overlap_ratio)

            # åˆ†å—
            chunks = chunk_document(documents, chunk_size, overlap)

            # æ„å»ºå‘é‡åº“
            vectorstore = build_vectorstore(chunks)

            # æµ‹è¯•æ£€ç´¢è´¨é‡
            recall_scores = []
            for query in queries:
                retrieved = vectorstore.similarity_search(query, k=3)
                # è¯„ä¼°æ˜¯å¦åŒ…å«å®Œæ•´ç­”æ¡ˆ
                recall = evaluate_answer_completeness(query, retrieved)
                recall_scores.append(recall)

            results.append({
                "chunk_size": chunk_size,
                "overlap_ratio": overlap_ratio,
                "recall": np.mean(recall_scores)
            })

    return pd.DataFrame(results)

# å…¸å‹ç»“æœ
"""
chunk_size | overlap_ratio | recall
-----------|---------------|--------
200        | 0.0           | 0.62
200        | 0.1           | 0.71
200        | 0.2           | 0.78  â† æ˜¾è‘—æå‡
200        | 0.3           | 0.79
500        | 0.0           | 0.74
500        | 0.2           | 0.86  â† æœ€ä¼˜ç‚¹
1000       | 0.2           | 0.82
"""

# ç»“è®º: chunk_size=500, overlap=100 (20%) æœ€ä½³
```

#### æ–¹æ¡ˆ 2: Parent-Child Chunking

```python
class ParentChildChunker:
    """çˆ¶å­æ–‡æ¡£åˆ†å—ç­–ç•¥"""

    def __init__(self, parent_size=2000, child_size=500, child_overlap=100):
        self.parent_size = parent_size
        self.child_size = child_size
        self.child_overlap = child_overlap

    def chunk(self, documents):
        """ç”Ÿæˆçˆ¶å­æ–‡æ¡£å¯¹"""
        parent_chunks = []
        child_chunks = []

        for doc in documents:
            # çˆ¶æ–‡æ¡£ï¼šå¤§å—ï¼ˆä¿ç•™å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
            parents = self._chunk_text(doc, self.parent_size, 0)

            for parent_id, parent in enumerate(parents):
                parent_chunks.append({
                    "id": f"parent_{parent_id}",
                    "content": parent,
                    "type": "parent"
                })

                # å­æ–‡æ¡£ï¼šå°å—ï¼ˆç”¨äºæ£€ç´¢ï¼‰
                children = self._chunk_text(parent, self.child_size, self.child_overlap)

                for child_id, child in enumerate(children):
                    child_chunks.append({
                        "id": f"child_{parent_id}_{child_id}",
                        "content": child,
                        "parent_id": f"parent_{parent_id}",
                        "type": "child"
                    })

        return parent_chunks, child_chunks

    def _chunk_text(self, text, size, overlap):
        """æ–‡æœ¬åˆ†å—"""
        chunks = []
        start = 0
        while start < len(text):
            end = min(start + size, len(text))
            chunks.append(text[start:end])
            start += (size - overlap)
        return chunks

class ParentChildRetriever:
    """çˆ¶å­æ–‡æ¡£æ£€ç´¢å™¨"""

    def __init__(self, parent_chunks, child_chunks, embedding_model):
        self.parent_chunks = {p['id']: p for p in parent_chunks}

        # åªä¸ºå­æ–‡æ¡£å»ºç´¢å¼•
        self.vectorstore = Chroma.from_texts(
            texts=[c['content'] for c in child_chunks],
            metadatas=[{"parent_id": c['parent_id'], "id": c['id']} for c in child_chunks],
            embedding=embedding_model
        )

    def retrieve(self, query, k=3):
        """æ£€ç´¢ â†’ è¿”å›çˆ¶æ–‡æ¡£"""
        # Step 1: æ£€ç´¢å­æ–‡æ¡£
        child_results = self.vectorstore.similarity_search(query, k=k*2)

        # Step 2: è·å–å¯¹åº”çš„çˆ¶æ–‡æ¡£
        parent_ids = set([doc.metadata['parent_id'] for doc in child_results])

        # Step 3: è¿”å›çˆ¶æ–‡æ¡£ï¼ˆå®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
        parent_docs = [self.parent_chunks[pid] for pid in parent_ids]

        return parent_docs[:k]

# ä½¿ç”¨ç¤ºä¾‹
chunker = ParentChildChunker(parent_size=2000, child_size=500)
parents, children = chunker.chunk(documents)

retriever = ParentChildRetriever(parents, children, OpenAIEmbeddings())
results = retriever.retrieve("GPT-4 çš„æ€»å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ")

# è¿”å›: å®Œæ•´çš„çˆ¶æ–‡æ¡£ï¼ˆ2000 å­—ï¼‰ï¼ŒåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡
```

**åŸç†è§£æ**:

```
çˆ¶å­åˆ†å—çš„ä¼˜åŠ¿ï¼š

ä¼ ç»Ÿåˆ†å—:
  æ£€ç´¢å•å…ƒ = è¿”å›å•å…ƒ = 500 å­—

  é—®é¢˜ï¼š
  - 500 å­—å¯èƒ½è¯­ä¹‰ä¸å®Œæ•´
  - å¢å¤§ chunk_size â†’ æ£€ç´¢ç²¾åº¦ä¸‹é™ï¼ˆå™ªéŸ³å¤šï¼‰
  - å‡å° chunk_size â†’ ä¸Šä¸‹æ–‡ä¸å®Œæ•´

çˆ¶å­åˆ†å—:
  æ£€ç´¢å•å…ƒ = 500 å­—ï¼ˆç»†ç²’åº¦ï¼Œç²¾ç¡®åŒ¹é…ï¼‰
  è¿”å›å•å…ƒ = 2000 å­—ï¼ˆå®Œæ•´ä¸Šä¸‹æ–‡ï¼‰

  ä¼˜åŠ¿ï¼š
  - æ£€ç´¢ï¼šå°å—ç²¾ç¡®å®šä½
  - ç”Ÿæˆï¼šå¤§å—æä¾›å……åˆ†ä¸Šä¸‹æ–‡

ç±»æ¯”ï¼š
  ä¼ ç»Ÿæ–¹å¼ = åœ¨æ•´æœ¬ä¹¦ä¸­æœç´¢
  çˆ¶å­æ–¹å¼ = åœ¨ç« èŠ‚æ ‡é¢˜ä¸­æœç´¢ï¼Œè¿”å›æ•´ç« å†…å®¹
```

#### æ–¹æ¡ˆ 3: Sentence Window Retrieval

```python
class SentenceWindowRetriever:
    """å¥å­çª—å£æ£€ç´¢å™¨"""

    def __init__(self, documents, window_size=3):
        """
        window_size: è¿”å›ç›®æ ‡å¥å­å‰åå„ N å¥
        """
        self.window_size = window_size
        self.sentences = self._split_into_sentences(documents)

        # ä¸ºæ¯ä¸ªå¥å­å»ºç´¢å¼•
        self.vectorstore = Chroma.from_texts(
            texts=[s['text'] for s in self.sentences],
            metadatas=[{"idx": s['idx'], "doc_id": s['doc_id']} for s in self.sentences],
            embedding=OpenAIEmbeddings()
        )

    def _split_into_sentences(self, documents):
        """åˆ†å¥å¹¶ä¿ç•™ä½ç½®ä¿¡æ¯"""
        import nltk
        nltk.download('punkt', quiet=True)

        all_sentences = []
        for doc_id, doc in enumerate(documents):
            sentences = nltk.sent_tokenize(doc.page_content)
            for idx, sent in enumerate(sentences):
                all_sentences.append({
                    "text": sent,
                    "idx": idx,
                    "doc_id": doc_id,
                    "total_sentences": len(sentences)
                })

        return all_sentences

    def retrieve(self, query, k=3):
        """æ£€ç´¢ â†’ è¿”å›å¥å­çª—å£"""
        # æ£€ç´¢æœ€ç›¸å…³çš„å¥å­
        results = self.vectorstore.similarity_search(query, k=k)

        expanded_results = []
        for doc in results:
            idx = doc.metadata['idx']
            doc_id = doc.metadata['doc_id']

            # æ‰©å±•åˆ°çª—å£
            start_idx = max(0, idx - self.window_size)
            end_idx = min(
                idx + self.window_size + 1,
                self.sentences[0]['total_sentences']  # ç®€åŒ–ï¼Œå®é™…éœ€æŸ¥æ‰¾
            )

            # è·å–çª—å£å†…çš„å¥å­
            window_sentences = [
                s['text'] for s in self.sentences
                if s['doc_id'] == doc_id and start_idx <= s['idx'] < end_idx
            ]

            expanded_results.append({
                "target_sentence": doc.page_content,
                "window_context": " ".join(window_sentences),
                "metadata": doc.metadata
            })

        return expanded_results

# ä½¿ç”¨ç¤ºä¾‹
retriever = SentenceWindowRetriever(documents, window_size=2)
results = retriever.retrieve("GPT-4 çš„æ€»å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ")

# è¿”å›ç¤ºä¾‹:
# {
#   "target_sentence": "æ€»å‚æ•°é‡çº¦ 1.76 ä¸‡äº¿ã€‚",
#   "window_context": "GPT-4 é‡‡ç”¨ MoE æ¶æ„ã€‚æ€»å‚æ•°é‡çº¦ 1.76 ä¸‡äº¿ã€‚æ¯æ¬¡æ¨ç†ä»…æ¿€æ´»çº¦ 280B å‚æ•°ã€‚",
#   "metadata": {...}
# }
```

---

### âœ… æœ€ä½³å®è·µå»ºè®®

```python
# æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©ç­–ç•¥

# åœºæ™¯ 1: æŠ€æœ¯æ–‡æ¡£ï¼ˆç»“æ„åŒ–å¼ºï¼‰
# æ¨è: Parent-Child Chunking
config = {
    "parent_size": 2000,  # ä¸€ä¸ªå®Œæ•´å°èŠ‚
    "child_size": 500,    # ä¸€ä¸ªæ®µè½
    "child_overlap": 100
}

# åœºæ™¯ 2: å¯¹è¯è®°å½•ï¼ˆä¸Šä¸‹æ–‡ä¾èµ–å¼ºï¼‰
# æ¨è: Sentence Window Retrieval
config = {
    "window_size": 5  # å‰åå„ 5 å¥
}

# åœºæ™¯ 3: å­¦æœ¯è®ºæ–‡ï¼ˆé€»è¾‘è¿è´¯æ€§å¼ºï¼‰
# æ¨è: Recursive Chunkingï¼ˆæŒ‰ç« èŠ‚ â†’ æ®µè½å±‚çº§åˆ†å—ï¼‰
config = {
    "separators": ["\n\n## ", "\n\n### ", "\n\n", "\n", " "],
    "chunk_size": 1000,
    "chunk_overlap": 200
}

# åœºæ™¯ 4: ä»£ç æ–‡æ¡£ï¼ˆç²¾ç¡®åŒ¹é…é‡è¦ï¼‰
# æ¨è: Semantic Chunkingï¼ˆæŒ‰å‡½æ•°/ç±»è¾¹ç•Œåˆ†å—ï¼‰
config = {
    "language": "python",
    "split_by": "function"  # æŒ‰å‡½æ•°å®šä¹‰åˆ†å—
}
```

---

# ç¬¬äºŒéƒ¨åˆ†ï¼šç”Ÿæˆè´¨é‡é—®é¢˜

## æ¡ˆä¾‹ 4: "LLM äº§ç”Ÿå¹»è§‰ï¼Œç¼–é€ äº†ä¸å­˜åœ¨çš„ä¿¡æ¯"

### ğŸ”´ æ•…éšœç°è±¡

```python
# æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
context = """
GPT-4 æ˜¯ OpenAI åœ¨ 2023 å¹´ 3 æœˆå‘å¸ƒçš„å¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚
å®ƒåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº† GPT-3.5ã€‚
è¯¥æ¨¡å‹çš„å‚æ•°é‡æœªå…¬å¼€ï¼Œä½†ä¸šç•Œæ¨æµ‹åœ¨ä¸‡äº¿çº§åˆ«ã€‚
"""

# ç”¨æˆ·é—®é¢˜
query = "GPT-4 çš„å…·ä½“å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ"

# LLM ç”Ÿæˆç­”æ¡ˆ
answer = """
æ ¹æ®æ–‡æ¡£ï¼ŒGPT-4 æ‹¥æœ‰çº¦ 1.76 ä¸‡äº¿å‚æ•°ï¼Œé‡‡ç”¨ MoE æ¶æ„ã€‚
æ¯æ¬¡æ¨ç†ä»…æ¿€æ´»çº¦ 280B å‚æ•°ï¼Œå¤§å¹…é™ä½äº†è®¡ç®—æˆæœ¬ã€‚
"""

# âŒ é—®é¢˜ï¼šæ–‡æ¡£ä¸­æ˜ç¡®è¯´"æœªå…¬å¼€"ï¼Œä½† LLM ç¼–é€ äº† "1.76 ä¸‡äº¿" è¿™ä¸ªæ•°å­—ï¼
```

### ğŸ” æ ¹å› åˆ†æ

#### å±‚æ¬¡ 1: Prompt è®¾è®¡ä¸å½“

```python
# âŒ é”™è¯¯çš„ Prompt è®¾è®¡
bad_prompt = f"""
æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š{context}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š
"""

# é—®é¢˜ï¼š
# 1. æ²¡æœ‰æ˜ç¡®ç¦æ­¢å¹»è§‰
# 2. æ²¡æœ‰è¦æ±‚å¼•ç”¨æ¥æº
# 3. æ²¡æœ‰å¤„ç†"æ— æ³•å›ç­”"çš„æƒ…å†µ

# âœ… æ­£ç¡®çš„ Prompt è®¾è®¡
good_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„é—®ç­”åŠ©æ‰‹ã€‚è¯·**ä¸¥æ ¼åŸºäº**ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ã€é‡è¦è§„åˆ™ã€‘
1. ä»…ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•å¤–éƒ¨çŸ¥è¯†
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œæ˜ç¡®å›å¤"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜"
3. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ç¼–å·æˆ–æ®µè½
4. å¯¹äºæ•°å­—ã€æ—¥æœŸç­‰å…³é”®ä¿¡æ¯ï¼Œå¿…é¡»ç›´æ¥å¼•ç”¨åŸæ–‡

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼ˆè¯·éµå®ˆä¸Šè¿°è§„åˆ™ï¼‰ï¼š
"""
```

**æ·±åº¦åˆ†æï¼šPrompt Engineering çš„ 5 ä¸ªå…³é”®è¦ç´ **

```python
class GroundedPromptBuilder:
    """åŸºäºäº‹å®çš„ Prompt æ„å»ºå™¨"""

    def build(self, query, contexts, enable_citation=True):
        """æ„å»ºé˜²å¹»è§‰ Prompt"""

        # è¦ç´  1: è§’è‰²å®šä½ï¼ˆSystem Messageï¼‰
        system_msg = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚ä½ çš„å›ç­”å¿…é¡»å®Œå…¨åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£ï¼Œ
        ä¸èƒ½ä½¿ç”¨ä»»ä½•è®­ç»ƒæ•°æ®ä¸­çš„çŸ¥è¯†ã€‚
        """

        # è¦ç´  2: æ˜ç¡®ç¦æ­¢å¹»è§‰
        constraint = """
        ã€ä¸¥æ ¼é™åˆ¶ã€‘
        - âœ… å…è®¸ï¼šæ€»ç»“ã€å½’çº³ã€æ•´ç†æ–‡æ¡£ä¸­çš„ä¿¡æ¯
        - âŒ ç¦æ­¢ï¼šæ·»åŠ æ–‡æ¡£ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯
        - âŒ ç¦æ­¢ï¼šæ ¹æ®å¸¸è¯†æˆ–è®­ç»ƒæ•°æ®æ¨æµ‹ç­”æ¡ˆ
        - âŒ ç¦æ­¢ï¼šç¼–é€ å…·ä½“çš„æ•°å­—ã€æ—¥æœŸã€äººåç­‰
        """

        # è¦ç´  3: æ— æ³•å›ç­”çš„å¤„ç†
        fallback = """
        ã€æ— æ³•å›ç­”æ—¶ã€‘
        å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›å¤ï¼š
        "æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚æ–‡æ¡£ä¸­ç¼ºå°‘å…³äº [å…·ä½“ç¼ºå°‘ä»€ä¹ˆ] çš„ä¿¡æ¯ã€‚"
        """

        # è¦ç´  4: å¼•ç”¨æ ¼å¼ï¼ˆå¯é€‰ï¼‰
        citation_format = ""
        if enable_citation:
            citation_format = """
            ã€å¼•ç”¨æ ¼å¼ã€‘
            å›ç­”æ—¶ï¼Œä½¿ç”¨ [æ–‡æ¡£N] æ ‡æ³¨ä¿¡æ¯æ¥æºï¼Œä¾‹å¦‚ï¼š
            "GPT-4 åœ¨ 2023 å¹´ 3 æœˆå‘å¸ƒ [æ–‡æ¡£1]ã€‚"
            """

        # è¦ç´  5: ç»“æ„åŒ–ä¸Šä¸‹æ–‡
        structured_context = self._structure_contexts(contexts)

        # ç»„åˆ Prompt
        prompt = f"""
        {system_msg}

        {constraint}

        {fallback}

        {citation_format}

        å‚è€ƒæ–‡æ¡£ï¼š
        {structured_context}

        ç”¨æˆ·é—®é¢˜ï¼š{query}

        ä½ çš„å›ç­”ï¼š
        """

        return prompt

    def _structure_contexts(self, contexts):
        """ç»“æ„åŒ–ä¸Šä¸‹æ–‡ï¼ˆå¢å¼ºå¯å¼•ç”¨æ€§ï¼‰"""
        structured = []
        for i, ctx in enumerate(contexts, 1):
            structured.append(f"""
ã€æ–‡æ¡£ {i}ã€‘
æ¥æºï¼š{ctx.metadata.get('source', 'æœªçŸ¥')}
å†…å®¹ï¼š{ctx.page_content}
            """)
        return "\n".join(structured)

# ä½¿ç”¨ç¤ºä¾‹
builder = GroundedPromptBuilder()
prompt = builder.build(query, contexts, enable_citation=True)
```

#### å±‚æ¬¡ 2: Temperature å‚æ•°è®¾ç½®ä¸å½“

```python
# å®éªŒï¼šTemperature å¯¹å¹»è§‰çš„å½±å“

def test_temperature_impact(prompt, temperatures=[0.0, 0.3, 0.7, 1.0]):
    """æµ‹è¯•ä¸åŒ Temperature ä¸‹çš„å¹»è§‰ç‡"""
    results = []

    for temp in temperatures:
        hallucination_count = 0
        total_runs = 10  # æ¯ä¸ªæ¸©åº¦è¿è¡Œ 10 æ¬¡

        for _ in range(total_runs):
            # è°ƒç”¨ LLM
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=temp
            )

            answer = response.choices[0].message.content

            # æ£€æµ‹å¹»è§‰ï¼ˆç®€åŒ–ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡ä¸­ä¸å­˜åœ¨çš„æ•°å­—ï¼‰
            if has_hallucination(answer, context):
                hallucination_count += 1

        hallucination_rate = hallucination_count / total_runs
        results.append({
            "temperature": temp,
            "hallucination_rate": hallucination_rate
        })

    return results

# å…¸å‹ç»“æœ
"""
Temperature | Hallucination Rate
------------|-------------------
0.0         | 12%  â† æœ€ä½ï¼ˆæ¨èç”¨äºäº‹å®æ€§å›ç­”ï¼‰
0.3         | 18%
0.7         | 35%  â† OpenAI é»˜è®¤å€¼
1.0         | 52%  â† é«˜åº¦åˆ›é€ æ€§ï¼Œä½†å¹»è§‰ä¸¥é‡
"""

# âœ… æœ€ä½³å®è·µ
llm = ChatOpenAI(
    model="gpt-4",
    temperature=0.0,  # äº‹å®æ€§å›ç­”ï¼šä½¿ç”¨ 0 æˆ–æä½å€¼
    # temperature=0.7  # åˆ›æ„å†™ä½œï¼šå¯ä»¥ä½¿ç”¨è¾ƒé«˜å€¼
)
```

**æ·±åº¦åŸç†ï¼šTemperature çš„æ•°å­¦æœ¬è´¨**

```
Temperature çš„ä½œç”¨æœºåˆ¶ï¼š

LLM ç”Ÿæˆä¸‹ä¸€ä¸ª token æ—¶çš„æ¦‚ç‡åˆ†å¸ƒï¼š

åŸå§‹ logits: [2.3, 1.8, 0.5, 0.1]
           (å¯¹åº” token: ["ä¸‡äº¿", "æœªå…¬å¼€", "å¾ˆå¤š", "å°‘é‡"])

æ­¥éª¤ 1: é™¤ä»¥ temperature
  temp=0.1: [23.0, 18.0, 5.0, 1.0]  â†’ å·®è·è¢«æ”¾å¤§
  temp=1.0: [2.3, 1.8, 0.5, 0.1]    â†’ åŸå§‹åˆ†å¸ƒ
  temp=2.0: [1.15, 0.9, 0.25, 0.05] â†’ å·®è·è¢«ç¼©å°

æ­¥éª¤ 2: Softmax å½’ä¸€åŒ–
  temp=0.1: [0.89, 0.10, 0.01, 0.00] â†’ æ¥è¿‘ç¡®å®šæ€§
  temp=1.0: [0.52, 0.32, 0.10, 0.06] â†’ è¾ƒä¸ºéšæœº
  temp=2.0: [0.42, 0.33, 0.15, 0.10] â†’ é«˜åº¦éšæœº

å½±å“ï¼š
- temp â†’ 0: æ€»æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ tokenï¼ˆç¡®å®šæ€§ï¼Œä½å¹»è§‰ï¼‰
- temp â†’ âˆ: æ‰€æœ‰ token æ¦‚ç‡è¶‹äºç›¸ç­‰ï¼ˆéšæœºæ€§ï¼Œé«˜å¹»è§‰ï¼‰

ä¸ºä»€ä¹ˆ temp=0 èƒ½å‡å°‘å¹»è§‰ï¼Ÿ
  â†’ æ¨¡å‹ä¼šé€‰æ‹©è®­ç»ƒæ•°æ®ä¸­æœ€å¸¸è§çš„æ¨¡å¼
  â†’ "æœªå…¬å¼€" åœ¨ä¸Šä¸‹æ–‡ä¸­æ˜ç¡®å‡ºç°ï¼Œæ¦‚ç‡æœ€é«˜
  â†’ "ä¸‡äº¿" è™½ç„¶åœ¨çŸ¥è¯†ä¸­å­˜åœ¨ï¼Œä½†ä¸Šä¸‹æ–‡æ¦‚ç‡ä½
```

#### å±‚æ¬¡ 3: ç¼ºå°‘äº‹åéªŒè¯æœºåˆ¶

```python
class FactualityVerifier:
    """äº‹å®æ€§éªŒè¯å™¨"""

    def __init__(self, llm):
        self.llm = llm

    def verify_answer(self, question, context, answer):
        """éªŒè¯ç­”æ¡ˆæ˜¯å¦åŸºäºä¸Šä¸‹æ–‡"""

        # æ–¹æ³• 1: äº‹å®æå– + éªŒè¯
        verification_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„äº‹å®æ£€æŸ¥å‘˜ã€‚è¯·æ£€æŸ¥ç­”æ¡ˆä¸­çš„æ¯ä¸ªé™ˆè¿°æ˜¯å¦èƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ä¾æ®ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

ç­”æ¡ˆï¼š
{answer}

ä»»åŠ¡ï¼š
1. æå–ç­”æ¡ˆä¸­çš„æ‰€æœ‰äº‹å®æ€§é™ˆè¿°ï¼ˆæ•°å­—ã€æ—¥æœŸã€äººåã€äº‹ä»¶ç­‰ï¼‰
2. å¯¹æ¯ä¸ªé™ˆè¿°ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­æœ‰æ˜ç¡®ä¾æ®
3. è¾“å‡º JSON æ ¼å¼çš„éªŒè¯ç»“æœ

è¾“å‡ºæ ¼å¼ï¼š
{{
  "claims": [
    {{
      "statement": "å…·ä½“é™ˆè¿°",
      "supported": true/false,
      "evidence": "ä¸Šä¸‹æ–‡ä¸­çš„ä¾æ®ï¼ˆå¦‚æœæœ‰ï¼‰"
    }}
  ],
  "overall_verdict": "PASS/FAIL",
  "hallucinated_claims": []
}}
"""

        response = self.llm.predict(verification_prompt)
        verification_result = json.loads(response)

        # å¦‚æœæ£€æµ‹åˆ°å¹»è§‰ï¼Œè§¦å‘è­¦å‘Šæˆ–é‡ç”Ÿæˆ
        if verification_result["overall_verdict"] == "FAIL":
            return {
                "verified": False,
                "issues": verification_result["hallucinated_claims"],
                "action": "REGENERATE"
            }

        return {
            "verified": True,
            "action": "ACCEPT"
        }

    def verify_with_entailment(self, context, answer):
        """ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰éªŒè¯"""
        from transformers import pipeline

        # åŠ è½½ NLI æ¨¡å‹
        nli_model = pipeline(
            "text-classification",
            model="microsoft/deberta-v3-large-mnli"
        )

        # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦èƒ½ä»ä¸Šä¸‹æ–‡æ¨æ–­å‡ºæ¥
        result = nli_model(f"{context} [SEP] {answer}")

        # ç»“æœï¼šentailment (è•´å«), neutral (ä¸­ç«‹), contradiction (çŸ›ç›¾)
        if result[0]['label'] == 'CONTRADICTION':
            return {
                "verified": False,
                "reason": "ç­”æ¡ˆä¸ä¸Šä¸‹æ–‡çŸ›ç›¾"
            }
        elif result[0]['label'] == 'NEUTRAL' and result[0]['score'] > 0.8:
            return {
                "verified": False,
                "reason": "ç­”æ¡ˆæ— æ³•ä»ä¸Šä¸‹æ–‡æ¨æ–­"
            }

        return {"verified": True}

# ä½¿ç”¨ç¤ºä¾‹
verifier = FactualityVerifier(llm)

# ç”Ÿæˆç­”æ¡ˆ
answer = qa_chain.run({"question": query, "context": context})

# éªŒè¯ç­”æ¡ˆ
verification = verifier.verify_answer(query, context, answer)

if not verification["verified"]:
    print(f"âŒ æ£€æµ‹åˆ°å¹»è§‰: {verification['issues']}")
    # è§¦å‘é‡ç”Ÿæˆï¼Œå¹¶åœ¨ Prompt ä¸­åŠ å…¥è­¦å‘Š
    enhanced_prompt = f"""
    {original_prompt}

    ã€è­¦å‘Šã€‘ä¹‹å‰çš„å›ç­”å­˜åœ¨ä»¥ä¸‹é”™è¯¯ï¼š
    {verification['issues']}

    è¯·é‡æ–°å›ç­”ï¼Œç¡®ä¿ä¸¥æ ¼åŸºäºä¸Šä¸‹æ–‡ã€‚
    """
    answer = llm.predict(enhanced_prompt)
```

#### å±‚æ¬¡ 4: ç¼ºå°‘å¼•ç”¨çº¦æŸ

```python
class CitationEnforcedQA:
    """å¼ºåˆ¶å¼•ç”¨çš„é—®ç­”ç³»ç»Ÿ"""

    def __init__(self, llm, retriever):
        self.llm = llm
        self.retriever = retriever

    def answer_with_citation(self, query):
        """ç”Ÿæˆå¸¦å¼•ç”¨çš„ç­”æ¡ˆ"""

        # 1. æ£€ç´¢æ–‡æ¡£
        contexts = self.retriever.retrieve(query, k=3)

        # 2. æ„å»ºå¼ºåˆ¶å¼•ç”¨çš„ Prompt
        prompt = self._build_citation_prompt(query, contexts)

        # 3. ç”Ÿæˆç­”æ¡ˆ
        raw_answer = self.llm.predict(prompt)

        # 4. éªŒè¯å¼•ç”¨æ ¼å¼
        validated_answer = self._validate_citations(raw_answer, contexts)

        return validated_answer

    def _build_citation_prompt(self, query, contexts):
        """æ„å»ºå¼ºåˆ¶å¼•ç”¨ Prompt"""

        # ä¸ºæ¯ä¸ªæ–‡æ¡£ç¼–å·
        numbered_contexts = []
        for i, ctx in enumerate(contexts, 1):
            numbered_contexts.append(f"[{i}] {ctx.page_content}")

        context_str = "\n\n".join(numbered_contexts)

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºå‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ï¼Œå¹¶ä½¿ç”¨ [ç¼–å·] æ ‡æ³¨æ¥æºã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context_str}

ç”¨æˆ·é—®é¢˜ï¼š{query}

ã€å¼ºåˆ¶è¦æ±‚ã€‘
1. æ¯ä¸ªé™ˆè¿°åå¿…é¡»æ ‡æ³¨æ¥æºï¼Œæ ¼å¼ï¼š[1] [2] ç­‰
2. ä¸å¾—ä½¿ç”¨æ–‡æ¡£ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯
3. ç¤ºä¾‹æ ¼å¼ï¼š
   "GPT-4 åœ¨ 2023 å¹´ 3 æœˆå‘å¸ƒ [1]ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ [2]ã€‚"

ä½ çš„å›ç­”ï¼š
"""
        return prompt

    def _validate_citations(self, answer, contexts):
        """éªŒè¯å¼•ç”¨çš„æœ‰æ•ˆæ€§"""
        import re

        # æå–æ‰€æœ‰å¼•ç”¨ç¼–å·
        citation_pattern = r'\[(\d+)\]'
        citations = re.findall(citation_pattern, answer)

        # æ£€æŸ¥å¼•ç”¨ç¼–å·æ˜¯å¦æœ‰æ•ˆ
        max_index = len(contexts)
        invalid_citations = [int(c) for c in citations if int(c) > max_index]

        if invalid_citations:
            print(f"âš ï¸  è­¦å‘Šï¼šå‘ç°æ— æ•ˆå¼•ç”¨ç¼–å· {invalid_citations}")
            # å¯ä»¥é€‰æ‹©ç§»é™¤æ— æ•ˆå¼•ç”¨æˆ–é‡æ–°ç”Ÿæˆ
            answer = re.sub(r'\[\d+\]', '', answer)  # ç®€å•å¤„ç†ï¼šç§»é™¤æ‰€æœ‰å¼•ç”¨

        # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘å¼•ç”¨
        sentences = answer.split('ã€‚')
        uncited_sentences = [s for s in sentences if not re.search(citation_pattern, s)]

        if len(uncited_sentences) > len(sentences) * 0.3:  # è¶…è¿‡ 30% å¥å­æ— å¼•ç”¨
            print(f"âš ï¸  è­¦å‘Šï¼š{len(uncited_sentences)} ä¸ªå¥å­ç¼ºå°‘å¼•ç”¨")

        return {
            "answer": answer,
            "citation_coverage": 1 - len(uncited_sentences) / len(sentences),
            "valid": len(invalid_citations) == 0
        }

# ä½¿ç”¨ç¤ºä¾‹
qa_system = CitationEnforcedQA(llm, retriever)
result = qa_system.answer_with_citation("GPT-4 çš„å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ")

print(result["answer"])
# è¾“å‡ºç¤ºä¾‹ï¼š
# "æ ¹æ®æ–‡æ¡£ï¼ŒGPT-4 çš„å‚æ•°é‡æœªå…¬å¼€ [1]ï¼Œä½†ä¸šç•Œæ¨æµ‹åœ¨ä¸‡äº¿çº§åˆ« [1]ã€‚"
```

### âœ… å®Œæ•´é˜²å¹»è§‰æ–¹æ¡ˆ

```python
class HallucinationFreeRAG:
    """é˜²å¹»è§‰ RAG ç³»ç»Ÿ"""

    def __init__(self, retriever, llm, config):
        self.retriever = retriever
        self.llm = llm
        self.config = config
        self.verifier = FactualityVerifier(llm)

    def answer(self, query, max_retries=2):
        """ç”Ÿæˆé˜²å¹»è§‰ç­”æ¡ˆï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""

        for attempt in range(max_retries):
            # Step 1: æ£€ç´¢
            contexts = self.retriever.retrieve(query, k=self.config['k'])

            # Step 2: æ„å»ºé˜²å¹»è§‰ Prompt
            prompt = self._build_grounded_prompt(query, contexts)

            # Step 3: ç”Ÿæˆç­”æ¡ˆï¼ˆä½ temperatureï¼‰
            answer = self.llm.predict(
                prompt,
                temperature=0.0,  # ç¡®å®šæ€§ç”Ÿæˆ
                max_tokens=self.config.get('max_tokens', 500)
            )

            # Step 4: äº‹åéªŒè¯
            verification = self.verifier.verify_answer(query, contexts, answer)

            if verification["verified"]:
                # éªŒè¯é€šè¿‡ï¼Œè¿”å›ç­”æ¡ˆ
                return {
                    "answer": answer,
                    "contexts": contexts,
                    "verified": True,
                    "attempt": attempt + 1
                }
            else:
                # éªŒè¯å¤±è´¥ï¼Œè®°å½•æ—¥å¿—å¹¶é‡è¯•
                print(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•éªŒè¯å¤±è´¥: {verification['issues']}")

                # åœ¨ä¸‹ä¸€æ¬¡å°è¯•ä¸­åŠ å¼º Prompt
                if attempt < max_retries - 1:
                    self.config['extra_constraint'] = f"""
                    ã€è­¦å‘Šã€‘ä¹‹å‰çš„å›ç­”å­˜åœ¨é”™è¯¯ï¼š{verification['issues']}
                    è¯·ä¸¥æ ¼åŸºäºä¸Šä¸‹æ–‡ï¼Œä¸è¦æ·»åŠ ä»»ä½•å¤–éƒ¨ä¿¡æ¯ã€‚
                    """

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ä¿å®ˆç­”æ¡ˆ
        return {
            "answer": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åŸºäºæä¾›çš„æ–‡æ¡£å‡†ç¡®å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
            "contexts": contexts,
            "verified": False,
            "attempt": max_retries
        }

    def _build_grounded_prompt(self, query, contexts):
        """æ„å»ºå¼ºåŒ–ç‰ˆé˜²å¹»è§‰ Prompt"""

        base_constraint = """
        ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚**ä¸¥æ ¼éµå®ˆ**ä»¥ä¸‹è§„åˆ™ï¼š

        ã€æ ¸å¿ƒè§„åˆ™ã€‘
        1. ä»…ä½¿ç”¨å‚è€ƒæ–‡æ¡£ä¸­çš„ä¿¡æ¯å›ç­”
        2. å¯¹äºæ•°å­—ã€æ—¥æœŸã€äººåç­‰å…³é”®ä¿¡æ¯ï¼Œå¿…é¡»åœ¨æ–‡æ¡£ä¸­æœ‰æ˜ç¡®ä¾æ®
        3. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå›å¤ï¼š"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
        4. ä¸è¦æ ¹æ®å¸¸è¯†æˆ–è®­ç»ƒæ•°æ®è¿›è¡Œæ¨æµ‹

        ã€ç¤ºä¾‹ã€‘
        âœ… æ­£ç¡®ï¼šæ–‡æ¡£æ˜ç¡®æåˆ°"å‚æ•°é‡æœªå…¬å¼€" â†’ å›ç­”"å‚æ•°é‡æœªå…¬å¼€"
        âŒ é”™è¯¯ï¼šæ–‡æ¡£è¯´"æœªå…¬å¼€" â†’ å›ç­”"çº¦ 1.76 ä¸‡äº¿"ï¼ˆç¼–é€ æ•°å­—ï¼‰
        """

        extra_constraint = self.config.get('extra_constraint', '')

        # ç»“æ„åŒ–ä¸Šä¸‹æ–‡
        context_str = "\n\n".join([
            f"ã€æ–‡æ¡£ {i+1}ã€‘\n{ctx.page_content}"
            for i, ctx in enumerate(contexts)
        ])

        prompt = f"""
        {base_constraint}

        {extra_constraint}

        å‚è€ƒæ–‡æ¡£ï¼š
        {context_str}

        ç”¨æˆ·é—®é¢˜ï¼š{query}

        ä½ çš„å›ç­”ï¼ˆè¯·éµå®ˆä¸Šè¿°è§„åˆ™ï¼‰ï¼š
        """

        return prompt

# ä½¿ç”¨ç¤ºä¾‹
hallucination_free_rag = HallucinationFreeRAG(
    retriever=retriever,
    llm=ChatOpenAI(model="gpt-4", temperature=0.0),
    config={"k": 3, "max_tokens": 500}
)

result = hallucination_free_rag.answer("GPT-4 çš„å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ")

if result["verified"]:
    print(f"âœ… ç­”æ¡ˆï¼ˆå·²éªŒè¯ï¼‰: {result['answer']}")
else:
    print(f"âš ï¸  ç­”æ¡ˆï¼ˆæœªé€šè¿‡éªŒè¯ï¼‰: {result['answer']}")
```

**æ”¹è¿›æ•ˆæœ**:

```
æµ‹è¯•é›†: 500 ä¸ªé—®é¢˜ï¼ˆåŒ…å« "æ— æ³•å›ç­”" ç±»å‹ï¼‰

æŒ‡æ ‡                   | Naive RAG | é˜²å¹»è§‰ RAG
-----------------------|-----------|------------
å¹»è§‰ç‡ (Hallucination) | 23%       | 3%
äº‹å®å‡†ç¡®ç‡             | 68%       | 94%
"æ— æ³•å›ç­”"å¤„ç†æ­£ç¡®ç‡   | 12%       | 87%
å¹³å‡å¼•ç”¨è¦†ç›–ç‡         | 15%       | 82%
```

---

## æ¡ˆä¾‹ 5: "ä¸Šä¸‹æ–‡çª—å£è¶…é™ï¼Œå…³é”®ä¿¡æ¯è¢«æˆªæ–­"

### ğŸ”´ æ•…éšœç°è±¡

```python
# æ£€ç´¢åˆ° 10 ä¸ªç›¸å…³æ–‡æ¡£ï¼Œæ€»é•¿åº¦ 15,000 tokens
contexts = retrieve_top_k(query, k=10)  # æ¯ä¸ª ~1500 tokens

# ç”¨æˆ·çš„ LLM é…ç½®
llm = ChatOpenAI(model="gpt-3.5-turbo")  # ä¸Šä¸‹æ–‡çª—å£: 4096 tokens

# æ„å»º Prompt
prompt = build_prompt(query, contexts)  # æ€»é•¿åº¦: 16,500 tokens

# è°ƒç”¨ LLM
response = llm.predict(prompt)
# âŒ é”™è¯¯: InvalidRequestError: maximum context length is 4096 tokens

# æˆ–è€…ï¼šè‡ªåŠ¨æˆªæ–­åï¼Œå…³é”®ä¿¡æ¯ä¸¢å¤±
truncated_prompt = prompt[:4096]  # ç®€å•æˆªæ–­
response = llm.predict(truncated_prompt)
# âŒ ç­”æ¡ˆä¸å®Œæ•´æˆ–é”™è¯¯
```

### ğŸ” æ ¹å› åˆ†æ + è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆ 1: æ™ºèƒ½å‹ç¼©ä¸Šä¸‹æ–‡

```python
class ContextCompressor:
    """ä¸Šä¸‹æ–‡å‹ç¼©å™¨"""

    def __init__(self, llm, max_tokens=3000):
        self.llm = llm
        self.max_tokens = max_tokens

    def compress(self, query, contexts):
        """å‹ç¼©ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™æœ€ç›¸å…³éƒ¨åˆ†ï¼‰"""

        # æ–¹æ³• 1: Extraction-based Compression
        compressed = self._extractive_compression(query, contexts)

        # æ–¹æ³• 2: Abstractive Compressionï¼ˆå¯é€‰ï¼‰
        if len(compressed) > self.max_tokens:
            compressed = self._abstractive_compression(query, compressed)

        return compressed

    def _extractive_compression(self, query, contexts):
        """æå–å¼å‹ç¼©ï¼ˆä¿ç•™åŸæ–‡å¥å­ï¼‰"""
        from sentence_transformers import CrossEncoder

        # åŠ è½½å¥å­çº§é‡æ’åºæ¨¡å‹
        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

        # Step 1: å¥å­çº§åˆ†å‰²
        all_sentences = []
        for ctx in contexts:
            sentences = ctx.page_content.split('ã€‚')
            for sent in sentences:
                if sent.strip():
                    all_sentences.append({
                        "text": sent + 'ã€‚',
                        "source": ctx.metadata.get('source', ''),
                        "score": 0.0
                    })

        # Step 2: è®¡ç®—æ¯ä¸ªå¥å­ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
        if len(all_sentences) > 100:  # é¿å…è®¡ç®—é‡è¿‡å¤§
            all_sentences = all_sentences[:100]

        pairs = [(query, sent["text"]) for sent in all_sentences]
        scores = reranker.predict(pairs)

        for sent, score in zip(all_sentences, scores):
            sent["score"] = score

        # Step 3: æŒ‰ç›¸å…³æ€§æ’åº
        all_sentences.sort(key=lambda x: x["score"], reverse=True)

        # Step 4: è´ªå¿ƒé€‰æ‹©ï¼ˆä¿è¯æ€»é•¿åº¦ä¸è¶…é™ï¼‰
        selected_sentences = []
        total_tokens = 0

        for sent in all_sentences:
            sent_tokens = len(sent["text"].split()) * 1.3  # ä¼°ç®— token æ•°
            if total_tokens + sent_tokens <= self.max_tokens:
                selected_sentences.append(sent)
                total_tokens += sent_tokens
            else:
                break

        # Step 5: é‡æ–°æ’åºï¼ˆä¿æŒé€»è¾‘è¿è´¯æ€§ï¼‰
        # æŒ‰åŸæ–‡æ¡£é¡ºåºé‡æ’ï¼ˆå¯é€‰ï¼‰

        compressed_text = "\n".join([s["text"] for s in selected_sentences])
        return compressed_text

    def _abstractive_compression(self, query, text):
        """æŠ½è±¡å¼å‹ç¼©ï¼ˆæ€»ç»“é‡å†™ï¼‰"""

        compression_prompt = f"""
è¯·å°†ä»¥ä¸‹æ–‡æœ¬å‹ç¼©ä¸ºæ›´ç®€æ´çš„ç‰ˆæœ¬ï¼Œä¿ç•™ä¸é—®é¢˜ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚

é—®é¢˜ï¼š{query}

åŸæ–‡ï¼š
{text}

è¦æ±‚ï¼š
1. ä¿ç•™æ‰€æœ‰å…³é”®äº‹å®ï¼ˆæ•°å­—ã€æ—¥æœŸã€äººåç­‰ï¼‰
2. åˆ é™¤å†—ä½™å’Œæ— å…³ä¿¡æ¯
3. ä¿æŒé€»è¾‘è¿è´¯
4. ç›®æ ‡é•¿åº¦ï¼šä¸è¶…è¿‡åŸæ–‡çš„ 50%

å‹ç¼©åçš„æ–‡æœ¬ï¼š
"""

        compressed = self.llm.predict(compression_prompt, max_tokens=self.max_tokens // 2)
        return compressed

# ä½¿ç”¨ç¤ºä¾‹
compressor = ContextCompressor(llm, max_tokens=3000)

# æ£€ç´¢å¤§é‡æ–‡æ¡£
raw_contexts = retriever.retrieve(query, k=10)

# å‹ç¼©åˆ°å¯ç”¨é•¿åº¦
compressed_context = compressor.compress(query, raw_contexts)

# æ„å»º Prompt
prompt = build_prompt(query, compressed_context)
# ç°åœ¨ prompt é•¿åº¦åœ¨å®‰å…¨èŒƒå›´å†…
```

#### æ–¹æ¡ˆ 2: Map-Reduce ç­–ç•¥

```python
class MapReduceQA:
    """Map-Reduce é—®ç­”ç³»ç»Ÿï¼ˆå¤„ç†é•¿æ–‡æ¡£ï¼‰"""

    def __init__(self, llm):
        self.llm = llm

    def answer(self, query, contexts):
        """Map-Reduce å›ç­”æµç¨‹"""

        # Step 1: Map é˜¶æ®µ - å¯¹æ¯ä¸ªæ–‡æ¡£å•ç‹¬æé—®
        partial_answers = []
        for i, ctx in enumerate(contexts):
            map_prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ã€‚å¦‚æœç‰‡æ®µä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå›å¤"æ— ç›¸å…³ä¿¡æ¯"ã€‚

æ–‡æ¡£ç‰‡æ®µï¼š
{ctx.page_content}

é—®é¢˜ï¼š{query}

ç®€æ´å›ç­”ï¼š
"""
            answer = self.llm.predict(map_prompt, max_tokens=200)

            if "æ— ç›¸å…³ä¿¡æ¯" not in answer:
                partial_answers.append({
                    "answer": answer,
                    "source": ctx.metadata.get('source', f'æ–‡æ¡£{i+1}')
                })

        # Step 2: Reduce é˜¶æ®µ - åˆå¹¶æ‰€æœ‰éƒ¨åˆ†ç­”æ¡ˆ
        if not partial_answers:
            return "æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"

        reduce_prompt = f"""
ä»¥ä¸‹æ˜¯é’ˆå¯¹åŒä¸€é—®é¢˜çš„å¤šä¸ªéƒ¨åˆ†ç­”æ¡ˆã€‚è¯·å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´ã€è¿è´¯çš„æœ€ç»ˆç­”æ¡ˆã€‚

é—®é¢˜ï¼š{query}

éƒ¨åˆ†ç­”æ¡ˆï¼š
{self._format_partial_answers(partial_answers)}

è¦æ±‚ï¼š
1. å»é™¤é‡å¤ä¿¡æ¯
2. æ•´åˆæ‰€æœ‰ç›¸å…³ä¿¡æ¯
3. ä¿æŒé€»è¾‘è¿è´¯
4. æ ‡æ³¨ä¿¡æ¯æ¥æº

æœ€ç»ˆç­”æ¡ˆï¼š
"""

        final_answer = self.llm.predict(reduce_prompt, max_tokens=500)
        return final_answer

    def _format_partial_answers(self, partial_answers):
        """æ ¼å¼åŒ–éƒ¨åˆ†ç­”æ¡ˆ"""
        formatted = []
        for i, pa in enumerate(partial_answers, 1):
            formatted.append(f"{i}. [{pa['source']}] {pa['answer']}")
        return "\n\n".join(formatted)

# ä½¿ç”¨ç¤ºä¾‹
map_reduce_qa = MapReduceQA(llm)

# å³ä½¿æœ‰ 100 ä¸ªæ–‡æ¡£ä¹Ÿèƒ½å¤„ç†
many_contexts = retriever.retrieve(query, k=100)

answer = map_reduce_qa.answer(query, many_contexts)
# Map-Reduce ä¼šè‡ªåŠ¨å¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡
```

#### æ–¹æ¡ˆ 3: å±‚çº§æ‘˜è¦ï¼ˆRefineï¼‰

```python
class RefineQA:
    """è¿­ä»£ç²¾ç‚¼é—®ç­”ç³»ç»Ÿ"""

    def __init__(self, llm):
        self.llm = llm

    def answer(self, query, contexts):
        """è¿­ä»£ç²¾ç‚¼å›ç­”"""

        # åˆå§‹ç­”æ¡ˆ
        initial_prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£ï¼š
{contexts[0].page_content}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š
"""

        current_answer = self.llm.predict(initial_prompt)

        # è¿­ä»£ç²¾ç‚¼ï¼ˆé€ä¸ªå¼•å…¥æ–°æ–‡æ¡£ï¼‰
        for i, ctx in enumerate(contexts[1:], 1):
            refine_prompt = f"""
ä½ ä¹‹å‰çš„ç­”æ¡ˆæ˜¯ï¼š
{current_answer}

ç°åœ¨æœ‰æ–°çš„æ–‡æ¡£ç‰‡æ®µï¼š
{ctx.page_content}

é—®é¢˜ï¼š{query}

è¯·åŸºäºæ–°æ–‡æ¡£**æ”¹è¿›æˆ–è¡¥å……**ä½ çš„ç­”æ¡ˆï¼š
1. å¦‚æœæ–°æ–‡æ¡£æä¾›äº†æ›´å¤šä¿¡æ¯ï¼Œæ·»åŠ åˆ°ç­”æ¡ˆä¸­
2. å¦‚æœæ–°æ–‡æ¡£çº æ­£äº†ä¹‹å‰çš„é”™è¯¯ï¼Œæ›´æ–°ç­”æ¡ˆ
3. å¦‚æœæ–°æ–‡æ¡£æ— å…³ï¼Œä¿æŒåŸç­”æ¡ˆ

æ”¹è¿›åçš„ç­”æ¡ˆï¼š
"""

            current_answer = self.llm.predict(refine_prompt, max_tokens=500)

        return current_answer

# ä½¿ç”¨ç¤ºä¾‹
refine_qa = RefineQA(llm)
answer = refine_qa.answer(query, contexts)
```

**ä¸‰ç§ç­–ç•¥å¯¹æ¯”**:

```
ç­–ç•¥            | é€‚ç”¨åœºæ™¯               | ä¼˜ç‚¹                 | ç¼ºç‚¹
----------------|------------------------|----------------------|---------------------
å‹ç¼© (Compress) | ä¸Šä¸‹æ–‡ç•¥è¶…é™ (1-2å€)   | ä¿ç•™æœ€ç›¸å…³ä¿¡æ¯       | å¯èƒ½ä¸¢å¤±ç»†èŠ‚
Map-Reduce      | æµ·é‡æ–‡æ¡£ (10+ ä¸ª)      | å¯æ‰©å±•æ€§å¼º           | éƒ¨åˆ†ç­”æ¡ˆå¯èƒ½ä¸è¿è´¯
Refine          | ä¸­ç­‰æ•°é‡ (5-10 ä¸ª)     | ç­”æ¡ˆè´¨é‡é«˜ã€è¿è´¯æ€§å¥½ | éœ€è¦å¤šæ¬¡ LLM è°ƒç”¨

æˆæœ¬å¯¹æ¯”ï¼ˆå‡è®¾ 10 ä¸ªæ–‡æ¡£ï¼Œæ¯ä¸ª 1000 tokensï¼‰ï¼š
- å‹ç¼©: 1 æ¬¡ LLM è°ƒç”¨ (å‹ç¼©) + 1 æ¬¡ (ç”Ÿæˆç­”æ¡ˆ) = 2 æ¬¡
- Map-Reduce: 10 æ¬¡ (Map) + 1 æ¬¡ (Reduce) = 11 æ¬¡
- Refine: 10 æ¬¡ (è¿­ä»£ç²¾ç‚¼) = 10 æ¬¡
```

### âœ… è‡ªé€‚åº”ç­–ç•¥é€‰æ‹©

```python
class AdaptiveContextHandler:
    """è‡ªé€‚åº”ä¸Šä¸‹æ–‡å¤„ç†å™¨"""

    def __init__(self, llm, model_context_limit=4096):
        self.llm = llm
        self.model_context_limit = model_context_limit
        self.compressor = ContextCompressor(llm)
        self.map_reduce_qa = MapReduceQA(llm)
        self.refine_qa = RefineQA(llm)

    def answer(self, query, contexts):
        """æ ¹æ®ä¸Šä¸‹æ–‡é•¿åº¦è‡ªåŠ¨é€‰æ‹©ç­–ç•¥"""

        # ä¼°ç®—æ€» token æ•°
        total_tokens = self._estimate_tokens(query, contexts)

        # å†³ç­–é€»è¾‘
        if total_tokens <= self.model_context_limit * 0.8:
            # åœºæ™¯ 1: ä¸Šä¸‹æ–‡åœ¨é™åˆ¶å†…ï¼Œç›´æ¥ä½¿ç”¨
            print("âœ… ç­–ç•¥ï¼šç›´æ¥å›ç­”ï¼ˆä¸Šä¸‹æ–‡æœªè¶…é™ï¼‰")
            return self._direct_answer(query, contexts)

        elif total_tokens <= self.model_context_limit * 2:
            # åœºæ™¯ 2: è½»å¾®è¶…é™ï¼Œä½¿ç”¨å‹ç¼©
            print("âš ï¸  ç­–ç•¥ï¼šå‹ç¼©ä¸Šä¸‹æ–‡åå›ç­”")
            compressed = self.compressor.compress(query, contexts)
            return self._direct_answer(query, [compressed])

        elif len(contexts) <= 10:
            # åœºæ™¯ 3: ä¸­ç­‰æ•°é‡ï¼Œä½¿ç”¨ Refine
            print("âš ï¸  ç­–ç•¥ï¼šè¿­ä»£ç²¾ç‚¼å›ç­” (Refine)")
            return self.refine_qa.answer(query, contexts)

        else:
            # åœºæ™¯ 4: å¤§é‡æ–‡æ¡£ï¼Œä½¿ç”¨ Map-Reduce
            print("âš ï¸  ç­–ç•¥ï¼šMap-Reduce å›ç­”")
            return self.map_reduce_qa.answer(query, contexts)

    def _estimate_tokens(self, query, contexts):
        """ä¼°ç®—æ€» token æ•°"""
        # ç®€åŒ–ä¼°ç®—ï¼š1 token â‰ˆ 0.75 ä¸ªè‹±æ–‡å•è¯ â‰ˆ 1.3 ä¸ªä¸­æ–‡å­—ç¬¦
        total_chars = len(query)
        for ctx in contexts:
            total_chars += len(ctx.page_content)

        return int(total_chars * 1.3)  # ä¿å®ˆä¼°è®¡

    def _direct_answer(self, query, contexts):
        """ç›´æ¥å›ç­”ï¼ˆä¸ä½¿ç”¨ç‰¹æ®Šç­–ç•¥ï¼‰"""
        context_str = "\n\n".join([
            ctx.page_content if isinstance(ctx, str) else ctx
            for ctx in contexts
        ])

        prompt = f"""
å‚è€ƒæ–‡æ¡£ï¼š
{context_str}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š
"""

        return self.llm.predict(prompt)

# ä½¿ç”¨ç¤ºä¾‹
adaptive_handler = AdaptiveContextHandler(
    llm=ChatOpenAI(model="gpt-3.5-turbo"),
    model_context_limit=4096
)

# è‡ªåŠ¨é€‚åº”ä¸åŒæƒ…å†µ
answer = adaptive_handler.answer(query, contexts)
# ç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥
```

---

# ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ€§èƒ½ä¸æˆæœ¬é—®é¢˜

## æ¡ˆä¾‹ 6: "å»¶è¿Ÿå¤ªé«˜ï¼Œç”¨æˆ·ç­‰å¾…æ—¶é—´è¿‡é•¿"

### ğŸ”´ æ•…éšœç°è±¡

```python
import time

start = time.time()
answer = rag_system.answer("GPT-4 çš„å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ")
latency = time.time() - start

print(f"æ€»å»¶è¿Ÿ: {latency:.2f}s")  # è¾“å‡º: æ€»å»¶è¿Ÿ: 8.5s
# âŒ é—®é¢˜ï¼šç”¨æˆ·ä½“éªŒå·®ï¼Œ8.5 ç§’å¤ªæ…¢
```

### ğŸ” å»¶è¿Ÿåˆ†æï¼ˆåˆ†è§£å„é˜¶æ®µè€—æ—¶ï¼‰

```python
class LatencyProfiler:
    """å»¶è¿Ÿåˆ†æå™¨"""

    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.metrics = {}

    def profile(self, query):
        """åˆ†æå„é˜¶æ®µå»¶è¿Ÿ"""
        import time

        total_start = time.time()

        # é˜¶æ®µ 1: Embedding æŸ¥è¯¢
        embed_start = time.time()
        query_embedding = self.rag_system.embedding_model.embed_query(query)
        embed_time = time.time() - embed_start

        # é˜¶æ®µ 2: å‘é‡æ£€ç´¢
        search_start = time.time()
        contexts = self.rag_system.vectorstore.similarity_search(query, k=5)
        search_time = time.time() - search_start

        # é˜¶æ®µ 3: é‡æ’åºï¼ˆå¦‚æœæœ‰ï¼‰
        rerank_start = time.time()
        if self.rag_system.reranker:
            contexts = self.rag_system.reranker.rerank(query, contexts)
        rerank_time = time.time() - rerank_start

        # é˜¶æ®µ 4: Prompt æ„å»º
        prompt_start = time.time()
        prompt = self.rag_system.build_prompt(query, contexts)
        prompt_time = time.time() - prompt_start

        # é˜¶æ®µ 5: LLM ç”Ÿæˆ
        llm_start = time.time()
        answer = self.rag_system.llm.predict(prompt)
        llm_time = time.time() - llm_start

        total_time = time.time() - total_start

        # è¿”å›åˆ†æç»“æœ
        return {
            "total_latency": total_time,
            "breakdown": {
                "embedding": embed_time,
                "search": search_time,
                "rerank": rerank_time,
                "prompt_build": prompt_time,
                "llm_generation": llm_time
            },
            "percentages": {
                "embedding": embed_time / total_time * 100,
                "search": search_time / total_time * 100,
                "rerank": rerank_time / total_time * 100,
                "prompt_build": prompt_time / total_time * 100,
                "llm_generation": llm_time / total_time * 100
            }
        }

# ä½¿ç”¨ç¤ºä¾‹
profiler = LatencyProfiler(rag_system)
profile_result = profiler.profile("GPT-4 çš„å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ")

print("=== å»¶è¿Ÿåˆ†æ ===")
for stage, latency in profile_result["breakdown"].items():
    percentage = profile_result["percentages"][stage]
    print(f"{stage:15s}: {latency:.3f}s ({percentage:.1f}%)")

# å…¸å‹è¾“å‡ºï¼š
"""
=== å»¶è¿Ÿåˆ†æ ===
embedding      : 0.080s (0.9%)
search         : 0.350s (4.1%)
rerank         : 1.200s (14.1%)
prompt_build   : 0.020s (0.2%)
llm_generation : 6.850s (80.6%)  â† ä¸»è¦ç“¶é¢ˆï¼
---------------------------------
Total          : 8.500s (100.0%)
"""
```

### ğŸ”§ æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

#### ä¼˜åŒ– 1: æµå¼è¾“å‡ºï¼ˆé™ä½æ„ŸçŸ¥å»¶è¿Ÿï¼‰

```python
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

class StreamingRAG:
    """æ”¯æŒæµå¼è¾“å‡ºçš„ RAG"""

    def __init__(self, llm_streaming, retriever):
        self.llm = llm_streaming
        self.retriever = retriever

    def answer_stream(self, query):
        """æµå¼ç”Ÿæˆç­”æ¡ˆ"""

        # 1. æ£€ç´¢ï¼ˆè¿™éƒ¨åˆ†ä»æ˜¯æ‰¹é‡ï¼‰
        contexts = self.retriever.retrieve(query, k=3)

        # 2. æ„å»º Prompt
        prompt = self.build_prompt(query, contexts)

        # 3. æµå¼ç”Ÿæˆ
        print("ğŸ’¡ ç­”æ¡ˆ: ", end="", flush=True)

        for chunk in self.llm.stream(prompt):
            print(chunk.content, end="", flush=True)  # å®æ—¶æ‰“å°
            yield chunk.content  # æµå¼è¿”å›

        print()  # æ¢è¡Œ

# ä½¿ç”¨ç¤ºä¾‹
from langchain.chat_models import ChatOpenAI

streaming_llm = ChatOpenAI(
    model="gpt-4",
    streaming=True,  # å¯ç”¨æµå¼
    callbacks=[StreamingStdOutCallbackHandler()]
)

streaming_rag = StreamingRAG(streaming_llm, retriever)

# æµå¼å›ç­”ï¼ˆé™ä½æ„ŸçŸ¥å»¶è¿Ÿï¼‰
for chunk in streaming_rag.answer_stream(query):
    pass  # chunk å·²è¢«æ‰“å°
# ç”¨æˆ·åœ¨ 0.5s åå°±èƒ½çœ‹åˆ°ç¬¬ä¸€ä¸ªè¯ï¼Œè€Œä¸æ˜¯ç­‰å¾… 8.5s
```

#### ä¼˜åŒ– 2: å¹¶è¡ŒåŒ–æ£€ç´¢å’Œé‡æ’åº

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class ParallelRAG:
    """å¹¶è¡ŒåŒ– RAG ç³»ç»Ÿ"""

    def __init__(self, vectorstore, reranker, llm):
        self.vectorstore = vectorstore
        self.reranker = reranker
        self.llm = llm
        self.executor = ThreadPoolExecutor(max_workers=4)

    async def answer_async(self, query):
        """å¼‚æ­¥å¹¶è¡Œå›ç­”"""

        # å¹¶è¡Œæ‰§è¡Œï¼šEmbedding + BM25æ£€ç´¢ï¼ˆå¦‚æœæœ‰ï¼‰
        tasks = [
            self._async_vector_search(query),
            # self._async_bm25_search(query)  # å¯é€‰
        ]

        search_results = await asyncio.gather(*tasks)
        vector_results = search_results[0]

        # é‡æ’åº
        reranked = await self._async_rerank(query, vector_results)

        # æ„å»º Prompt
        prompt = self.build_prompt(query, reranked)

        # LLM ç”Ÿæˆ
        answer = await self._async_llm_generate(prompt)

        return answer

    async def _async_vector_search(self, query):
        """å¼‚æ­¥å‘é‡æ£€ç´¢"""
        loop = asyncio.get_event_loop()
        results = await loop.run_in_executor(
            self.executor,
            self.vectorstore.similarity_search,
            query,
            5
        )
        return results

    async def _async_rerank(self, query, candidates):
        """å¼‚æ­¥é‡æ’åº"""
        loop = asyncio.get_event_loop()
        reranked = await loop.run_in_executor(
            self.executor,
            self.reranker.rerank,
            query,
            candidates
        )
        return reranked

    async def _async_llm_generate(self, prompt):
        """å¼‚æ­¥ LLM ç”Ÿæˆ"""
        loop = asyncio.get_event_loop()
        answer = await loop.run_in_executor(
            self.executor,
            self.llm.predict,
            prompt
        )
        return answer

# ä½¿ç”¨ç¤ºä¾‹
parallel_rag = ParallelRAG(vectorstore, reranker, llm)

# å¼‚æ­¥è°ƒç”¨
import asyncio
answer = asyncio.run(parallel_rag.answer_async(query))

# æ”¹è¿›æ•ˆæœ:
# Before: 8.5s (ä¸²è¡Œ)
# After:  5.2s (å¹¶è¡Œï¼ŒèŠ‚çœçº¦ 40%)
```

#### ä¼˜åŒ– 3: ç¼“å­˜çƒ­é—¨æŸ¥è¯¢

```python
from functools import lru_cache
import hashlib

class CachedRAG:
    """å¸¦ç¼“å­˜çš„ RAG ç³»ç»Ÿ"""

    def __init__(self, rag_system, cache_size=100):
        self.rag_system = rag_system
        self.cache_size = cache_size
        self.cache = {}

    def answer(self, query):
        """å¸¦ç¼“å­˜çš„å›ç­”"""

        # è®¡ç®—æŸ¥è¯¢å“ˆå¸Œ
        query_hash = hashlib.md5(query.encode()).hexdigest()

        # æ£€æŸ¥ç¼“å­˜
        if query_hash in self.cache:
            print("âœ… å‘½ä¸­ç¼“å­˜")
            return self.cache[query_hash]

        # æœªå‘½ä¸­ï¼Œè°ƒç”¨åŸç³»ç»Ÿ
        answer = self.rag_system.answer(query)

        # å†™å…¥ç¼“å­˜ï¼ˆLRUç­–ç•¥ï¼‰
        if len(self.cache) >= self.cache_size:
            # ç§»é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆç®€åŒ–å®ç°ï¼‰
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        self.cache[query_hash] = answer

        return answer

# ä½¿ç”¨ç¤ºä¾‹
cached_rag = CachedRAG(rag_system, cache_size=100)

# ç¬¬ä¸€æ¬¡æŸ¥è¯¢: 8.5s
answer1 = cached_rag.answer("GPT-4 çš„å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ")

# ç¬¬äºŒæ¬¡ç›¸åŒæŸ¥è¯¢: < 0.001sï¼ˆå‘½ä¸­ç¼“å­˜ï¼‰
answer2 = cached_rag.answer("GPT-4 çš„å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ")
```

#### ä¼˜åŒ– 4: ä½¿ç”¨æ›´å¿«çš„ Embedding æ¨¡å‹

```python
# å¯¹æ¯”ä¸åŒ Embedding æ¨¡å‹çš„é€Ÿåº¦

models_comparison = [
    {
        "name": "text-embedding-ada-002",
        "dimension": 1536,
        "latency_per_query": "80ms",
        "quality": "â­â­â­â­â­",
        "cost": "$$$$"
    },
    {
        "name": "all-MiniLM-L6-v2 (local)",
        "dimension": 384,
        "latency_per_query": "15ms",  # â† å¿« 5 å€ï¼
        "quality": "â­â­â­â­",
        "cost": "$0 (æœ¬åœ°)"
    },
    {
        "name": "bge-small-zh-v1.5 (ä¸­æ–‡)",
        "dimension": 512,
        "latency_per_query": "20ms",
        "quality": "â­â­â­â­",
        "cost": "$0 (æœ¬åœ°)"
    }
]

# ä½¿ç”¨æœ¬åœ° Embedding æ¨¡å‹
from sentence_transformers import SentenceTransformer

class LocalEmbeddingRAG:
    """ä½¿ç”¨æœ¬åœ° Embedding çš„ RAG"""

    def __init__(self, model_name="all-MiniLM-L6-v2"):
        # åŠ è½½æœ¬åœ°æ¨¡å‹
        self.embedding_model = SentenceTransformer(model_name)

        # å…¶ä»–ç»„ä»¶ä¿æŒä¸å˜
        # ...

    def embed_query(self, query):
        """æœ¬åœ° Embeddingï¼ˆæ— ç½‘ç»œå»¶è¿Ÿï¼‰"""
        return self.embedding_model.encode(query)

# ä½¿ç”¨ç¤ºä¾‹
local_rag = LocalEmbeddingRAG()

# Embedding å»¶è¿Ÿ: 80ms â†’ 15msï¼ˆèŠ‚çœ 65msï¼‰
```

#### ä¼˜åŒ– 5: å‡å°‘é‡æ’åºå€™é€‰æ•°é‡

```python
# é‡æ’åºæ˜¯ç“¶é¢ˆä¹‹ä¸€ï¼ˆCrossEncoder æ…¢ï¼‰

# âŒ ä½æ•ˆï¼šé‡æ’åºæ‰€æœ‰å€™é€‰
vector_results = vectorstore.similarity_search(query, k=20)  # 20 ä¸ªå€™é€‰
reranked = reranker.rerank(query, vector_results)  # é‡æ’åº 20 ä¸ªï¼ˆæ…¢ï¼ï¼‰

# âœ… é«˜æ•ˆï¼šåªé‡æ’åº Top-K
vector_results = vectorstore.similarity_search(query, k=20)
top_candidates = vector_results[:10]  # åªå–å‰ 10 ä¸ª
reranked = reranker.rerank(query, top_candidates)  # é‡æ’åº 10 ä¸ªï¼ˆå¿« 2 å€ï¼‰

# æ”¹è¿›æ•ˆæœ:
# Rerank 20 ä¸ª: 1.2s
# Rerank 10 ä¸ª: 0.6sï¼ˆèŠ‚çœ 50%ï¼‰
```

### âœ… å®Œæ•´æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

```python
class OptimizedRAG:
    """å…¨æ–¹ä½ä¼˜åŒ–çš„ RAG ç³»ç»Ÿ"""

    def __init__(self, config):
        # 1. ä½¿ç”¨æœ¬åœ° Embeddingï¼ˆå‡å°‘ç½‘ç»œå»¶è¿Ÿï¼‰
        self.embedding_model = SentenceTransformer(config['embedding_model'])

        # 2. å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨ HNSW ç´¢å¼•ï¼‰
        self.vectorstore = Chroma(
            embedding_function=self.embedding_model,
            collection_metadata={"hnsw:space": "cosine"}
        )

        # 3. è½»é‡çº§é‡æ’åº
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')  # å°æ¨¡å‹

        # 4. æµå¼ LLM
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",  # æ¯” GPT-4 å¿« 3 å€
            streaming=True,
            temperature=0.0
        )

        # 5. ç¼“å­˜
        self.cache = {}

    def answer(self, query, use_cache=True):
        """ä¼˜åŒ–åçš„å›ç­”æµç¨‹"""

        # ç¼“å­˜æ£€æŸ¥
        if use_cache and query in self.cache:
            return self.cache[query]

        # å¹¶è¡Œæ£€ç´¢
        contexts = self._parallel_retrieve(query)

        # æ„å»º Prompt
        prompt = self._build_prompt(query, contexts)

        # æµå¼ç”Ÿæˆ
        answer = ""
        for chunk in self.llm.stream(prompt):
            answer += chunk.content
            print(chunk.content, end="", flush=True)

        # å†™å…¥ç¼“å­˜
        if use_cache:
            self.cache[query] = answer

        return answer

    def _parallel_retrieve(self, query):
        """å¹¶è¡Œæ£€ç´¢ï¼ˆå‘é‡ + é‡æ’åºï¼‰"""
        import time

        # å‘é‡æ£€ç´¢
        start = time.time()
        vector_results = self.vectorstore.similarity_search(query, k=10)
        print(f"ğŸ” å‘é‡æ£€ç´¢: {time.time() - start:.3f}s")

        # é‡æ’åºï¼ˆåªé‡æ’å‰ 5 ä¸ªï¼‰
        start = time.time()
        top_candidates = vector_results[:5]
        reranked = self._rerank(query, top_candidates)
        print(f"ğŸ”„ é‡æ’åº: {time.time() - start:.3f}s")

        return reranked[:3]  # è¿”å› Top-3

# ä½¿ç”¨ç¤ºä¾‹
optimized_rag = OptimizedRAG({
    "embedding_model": "all-MiniLM-L6-v2"
})

answer = optimized_rag.answer("GPT-4 çš„å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ")

# æ€§èƒ½å¯¹æ¯”:
# Before: 8.5s
# After:  2.1sï¼ˆæå‡ 75%ï¼‰
```

**ä¼˜åŒ–æ•ˆæœæ±‡æ€»**:

```
ä¼˜åŒ–é¡¹                  | å»¶è¿Ÿå‡å°‘  | æˆæœ¬å½±å“
------------------------|-----------|------------
æµå¼è¾“å‡º                | æ„ŸçŸ¥ -90% | æ— 
æœ¬åœ° Embedding          | -65ms     | èŠ‚çœ API è´¹ç”¨
å¹¶è¡ŒåŒ–                  | -40%      | æ— 
ç¼“å­˜ (å‘½ä¸­æ—¶)           | -99%      | æ— 
å‡å°‘é‡æ’åºå€™é€‰          | -50%      | æ— 
ä½¿ç”¨ GPT-3.5 æ›¿ä»£ GPT-4 | -70%      | èŠ‚çœ 90% è´¹ç”¨

ç»¼åˆä¼˜åŒ–åï¼š
- æ€»å»¶è¿Ÿ: 8.5s â†’ 2.1sï¼ˆæå‡ 75%ï¼‰
- æµå¼é¦–å­—å»¶è¿Ÿ: < 0.5s
- æˆæœ¬: å‡å°‘çº¦ 60%
```

---

## æ¡ˆä¾‹ 7: "æˆæœ¬å¤ªé«˜ï¼Œæ¯æœˆ API è´¹ç”¨è¿‡ä¸‡"

### ğŸ”´ æ•…éšœç°è±¡

```python
# æœˆåº¦æˆæœ¬åˆ†æ
monthly_stats = {
    "total_queries": 50000,
    "avg_context_tokens": 3000,
    "avg_answer_tokens": 500,
    "model": "gpt-4"
}

# GPT-4 å®šä»·
gpt4_pricing = {
    "input": 0.03 / 1000,   # $0.03 per 1K tokens
    "output": 0.06 / 1000   # $0.06 per 1K tokens
}

# è®¡ç®—æœˆåº¦æˆæœ¬
input_cost = monthly_stats["total_queries"] * monthly_stats["avg_context_tokens"] * gpt4_pricing["input"]
output_cost = monthly_stats["total_queries"] * monthly_stats["avg_answer_tokens"] * gpt4_pricing["output"]

total_cost = input_cost + output_cost
print(f"æœˆåº¦æ€»æˆæœ¬: ${total_cost:.2f}")  # è¾“å‡º: $6,000

# âŒ é—®é¢˜ï¼šæˆæœ¬è¿‡é«˜ï¼Œéœ€è¦ä¼˜åŒ–
```

### ğŸ”§ æˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆ

#### ç­–ç•¥ 1: æ··åˆæ¨¡å‹ç­–ç•¥

```python
class HybridModelRAG:
    """æ··åˆæ¨¡å‹ RAGï¼ˆæ ¹æ®éš¾åº¦é€‰æ‹©æ¨¡å‹ï¼‰"""

    def __init__(self):
        # ä¾¿å®œçš„æ¨¡å‹ï¼ˆç®€å•é—®é¢˜ï¼‰
        self.cheap_llm = ChatOpenAI(
            model="gpt-3.5-turbo",  # $0.001/1K tokens
            temperature=0.0
        )

        # æ˜‚è´µçš„æ¨¡å‹ï¼ˆå¤æ‚é—®é¢˜ï¼‰
        self.expensive_llm = ChatOpenAI(
            model="gpt-4",  # $0.03/1K tokens
            temperature=0.0
        )

        self.complexity_classifier = self._load_classifier()

    def answer(self, query, contexts):
        """æ ¹æ®é—®é¢˜å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""

        # åˆ¤æ–­é—®é¢˜å¤æ‚åº¦
        complexity = self._classify_complexity(query)

        if complexity == "simple":
            print("âœ… ä½¿ç”¨ GPT-3.5 (ç®€å•é—®é¢˜)")
            llm = self.cheap_llm
        else:
            print("âš ï¸  ä½¿ç”¨ GPT-4 (å¤æ‚é—®é¢˜)")
            llm = self.expensive_llm

        # ç”Ÿæˆç­”æ¡ˆ
        prompt = self._build_prompt(query, contexts)
        answer = llm.predict(prompt)

        return answer

    def _classify_complexity(self, query):
        """åˆ†ç±»é—®é¢˜å¤æ‚åº¦"""

        # æ–¹æ³• 1: åŸºäºè§„åˆ™
        simple_patterns = [
            "ä»€ä¹ˆæ˜¯",
            "å¦‚ä½•å®šä¹‰",
            "æœ‰å“ªäº›",
            "åˆ—ä¸¾",
            "å¤šå°‘"
        ]

        for pattern in simple_patterns:
            if pattern in query:
                return "simple"

        # æ–¹æ³• 2: ä½¿ç”¨å°å‹åˆ†ç±»å™¨ï¼ˆå¯é€‰ï¼‰
        # complexity_score = self.complexity_classifier(query)
        # return "simple" if complexity_score < 0.5 else "complex"

        return "complex"  # é»˜è®¤å¤æ‚

# ä½¿ç”¨ç¤ºä¾‹
hybrid_rag = HybridModelRAG()

# ç®€å•é—®é¢˜ â†’ GPT-3.5ï¼ˆèŠ‚çœ 96% æˆæœ¬ï¼‰
answer1 = hybrid_rag.answer("GPT-4 æ˜¯ä»€ä¹ˆï¼Ÿ", contexts)

# å¤æ‚é—®é¢˜ â†’ GPT-4
answer2 = hybrid_rag.answer("æ¯”è¾ƒ GPT-4 å’Œ Claude çš„æ¶æ„å·®å¼‚ï¼Œå¹¶åˆ†æå„è‡ªçš„ä¼˜åŠ£åŠ¿", contexts)

# æˆæœ¬èŠ‚çœ:
# å‡è®¾ 70% æ˜¯ç®€å•é—®é¢˜
# Before: 100% ä½¿ç”¨ GPT-4 â†’ $6,000/æœˆ
# After:  70% GPT-3.5 + 30% GPT-4 â†’ $2,100/æœˆï¼ˆèŠ‚çœ 65%ï¼‰
```

#### ç­–ç•¥ 2: å‹ç¼©ä¸Šä¸‹æ–‡ï¼ˆå‡å°‘ Input Tokensï¼‰

```python
# æˆæœ¬åˆ†è§£:
# Input tokens (ä¸Šä¸‹æ–‡): 3000 tokens Ã— $0.03/1K = $0.09
# Output tokens (ç­”æ¡ˆ): 500 tokens Ã— $0.06/1K = $0.03
# â†’ Input å æ€»æˆæœ¬çš„ 75%ï¼

class ContextCompressorForCost:
    """ä¸“æ³¨æˆæœ¬ä¼˜åŒ–çš„ä¸Šä¸‹æ–‡å‹ç¼©å™¨"""

    def compress(self, contexts, target_ratio=0.5):
        """å‹ç¼©åˆ°åŸé•¿åº¦çš„ 50%"""

        # ä½¿ç”¨è½»é‡çº§æ€»ç»“æ¨¡å‹ï¼ˆæ›´ä¾¿å®œï¼‰
        summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

        compressed_contexts = []
        for ctx in contexts:
            summary = summarizer(
                ctx.page_content,
                max_length=len(ctx.page_content.split()) // 2,
                min_length=30,
                do_sample=False
            )
            compressed_contexts.append(summary[0]['summary_text'])

        return compressed_contexts

# ä½¿ç”¨ç¤ºä¾‹
compressor = ContextCompressorForCost()
compressed = compressor.compress(contexts, target_ratio=0.5)

# æˆæœ¬èŠ‚çœ:
# Before: 3000 tokens context Ã— $0.03/1K = $0.09
# After:  1500 tokens context Ã— $0.03/1K = $0.045ï¼ˆèŠ‚çœ 50%ï¼‰
```

#### ç­–ç•¥ 3: æ‰¹é‡å¤„ç†

```python
class BatchRAG:
    """æ‰¹é‡å¤„ç† RAGï¼ˆé™ä½è¯·æ±‚é¢‘ç‡ï¼‰"""

    def __init__(self, llm, batch_size=10):
        self.llm = llm
        self.batch_size = batch_size
        self.query_buffer = []

    def answer_batch(self, queries):
        """æ‰¹é‡å›ç­”ï¼ˆä¸€æ¬¡ API è°ƒç”¨å¤„ç†å¤šä¸ªé—®é¢˜ï¼‰"""

        # æ„å»ºæ‰¹é‡ Prompt
        batch_prompt = """
è¯·å›ç­”ä»¥ä¸‹ {count} ä¸ªé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜å•ç‹¬å›ç­”ï¼š

{questions}

æ ¼å¼ï¼š
é—®é¢˜1çš„ç­”æ¡ˆ: ...
é—®é¢˜2çš„ç­”æ¡ˆ: ...
...
"""

        questions_text = "\n".join([
            f"é—®é¢˜{i+1}: {q}"
            for i, q in enumerate(queries)
        ])

        formatted_prompt = batch_prompt.format(
            count=len(queries),
            questions=questions_text
        )

        # ä¸€æ¬¡æ€§è°ƒç”¨ LLM
        batch_answer = self.llm.predict(formatted_prompt)

        # è§£ææ‰¹é‡ç­”æ¡ˆ
        answers = self._parse_batch_answer(batch_answer, len(queries))

        return answers

    def _parse_batch_answer(self, batch_answer, num_questions):
        """è§£ææ‰¹é‡ç­”æ¡ˆ"""
        # ç®€åŒ–å®ç°ï¼šæŒ‰ "ç­”æ¡ˆN:" åˆ†å‰²
        import re
        pattern = r'é—®é¢˜\d+çš„ç­”æ¡ˆ:\s*(.+?)(?=é—®é¢˜\d+çš„ç­”æ¡ˆ:|$)'
        matches = re.findall(pattern, batch_answer, re.DOTALL)
        return matches[:num_questions]

# ä½¿ç”¨ç¤ºä¾‹
batch_rag = BatchRAG(llm, batch_size=10)

# æ‰¹é‡å¤„ç† 10 ä¸ªé—®é¢˜
queries = [
    "GPT-4 æ˜¯ä»€ä¹ˆï¼Ÿ",
    "GPT-4 çš„å‚æ•°é‡ï¼Ÿ",
    # ... 8 more questions
]

answers = batch_rag.answer_batch(queries)

# æˆæœ¬èŠ‚çœ:
# Before: 10 æ¬¡ç‹¬ç«‹è°ƒç”¨ Ã— ($0.09 + $0.03) = $1.20
# After:  1 æ¬¡æ‰¹é‡è°ƒç”¨ = $0.15ï¼ˆèŠ‚çœ 87%ï¼‰
```

#### ç­–ç•¥ 4: ä½¿ç”¨å¼€æºæ¨¡å‹ï¼ˆè‡ªéƒ¨ç½²ï¼‰

```python
# å®Œå…¨æ¶ˆé™¤ API æˆæœ¬

from transformers import AutoModelForCausalLM, AutoTokenizer

class SelfHostedRAG:
    """è‡ªéƒ¨ç½²å¼€æºæ¨¡å‹çš„ RAG"""

    def __init__(self, model_name="meta-llama/Llama-2-13b-chat-hf"):
        # åŠ è½½å¼€æºæ¨¡å‹ï¼ˆéœ€è¦ GPUï¼‰
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",  # è‡ªåŠ¨åˆ†é… GPU
            load_in_8bit=True   # é‡åŒ–å‡å°‘æ˜¾å­˜
        )

    def answer(self, query, contexts):
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹å›ç­”"""

        prompt = self._build_prompt(query, contexts)

        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=500,
            temperature=0.7,
            do_sample=True
        )

        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return answer

# æˆæœ¬å¯¹æ¯”:
"""
æ–¹æ¡ˆ            | æœˆåº¦æˆæœ¬ (50K queries) | åˆæœŸæŠ•å…¥      | æ€§èƒ½
----------------|------------------------|---------------|--------
GPT-4 API       | $6,000                 | $0            | â­â­â­â­â­
GPT-3.5 API     | $250                   | $0            | â­â­â­â­
Llama-2 (è‡ªéƒ¨ç½²)| $0 (ä»…ç”µè´¹çº¦ $50)      | GPU æœåŠ¡å™¨ $2K| â­â­â­

æ¨èç­–ç•¥ï¼š
- åˆåˆ›å…¬å¸/ä½æµé‡ï¼šGPT-3.5 API
- ä¸­ç­‰æµé‡ï¼ˆæœˆ 10K+ queriesï¼‰ï¼šæ··åˆæ¨¡å‹
- é«˜æµé‡ï¼ˆæœˆ 100K+ queriesï¼‰ï¼šè‡ªéƒ¨ç½²å¼€æºæ¨¡å‹
"""
```

### âœ… å®Œæ•´æˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆ

```python
class CostOptimizedRAG:
    """å…¨æ–¹ä½æˆæœ¬ä¼˜åŒ–çš„ RAG"""

    def __init__(self, config):
        self.config = config

        # 1. æ··åˆæ¨¡å‹
        self.cheap_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.0)
        self.expensive_llm = ChatOpenAI(model="gpt-4", temperature=0.0)

        # 2. ä¸Šä¸‹æ–‡å‹ç¼©å™¨
        self.compressor = ContextCompressorForCost()

        # 3. ç¼“å­˜
        self.cache = {}

    def answer(self, query, use_compression=True, use_cache=True):
        """æˆæœ¬ä¼˜åŒ–çš„å›ç­”æµç¨‹"""

        # Step 1: æ£€æŸ¥ç¼“å­˜ï¼ˆæˆæœ¬æœ€ä½ï¼‰
        if use_cache and query in self.cache:
            print("âœ… å‘½ä¸­ç¼“å­˜ï¼ˆæˆæœ¬: $0ï¼‰")
            return self.cache[query]

        # Step 2: æ£€ç´¢
        contexts = self._retrieve(query)

        # Step 3: å‹ç¼©ä¸Šä¸‹æ–‡ï¼ˆå‡å°‘ input tokensï¼‰
        if use_compression:
            contexts = self.compressor.compress(contexts, target_ratio=0.6)
            print("âœ… ä¸Šä¸‹æ–‡å·²å‹ç¼©ï¼ˆèŠ‚çœ 40% input æˆæœ¬ï¼‰")

        # Step 4: é€‰æ‹©æ¨¡å‹ï¼ˆæ ¹æ®å¤æ‚åº¦ï¼‰
        complexity = self._classify_complexity(query)
        llm = self.cheap_llm if complexity == "simple" else self.expensive_llm

        model_name = "GPT-3.5" if complexity == "simple" else "GPT-4"
        print(f"âœ… ä½¿ç”¨ {model_name}")

        # Step 5: ç”Ÿæˆç­”æ¡ˆ
        prompt = self._build_prompt(query, contexts)
        answer = llm.predict(prompt)

        # Step 6: å†™å…¥ç¼“å­˜
        if use_cache:
            self.cache[query] = answer

        return answer

# ä½¿ç”¨ç¤ºä¾‹
cost_optimized_rag = CostOptimizedRAG(config={})

answer = cost_optimized_rag.answer(
    "GPT-4 çš„å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ",
    use_compression=True,
    use_cache=True
)

# æˆæœ¬å¯¹æ¯”ï¼ˆå•æ¬¡æŸ¥è¯¢ï¼‰:
"""
åœºæ™¯                      | æˆæœ¬      | è¯´æ˜
--------------------------|-----------|-----------------------------
åŸå§‹ (GPT-4, æ— ä¼˜åŒ–)      | $0.12     | 3000 input + 500 output
å‹ç¼©ä¸Šä¸‹æ–‡ (GPT-4)        | $0.08     | 1800 input + 500 output
ä½¿ç”¨ GPT-3.5              | $0.004    | 3000 input + 500 output
å‹ç¼© + GPT-3.5            | $0.0024   | 1800 input + 500 output
å‘½ä¸­ç¼“å­˜                  | $0        | æ—  API è°ƒç”¨

æœˆåº¦æˆæœ¬ (50K queries, 70% simple, 20% cache hit):
- Before: $6,000
- After:  $480ï¼ˆèŠ‚çœ 92%ï¼‰
"""
```

---

# ç¬¬å››éƒ¨åˆ†ï¼šç³»ç»Ÿç¨³å®šæ€§é—®é¢˜

## æ¡ˆä¾‹ 8: "API é™æµå¯¼è‡´æœåŠ¡ä¸å¯ç”¨"

### ğŸ”´ æ•…éšœç°è±¡

```python
# é«˜å¹¶å‘åœºæ™¯
import time
from concurrent.futures import ThreadPoolExecutor

def process_query(query):
    return rag_system.answer(query)

queries = ["é—®é¢˜1", "é—®é¢˜2", ...] * 100  # 100 ä¸ªæŸ¥è¯¢

# å¹¶å‘æ‰§è¡Œ
with ThreadPoolExecutor(max_workers=10) as executor:
    results = list(executor.map(process_query, queries))

# âŒ é”™è¯¯: RateLimitError: Rate limit reached for requests
```

### ğŸ”§ è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆ 1: é™æµå™¨ (Rate Limiter)

```python
import time
from threading import Lock

class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨ï¼ˆä»¤ç‰Œæ¡¶ç®—æ³•ï¼‰"""

    def __init__(self, max_requests_per_minute=60):
        self.max_requests = max_requests_per_minute
        self.requests = []
        self.lock = Lock()

    def acquire(self):
        """è·å–ä»¤ç‰Œï¼ˆé˜»å¡ç›´åˆ°å¯ç”¨ï¼‰"""
        with self.lock:
            now = time.time()

            # ç§»é™¤ 1 åˆ†é’Ÿå‰çš„è¯·æ±‚è®°å½•
            self.requests = [req for req in self.requests if now - req < 60]

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é™åˆ¶
            if len(self.requests) >= self.max_requests:
                # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
                oldest_request = self.requests[0]
                wait_time = 60 - (now - oldest_request) + 0.1
                print(f"âš ï¸  è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f}s")
                time.sleep(wait_time)

                # é€’å½’é‡è¯•
                return self.acquire()

            # è®°å½•æœ¬æ¬¡è¯·æ±‚
            self.requests.append(now)

class RateLimitedRAG:
    """å¸¦é™æµçš„ RAG"""

    def __init__(self, rag_system, rpm_limit=60):
        self.rag_system = rag_system
        self.rate_limiter = RateLimiter(max_requests_per_minute=rpm_limit)

    def answer(self, query):
        """é™æµåçš„å›ç­”"""
        # è·å–ä»¤ç‰Œï¼ˆå¯èƒ½é˜»å¡ï¼‰
        self.rate_limiter.acquire()

        # æ‰§è¡Œå®é™…è¯·æ±‚
        return self.rag_system.answer(query)

# ä½¿ç”¨ç¤ºä¾‹
rate_limited_rag = RateLimitedRAG(rag_system, rpm_limit=60)

# å³ä½¿å¹¶å‘ 100 ä¸ªè¯·æ±‚ï¼Œä¹Ÿä¸ä¼šè§¦å‘é™æµé”™è¯¯
with ThreadPoolExecutor(max_workers=10) as executor:
    results = list(executor.map(rate_limited_rag.answer, queries))
# ä¼šè‡ªåŠ¨é™é€Ÿï¼Œç¡®ä¿ä¸è¶…è¿‡ 60 RPM
```

#### æ–¹æ¡ˆ 2: æŒ‡æ•°é€€é¿é‡è¯•

```python
import time
import random
from functools import wraps

def retry_with_exponential_backoff(
    max_retries=5,
    initial_delay=1,
    exponential_base=2,
    jitter=True
):
    """æŒ‡æ•°é€€é¿è£…é¥°å™¨"""

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            delay = initial_delay

            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)

                except Exception as e:
                    error_message = str(e)

                    # æ£€æµ‹æ˜¯å¦æ˜¯é™æµé”™è¯¯
                    if "rate limit" in error_message.lower():
                        if attempt == max_retries - 1:
                            raise  # æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸

                        # æ·»åŠ éšæœºæŠ–åŠ¨
                        if jitter:
                            delay = delay * exponential_base * (0.5 + random.random())
                        else:
                            delay = delay * exponential_base

                        print(f"âš ï¸  é™æµé”™è¯¯ï¼Œ{delay:.1f}s åé‡è¯•ï¼ˆç¬¬ {attempt+1}/{max_retries} æ¬¡ï¼‰")
                        time.sleep(delay)
                    else:
                        # éé™æµé”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                        raise

            raise Exception(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")

        return wrapper
    return decorator

class RetryRAG:
    """å¸¦é‡è¯•çš„ RAG"""

    def __init__(self, rag_system):
        self.rag_system = rag_system

    @retry_with_exponential_backoff(max_retries=5, initial_delay=1)
    def answer(self, query):
        """å¸¦è‡ªåŠ¨é‡è¯•çš„å›ç­”"""
        return self.rag_system.answer(query)

# ä½¿ç”¨ç¤ºä¾‹
retry_rag = RetryRAG(rag_system)

try:
    answer = retry_rag.answer(query)
except Exception as e:
    print(f"âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥: {e}")
```

#### æ–¹æ¡ˆ 3: è¯·æ±‚é˜Ÿåˆ—

```python
import queue
import threading
import time

class QueuedRAG:
    """é˜Ÿåˆ—åŒ– RAGï¼ˆæ§åˆ¶å¹¶å‘ï¼‰"""

    def __init__(self, rag_system, max_workers=3):
        self.rag_system = rag_system
        self.max_workers = max_workers

        # è¯·æ±‚é˜Ÿåˆ—
        self.request_queue = queue.Queue()
        self.result_queue = queue.Queue()

        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        self.workers = []
        for _ in range(max_workers):
            worker = threading.Thread(target=self._worker, daemon=True)
            worker.start()
            self.workers.append(worker)

    def _worker(self):
        """å·¥ä½œçº¿ç¨‹ï¼ˆä»é˜Ÿåˆ—ä¸­å–ä»»åŠ¡ï¼‰"""
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                task = self.request_queue.get(timeout=1)

                if task is None:  # ç»“æŸä¿¡å·
                    break

                query, result_id = task

                # å¤„ç†ä»»åŠ¡
                try:
                    answer = self.rag_system.answer(query)
                    self.result_queue.put((result_id, answer, None))
                except Exception as e:
                    self.result_queue.put((result_id, None, e))

                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.request_queue.task_done()

            except queue.Empty:
                continue

    def answer(self, query, timeout=30):
        """å¼‚æ­¥æäº¤ä»»åŠ¡"""
        result_id = id(query)

        # æäº¤åˆ°é˜Ÿåˆ—
        self.request_queue.put((query, result_id))

        # ç­‰å¾…ç»“æœ
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                rid, answer, error = self.result_queue.get(timeout=0.1)

                if rid == result_id:
                    if error:
                        raise error
                    return answer

            except queue.Empty:
                continue

        raise TimeoutError(f"æŸ¥è¯¢è¶…æ—¶ï¼ˆ{timeout}sï¼‰")

    def shutdown(self):
        """å…³é—­é˜Ÿåˆ—"""
        for _ in self.workers:
            self.request_queue.put(None)

        for worker in self.workers:
            worker.join()

# ä½¿ç”¨ç¤ºä¾‹
queued_rag = QueuedRAG(rag_system, max_workers=3)

# å¹¶å‘æäº¤ 100 ä¸ªè¯·æ±‚ï¼ˆä½†åªæœ‰ 3 ä¸ªå¹¶å‘æ‰§è¡Œï¼‰
with ThreadPoolExecutor(max_workers=20) as executor:
    futures = [executor.submit(queued_rag.answer, q) for q in queries]
    results = [f.result() for f in futures]

queued_rag.shutdown()
```

### âœ… å®Œæ•´å®¹é”™æ–¹æ¡ˆ

```python
class RobustRAG:
    """å¥å£®çš„ RAG ç³»ç»Ÿï¼ˆé›†æˆé™æµã€é‡è¯•ã€é™çº§ï¼‰"""

    def __init__(self, rag_system, config):
        self.rag_system = rag_system
        self.config = config

        # é™æµå™¨
        self.rate_limiter = RateLimiter(
            max_requests_per_minute=config.get('rpm_limit', 60)
        )

        # ç¼“å­˜
        self.cache = {}

        # é™çº§å“åº”
        self.fallback_enabled = config.get('enable_fallback', True)

    @retry_with_exponential_backoff(max_retries=3)
    def answer(self, query, timeout=30):
        """å¥å£®çš„å›ç­”æµç¨‹"""

        # Step 1: æ£€æŸ¥ç¼“å­˜
        if query in self.cache:
            return self.cache[query]

        # Step 2: é™æµ
        self.rate_limiter.acquire()

        try:
            # Step 3: æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå¸¦è¶…æ—¶ï¼‰
            answer = self._answer_with_timeout(query, timeout)

            # Step 4: å†™å…¥ç¼“å­˜
            self.cache[query] = answer

            return answer

        except Exception as e:
            # Step 5: é™çº§ç­–ç•¥
            if self.fallback_enabled:
                return self._fallback_answer(query, e)
            else:
                raise

    def _answer_with_timeout(self, query, timeout):
        """å¸¦è¶…æ—¶çš„å›ç­”"""
        import signal

        def timeout_handler(signum, frame):
            raise TimeoutError("æŸ¥è¯¢è¶…æ—¶")

        # è®¾ç½®è¶…æ—¶
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout)

        try:
            answer = self.rag_system.answer(query)
            signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
            return answer
        except:
            signal.alarm(0)
            raise

    def _fallback_answer(self, query, error):
        """é™çº§å“åº”"""
        print(f"âš ï¸  ä¸»ç³»ç»Ÿæ•…éšœï¼Œä½¿ç”¨é™çº§å“åº”: {error}")

        # é™çº§ç­–ç•¥ 1: è¿”å›ç®€åŒ–ç­”æ¡ˆ
        return f"æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚\né”™è¯¯ä¿¡æ¯: {str(error)[:100]}"

        # é™çº§ç­–ç•¥ 2: ä½¿ç”¨å¤‡ç”¨æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        # return self.backup_llm.predict(f"ç®€å•å›ç­”ï¼š{query}")

# ä½¿ç”¨ç¤ºä¾‹
robust_rag = RobustRAG(
    rag_system=rag_system,
    config={
        "rpm_limit": 60,
        "enable_fallback": True
    }
)

# å³ä½¿åœ¨é«˜å¹¶å‘ã€ç½‘ç»œä¸ç¨³å®šçš„æƒ…å†µä¸‹ä¹Ÿèƒ½æ­£å¸¸è¿è¡Œ
answer = robust_rag.answer(query, timeout=30)
```

---

# ç¬¬äº”éƒ¨åˆ†ï¼šè°ƒè¯•å·¥å…·ä¸æµ‹è¯•æ–¹æ³•

## å·¥å…· 1: RAG å¯è§†åŒ–è°ƒè¯•å™¨

```python
import json
from datetime import datetime

class RAGDebugger:
    """RAG è°ƒè¯•å™¨ï¼ˆè®°å½•å®Œæ•´æ‰§è¡Œé“¾è·¯ï¼‰"""

    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.debug_logs = []

    def debug_answer(self, query):
        """è°ƒè¯•æ¨¡å¼å›ç­”ï¼ˆè®°å½•æ‰€æœ‰ä¸­é—´æ­¥éª¤ï¼‰"""

        debug_session = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "steps": []
        }

        # Step 1: Embedding
        step1_start = time.time()
        query_embedding = self.rag_system.embedding_model.embed_query(query)
        debug_session["steps"].append({
            "name": "embedding",
            "duration": time.time() - step1_start,
            "output_preview": f"Vector dimension: {len(query_embedding)}"
        })

        # Step 2: Retrieval
        step2_start = time.time()
        contexts = self.rag_system.vectorstore.similarity_search_with_score(query, k=5)
        debug_session["steps"].append({
            "name": "retrieval",
            "duration": time.time() - step2_start,
            "output": [
                {
                    "content": doc.page_content[:100],
                    "score": float(score),
                    "metadata": doc.metadata
                }
                for doc, score in contexts
            ]
        })

        # Step 3: Prompt Building
        step3_start = time.time()
        prompt = self.rag_system.build_prompt(query, [doc for doc, _ in contexts])
        debug_session["steps"].append({
            "name": "prompt_building",
            "duration": time.time() - step3_start,
            "output": {
                "prompt_length": len(prompt),
                "prompt_preview": prompt[:200]
            }
        })

        # Step 4: LLM Generation
        step4_start = time.time()
        answer = self.rag_system.llm.predict(prompt)
        debug_session["steps"].append({
            "name": "llm_generation",
            "duration": time.time() - step4_start,
            "output": answer
        })

        # ä¿å­˜è°ƒè¯•æ—¥å¿—
        self.debug_logs.append(debug_session)

        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        self._generate_debug_report(debug_session)

        return answer

    def _generate_debug_report(self, session):
        """ç”Ÿæˆå¯è§†åŒ–è°ƒè¯•æŠ¥å‘Š"""
        print("\n" + "="*70)
        print("ğŸ” RAG è°ƒè¯•æŠ¥å‘Š")
        print("="*70)

        print(f"\nâ“ æŸ¥è¯¢: {session['query']}")
        print(f"ğŸ• æ—¶é—´: {session['timestamp']}")

        print("\nğŸ“Š æ‰§è¡Œæ­¥éª¤:")
        for i, step in enumerate(session['steps'], 1):
            print(f"\n{i}. {step['name'].upper()}")
            print(f"   â±ï¸  è€—æ—¶: {step['duration']:.3f}s")

            if 'output' in step:
                if step['name'] == 'retrieval':
                    print(f"   ğŸ“„ æ£€ç´¢åˆ° {len(step['output'])} ä¸ªæ–‡æ¡£:")
                    for j, doc in enumerate(step['output'][:3], 1):
                        print(f"      {j}. ç›¸ä¼¼åº¦: {doc['score']:.3f}")
                        print(f"         å†…å®¹: {doc['content']}...")

                elif step['name'] == 'llm_generation':
                    print(f"   ğŸ’¡ ç­”æ¡ˆ: {step['output'][:200]}...")

        print("\n" + "="*70)

    def export_debug_logs(self, filepath="rag_debug_logs.json"):
        """å¯¼å‡ºè°ƒè¯•æ—¥å¿—"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.debug_logs, f, ensure_ascii=False, indent=2)

        print(f"âœ… è°ƒè¯•æ—¥å¿—å·²å¯¼å‡ºåˆ°: {filepath}")

# ä½¿ç”¨ç¤ºä¾‹
debugger = RAGDebugger(rag_system)

# è°ƒè¯•æ¨¡å¼å›ç­”
answer = debugger.debug_answer("GPT-4 çš„å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ")

# å¯¼å‡ºæ—¥å¿—
debugger.export_debug_logs()
```

## å·¥å…· 2: RAG è¯„ä¼°æ¡†æ¶

```python
class RAGEvaluator:
    """RAG è¯„ä¼°æ¡†æ¶"""

    def __init__(self, rag_system):
        self.rag_system = rag_system

    def evaluate(self, test_set):
        """
        è¯„ä¼° RAG ç³»ç»Ÿ

        test_set: [{"query": "...", "expected_answer": "...", "ground_truth_docs": [...]}, ...]
        """
        metrics = {
            "retrieval": {
                "recall@3": [],
                "recall@5": [],
                "precision@3": [],
                "mrr": []  # Mean Reciprocal Rank
            },
            "generation": {
                "accuracy": [],
                "hallucination_rate": [],
                "citation_coverage": []
            },
            "latency": []
        }

        for test_case in test_set:
            # è¯„ä¼°æ£€ç´¢
            retrieval_metrics = self._evaluate_retrieval(
                test_case["query"],
                test_case["ground_truth_docs"]
            )

            metrics["retrieval"]["recall@3"].append(retrieval_metrics["recall@3"])
            metrics["retrieval"]["recall@5"].append(retrieval_metrics["recall@5"])
            metrics["retrieval"]["mrr"].append(retrieval_metrics["mrr"])

            # è¯„ä¼°ç”Ÿæˆ
            generation_metrics = self._evaluate_generation(
                test_case["query"],
                test_case["expected_answer"]
            )

            metrics["generation"]["accuracy"].append(generation_metrics["accuracy"])
            metrics["generation"]["hallucination_rate"].append(generation_metrics["hallucination"])

            # è¯„ä¼°å»¶è¿Ÿ
            start = time.time()
            self.rag_system.answer(test_case["query"])
            metrics["latency"].append(time.time() - start)

        # è®¡ç®—å¹³å‡å€¼
        final_metrics = {
            "retrieval_recall@3": np.mean(metrics["retrieval"]["recall@3"]),
            "retrieval_recall@5": np.mean(metrics["retrieval"]["recall@5"]),
            "generation_accuracy": np.mean(metrics["generation"]["accuracy"]),
            "hallucination_rate": np.mean(metrics["generation"]["hallucination_rate"]),
            "avg_latency": np.mean(metrics["latency"])
        }

        return final_metrics

    def _evaluate_retrieval(self, query, ground_truth_docs):
        """è¯„ä¼°æ£€ç´¢è´¨é‡"""
        # æ£€ç´¢ Top-5
        retrieved = self.rag_system.retriever.retrieve(query, k=5)
        retrieved_ids = [doc.metadata.get('id') for doc in retrieved]

        # è®¡ç®— Recall@K
        recall_at_3 = len(set(retrieved_ids[:3]) & set(ground_truth_docs)) / len(ground_truth_docs)
        recall_at_5 = len(set(retrieved_ids[:5]) & set(ground_truth_docs)) / len(ground_truth_docs)

        # è®¡ç®— MRR
        mrr = 0.0
        for i, doc_id in enumerate(retrieved_ids, 1):
            if doc_id in ground_truth_docs:
                mrr = 1 / i
                break

        return {
            "recall@3": recall_at_3,
            "recall@5": recall_at_5,
            "mrr": mrr
        }

    def _evaluate_generation(self, query, expected_answer):
        """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
        # ç”Ÿæˆç­”æ¡ˆ
        generated_answer = self.rag_system.answer(query)

        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½œä¸ºå‡†ç¡®ç‡ä»£ç†ï¼‰
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('all-MiniLM-L6-v2')

        emb1 = model.encode(generated_answer)
        emb2 = model.encode(expected_answer)

        accuracy = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))

        # æ£€æµ‹å¹»è§‰ï¼ˆç®€åŒ–ï¼‰
        hallucination = self._detect_hallucination(generated_answer)

        return {
            "accuracy": accuracy,
            "hallucination": hallucination
        }

    def _detect_hallucination(self, answer):
        """ç®€åŒ–çš„å¹»è§‰æ£€æµ‹"""
        # å®é™…åº”ä½¿ç”¨ NLI æ¨¡å‹
        hallucination_keywords = ["æ®æˆ‘æ‰€çŸ¥", "ä¸€èˆ¬æ¥è¯´", "å¯èƒ½", "å¤§æ¦‚"]
        return any(kw in answer for kw in hallucination_keywords)

# ä½¿ç”¨ç¤ºä¾‹
evaluator = RAGEvaluator(rag_system)

test_set = [
    {
        "query": "GPT-4 çš„å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ",
        "expected_answer": "GPT-4 çš„å‚æ•°é‡æœªå…¬å¼€",
        "ground_truth_docs": ["doc_123", "doc_456"]
    },
    # ... æ›´å¤šæµ‹è¯•ç”¨ä¾‹
]

results = evaluator.evaluate(test_set)

print("=== RAG è¯„ä¼°ç»“æœ ===")
for metric, value in results.items():
    print(f"{metric}: {value:.3f}")
```

---

## æ€»ç»“ï¼šæ•…éšœæ’æŸ¥å†³ç­–æ ‘

```
RAG ç³»ç»Ÿæ•…éšœ
â”‚
â”œâ”€ ç­”æ¡ˆè´¨é‡å·®
â”‚  â”œâ”€ æ£€ç´¢ä¸åˆ°ç›¸å…³æ–‡æ¡£
â”‚  â”‚  â”œâ”€ æ£€æŸ¥ 1: åˆ†å—ç­–ç•¥ï¼ˆchunk_size, overlapï¼‰
â”‚  â”‚  â”œâ”€ æ£€æŸ¥ 2: Embedding æ¨¡å‹è´¨é‡
â”‚  â”‚  â”œâ”€ æ£€æŸ¥ 3: è·ç¦»åº¦é‡ï¼ˆL2 vs Cosineï¼‰
â”‚  â”‚  â”œâ”€ æ£€æŸ¥ 4: Top-K è®¾ç½®
â”‚  â”‚  â””â”€ è§£å†³æ–¹æ¡ˆ: æ··åˆæ£€ç´¢ + é‡æ’åº
â”‚  â”‚
â”‚  â”œâ”€ æ£€ç´¢åˆ°ç›¸å…³ä½†å®ä½“ä¸åŒ¹é…
â”‚  â”‚  â””â”€ è§£å†³æ–¹æ¡ˆ: å®ä½“éªŒè¯ + CrossEncoder é‡æ’åº
â”‚  â”‚
â”‚  â”œâ”€ ç­”æ¡ˆåŒ…å«å¹»è§‰
â”‚  â”‚  â”œâ”€ æ£€æŸ¥ 1: Prompt è®¾è®¡
â”‚  â”‚  â”œâ”€ æ£€æŸ¥ 2: Temperature å‚æ•°
â”‚  â”‚  â””â”€ è§£å†³æ–¹æ¡ˆ: é˜²å¹»è§‰ Prompt + äº‹åéªŒè¯ + å¼•ç”¨çº¦æŸ
â”‚  â”‚
â”‚  â””â”€ ä¸Šä¸‹æ–‡è¢«æˆªæ–­
â”‚     â””â”€ è§£å†³æ–¹æ¡ˆ: ä¸Šä¸‹æ–‡å‹ç¼© / Map-Reduce / Refine
â”‚
â”œâ”€ æ€§èƒ½é—®é¢˜
â”‚  â”œâ”€ å»¶è¿Ÿé«˜
â”‚  â”‚  â”œâ”€ æ£€æŸ¥ 1: å„é˜¶æ®µè€—æ—¶åˆ†æ
â”‚  â”‚  â”œâ”€ è§£å†³æ–¹æ¡ˆ 1: æµå¼è¾“å‡ºï¼ˆé™ä½æ„ŸçŸ¥å»¶è¿Ÿï¼‰
â”‚  â”‚  â”œâ”€ è§£å†³æ–¹æ¡ˆ 2: å¹¶è¡ŒåŒ–
â”‚  â”‚  â”œâ”€ è§£å†³æ–¹æ¡ˆ 3: ç¼“å­˜
â”‚  â”‚  â””â”€ è§£å†³æ–¹æ¡ˆ 4: æœ¬åœ° Embedding
â”‚  â”‚
â”‚  â””â”€ æˆæœ¬é«˜
â”‚     â”œâ”€ è§£å†³æ–¹æ¡ˆ 1: æ··åˆæ¨¡å‹ç­–ç•¥
â”‚     â”œâ”€ è§£å†³æ–¹æ¡ˆ 2: å‹ç¼©ä¸Šä¸‹æ–‡
â”‚     â”œâ”€ è§£å†³æ–¹æ¡ˆ 3: æ‰¹é‡å¤„ç†
â”‚     â””â”€ è§£å†³æ–¹æ¡ˆ 4: è‡ªéƒ¨ç½²å¼€æºæ¨¡å‹
â”‚
â””â”€ ç¨³å®šæ€§é—®é¢˜
   â”œâ”€ API é™æµ
   â”‚  â”œâ”€ è§£å†³æ–¹æ¡ˆ 1: é€Ÿç‡é™åˆ¶å™¨
   â”‚  â”œâ”€ è§£å†³æ–¹æ¡ˆ 2: æŒ‡æ•°é€€é¿é‡è¯•
   â”‚  â””â”€ è§£å†³æ–¹æ¡ˆ 3: è¯·æ±‚é˜Ÿåˆ—
   â”‚
   â”œâ”€ è¶…æ—¶
   â”‚  â””â”€ è§£å†³æ–¹æ¡ˆ: è¶…æ—¶æ§åˆ¶ + é™çº§ç­–ç•¥
   â”‚
   â””â”€ å¹¶å‘é”™è¯¯
      â””â”€ è§£å†³æ–¹æ¡ˆ: çº¿ç¨‹å®‰å…¨ + é˜Ÿåˆ—åŒ–
```

---

## é™„å½•ï¼šå¿«é€Ÿè¯Šæ–­æ£€æŸ¥æ¸…å•

### æ£€ç´¢è´¨é‡é—®é¢˜
- [ ] åˆ†å—å‚æ•°åˆç†ï¼Ÿï¼ˆchunk_size: 500-1000, overlap: 10-20%ï¼‰
- [ ] è·ç¦»åº¦é‡æ­£ç¡®ï¼Ÿï¼ˆæ–‡æœ¬æ¨è cosineï¼‰
- [ ] Top-K å€¼åˆç†ï¼Ÿï¼ˆæ¨è 3-5ï¼‰
- [ ] æ˜¯å¦éœ€è¦æ··åˆæ£€ç´¢ï¼Ÿï¼ˆç²¾ç¡®åŒ¹é…åœºæ™¯ï¼‰
- [ ] æ˜¯å¦éœ€è¦é‡æ’åºï¼Ÿï¼ˆCrossEncoderï¼‰

### ç”Ÿæˆè´¨é‡é—®é¢˜
- [ ] Prompt æ˜¯å¦ç¦æ­¢å¹»è§‰ï¼Ÿ
- [ ] Temperature æ˜¯å¦è¿‡é«˜ï¼Ÿï¼ˆäº‹å®æ€§å›ç­”æ¨è 0-0.3ï¼‰
- [ ] æ˜¯å¦å¼ºåˆ¶å¼•ç”¨ï¼Ÿ
- [ ] æ˜¯å¦æœ‰äº‹åéªŒè¯ï¼Ÿ
- [ ] ä¸Šä¸‹æ–‡æ˜¯å¦è¶…é™ï¼Ÿ

### æ€§èƒ½é—®é¢˜
- [ ] æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼Ÿ
- [ ] Embedding æ˜¯å¦å¯æœ¬åœ°åŒ–ï¼Ÿ
- [ ] é‡æ’åºå€™é€‰æ•°æ˜¯å¦è¿‡å¤šï¼Ÿ
- [ ] æ˜¯å¦å¯ç”¨ç¼“å­˜ï¼Ÿ
- [ ] æ¨¡å‹é€‰æ‹©æ˜¯å¦åˆç†ï¼Ÿ

### æˆæœ¬é—®é¢˜
- [ ] æ˜¯å¦å¯ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹ï¼Ÿ
- [ ] ä¸Šä¸‹æ–‡æ˜¯å¦å¯å‹ç¼©ï¼Ÿ
- [ ] æ˜¯å¦å¯ç”¨ç¼“å­˜ï¼Ÿ
- [ ] æ˜¯å¦è€ƒè™‘è‡ªéƒ¨ç½²ï¼Ÿ

### ç¨³å®šæ€§é—®é¢˜
- [ ] æ˜¯å¦æœ‰é€Ÿç‡é™åˆ¶ï¼Ÿ
- [ ] æ˜¯å¦æœ‰é‡è¯•æœºåˆ¶ï¼Ÿ
- [ ] æ˜¯å¦æœ‰é™çº§ç­–ç•¥ï¼Ÿ
- [ ] æ˜¯å¦æœ‰è¶…æ—¶æ§åˆ¶ï¼Ÿ

---

**æœ¬æŒ‡å—æ¶µç›– RAG ç³»ç»Ÿ 90% çš„å¸¸è§æ•…éšœåŠè§£å†³æ–¹æ¡ˆï¼Œé€‚åˆä½œä¸ºç”Ÿäº§ç¯å¢ƒçš„æ’æŸ¥æ‰‹å†Œã€‚**
