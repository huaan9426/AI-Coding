# 第6章：配置与安全管理

> 在第5章中，我们系统学习了Kubernetes的存储管理，包括ConfigMap和Secret的基础用法。本章将在此基础上，深入探讨Kubernetes的配置管理和安全机制，构建生产级的安全防护体系。

**本章学习目标：**
- 掌握资源配额（ResourceQuota）和限制范围（LimitRange）
- 理解Pod安全策略（Pod Security）
- 深入学习RBAC权限控制体系
- 掌握网络策略（NetworkPolicy）
- 了解审计日志和安全扫描
- 构建完整的安全防护体系

---

## 6.1 资源配额与限制

在多租户Kubernetes集群中，资源管理是一个核心问题。如何防止某个应用或租户过度消耗资源？如何确保集群资源的公平分配？本节将深入学习Kubernetes的资源配额和限制机制。

### 6.1.1 为什么需要资源配额

#### 6.1.1.1 多租户环境的挑战

在实际生产环境中，一个Kubernetes集群通常会被多个团队或应用共享：

```
┌─────────────────────────────────────────────────┐
│          多租户集群资源竞争场景                  │
├─────────────────────────────────────────────────┤
│  团队A：电商应用（高峰期流量大）                │
│  团队B：数据处理（计算密集型）                  │
│  团队C：开发测试（资源需求不稳定）              │
│                                                  │
│  问题：                                          │
│  - 团队A在促销期间疯狂扩容，占用80%资源         │
│  - 团队B的批处理任务导致其他服务响应变慢       │
│  - 团队C的测试Pod泄漏，持续消耗资源            │
│                                                  │
│  后果：                                          │
│  ❌ 资源争抢导致服务不稳定                      │
│  ❌ 关键应用无法获得足够资源                    │
│  ❌ 成本失控，账单暴增                          │
└─────────────────────────────────────────────────┘
```

#### 6.1.1.2 资源管理的痛点

**没有资源配额的情况：**

| 问题 | 场景 | 影响 |
|-----|------|------|
| **资源霸占** | 单个应用申请过多资源 | 其他应用无法部署 |
| **成本失控** | 无限制创建资源 | 云账单暴增 |
| **稳定性问题** | 资源耗尽导致集群不稳定 | 服务大面积故障 |
| **公平性缺失** | 先到先得的资源分配 | 重要应用资源不足 |
| **容量规划困难** | 无法预测资源使用情况 | 扩容滞后 |

**真实案例：**

```yaml
# ❌ 不受控制的资源申请
apiVersion: apps/v1
kind: Deployment
metadata:
  name: greedy-app
  namespace: team-a
spec:
  replicas: 100        # 过度扩容
  template:
    spec:
      containers:
      - name: app
        image: myapp:1.0
        resources:
          requests:
            memory: "8Gi"      # 每个Pod申请8Gi内存
            cpu: "4"           # 每个Pod申请4核CPU
        # 总需求：800Gi内存 + 400核CPU
        # 如果集群只有1000Gi内存，其他应用将无法运行！
```

#### 6.1.1.3 资源配额的价值

**引入资源配额后：**

```
┌─────────────────────────────────────────────────┐
│          资源配额带来的好处                      │
├─────────────────────────────────────────────────┤
│  ✅ 资源隔离                                     │
│     每个Namespace有明确的资源上限                │
│                                                  │
│  ✅ 成本控制                                     │
│     防止资源滥用，控制云成本                    │
│                                                  │
│  ✅ 公平性                                       │
│     确保资源公平分配给各团队                    │
│                                                  │
│  ✅ 稳定性                                       │
│     防止单点故障影响整个集群                    │
│                                                  │
│  ✅ 容量规划                                     │
│     清晰的资源使用预期，便于扩容决策            │
└─────────────────────────────────────────────────┘
```

### 6.1.2 ResourceQuota资源配额

#### 6.1.2.1 ResourceQuota核心概念

ResourceQuota是Namespace级别的资源，用于限制该Namespace下所有资源的总量。

**基本结构：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota-example
  namespace: team-a
spec:
  hard:
    # 计算资源配额
    requests.cpu: "10"           # CPU请求总量：10核
    requests.memory: "20Gi"      # 内存请求总量：20Gi
    limits.cpu: "20"             # CPU限制总量：20核
    limits.memory: "40Gi"        # 内存限制总量：40Gi

    # 对象数量配额
    pods: "50"                   # 最多50个Pod
    services: "10"               # 最多10个Service
    persistentvolumeclaims: "5"  # 最多5个PVC

    # 存储配额
    requests.storage: "100Gi"    # 存储请求总量：100Gi
```

**工作原理：**

```
┌─────────────────────────────────────────────────┐
│          ResourceQuota工作流程                   │
├─────────────────────────────────────────────────┤
│                                                  │
│  1. 用户提交Pod/Deployment                      │
│           ↓                                      │
│  2. API Server检查ResourceQuota                 │
│           ↓                                      │
│  3. 计算当前Namespace资源使用量                 │
│           ↓                                      │
│  4. 判断：新资源 + 已用资源 <= 配额？           │
│           ↓                 ↓                    │
│         YES               NO                     │
│           ↓                 ↓                    │
│      允许创建          拒绝创建                 │
│           ↓                 ↓                    │
│      资源计数+1    返回错误：exceeded quota      │
└─────────────────────────────────────────────────┘
```

#### 6.1.2.2 计算资源配额

**示例1：限制CPU和内存**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: development
spec:
  hard:
    # CPU配额
    requests.cpu: "4"      # 该Namespace下所有Pod的CPU请求总和不超过4核
    limits.cpu: "8"        # 该Namespace下所有Pod的CPU限制总和不超过8核

    # 内存配额
    requests.memory: "8Gi"   # 内存请求总和不超过8Gi
    limits.memory: "16Gi"    # 内存限制总和不超过16Gi
```

**应用到Namespace：**

```bash
kubectl apply -f compute-quota.yaml
kubectl get resourcequota -n development
```

**测试配额：**

```yaml
# 尝试创建Pod
apiVersion: v1
kind: Pod
metadata:
  name: test-pod-1
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "4Gi"
        cpu: "2"
```

```bash
# 创建第1个Pod - 成功（使用：CPU 1核，内存2Gi）
kubectl apply -f test-pod-1.yaml

# 创建第2个Pod - 成功（累计：CPU 2核，内存4Gi）
kubectl apply -f test-pod-2.yaml

# 创建第3个Pod - 成功（累计：CPU 3核，内存6Gi）
kubectl apply -f test-pod-3.yaml

# 创建第4个Pod - 成功（累计：CPU 4核，内存8Gi，达到requests上限）
kubectl apply -f test-pod-4.yaml

# 创建第5个Pod - 失败！
kubectl apply -f test-pod-5.yaml
# Error: exceeded quota: compute-quota,
# requested: requests.cpu=1,requests.memory=2Gi,
# used: requests.cpu=4,requests.memory=8Gi,
# limited: requests.cpu=4,requests.memory=8Gi
```

**查看配额使用情况：**

```bash
kubectl describe resourcequota compute-quota -n development
```

输出：
```
Name:            compute-quota
Namespace:       development
Resource         Used   Hard
--------         ----   ----
limits.cpu       8      8
limits.memory    16Gi   16Gi
requests.cpu     4      4
requests.memory  8Gi    8Gi
```

#### 6.1.2.3 对象数量配额

**示例2：限制对象数量**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota
  namespace: development
spec:
  hard:
    # Pod数量
    pods: "20"

    # Service数量
    services: "5"
    services.loadbalancers: "2"      # LoadBalancer类型Service
    services.nodeports: "3"          # NodePort类型Service

    # PVC数量
    persistentvolumeclaims: "10"

    # ConfigMap和Secret数量
    configmaps: "20"
    secrets: "20"

    # ReplicationController数量
    replicationcontrollers: "10"
```

**按存储类限制PVC：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: development
spec:
  hard:
    # 总存储配额
    requests.storage: "100Gi"
    persistentvolumeclaims: "10"

    # 按StorageClass限制
    nfs-client.storageclass.storage.k8s.io/requests.storage: "50Gi"
    nfs-client.storageclass.storage.k8s.io/persistentvolumeclaims: "5"

    local-storage.storageclass.storage.k8s.io/requests.storage: "30Gi"
    local-storage.storageclass.storage.k8s.io/persistentvolumeclaims: "3"
```

#### 6.1.2.4 作用域选择器（Scope Selector）

ResourceQuota可以根据Pod的优先级或QoS类别进行限制。

**按优先级限制：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: high-priority-quota
  namespace: production
spec:
  hard:
    pods: "100"
    requests.cpu: "50"
    requests.memory: "100Gi"
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["high-priority"]   # 只对high-priority优先级的Pod生效
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: low-priority-quota
  namespace: production
spec:
  hard:
    pods: "20"
    requests.cpu: "10"
    requests.memory: "20Gi"
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["low-priority"]    # 只对low-priority优先级的Pod生效
```

**按QoS类别限制：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-quota
  namespace: development
spec:
  hard:
    pods: "10"            # BestEffort类Pod最多10个
  scopes:
  - BestEffort            # 只对BestEffort QoS的Pod生效
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: notbesteffort-quota
  namespace: development
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
  scopes:
  - NotBestEffort         # 对Burstable和Guaranteed QoS的Pod生效
```

**QoS类别说明：**

| QoS类别 | 条件 | 特点 |
|--------|------|------|
| **Guaranteed** | requests = limits（且所有容器都设置） | 最高优先级，不会被OOM Kill |
| **Burstable** | 设置了requests，但requests < limits | 中等优先级，资源不足时可能被Kill |
| **BestEffort** | 未设置requests和limits | 最低优先级，优先被Kill |

### 6.1.3 LimitRange限制范围

#### 6.1.3.1 LimitRange核心概念

LimitRange是Namespace级别的资源，用于限制单个资源对象的大小。

**ResourceQuota vs LimitRange：**

```
┌──────────────────┬─────────────────┬─────────────────┐
│    特性          │  ResourceQuota  │   LimitRange    │
├──────────────────┼─────────────────┼─────────────────┤
│  作用范围        │  Namespace总量  │  单个对象       │
│  限制内容        │  总资源上限     │  单个资源上下限 │
│  典型用途        │  多租户资源隔离 │  防止单点过大   │
│  示例            │  总CPU不超过10核│  单Pod不超过2核 │
└──────────────────┴─────────────────┴─────────────────┘
```

**组合使用场景：**

```yaml
# ResourceQuota: 限制总量
# development namespace总共最多使用10核CPU

# LimitRange: 限制单个对象
# 每个Pod最多使用2核CPU
# 这样可以确保至少能运行5个Pod
```

#### 6.1.3.2 限制Pod和容器资源

**示例1：限制容器资源范围**

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: container-limit-range
  namespace: development
spec:
  limits:
  - type: Container
    # 默认值（如果未指定）
    default:
      cpu: "500m"        # 默认limit
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"        # 默认request
      memory: "256Mi"

    # 最大值
    max:
      cpu: "2"           # 单个容器最多2核
      memory: "4Gi"      # 单个容器最多4Gi内存

    # 最小值
    min:
      cpu: "100m"        # 单个容器至少100m CPU
      memory: "128Mi"    # 单个容器至少128Mi内存

    # 最大/最小比例
    maxLimitRequestRatio:
      cpu: "4"           # limit最多是request的4倍
      memory: "4"
```

**效果演示：**

```yaml
# 案例1：未指定资源 - 自动应用默认值
apiVersion: v1
kind: Pod
metadata:
  name: pod-default
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    # 未指定resources，自动应用：
    # requests: cpu=250m, memory=256Mi
    # limits: cpu=500m, memory=512Mi
```

```yaml
# 案例2：超过最大值 - 拒绝创建
apiVersion: v1
kind: Pod
metadata:
  name: pod-too-large
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "3"      # ❌ 超过max（2核）
# Error: [spec.containers[0].resources.requests.cpu: Invalid value: "3": must be less than or equal to cpu limit of 2]
```

```yaml
# 案例3：低于最小值 - 拒绝创建
apiVersion: v1
kind: Pod
metadata:
  name: pod-too-small
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "50m"    # ❌ 低于min（100m）
# Error: minimum cpu usage per Container is 100m
```

```yaml
# 案例4：比例不合理 - 拒绝创建
apiVersion: v1
kind: Pod
metadata:
  name: pod-bad-ratio
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "200m"
      limits:
        cpu: "1"       # ❌ 比例5倍，超过maxLimitRequestRatio（4倍）
# Error: cpu max limit to request ratio per Container is 4
```

**示例2：限制Pod资源范围**

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: pod-limit-range
  namespace: development
spec:
  limits:
  - type: Pod
    max:
      cpu: "4"           # 单个Pod（所有容器总和）最多4核
      memory: "8Gi"      # 单个Pod（所有容器总和）最多8Gi
    min:
      cpu: "200m"
      memory: "256Mi"
```

**多容器Pod示例：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "1"
        memory: "2Gi"

  - name: sidecar
    image: busybox
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"

  # Pod总资源：cpu=1.5, memory=3Gi
  # 符合LimitRange: max pod cpu=4, memory=8Gi ✅
```

#### 6.1.3.3 限制PVC存储

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: storage-limit-range
  namespace: development
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: "50Gi"    # 单个PVC最大50Gi
    min:
      storage: "1Gi"     # 单个PVC最小1Gi
```

**测试：**

```yaml
# ✅ 符合范围
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-valid
  namespace: development
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi    # 在1Gi-50Gi范围内

# ❌ 超过最大值
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-too-large
  namespace: development
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi   # 超过50Gi上限
# Error: maximum storage usage per PersistentVolumeClaim is 50Gi
```

### 6.1.4 实战案例：多环境资源配额

#### 6.1.4.1 场景设计

为一家公司的三个环境（Dev、Test、Prod）配置不同的资源配额：

| 环境 | Pod数量 | CPU总量 | 内存总量 | 存储总量 | 优先级 |
|-----|--------|---------|---------|---------|--------|
| **Dev** | 50 | 10核 | 20Gi | 100Gi | 低 |
| **Test** | 30 | 20核 | 40Gi | 200Gi | 中 |
| **Prod** | 100 | 50核 | 100Gi | 500Gi | 高 |

#### 6.1.4.2 Dev环境配置

**dev-namespace.yaml：**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    environment: development
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-resource-quota
  namespace: dev
spec:
  hard:
    # 计算资源
    requests.cpu: "10"
    requests.memory: "20Gi"
    limits.cpu: "15"
    limits.memory: "30Gi"

    # 对象数量
    pods: "50"
    services: "10"
    persistentvolumeclaims: "10"
    configmaps: "30"
    secrets: "30"

    # 存储
    requests.storage: "100Gi"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limit-range
  namespace: dev
spec:
  limits:
  # 容器限制
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "200m"
      memory: "256Mi"
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "50m"
      memory: "64Mi"
    maxLimitRequestRatio:
      cpu: "4"
      memory: "4"

  # Pod限制
  - type: Pod
    max:
      cpu: "4"
      memory: "8Gi"

  # PVC限制
  - type: PersistentVolumeClaim
    max:
      storage: "20Gi"
    min:
      storage: "1Gi"
```

#### 6.1.4.3 Prod环境配置

**prod-namespace.yaml：**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: prod
  labels:
    environment: production
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: prod-resource-quota
  namespace: prod
spec:
  hard:
    # 计算资源（更高配额）
    requests.cpu: "50"
    requests.memory: "100Gi"
    limits.cpu: "80"
    limits.memory: "150Gi"

    # 对象数量
    pods: "100"
    services: "20"
    persistentvolumeclaims: "30"
    configmaps: "50"
    secrets: "50"

    # 存储
    requests.storage: "500Gi"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: prod-limit-range
  namespace: prod
spec:
  limits:
  # 容器限制（更宽松）
  - type: Container
    default:
      cpu: "1"
      memory: "2Gi"
    defaultRequest:
      cpu: "500m"
      memory: "1Gi"
    max:
      cpu: "8"          # 生产环境允许更大的容器
      memory: "16Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    maxLimitRequestRatio:
      cpu: "4"
      memory: "4"

  # Pod限制
  - type: Pod
    max:
      cpu: "16"
      memory: "32Gi"

  # PVC限制
  - type: PersistentVolumeClaim
    max:
      storage: "100Gi"
    min:
      storage: "5Gi"
```

#### 6.1.4.4 部署和验证

```bash
# 部署环境配置
kubectl apply -f dev-namespace.yaml
kubectl apply -f test-namespace.yaml
kubectl apply -f prod-namespace.yaml

# 查看配额
kubectl get resourcequota --all-namespaces
kubectl get limitrange --all-namespaces

# 查看Dev环境详情
kubectl describe resourcequota dev-resource-quota -n dev
kubectl describe limitrange dev-limit-range -n dev

# 查看Prod环境详情
kubectl describe resourcequota prod-resource-quota -n prod
kubectl describe limitrange prod-limit-range -n prod
```

**部署测试应用：**

```yaml
# dev-test-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-app
  namespace: dev
spec:
  replicas: 3
  selector:
    matchLabels:
      app: test-app
  template:
    metadata:
      labels:
        app: test-app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        # 未指定资源，自动应用LimitRange默认值：
        # requests: cpu=200m, memory=256Mi
        # limits: cpu=500m, memory=512Mi
```

```bash
kubectl apply -f dev-test-deployment.yaml

# 查看资源使用情况
kubectl describe resourcequota dev-resource-quota -n dev
# 输出：
# Resource         Used    Hard
# --------         ----    ----
# pods             3       50
# requests.cpu     600m    10
# requests.memory  768Mi   20Gi
# ...
```

### 6.1.5 最佳实践

#### 6.1.5.1 资源配额设计原则

**1. 合理规划配额：**

```yaml
# ✅ 推荐：基于实际监控数据设置
# 监控1个月后发现：
# - 平均CPU使用：8核
# - 峰值CPU使用：12核
# - 设置配额：15核（留有余量）

apiVersion: v1
kind: ResourceQuota
metadata:
  name: data-driven-quota
spec:
  hard:
    requests.cpu: "15"    # 峰值 + 25% buffer
    requests.memory: "30Gi"
```

**2. 分层配额策略：**

```
┌─────────────────────────────────────────┐
│         分层配额架构                     │
├─────────────────────────────────────────┤
│  第1层：集群级别（总资源池）             │
│    └─ 总CPU: 100核, 总内存: 200Gi      │
│                                          │
│  第2层：环境级别（Namespace配额）        │
│    ├─ Prod: 50核, 100Gi (50%)          │
│    ├─ Test: 30核, 60Gi (30%)           │
│    └─ Dev: 20核, 40Gi (20%)            │
│                                          │
│  第3层：应用级别（LimitRange限制）       │
│    └─ 单Pod: ≤4核, ≤8Gi                │
└─────────────────────────────────────────┘
```

**3. 强制要求资源设置：**

```yaml
# 部署ResourceQuota后，必须为所有Pod设置资源
# 否则Pod无法创建

# ❌ 没有ResourceQuota的情况
apiVersion: v1
kind: Pod
metadata:
  name: pod-no-resources
spec:
  containers:
  - name: app
    image: nginx:1.21
    # 未设置resources - 可以创建

# ✅ 有ResourceQuota的情况
# 必须设置resources或配置LimitRange默认值
```

**配置LimitRange提供默认值：**

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
spec:
  limits:
  - type: Container
    defaultRequest:    # 为未指定的Pod提供默认request
      cpu: "100m"
      memory: "128Mi"
    default:           # 为未指定的Pod提供默认limit
      cpu: "200m"
      memory: "256Mi"
```

#### 6.1.5.2 监控和告警

**查看配额使用情况：**

```bash
# 查看所有Namespace的配额
kubectl get resourcequota --all-namespaces

# 详细查看某个配额
kubectl describe resourcequota <name> -n <namespace>

# 查看配额使用率（自定义脚本）
kubectl get resourcequota dev-resource-quota -n dev -o json | \
  jq '.status.used, .status.hard'
```

**配置Prometheus告警：**

```yaml
# prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: resourcequota-alerts
spec:
  groups:
  - name: resourcequota
    interval: 30s
    rules:
    # CPU配额使用率超过80%
    - alert: NamespaceCPUQuotaExceeding
      expr: |
        sum(kube_resourcequota{resource="requests.cpu", type="used"}) by (namespace)
        /
        sum(kube_resourcequota{resource="requests.cpu", type="hard"}) by (namespace)
        > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Namespace {{ $labels.namespace }} CPU quota exceeding 80%"

    # 内存配额使用率超过80%
    - alert: NamespaceMemoryQuotaExceeding
      expr: |
        sum(kube_resourcequota{resource="requests.memory", type="used"}) by (namespace)
        /
        sum(kube_resourcequota{resource="requests.memory", type="hard"}) by (namespace)
        > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Namespace {{ $labels.namespace }} memory quota exceeding 80%"
```

#### 6.1.5.3 常见问题排查

**问题1：Pod无法创建，提示exceeded quota**

```bash
# 错误信息
Error from server (Forbidden): error when creating "pod.yaml":
pods "myapp" is forbidden: exceeded quota: compute-quota,
requested: requests.cpu=2,requests.memory=4Gi,
used: requests.cpu=9,requests.memory=18Gi,
limited: requests.cpu=10,requests.memory=20Gi

# 排查步骤
# 1. 查看当前配额使用情况
kubectl describe resourcequota compute-quota -n <namespace>

# 2. 找出资源消耗大的Pod
kubectl top pods -n <namespace> --sort-by=cpu
kubectl top pods -n <namespace> --sort-by=memory

# 3. 解决方案
# 方案A：删除不必要的Pod
kubectl delete pod <unused-pod> -n <namespace>

# 方案B：减少新Pod的资源请求
# 修改Deployment的resources.requests

# 方案C：增加配额（需要管理员权限）
kubectl edit resourcequota compute-quota -n <namespace>
```

**问题2：Pod一直Pending，提示Insufficient CPU/Memory**

```bash
# 查看Pod事件
kubectl describe pod <pod-name> -n <namespace>
# Events:
#   Warning  FailedScheduling  Pod didn't fit: Insufficient CPU

# 原因分析：
# 1. ResourceQuota限制了总量
# 2. LimitRange限制了单个Pod大小
# 3. 节点实际资源不足

# 检查LimitRange
kubectl describe limitrange -n <namespace>

# 检查节点资源
kubectl top nodes
kubectl describe node <node-name>
```

**问题3：ResourceQuota未生效**

```bash
# 可能原因1：Pod未设置resources
# 解决：配置LimitRange提供默认值

# 可能原因2：ResourceQuota配置错误
kubectl get resourcequota <name> -n <namespace> -o yaml
# 检查spec.hard字段是否正确

# 可能原因3：权限问题
kubectl auth can-i create resourcequotas --as=system:serviceaccount:<namespace>:<serviceaccount>
```

本节我们深入学习了Kubernetes的资源配额和限制机制。通过ResourceQuota可以限制Namespace的总资源，通过LimitRange可以限制单个对象的资源范围。合理使用这些机制，可以实现多租户环境的资源隔离、成本控制和稳定性保障。

接下来，我们将学习Pod安全策略，进一步加固集群的安全防护。

---

## 6.2 Pod安全策略

容器安全是Kubernetes集群安全的核心。恶意或配置不当的容器可能导致容器逃逸、权限提升、数据泄露等严重安全问题。本节将深入学习Kubernetes的Pod安全机制，构建纵深防御体系。

### 6.2.1 为什么需要Pod安全策略

#### 6.2.1.1 容器安全威胁

**真实的安全风险：**

```
┌─────────────────────────────────────────────────┐
│          容器常见安全威胁                        │
├─────────────────────────────────────────────────┤
│  1. 特权容器（Privileged Container）            │
│     - 拥有宿主机root权限                        │
│     - 可以访问宿主机所有设备                    │
│     - 可以修改宿主机系统配置                    │
│                                                  │
│  2. 容器逃逸（Container Escape）                │
│     - 突破容器隔离，访问宿主机                  │
│     - 利用内核漏洞或错误配置                    │
│     - 影响同节点所有容器                        │
│                                                  │
│  3. 权限提升（Privilege Escalation）            │
│     - 通过setuid/setgid提升权限                 │
│     - 访问敏感的宿主机路径                      │
│     - 修改宿主机文件系统                        │
│                                                  │
│  4. 敏感数据泄露                                 │
│     - 挂载宿主机敏感目录                        │
│     - 访问其他容器的数据                        │
│     - 读取Kubernetes Secret                     │
└─────────────────────────────────────────────────┘
```

**危险的Pod配置示例：**

```yaml
# ❌ 极度危险的Pod配置
apiVersion: v1
kind: Pod
metadata:
  name: dangerous-pod
spec:
  hostNetwork: true           # 使用宿主机网络
  hostPID: true               # 使用宿主机PID命名空间
  hostIPC: true               # 使用宿主机IPC命名空间

  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      privileged: true        # 特权模式
      runAsUser: 0            # 以root用户运行
      allowPrivilegeEscalation: true
      capabilities:
        add:
        - ALL                 # 添加所有Linux Capabilities

    volumeMounts:
    - name: host-root
      mountPath: /host

  volumes:
  - name: host-root
    hostPath:
      path: /                 # 挂载宿主机根目录！
      type: Directory
```

**攻击场景演示：**

```bash
# 恶意Pod可以做什么？

# 1. 访问宿主机文件系统
kubectl exec dangerous-pod -- ls /host
# 输出：bin  boot  dev  etc  home  root  ...
# 看到宿主机的完整文件系统！

# 2. 查看宿主机进程
kubectl exec dangerous-pod -- ps aux
# 可以看到宿主机上所有进程

# 3. 修改宿主机配置
kubectl exec dangerous-pod -- chroot /host bash
# 已经完全控制宿主机！

# 4. 访问其他容器数据
kubectl exec dangerous-pod -- ls /host/var/lib/docker/containers
# 可以访问其他容器的数据
```

#### 6.2.1.2 安全防护的必要性

**没有安全策略的后果：**

| 风险 | 影响 | 真实案例 |
|-----|------|---------|
| **容器逃逸** | 攻击者控制宿主机和所有容器 | CVE-2019-5736 runc漏洞 |
| **加密货币挖矿** | 消耗大量CPU资源，成本暴增 | Tesla K8s集群被入侵挖矿 |
| **数据泄露** | 敏感数据被窃取 | 配置不当导致Secret暴露 |
| **横向渗透** | 从一个容器攻击整个集群 | 网络未隔离导致蔓延 |

**Pod安全的核心目标：**

```
┌─────────────────────────────────────────────────┐
│          Pod安全防护目标                         │
├─────────────────────────────────────────────────┤
│  ✅ 最小权限原则                                 │
│     容器只获得完成任务所需的最小权限            │
│                                                  │
│  ✅ 隔离性                                       │
│     容器与宿主机、容器与容器之间充分隔离        │
│                                                  │
│  ✅ 不可变性                                     │
│     容器文件系统只读，防止恶意修改              │
│                                                  │
│  ✅ 可审计性                                     │
│     所有安全相关操作都可追溯                    │
│                                                  │
│  ✅ 纵深防御                                     │
│     多层防护，单点失效不导致整体沦陷            │
└─────────────────────────────────────────────────┘
```

### 6.2.2 Pod Security Standards（PSS）

从Kubernetes 1.25开始，Pod Security Policy（PSP）被废弃，取而代之的是Pod Security Standards（PSS）和Pod Security Admission（PSA）。

#### 6.2.2.1 三种安全级别

**Pod Security Standards定义了三种安全级别：**

| 级别 | 限制程度 | 适用场景 | 主要限制 |
|-----|---------|---------|---------|
| **Privileged** | 无限制 | 信任的系统组件 | 允许任何配置 |
| **Baseline** | 最小限制 | 一般应用 | 禁止已知的特权提升 |
| **Restricted** | 严格限制 | 安全敏感应用 | 强制执行最佳安全实践 |

**安全级别对比：**

```
┌──────────────┬─────────────┬─────────────┬─────────────┐
│   配置项     │ Privileged  │  Baseline   │ Restricted  │
├──────────────┼─────────────┼─────────────┼─────────────┤
│ privileged   │     ✅      │      ❌     │      ❌     │
│ hostNetwork  │     ✅      │      ❌     │      ❌     │
│ hostPID      │     ✅      │      ❌     │      ❌     │
│ hostIPC      │     ✅      │      ❌     │      ❌     │
│ hostPath     │     ✅      │   ⚠️ 部分   │      ❌     │
│ runAsNonRoot │     -       │      -      │      ✅     │
│ capabilities │     ✅      │   ⚠️ 限制   │   ❌ 严格   │
│ seccompProfile│    -       │      -      │      ✅     │
└──────────────┴─────────────┴─────────────┴─────────────┘

✅ = 允许
❌ = 禁止
⚠️ = 部分允许/限制
-  = 不强制
```

#### 6.2.2.2 Baseline级别详解

**Baseline级别禁止的配置：**

```yaml
# Baseline级别不允许的配置

# ❌ 特权容器
spec:
  containers:
  - name: app
    securityContext:
      privileged: true        # 禁止

# ❌ 宿主机命名空间
spec:
  hostNetwork: true            # 禁止
  hostPID: true                # 禁止
  hostIPC: true                # 禁止

# ❌ 不安全的hostPath
spec:
  volumes:
  - name: host-path
    hostPath:
      path: /                  # 禁止挂载根目录
      path: /etc               # 禁止挂载敏感目录
      path: /sys               # 禁止

# ❌ 危险的Capabilities
spec:
  containers:
  - name: app
    securityContext:
      capabilities:
        add:
        - SYS_ADMIN           # 禁止
        - NET_ADMIN           # 部分场景禁止
        - ALL                 # 禁止

# ❌ hostPorts
spec:
  containers:
  - name: app
    ports:
    - containerPort: 80
      hostPort: 80            # 禁止（0-1024端口）

# ❌ AppArmor配置
metadata:
  annotations:
    container.apparmor.security.beta.kubernetes.io/app: unconfined  # 禁止

# ❌ SELinux自定义选项
spec:
  securityContext:
    seLinuxOptions:
      type: custom_t          # 禁止自定义
```

**Baseline级别允许的配置：**

```yaml
# ✅ Baseline级别允许的安全配置
apiVersion: v1
kind: Pod
metadata:
  name: baseline-compliant-pod
spec:
  # 非特权模式
  containers:
  - name: app
    image: nginx:1.21

    securityContext:
      # ✅ 非root用户
      runAsUser: 1000
      runAsGroup: 3000

      # ✅ 禁止特权提升
      allowPrivilegeEscalation: false

      # ✅ 只读根文件系统
      readOnlyRootFilesystem: true

      # ✅ 允许的capabilities
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE   # 允许绑定1024以下端口

    # ✅ 允许的volume类型
    volumeMounts:
    - name: config
      mountPath: /etc/config
    - name: cache
      mountPath: /tmp

  volumes:
  - name: config
    configMap:              # ✅ ConfigMap
      name: app-config
  - name: cache
    emptyDir: {}            # ✅ emptyDir
```

#### 6.2.2.3 Restricted级别详解

**Restricted级别的严格要求：**

```yaml
# ✅ Restricted级别合规的Pod
apiVersion: v1
kind: Pod
metadata:
  name: restricted-compliant-pod
spec:
  # 必须：Pod级别安全上下文
  securityContext:
    runAsNonRoot: true        # 强制非root
    seccompProfile:           # 强制Seccomp
      type: RuntimeDefault

  containers:
  - name: app
    image: nginx:1.21

    # 必须：容器级别安全上下文
    securityContext:
      # 强制要求
      allowPrivilegeEscalation: false
      runAsNonRoot: true

      # 强制Capabilities配置
      capabilities:
        drop:
        - ALL                 # 必须drop ALL

      # 强制只读文件系统
      readOnlyRootFilesystem: true

      # 强制Seccomp
      seccompProfile:
        type: RuntimeDefault

    # 只能使用受限的volume类型
    volumeMounts:
    - name: config
      mountPath: /etc/config
      readOnly: true
    - name: cache
      mountPath: /tmp

  volumes:
  # ✅ 允许的volume类型
  - name: config
    configMap:
      name: app-config
  - name: cache
    emptyDir: {}

  # ❌ 不允许的volume类型
  # - hostPath
  # - gcePersistentDisk
  # - awsElasticBlockStore
  # - gitRepo
  # - nfs
  # - iscsi
  # - glusterfs
  # - rbd
  # - flexVolume
  # - cinder
  # - cephfs
  # - flocker
  # - fc
  # - azureFile
  # - vsphereVolume
  # - quobyte
  # - azureDisk
  # - portworxVolume
  # - scaleIO
  # - storageos
  # - csi (部分)
```

**Restricted vs Baseline对比：**

```yaml
# Baseline: 基础安全
apiVersion: v1
kind: Pod
metadata:
  name: baseline-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      allowPrivilegeEscalation: false
      # runAsNonRoot: 可选
      # readOnlyRootFilesystem: 可选

---
# Restricted: 严格安全
apiVersion: v1
kind: Pod
metadata:
  name: restricted-pod
spec:
  securityContext:
    runAsNonRoot: true          # ← 必需
    seccompProfile:             # ← 必需
      type: RuntimeDefault

  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      allowPrivilegeEscalation: false
      runAsNonRoot: true                    # ← 必需
      readOnlyRootFilesystem: true          # ← 必需
      capabilities:
        drop:
        - ALL                               # ← 必需
      seccompProfile:                       # ← 必需
        type: RuntimeDefault
```

### 6.2.3 Pod Security Admission（PSA）

#### 6.2.3.1 PSA工作原理

Pod Security Admission是内置的准入控制器，用于强制执行Pod Security Standards。

**工作流程：**

```
┌─────────────────────────────────────────────────┐
│       Pod Security Admission工作流程            │
├─────────────────────────────────────────────────┤
│                                                  │
│  1. 用户提交Pod/Deployment                      │
│           ↓                                      │
│  2. API Server调用PSA准入控制器                 │
│           ↓                                      │
│  3. PSA检查Pod是否符合Namespace标签定义的级别   │
│           ↓                                      │
│  4. 根据模式（enforce/audit/warn）执行动作      │
│           ↓                                      │
│     ┌─────┴─────┬─────────┬─────────┐           │
│     ↓           ↓         ↓         ↓           │
│  enforce:    audit:    warn:    通过            │
│  拒绝创建   记录日志   返回警告   允许创建      │
└─────────────────────────────────────────────────┘
```

#### 6.2.3.2 配置PSA

**通过Namespace标签配置PSA：**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: my-app
  labels:
    # enforce模式：强制执行，违规拒绝创建
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/enforce-version: latest

    # audit模式：记录违规行为到审计日志
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/audit-version: latest

    # warn模式：向用户返回警告信息
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: latest
```

**三种模式说明：**

| 模式 | 行为 | 用途 |
|-----|------|------|
| **enforce** | 拒绝不符合标准的Pod | 生产环境强制执行 |
| **audit** | 记录违规到审计日志 | 监控和合规检查 |
| **warn** | 返回警告但允许创建 | 开发环境友好提示 |

**示例1：宽松的开发环境**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    # 开发环境：只警告，不强制
    pod-security.kubernetes.io/enforce: privileged    # 不限制
    pod-security.kubernetes.io/warn: baseline         # 警告不安全配置
    pod-security.kubernetes.io/audit: baseline        # 记录日志
```

**示例2：严格的生产环境**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    # 生产环境：强制执行严格标准
    pod-security.kubernetes.io/enforce: restricted    # 强制Restricted
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/audit-version: latest
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: latest
```

**示例3：渐进式迁移**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: legacy-app
  labels:
    # 当前强制Baseline，但audit/warn Restricted
    # 帮助逐步迁移到更严格的标准
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: restricted      # 记录不符合Restricted的情况
    pod-security.kubernetes.io/warn: restricted       # 警告开发者
```

#### 6.2.3.3 测试PSA

**测试Baseline级别：**

```yaml
# 创建Baseline级别的Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: test-baseline
  labels:
    pod-security.kubernetes.io/enforce: baseline
---
# 尝试创建特权Pod - 应该被拒绝
apiVersion: v1
kind: Pod
metadata:
  name: privileged-pod
  namespace: test-baseline
spec:
  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      privileged: true        # ❌ Baseline禁止
```

```bash
kubectl apply -f privileged-pod.yaml
# Error from server (Forbidden): error when creating "privileged-pod.yaml":
# pods "privileged-pod" is forbidden: violates PodSecurity "baseline:latest":
# privileged (container "app" must not set securityContext.privileged=true)
```

**测试Restricted级别：**

```yaml
# 创建Restricted级别的Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: test-restricted
  labels:
    pod-security.kubernetes.io/enforce: restricted
---
# 尝试创建缺少安全配置的Pod - 应该被拒绝
apiVersion: v1
kind: Pod
metadata:
  name: insecure-pod
  namespace: test-restricted
spec:
  containers:
  - name: app
    image: nginx:1.21
    # ❌ 缺少必需的安全配置
```

```bash
kubectl apply -f insecure-pod.yaml
# Error from server (Forbidden): pods "insecure-pod" is forbidden:
# violates PodSecurity "restricted:latest":
# allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false),
# unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]),
# runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true),
# seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
```

**创建合规的Pod：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: compliant-pod
  namespace: test-restricted
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    seccompProfile:
      type: RuntimeDefault

  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      allowPrivilegeEscalation: false
      runAsNonRoot: true
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
      seccompProfile:
        type: RuntimeDefault

    volumeMounts:
    - name: cache
      mountPath: /var/cache/nginx
    - name: run
      mountPath: /var/run

  volumes:
  - name: cache
    emptyDir: {}
  - name: run
    emptyDir: {}
```

```bash
kubectl apply -f compliant-pod.yaml
# pod/compliant-pod created ✅
```

### 6.2.4 SecurityContext安全上下文

#### 6.2.4.1 Pod级别SecurityContext

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-security-context
spec:
  # Pod级别安全上下文
  securityContext:
    # 以非root用户运行
    runAsUser: 1000           # UID
    runAsGroup: 3000          # GID
    fsGroup: 2000             # 文件系统组ID

    # 强制非root
    runAsNonRoot: true

    # 补充组
    supplementalGroups:
    - 4000
    - 5000

    # FSGroup变更策略
    fsGroupChangePolicy: "OnRootMismatch"

    # Seccomp配置
    seccompProfile:
      type: RuntimeDefault

    # SELinux配置
    seLinuxOptions:
      level: "s0:c123,c456"

    # Windows配置（如果使用Windows节点）
    windowsOptions:
      gmsaCredentialSpecName: "webapp-gmsa"

  containers:
  - name: app
    image: nginx:1.21
```

**UID/GID说明：**

```bash
# Pod内查看用户
kubectl exec pod-security-context -- id
# 输出：
# uid=1000 gid=3000 groups=3000,2000,4000,5000

# 查看文件权限
kubectl exec pod-security-context -- ls -ln /data
# 文件属主为1000:2000（fsGroup生效）
```

#### 6.2.4.2 容器级别SecurityContext

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: container-security-context
spec:
  containers:
  - name: app
    image: nginx:1.21

    # 容器级别安全上下文（优先级高于Pod级别）
    securityContext:
      # 运行用户
      runAsUser: 2000
      runAsGroup: 3000
      runAsNonRoot: true

      # 特权和权限提升
      privileged: false
      allowPrivilegeEscalation: false

      # 只读根文件系统
      readOnlyRootFilesystem: true

      # Capabilities
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE
        - CHOWN

      # Seccomp
      seccompProfile:
        type: RuntimeDefault

      # SELinux
      seLinuxOptions:
        level: "s0:c123,c456"
        role: "system_r"
        type: "container_t"
        user: "system_u"

      # AppArmor（通过annotation）
      # container.apparmor.security.beta.kubernetes.io/app: localhost/k8s-apparmor-example

      # Proc Mount
      procMount: Default      # Default | Unmasked
```

#### 6.2.4.3 Linux Capabilities详解

Linux Capabilities将root权限分解为更细粒度的权限单元。

**常用Capabilities：**

| Capability | 作用 | 风险 |
|-----------|------|------|
| **CAP_CHOWN** | 修改文件所有者 | 低 |
| **CAP_NET_BIND_SERVICE** | 绑定<1024端口 | 低 |
| **CAP_NET_RAW** | 使用RAW和PACKET套接字 | 中 |
| **CAP_SYS_ADMIN** | 几乎所有管理操作 | 极高 |
| **CAP_SYS_PTRACE** | 跟踪任意进程 | 高 |
| **CAP_SYS_MODULE** | 加载内核模块 | 极高 |
| **CAP_DAC_OVERRIDE** | 绕过文件权限检查 | 高 |
| **CAP_SETUID/SETGID** | 修改用户/组ID | 高 |

**安全的Capabilities配置：**

```yaml
# ✅ 推荐：最小权限
securityContext:
  capabilities:
    drop:
    - ALL                    # 先删除所有
    add:
    - NET_BIND_SERVICE       # 只添加必需的

# ❌ 危险：保留所有权限
securityContext:
  capabilities:
    add:
    - ALL                    # 极度危险！

# ❌ 危险：添加高风险Capability
securityContext:
  capabilities:
    add:
    - SYS_ADMIN              # 几乎等同于root
```

**实战示例：Nginx绑定80端口**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-unprivileged
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000

  containers:
  - name: nginx
    image: nginx:1.21

    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE    # 允许绑定80端口
      readOnlyRootFilesystem: true

    ports:
    - containerPort: 80

    volumeMounts:
    - name: cache
      mountPath: /var/cache/nginx
    - name: run
      mountPath: /var/run

  volumes:
  - name: cache
    emptyDir: {}
  - name: run
    emptyDir: {}
```

#### 6.2.4.4 只读根文件系统

**readOnlyRootFilesystem的好处：**

```
┌─────────────────────────────────────────────────┐
│       只读根文件系统的安全价值                   │
├─────────────────────────────────────────────────┤
│  ✅ 防止恶意软件持久化                          │
│     攻击者无法在容器内写入后门                  │
│                                                  │
│  ✅ 防止配置篡改                                 │
│     应用配置文件无法被修改                      │
│                                                  │
│  ✅ 符合不可变基础设施理念                      │
│     容器应该是一次性的、可替换的                │
│                                                  │
│  ✅ 便于审计                                     │
│     所有变更都在Volume中，易于追踪              │
└─────────────────────────────────────────────────┘
```

**配置只读文件系统：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: readonly-fs-pod
spec:
  containers:
  - name: app
    image: myapp:1.0

    securityContext:
      readOnlyRootFilesystem: true    # 根文件系统只读

    volumeMounts:
    # 为需要写入的目录挂载emptyDir
    - name: tmp
      mountPath: /tmp                 # 临时文件
    - name: cache
      mountPath: /var/cache           # 缓存
    - name: logs
      mountPath: /var/log             # 日志

  volumes:
  - name: tmp
    emptyDir: {}
  - name: cache
    emptyDir: {}
  - name: logs
    emptyDir: {}
```

**测试只读文件系统：**

```bash
# 尝试写入根文件系统 - 应该失败
kubectl exec readonly-fs-pod -- touch /test.txt
# Error: touch: /test.txt: Read-only file system

# 写入允许的目录 - 成功
kubectl exec readonly-fs-pod -- touch /tmp/test.txt
# （成功，无输出）
```

### 6.2.5 Seccomp和AppArmor

#### 6.2.5.1 Seccomp（Secure Computing Mode）

Seccomp用于限制容器可以调用的系统调用（syscalls）。

**Seccomp Profile类型：**

| 类型 | 说明 | 安全性 |
|-----|------|--------|
| **Unconfined** | 不限制系统调用 | 无保护 |
| **RuntimeDefault** | 使用运行时默认配置 | 推荐 |
| **Localhost** | 使用自定义profile | 最安全 |

**配置RuntimeDefault：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: seccomp-default
spec:
  securityContext:
    seccompProfile:
      type: RuntimeDefault    # 使用默认配置

  containers:
  - name: app
    image: nginx:1.21
```

**自定义Seccomp Profile：**

```json
// /var/lib/kubelet/seccomp/nginx-profile.json
{
  "defaultAction": "SCMP_ACT_ERRNO",
  "architectures": [
    "SCMP_ARCH_X86_64",
    "SCMP_ARCH_X86",
    "SCMP_ARCH_X32"
  ],
  "syscalls": [
    {
      "names": [
        "accept4",
        "bind",
        "clone",
        "close",
        "connect",
        "dup",
        "epoll_create1",
        "epoll_ctl",
        "epoll_wait",
        "exit_group",
        "fstat",
        "getpid",
        "listen",
        "mmap",
        "openat",
        "read",
        "rt_sigaction",
        "rt_sigprocmask",
        "setsockopt",
        "socket",
        "write"
      ],
      "action": "SCMP_ACT_ALLOW"
    }
  ]
}
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: seccomp-custom
spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: nginx-profile.json

  containers:
  - name: nginx
    image: nginx:1.21
```

#### 6.2.5.2 AppArmor

AppArmor是Linux安全模块，通过配置文件限制程序的行为。

**检查AppArmor是否启用：**

```bash
# 在节点上检查
cat /sys/module/apparmor/parameters/enabled
# Y 表示已启用
```

**AppArmor Profile示例：**

```
# /etc/apparmor.d/k8s-nginx
#include <tunables/global>

profile k8s-nginx flags=(attach_disconnected,mediate_deleted) {
  #include <abstractions/base>

  # 允许网络
  network inet tcp,
  network inet udp,

  # 允许读取配置文件
  /etc/nginx/** r,
  /usr/share/nginx/** r,

  # 允许写入日志
  /var/log/nginx/** w,

  # 允许读写缓存
  /var/cache/nginx/** rw,

  # 禁止执行命令
  deny /bin/** wrix,
  deny /usr/bin/** wrix,
  deny /sbin/** wrix,
  deny /usr/sbin/** wrix,

  # 禁止访问敏感目录
  deny /root/** rwx,
  deny /home/** rwx,
  deny /etc/shadow r,
}
```

**在Pod中使用AppArmor：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: apparmor-pod
  annotations:
    # 指定容器使用的AppArmor profile
    container.apparmor.security.beta.kubernetes.io/nginx: localhost/k8s-nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.21
```

**验证AppArmor生效：**

```bash
# 查看容器的AppArmor配置
kubectl exec apparmor-pod -- cat /proc/1/attr/current
# 输出：k8s-nginx (enforce)

# 尝试执行被禁止的命令
kubectl exec apparmor-pod -- /bin/bash
# Permission denied (AppArmor阻止)
```

### 6.2.6 实战案例：构建安全的应用

#### 6.2.6.1 场景：部署安全的Web应用

**需求：**
- 非root用户运行
- 只读根文件系统
- 最小Capabilities
- 启用Seccomp
- 符合Restricted标准

**完整配置：**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: secure-web
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  namespace: secure-web
data:
  nginx.conf: |
    user nginx;
    worker_processes auto;
    error_log /var/log/nginx/error.log;
    pid /var/run/nginx.pid;

    events {
      worker_connections 1024;
    }

    http {
      include /etc/nginx/mime.types;
      default_type application/octet-stream;
      access_log /var/log/nginx/access.log;

      server {
        listen 8080;
        server_name _;
        root /usr/share/nginx/html;
        index index.html;

        location / {
          try_files $uri $uri/ =404;
        }
      }
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-nginx
  namespace: secure-web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: secure-nginx
  template:
    metadata:
      labels:
        app: secure-nginx
    spec:
      # Pod级别安全上下文
      securityContext:
        runAsNonRoot: true
        runAsUser: 101          # nginx用户
        runAsGroup: 101
        fsGroup: 101
        seccompProfile:
          type: RuntimeDefault

      containers:
      - name: nginx
        image: nginx:1.21-alpine

        # 容器级别安全上下文
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 101
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
          readOnlyRootFilesystem: true
          seccompProfile:
            type: RuntimeDefault

        ports:
        - containerPort: 8080
          name: http

        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
          readOnly: true
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
        - name: logs
          mountPath: /var/log/nginx

        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5

      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
      - name: logs
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: secure-nginx
  namespace: secure-web
spec:
  selector:
    app: secure-nginx
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP
```

**部署和验证：**

```bash
# 部署应用
kubectl apply -f secure-web-deployment.yaml

# 验证Pod符合Restricted标准
kubectl get pods -n secure-web
kubectl describe pod -n secure-web <pod-name>

# 验证安全配置
kubectl exec -n secure-web <pod-name> -- id
# uid=101(nginx) gid=101(nginx)

# 验证只读文件系统
kubectl exec -n secure-web <pod-name> -- touch /test
# touch: /test: Read-only file system ✅

# 验证无特权
kubectl exec -n secure-web <pod-name> -- cat /proc/1/status | grep Cap
# 应该看到受限的Capabilities
```

本节我们深入学习了Kubernetes的Pod安全机制。通过Pod Security Standards和SecurityContext，可以构建纵深防御体系，有效防范容器安全威胁。在下一节，我们将学习RBAC权限控制，进一步加固集群安全。

---

