# 第6章：配置与安全管理

> 在第5章中，我们系统学习了Kubernetes的存储管理，包括ConfigMap和Secret的基础用法。本章将在此基础上，深入探讨Kubernetes的配置管理和安全机制，构建生产级的安全防护体系。

**本章学习目标：**
- 掌握资源配额（ResourceQuota）和限制范围（LimitRange）
- 理解Pod安全策略（Pod Security）
- 深入学习RBAC权限控制体系
- 掌握网络策略（NetworkPolicy）
- 了解审计日志和安全扫描
- 构建完整的安全防护体系

---

## 6.1 资源配额与限制

在多租户Kubernetes集群中，资源管理是一个核心问题。如何防止某个应用或租户过度消耗资源？如何确保集群资源的公平分配？本节将深入学习Kubernetes的资源配额和限制机制。

### 6.1.1 为什么需要资源配额

#### 6.1.1.1 多租户环境的挑战

在实际生产环境中，一个Kubernetes集群通常会被多个团队或应用共享：

```
┌─────────────────────────────────────────────────┐
│          多租户集群资源竞争场景                  │
├─────────────────────────────────────────────────┤
│  团队A：电商应用（高峰期流量大）                │
│  团队B：数据处理（计算密集型）                  │
│  团队C：开发测试（资源需求不稳定）              │
│                                                  │
│  问题：                                          │
│  - 团队A在促销期间疯狂扩容，占用80%资源         │
│  - 团队B的批处理任务导致其他服务响应变慢       │
│  - 团队C的测试Pod泄漏，持续消耗资源            │
│                                                  │
│  后果：                                          │
│  ❌ 资源争抢导致服务不稳定                      │
│  ❌ 关键应用无法获得足够资源                    │
│  ❌ 成本失控，账单暴增                          │
└─────────────────────────────────────────────────┘
```

#### 6.1.1.2 资源管理的痛点

**没有资源配额的情况：**

| 问题 | 场景 | 影响 |
|-----|------|------|
| **资源霸占** | 单个应用申请过多资源 | 其他应用无法部署 |
| **成本失控** | 无限制创建资源 | 云账单暴增 |
| **稳定性问题** | 资源耗尽导致集群不稳定 | 服务大面积故障 |
| **公平性缺失** | 先到先得的资源分配 | 重要应用资源不足 |
| **容量规划困难** | 无法预测资源使用情况 | 扩容滞后 |

**真实案例：**

```yaml
# ❌ 不受控制的资源申请
apiVersion: apps/v1
kind: Deployment
metadata:
  name: greedy-app
  namespace: team-a
spec:
  replicas: 100        # 过度扩容
  template:
    spec:
      containers:
      - name: app
        image: myapp:1.0
        resources:
          requests:
            memory: "8Gi"      # 每个Pod申请8Gi内存
            cpu: "4"           # 每个Pod申请4核CPU
        # 总需求：800Gi内存 + 400核CPU
        # 如果集群只有1000Gi内存，其他应用将无法运行！
```

#### 6.1.1.3 资源配额的价值

**引入资源配额后：**

```
┌─────────────────────────────────────────────────┐
│          资源配额带来的好处                      │
├─────────────────────────────────────────────────┤
│  ✅ 资源隔离                                     │
│     每个Namespace有明确的资源上限                │
│                                                  │
│  ✅ 成本控制                                     │
│     防止资源滥用，控制云成本                    │
│                                                  │
│  ✅ 公平性                                       │
│     确保资源公平分配给各团队                    │
│                                                  │
│  ✅ 稳定性                                       │
│     防止单点故障影响整个集群                    │
│                                                  │
│  ✅ 容量规划                                     │
│     清晰的资源使用预期，便于扩容决策            │
└─────────────────────────────────────────────────┘
```

### 6.1.2 ResourceQuota资源配额

#### 6.1.2.1 ResourceQuota核心概念

ResourceQuota是Namespace级别的资源，用于限制该Namespace下所有资源的总量。

**基本结构：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota-example
  namespace: team-a
spec:
  hard:
    # 计算资源配额
    requests.cpu: "10"           # CPU请求总量：10核
    requests.memory: "20Gi"      # 内存请求总量：20Gi
    limits.cpu: "20"             # CPU限制总量：20核
    limits.memory: "40Gi"        # 内存限制总量：40Gi

    # 对象数量配额
    pods: "50"                   # 最多50个Pod
    services: "10"               # 最多10个Service
    persistentvolumeclaims: "5"  # 最多5个PVC

    # 存储配额
    requests.storage: "100Gi"    # 存储请求总量：100Gi
```

**工作原理：**

```
┌─────────────────────────────────────────────────┐
│          ResourceQuota工作流程                   │
├─────────────────────────────────────────────────┤
│                                                  │
│  1. 用户提交Pod/Deployment                      │
│           ↓                                      │
│  2. API Server检查ResourceQuota                 │
│           ↓                                      │
│  3. 计算当前Namespace资源使用量                 │
│           ↓                                      │
│  4. 判断：新资源 + 已用资源 <= 配额？           │
│           ↓                 ↓                    │
│         YES               NO                     │
│           ↓                 ↓                    │
│      允许创建          拒绝创建                 │
│           ↓                 ↓                    │
│      资源计数+1    返回错误：exceeded quota      │
└─────────────────────────────────────────────────┘
```

#### 6.1.2.2 计算资源配额

**示例1：限制CPU和内存**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: development
spec:
  hard:
    # CPU配额
    requests.cpu: "4"      # 该Namespace下所有Pod的CPU请求总和不超过4核
    limits.cpu: "8"        # 该Namespace下所有Pod的CPU限制总和不超过8核

    # 内存配额
    requests.memory: "8Gi"   # 内存请求总和不超过8Gi
    limits.memory: "16Gi"    # 内存限制总和不超过16Gi
```

**应用到Namespace：**

```bash
kubectl apply -f compute-quota.yaml
kubectl get resourcequota -n development
```

**测试配额：**

```yaml
# 尝试创建Pod
apiVersion: v1
kind: Pod
metadata:
  name: test-pod-1
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "4Gi"
        cpu: "2"
```

```bash
# 创建第1个Pod - 成功（使用：CPU 1核，内存2Gi）
kubectl apply -f test-pod-1.yaml

# 创建第2个Pod - 成功（累计：CPU 2核，内存4Gi）
kubectl apply -f test-pod-2.yaml

# 创建第3个Pod - 成功（累计：CPU 3核，内存6Gi）
kubectl apply -f test-pod-3.yaml

# 创建第4个Pod - 成功（累计：CPU 4核，内存8Gi，达到requests上限）
kubectl apply -f test-pod-4.yaml

# 创建第5个Pod - 失败！
kubectl apply -f test-pod-5.yaml
# Error: exceeded quota: compute-quota,
# requested: requests.cpu=1,requests.memory=2Gi,
# used: requests.cpu=4,requests.memory=8Gi,
# limited: requests.cpu=4,requests.memory=8Gi
```

**查看配额使用情况：**

```bash
kubectl describe resourcequota compute-quota -n development
```

输出：
```
Name:            compute-quota
Namespace:       development
Resource         Used   Hard
--------         ----   ----
limits.cpu       8      8
limits.memory    16Gi   16Gi
requests.cpu     4      4
requests.memory  8Gi    8Gi
```

#### 6.1.2.3 对象数量配额

**示例2：限制对象数量**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota
  namespace: development
spec:
  hard:
    # Pod数量
    pods: "20"

    # Service数量
    services: "5"
    services.loadbalancers: "2"      # LoadBalancer类型Service
    services.nodeports: "3"          # NodePort类型Service

    # PVC数量
    persistentvolumeclaims: "10"

    # ConfigMap和Secret数量
    configmaps: "20"
    secrets: "20"

    # ReplicationController数量
    replicationcontrollers: "10"
```

**按存储类限制PVC：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: development
spec:
  hard:
    # 总存储配额
    requests.storage: "100Gi"
    persistentvolumeclaims: "10"

    # 按StorageClass限制
    nfs-client.storageclass.storage.k8s.io/requests.storage: "50Gi"
    nfs-client.storageclass.storage.k8s.io/persistentvolumeclaims: "5"

    local-storage.storageclass.storage.k8s.io/requests.storage: "30Gi"
    local-storage.storageclass.storage.k8s.io/persistentvolumeclaims: "3"
```

#### 6.1.2.4 作用域选择器（Scope Selector）

ResourceQuota可以根据Pod的优先级或QoS类别进行限制。

**按优先级限制：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: high-priority-quota
  namespace: production
spec:
  hard:
    pods: "100"
    requests.cpu: "50"
    requests.memory: "100Gi"
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["high-priority"]   # 只对high-priority优先级的Pod生效
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: low-priority-quota
  namespace: production
spec:
  hard:
    pods: "20"
    requests.cpu: "10"
    requests.memory: "20Gi"
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["low-priority"]    # 只对low-priority优先级的Pod生效
```

**按QoS类别限制：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-quota
  namespace: development
spec:
  hard:
    pods: "10"            # BestEffort类Pod最多10个
  scopes:
  - BestEffort            # 只对BestEffort QoS的Pod生效
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: notbesteffort-quota
  namespace: development
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
  scopes:
  - NotBestEffort         # 对Burstable和Guaranteed QoS的Pod生效
```

**QoS类别说明：**

| QoS类别 | 条件 | 特点 |
|--------|------|------|
| **Guaranteed** | requests = limits（且所有容器都设置） | 最高优先级，不会被OOM Kill |
| **Burstable** | 设置了requests，但requests < limits | 中等优先级，资源不足时可能被Kill |
| **BestEffort** | 未设置requests和limits | 最低优先级，优先被Kill |

### 6.1.3 LimitRange限制范围

#### 6.1.3.1 LimitRange核心概念

LimitRange是Namespace级别的资源，用于限制单个资源对象的大小。

**ResourceQuota vs LimitRange：**

```
┌──────────────────┬─────────────────┬─────────────────┐
│    特性          │  ResourceQuota  │   LimitRange    │
├──────────────────┼─────────────────┼─────────────────┤
│  作用范围        │  Namespace总量  │  单个对象       │
│  限制内容        │  总资源上限     │  单个资源上下限 │
│  典型用途        │  多租户资源隔离 │  防止单点过大   │
│  示例            │  总CPU不超过10核│  单Pod不超过2核 │
└──────────────────┴─────────────────┴─────────────────┘
```

**组合使用场景：**

```yaml
# ResourceQuota: 限制总量
# development namespace总共最多使用10核CPU

# LimitRange: 限制单个对象
# 每个Pod最多使用2核CPU
# 这样可以确保至少能运行5个Pod
```

#### 6.1.3.2 限制Pod和容器资源

**示例1：限制容器资源范围**

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: container-limit-range
  namespace: development
spec:
  limits:
  - type: Container
    # 默认值（如果未指定）
    default:
      cpu: "500m"        # 默认limit
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"        # 默认request
      memory: "256Mi"

    # 最大值
    max:
      cpu: "2"           # 单个容器最多2核
      memory: "4Gi"      # 单个容器最多4Gi内存

    # 最小值
    min:
      cpu: "100m"        # 单个容器至少100m CPU
      memory: "128Mi"    # 单个容器至少128Mi内存

    # 最大/最小比例
    maxLimitRequestRatio:
      cpu: "4"           # limit最多是request的4倍
      memory: "4"
```

**效果演示：**

```yaml
# 案例1：未指定资源 - 自动应用默认值
apiVersion: v1
kind: Pod
metadata:
  name: pod-default
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    # 未指定resources，自动应用：
    # requests: cpu=250m, memory=256Mi
    # limits: cpu=500m, memory=512Mi
```

```yaml
# 案例2：超过最大值 - 拒绝创建
apiVersion: v1
kind: Pod
metadata:
  name: pod-too-large
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "3"      # ❌ 超过max（2核）
# Error: [spec.containers[0].resources.requests.cpu: Invalid value: "3": must be less than or equal to cpu limit of 2]
```

```yaml
# 案例3：低于最小值 - 拒绝创建
apiVersion: v1
kind: Pod
metadata:
  name: pod-too-small
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "50m"    # ❌ 低于min（100m）
# Error: minimum cpu usage per Container is 100m
```

```yaml
# 案例4：比例不合理 - 拒绝创建
apiVersion: v1
kind: Pod
metadata:
  name: pod-bad-ratio
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "200m"
      limits:
        cpu: "1"       # ❌ 比例5倍，超过maxLimitRequestRatio（4倍）
# Error: cpu max limit to request ratio per Container is 4
```

**示例2：限制Pod资源范围**

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: pod-limit-range
  namespace: development
spec:
  limits:
  - type: Pod
    max:
      cpu: "4"           # 单个Pod（所有容器总和）最多4核
      memory: "8Gi"      # 单个Pod（所有容器总和）最多8Gi
    min:
      cpu: "200m"
      memory: "256Mi"
```

**多容器Pod示例：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "1"
        memory: "2Gi"

  - name: sidecar
    image: busybox
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"

  # Pod总资源：cpu=1.5, memory=3Gi
  # 符合LimitRange: max pod cpu=4, memory=8Gi ✅
```

#### 6.1.3.3 限制PVC存储

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: storage-limit-range
  namespace: development
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: "50Gi"    # 单个PVC最大50Gi
    min:
      storage: "1Gi"     # 单个PVC最小1Gi
```

**测试：**

```yaml
# ✅ 符合范围
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-valid
  namespace: development
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi    # 在1Gi-50Gi范围内

# ❌ 超过最大值
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-too-large
  namespace: development
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi   # 超过50Gi上限
# Error: maximum storage usage per PersistentVolumeClaim is 50Gi
```

### 6.1.4 实战案例：多环境资源配额

#### 6.1.4.1 场景设计

为一家公司的三个环境（Dev、Test、Prod）配置不同的资源配额：

| 环境 | Pod数量 | CPU总量 | 内存总量 | 存储总量 | 优先级 |
|-----|--------|---------|---------|---------|--------|
| **Dev** | 50 | 10核 | 20Gi | 100Gi | 低 |
| **Test** | 30 | 20核 | 40Gi | 200Gi | 中 |
| **Prod** | 100 | 50核 | 100Gi | 500Gi | 高 |

#### 6.1.4.2 Dev环境配置

**dev-namespace.yaml：**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    environment: development
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-resource-quota
  namespace: dev
spec:
  hard:
    # 计算资源
    requests.cpu: "10"
    requests.memory: "20Gi"
    limits.cpu: "15"
    limits.memory: "30Gi"

    # 对象数量
    pods: "50"
    services: "10"
    persistentvolumeclaims: "10"
    configmaps: "30"
    secrets: "30"

    # 存储
    requests.storage: "100Gi"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limit-range
  namespace: dev
spec:
  limits:
  # 容器限制
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "200m"
      memory: "256Mi"
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "50m"
      memory: "64Mi"
    maxLimitRequestRatio:
      cpu: "4"
      memory: "4"

  # Pod限制
  - type: Pod
    max:
      cpu: "4"
      memory: "8Gi"

  # PVC限制
  - type: PersistentVolumeClaim
    max:
      storage: "20Gi"
    min:
      storage: "1Gi"
```

#### 6.1.4.3 Prod环境配置

**prod-namespace.yaml：**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: prod
  labels:
    environment: production
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: prod-resource-quota
  namespace: prod
spec:
  hard:
    # 计算资源（更高配额）
    requests.cpu: "50"
    requests.memory: "100Gi"
    limits.cpu: "80"
    limits.memory: "150Gi"

    # 对象数量
    pods: "100"
    services: "20"
    persistentvolumeclaims: "30"
    configmaps: "50"
    secrets: "50"

    # 存储
    requests.storage: "500Gi"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: prod-limit-range
  namespace: prod
spec:
  limits:
  # 容器限制（更宽松）
  - type: Container
    default:
      cpu: "1"
      memory: "2Gi"
    defaultRequest:
      cpu: "500m"
      memory: "1Gi"
    max:
      cpu: "8"          # 生产环境允许更大的容器
      memory: "16Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    maxLimitRequestRatio:
      cpu: "4"
      memory: "4"

  # Pod限制
  - type: Pod
    max:
      cpu: "16"
      memory: "32Gi"

  # PVC限制
  - type: PersistentVolumeClaim
    max:
      storage: "100Gi"
    min:
      storage: "5Gi"
```

#### 6.1.4.4 部署和验证

```bash
# 部署环境配置
kubectl apply -f dev-namespace.yaml
kubectl apply -f test-namespace.yaml
kubectl apply -f prod-namespace.yaml

# 查看配额
kubectl get resourcequota --all-namespaces
kubectl get limitrange --all-namespaces

# 查看Dev环境详情
kubectl describe resourcequota dev-resource-quota -n dev
kubectl describe limitrange dev-limit-range -n dev

# 查看Prod环境详情
kubectl describe resourcequota prod-resource-quota -n prod
kubectl describe limitrange prod-limit-range -n prod
```

**部署测试应用：**

```yaml
# dev-test-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-app
  namespace: dev
spec:
  replicas: 3
  selector:
    matchLabels:
      app: test-app
  template:
    metadata:
      labels:
        app: test-app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        # 未指定资源，自动应用LimitRange默认值：
        # requests: cpu=200m, memory=256Mi
        # limits: cpu=500m, memory=512Mi
```

```bash
kubectl apply -f dev-test-deployment.yaml

# 查看资源使用情况
kubectl describe resourcequota dev-resource-quota -n dev
# 输出：
# Resource         Used    Hard
# --------         ----    ----
# pods             3       50
# requests.cpu     600m    10
# requests.memory  768Mi   20Gi
# ...
```

### 6.1.5 最佳实践

#### 6.1.5.1 资源配额设计原则

**1. 合理规划配额：**

```yaml
# ✅ 推荐：基于实际监控数据设置
# 监控1个月后发现：
# - 平均CPU使用：8核
# - 峰值CPU使用：12核
# - 设置配额：15核（留有余量）

apiVersion: v1
kind: ResourceQuota
metadata:
  name: data-driven-quota
spec:
  hard:
    requests.cpu: "15"    # 峰值 + 25% buffer
    requests.memory: "30Gi"
```

**2. 分层配额策略：**

```
┌─────────────────────────────────────────┐
│         分层配额架构                     │
├─────────────────────────────────────────┤
│  第1层：集群级别（总资源池）             │
│    └─ 总CPU: 100核, 总内存: 200Gi      │
│                                          │
│  第2层：环境级别（Namespace配额）        │
│    ├─ Prod: 50核, 100Gi (50%)          │
│    ├─ Test: 30核, 60Gi (30%)           │
│    └─ Dev: 20核, 40Gi (20%)            │
│                                          │
│  第3层：应用级别（LimitRange限制）       │
│    └─ 单Pod: ≤4核, ≤8Gi                │
└─────────────────────────────────────────┘
```

**3. 强制要求资源设置：**

```yaml
# 部署ResourceQuota后，必须为所有Pod设置资源
# 否则Pod无法创建

# ❌ 没有ResourceQuota的情况
apiVersion: v1
kind: Pod
metadata:
  name: pod-no-resources
spec:
  containers:
  - name: app
    image: nginx:1.21
    # 未设置resources - 可以创建

# ✅ 有ResourceQuota的情况
# 必须设置resources或配置LimitRange默认值
```

**配置LimitRange提供默认值：**

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
spec:
  limits:
  - type: Container
    defaultRequest:    # 为未指定的Pod提供默认request
      cpu: "100m"
      memory: "128Mi"
    default:           # 为未指定的Pod提供默认limit
      cpu: "200m"
      memory: "256Mi"
```

#### 6.1.5.2 监控和告警

**查看配额使用情况：**

```bash
# 查看所有Namespace的配额
kubectl get resourcequota --all-namespaces

# 详细查看某个配额
kubectl describe resourcequota <name> -n <namespace>

# 查看配额使用率（自定义脚本）
kubectl get resourcequota dev-resource-quota -n dev -o json | \
  jq '.status.used, .status.hard'
```

**配置Prometheus告警：**

```yaml
# prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: resourcequota-alerts
spec:
  groups:
  - name: resourcequota
    interval: 30s
    rules:
    # CPU配额使用率超过80%
    - alert: NamespaceCPUQuotaExceeding
      expr: |
        sum(kube_resourcequota{resource="requests.cpu", type="used"}) by (namespace)
        /
        sum(kube_resourcequota{resource="requests.cpu", type="hard"}) by (namespace)
        > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Namespace {{ $labels.namespace }} CPU quota exceeding 80%"

    # 内存配额使用率超过80%
    - alert: NamespaceMemoryQuotaExceeding
      expr: |
        sum(kube_resourcequota{resource="requests.memory", type="used"}) by (namespace)
        /
        sum(kube_resourcequota{resource="requests.memory", type="hard"}) by (namespace)
        > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Namespace {{ $labels.namespace }} memory quota exceeding 80%"
```

#### 6.1.5.3 常见问题排查

**问题1：Pod无法创建，提示exceeded quota**

```bash
# 错误信息
Error from server (Forbidden): error when creating "pod.yaml":
pods "myapp" is forbidden: exceeded quota: compute-quota,
requested: requests.cpu=2,requests.memory=4Gi,
used: requests.cpu=9,requests.memory=18Gi,
limited: requests.cpu=10,requests.memory=20Gi

# 排查步骤
# 1. 查看当前配额使用情况
kubectl describe resourcequota compute-quota -n <namespace>

# 2. 找出资源消耗大的Pod
kubectl top pods -n <namespace> --sort-by=cpu
kubectl top pods -n <namespace> --sort-by=memory

# 3. 解决方案
# 方案A：删除不必要的Pod
kubectl delete pod <unused-pod> -n <namespace>

# 方案B：减少新Pod的资源请求
# 修改Deployment的resources.requests

# 方案C：增加配额（需要管理员权限）
kubectl edit resourcequota compute-quota -n <namespace>
```

**问题2：Pod一直Pending，提示Insufficient CPU/Memory**

```bash
# 查看Pod事件
kubectl describe pod <pod-name> -n <namespace>
# Events:
#   Warning  FailedScheduling  Pod didn't fit: Insufficient CPU

# 原因分析：
# 1. ResourceQuota限制了总量
# 2. LimitRange限制了单个Pod大小
# 3. 节点实际资源不足

# 检查LimitRange
kubectl describe limitrange -n <namespace>

# 检查节点资源
kubectl top nodes
kubectl describe node <node-name>
```

**问题3：ResourceQuota未生效**

```bash
# 可能原因1：Pod未设置resources
# 解决：配置LimitRange提供默认值

# 可能原因2：ResourceQuota配置错误
kubectl get resourcequota <name> -n <namespace> -o yaml
# 检查spec.hard字段是否正确

# 可能原因3：权限问题
kubectl auth can-i create resourcequotas --as=system:serviceaccount:<namespace>:<serviceaccount>
```

本节我们深入学习了Kubernetes的资源配额和限制机制。通过ResourceQuota可以限制Namespace的总资源，通过LimitRange可以限制单个对象的资源范围。合理使用这些机制，可以实现多租户环境的资源隔离、成本控制和稳定性保障。

接下来，我们将学习Pod安全策略，进一步加固集群的安全防护。

---

## 6.2 Pod安全策略

容器安全是Kubernetes集群安全的核心。恶意或配置不当的容器可能导致容器逃逸、权限提升、数据泄露等严重安全问题。本节将深入学习Kubernetes的Pod安全机制，构建纵深防御体系。

### 6.2.1 为什么需要Pod安全策略

#### 6.2.1.1 容器安全威胁

**真实的安全风险：**

```
┌─────────────────────────────────────────────────┐
│          容器常见安全威胁                        │
├─────────────────────────────────────────────────┤
│  1. 特权容器（Privileged Container）            │
│     - 拥有宿主机root权限                        │
│     - 可以访问宿主机所有设备                    │
│     - 可以修改宿主机系统配置                    │
│                                                  │
│  2. 容器逃逸（Container Escape）                │
│     - 突破容器隔离，访问宿主机                  │
│     - 利用内核漏洞或错误配置                    │
│     - 影响同节点所有容器                        │
│                                                  │
│  3. 权限提升（Privilege Escalation）            │
│     - 通过setuid/setgid提升权限                 │
│     - 访问敏感的宿主机路径                      │
│     - 修改宿主机文件系统                        │
│                                                  │
│  4. 敏感数据泄露                                 │
│     - 挂载宿主机敏感目录                        │
│     - 访问其他容器的数据                        │
│     - 读取Kubernetes Secret                     │
└─────────────────────────────────────────────────┘
```

**危险的Pod配置示例：**

```yaml
# ❌ 极度危险的Pod配置
apiVersion: v1
kind: Pod
metadata:
  name: dangerous-pod
spec:
  hostNetwork: true           # 使用宿主机网络
  hostPID: true               # 使用宿主机PID命名空间
  hostIPC: true               # 使用宿主机IPC命名空间

  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      privileged: true        # 特权模式
      runAsUser: 0            # 以root用户运行
      allowPrivilegeEscalation: true
      capabilities:
        add:
        - ALL                 # 添加所有Linux Capabilities

    volumeMounts:
    - name: host-root
      mountPath: /host

  volumes:
  - name: host-root
    hostPath:
      path: /                 # 挂载宿主机根目录！
      type: Directory
```

**攻击场景演示：**

```bash
# 恶意Pod可以做什么？

# 1. 访问宿主机文件系统
kubectl exec dangerous-pod -- ls /host
# 输出：bin  boot  dev  etc  home  root  ...
# 看到宿主机的完整文件系统！

# 2. 查看宿主机进程
kubectl exec dangerous-pod -- ps aux
# 可以看到宿主机上所有进程

# 3. 修改宿主机配置
kubectl exec dangerous-pod -- chroot /host bash
# 已经完全控制宿主机！

# 4. 访问其他容器数据
kubectl exec dangerous-pod -- ls /host/var/lib/docker/containers
# 可以访问其他容器的数据
```

#### 6.2.1.2 安全防护的必要性

**没有安全策略的后果：**

| 风险 | 影响 | 真实案例 |
|-----|------|---------|
| **容器逃逸** | 攻击者控制宿主机和所有容器 | CVE-2019-5736 runc漏洞 |
| **加密货币挖矿** | 消耗大量CPU资源，成本暴增 | Tesla K8s集群被入侵挖矿 |
| **数据泄露** | 敏感数据被窃取 | 配置不当导致Secret暴露 |
| **横向渗透** | 从一个容器攻击整个集群 | 网络未隔离导致蔓延 |

**Pod安全的核心目标：**

```
┌─────────────────────────────────────────────────┐
│          Pod安全防护目标                         │
├─────────────────────────────────────────────────┤
│  ✅ 最小权限原则                                 │
│     容器只获得完成任务所需的最小权限            │
│                                                  │
│  ✅ 隔离性                                       │
│     容器与宿主机、容器与容器之间充分隔离        │
│                                                  │
│  ✅ 不可变性                                     │
│     容器文件系统只读，防止恶意修改              │
│                                                  │
│  ✅ 可审计性                                     │
│     所有安全相关操作都可追溯                    │
│                                                  │
│  ✅ 纵深防御                                     │
│     多层防护，单点失效不导致整体沦陷            │
└─────────────────────────────────────────────────┘
```

### 6.2.2 Pod Security Standards（PSS）

从Kubernetes 1.25开始，Pod Security Policy（PSP）被废弃，取而代之的是Pod Security Standards（PSS）和Pod Security Admission（PSA）。

#### 6.2.2.1 三种安全级别

**Pod Security Standards定义了三种安全级别：**

| 级别 | 限制程度 | 适用场景 | 主要限制 |
|-----|---------|---------|---------|
| **Privileged** | 无限制 | 信任的系统组件 | 允许任何配置 |
| **Baseline** | 最小限制 | 一般应用 | 禁止已知的特权提升 |
| **Restricted** | 严格限制 | 安全敏感应用 | 强制执行最佳安全实践 |

**安全级别对比：**

```
┌──────────────┬─────────────┬─────────────┬─────────────┐
│   配置项     │ Privileged  │  Baseline   │ Restricted  │
├──────────────┼─────────────┼─────────────┼─────────────┤
│ privileged   │     ✅      │      ❌     │      ❌     │
│ hostNetwork  │     ✅      │      ❌     │      ❌     │
│ hostPID      │     ✅      │      ❌     │      ❌     │
│ hostIPC      │     ✅      │      ❌     │      ❌     │
│ hostPath     │     ✅      │   ⚠️ 部分   │      ❌     │
│ runAsNonRoot │     -       │      -      │      ✅     │
│ capabilities │     ✅      │   ⚠️ 限制   │   ❌ 严格   │
│ seccompProfile│    -       │      -      │      ✅     │
└──────────────┴─────────────┴─────────────┴─────────────┘

✅ = 允许
❌ = 禁止
⚠️ = 部分允许/限制
-  = 不强制
```

#### 6.2.2.2 Baseline级别详解

**Baseline级别禁止的配置：**

```yaml
# Baseline级别不允许的配置

# ❌ 特权容器
spec:
  containers:
  - name: app
    securityContext:
      privileged: true        # 禁止

# ❌ 宿主机命名空间
spec:
  hostNetwork: true            # 禁止
  hostPID: true                # 禁止
  hostIPC: true                # 禁止

# ❌ 不安全的hostPath
spec:
  volumes:
  - name: host-path
    hostPath:
      path: /                  # 禁止挂载根目录
      path: /etc               # 禁止挂载敏感目录
      path: /sys               # 禁止

# ❌ 危险的Capabilities
spec:
  containers:
  - name: app
    securityContext:
      capabilities:
        add:
        - SYS_ADMIN           # 禁止
        - NET_ADMIN           # 部分场景禁止
        - ALL                 # 禁止

# ❌ hostPorts
spec:
  containers:
  - name: app
    ports:
    - containerPort: 80
      hostPort: 80            # 禁止（0-1024端口）

# ❌ AppArmor配置
metadata:
  annotations:
    container.apparmor.security.beta.kubernetes.io/app: unconfined  # 禁止

# ❌ SELinux自定义选项
spec:
  securityContext:
    seLinuxOptions:
      type: custom_t          # 禁止自定义
```

**Baseline级别允许的配置：**

```yaml
# ✅ Baseline级别允许的安全配置
apiVersion: v1
kind: Pod
metadata:
  name: baseline-compliant-pod
spec:
  # 非特权模式
  containers:
  - name: app
    image: nginx:1.21

    securityContext:
      # ✅ 非root用户
      runAsUser: 1000
      runAsGroup: 3000

      # ✅ 禁止特权提升
      allowPrivilegeEscalation: false

      # ✅ 只读根文件系统
      readOnlyRootFilesystem: true

      # ✅ 允许的capabilities
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE   # 允许绑定1024以下端口

    # ✅ 允许的volume类型
    volumeMounts:
    - name: config
      mountPath: /etc/config
    - name: cache
      mountPath: /tmp

  volumes:
  - name: config
    configMap:              # ✅ ConfigMap
      name: app-config
  - name: cache
    emptyDir: {}            # ✅ emptyDir
```

#### 6.2.2.3 Restricted级别详解

**Restricted级别的严格要求：**

```yaml
# ✅ Restricted级别合规的Pod
apiVersion: v1
kind: Pod
metadata:
  name: restricted-compliant-pod
spec:
  # 必须：Pod级别安全上下文
  securityContext:
    runAsNonRoot: true        # 强制非root
    seccompProfile:           # 强制Seccomp
      type: RuntimeDefault

  containers:
  - name: app
    image: nginx:1.21

    # 必须：容器级别安全上下文
    securityContext:
      # 强制要求
      allowPrivilegeEscalation: false
      runAsNonRoot: true

      # 强制Capabilities配置
      capabilities:
        drop:
        - ALL                 # 必须drop ALL

      # 强制只读文件系统
      readOnlyRootFilesystem: true

      # 强制Seccomp
      seccompProfile:
        type: RuntimeDefault

    # 只能使用受限的volume类型
    volumeMounts:
    - name: config
      mountPath: /etc/config
      readOnly: true
    - name: cache
      mountPath: /tmp

  volumes:
  # ✅ 允许的volume类型
  - name: config
    configMap:
      name: app-config
  - name: cache
    emptyDir: {}

  # ❌ 不允许的volume类型
  # - hostPath
  # - gcePersistentDisk
  # - awsElasticBlockStore
  # - gitRepo
  # - nfs
  # - iscsi
  # - glusterfs
  # - rbd
  # - flexVolume
  # - cinder
  # - cephfs
  # - flocker
  # - fc
  # - azureFile
  # - vsphereVolume
  # - quobyte
  # - azureDisk
  # - portworxVolume
  # - scaleIO
  # - storageos
  # - csi (部分)
```

**Restricted vs Baseline对比：**

```yaml
# Baseline: 基础安全
apiVersion: v1
kind: Pod
metadata:
  name: baseline-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      allowPrivilegeEscalation: false
      # runAsNonRoot: 可选
      # readOnlyRootFilesystem: 可选

---
# Restricted: 严格安全
apiVersion: v1
kind: Pod
metadata:
  name: restricted-pod
spec:
  securityContext:
    runAsNonRoot: true          # ← 必需
    seccompProfile:             # ← 必需
      type: RuntimeDefault

  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      allowPrivilegeEscalation: false
      runAsNonRoot: true                    # ← 必需
      readOnlyRootFilesystem: true          # ← 必需
      capabilities:
        drop:
        - ALL                               # ← 必需
      seccompProfile:                       # ← 必需
        type: RuntimeDefault
```

### 6.2.3 Pod Security Admission（PSA）

#### 6.2.3.1 PSA工作原理

Pod Security Admission是内置的准入控制器，用于强制执行Pod Security Standards。

**工作流程：**

```
┌─────────────────────────────────────────────────┐
│       Pod Security Admission工作流程            │
├─────────────────────────────────────────────────┤
│                                                  │
│  1. 用户提交Pod/Deployment                      │
│           ↓                                      │
│  2. API Server调用PSA准入控制器                 │
│           ↓                                      │
│  3. PSA检查Pod是否符合Namespace标签定义的级别   │
│           ↓                                      │
│  4. 根据模式（enforce/audit/warn）执行动作      │
│           ↓                                      │
│     ┌─────┴─────┬─────────┬─────────┐           │
│     ↓           ↓         ↓         ↓           │
│  enforce:    audit:    warn:    通过            │
│  拒绝创建   记录日志   返回警告   允许创建      │
└─────────────────────────────────────────────────┘
```

#### 6.2.3.2 配置PSA

**通过Namespace标签配置PSA：**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: my-app
  labels:
    # enforce模式：强制执行，违规拒绝创建
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/enforce-version: latest

    # audit模式：记录违规行为到审计日志
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/audit-version: latest

    # warn模式：向用户返回警告信息
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: latest
```

**三种模式说明：**

| 模式 | 行为 | 用途 |
|-----|------|------|
| **enforce** | 拒绝不符合标准的Pod | 生产环境强制执行 |
| **audit** | 记录违规到审计日志 | 监控和合规检查 |
| **warn** | 返回警告但允许创建 | 开发环境友好提示 |

**示例1：宽松的开发环境**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    # 开发环境：只警告，不强制
    pod-security.kubernetes.io/enforce: privileged    # 不限制
    pod-security.kubernetes.io/warn: baseline         # 警告不安全配置
    pod-security.kubernetes.io/audit: baseline        # 记录日志
```

**示例2：严格的生产环境**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    # 生产环境：强制执行严格标准
    pod-security.kubernetes.io/enforce: restricted    # 强制Restricted
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/audit-version: latest
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: latest
```

**示例3：渐进式迁移**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: legacy-app
  labels:
    # 当前强制Baseline，但audit/warn Restricted
    # 帮助逐步迁移到更严格的标准
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: restricted      # 记录不符合Restricted的情况
    pod-security.kubernetes.io/warn: restricted       # 警告开发者
```

#### 6.2.3.3 测试PSA

**测试Baseline级别：**

```yaml
# 创建Baseline级别的Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: test-baseline
  labels:
    pod-security.kubernetes.io/enforce: baseline
---
# 尝试创建特权Pod - 应该被拒绝
apiVersion: v1
kind: Pod
metadata:
  name: privileged-pod
  namespace: test-baseline
spec:
  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      privileged: true        # ❌ Baseline禁止
```

```bash
kubectl apply -f privileged-pod.yaml
# Error from server (Forbidden): error when creating "privileged-pod.yaml":
# pods "privileged-pod" is forbidden: violates PodSecurity "baseline:latest":
# privileged (container "app" must not set securityContext.privileged=true)
```

**测试Restricted级别：**

```yaml
# 创建Restricted级别的Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: test-restricted
  labels:
    pod-security.kubernetes.io/enforce: restricted
---
# 尝试创建缺少安全配置的Pod - 应该被拒绝
apiVersion: v1
kind: Pod
metadata:
  name: insecure-pod
  namespace: test-restricted
spec:
  containers:
  - name: app
    image: nginx:1.21
    # ❌ 缺少必需的安全配置
```

```bash
kubectl apply -f insecure-pod.yaml
# Error from server (Forbidden): pods "insecure-pod" is forbidden:
# violates PodSecurity "restricted:latest":
# allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false),
# unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]),
# runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true),
# seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
```

**创建合规的Pod：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: compliant-pod
  namespace: test-restricted
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    seccompProfile:
      type: RuntimeDefault

  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      allowPrivilegeEscalation: false
      runAsNonRoot: true
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
      seccompProfile:
        type: RuntimeDefault

    volumeMounts:
    - name: cache
      mountPath: /var/cache/nginx
    - name: run
      mountPath: /var/run

  volumes:
  - name: cache
    emptyDir: {}
  - name: run
    emptyDir: {}
```

```bash
kubectl apply -f compliant-pod.yaml
# pod/compliant-pod created ✅
```

### 6.2.4 SecurityContext安全上下文

#### 6.2.4.1 Pod级别SecurityContext

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-security-context
spec:
  # Pod级别安全上下文
  securityContext:
    # 以非root用户运行
    runAsUser: 1000           # UID
    runAsGroup: 3000          # GID
    fsGroup: 2000             # 文件系统组ID

    # 强制非root
    runAsNonRoot: true

    # 补充组
    supplementalGroups:
    - 4000
    - 5000

    # FSGroup变更策略
    fsGroupChangePolicy: "OnRootMismatch"

    # Seccomp配置
    seccompProfile:
      type: RuntimeDefault

    # SELinux配置
    seLinuxOptions:
      level: "s0:c123,c456"

    # Windows配置（如果使用Windows节点）
    windowsOptions:
      gmsaCredentialSpecName: "webapp-gmsa"

  containers:
  - name: app
    image: nginx:1.21
```

**UID/GID说明：**

```bash
# Pod内查看用户
kubectl exec pod-security-context -- id
# 输出：
# uid=1000 gid=3000 groups=3000,2000,4000,5000

# 查看文件权限
kubectl exec pod-security-context -- ls -ln /data
# 文件属主为1000:2000（fsGroup生效）
```

#### 6.2.4.2 容器级别SecurityContext

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: container-security-context
spec:
  containers:
  - name: app
    image: nginx:1.21

    # 容器级别安全上下文（优先级高于Pod级别）
    securityContext:
      # 运行用户
      runAsUser: 2000
      runAsGroup: 3000
      runAsNonRoot: true

      # 特权和权限提升
      privileged: false
      allowPrivilegeEscalation: false

      # 只读根文件系统
      readOnlyRootFilesystem: true

      # Capabilities
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE
        - CHOWN

      # Seccomp
      seccompProfile:
        type: RuntimeDefault

      # SELinux
      seLinuxOptions:
        level: "s0:c123,c456"
        role: "system_r"
        type: "container_t"
        user: "system_u"

      # AppArmor（通过annotation）
      # container.apparmor.security.beta.kubernetes.io/app: localhost/k8s-apparmor-example

      # Proc Mount
      procMount: Default      # Default | Unmasked
```

#### 6.2.4.3 Linux Capabilities详解

Linux Capabilities将root权限分解为更细粒度的权限单元。

**常用Capabilities：**

| Capability | 作用 | 风险 |
|-----------|------|------|
| **CAP_CHOWN** | 修改文件所有者 | 低 |
| **CAP_NET_BIND_SERVICE** | 绑定<1024端口 | 低 |
| **CAP_NET_RAW** | 使用RAW和PACKET套接字 | 中 |
| **CAP_SYS_ADMIN** | 几乎所有管理操作 | 极高 |
| **CAP_SYS_PTRACE** | 跟踪任意进程 | 高 |
| **CAP_SYS_MODULE** | 加载内核模块 | 极高 |
| **CAP_DAC_OVERRIDE** | 绕过文件权限检查 | 高 |
| **CAP_SETUID/SETGID** | 修改用户/组ID | 高 |

**安全的Capabilities配置：**

```yaml
# ✅ 推荐：最小权限
securityContext:
  capabilities:
    drop:
    - ALL                    # 先删除所有
    add:
    - NET_BIND_SERVICE       # 只添加必需的

# ❌ 危险：保留所有权限
securityContext:
  capabilities:
    add:
    - ALL                    # 极度危险！

# ❌ 危险：添加高风险Capability
securityContext:
  capabilities:
    add:
    - SYS_ADMIN              # 几乎等同于root
```

**实战示例：Nginx绑定80端口**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-unprivileged
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000

  containers:
  - name: nginx
    image: nginx:1.21

    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE    # 允许绑定80端口
      readOnlyRootFilesystem: true

    ports:
    - containerPort: 80

    volumeMounts:
    - name: cache
      mountPath: /var/cache/nginx
    - name: run
      mountPath: /var/run

  volumes:
  - name: cache
    emptyDir: {}
  - name: run
    emptyDir: {}
```

#### 6.2.4.4 只读根文件系统

**readOnlyRootFilesystem的好处：**

```
┌─────────────────────────────────────────────────┐
│       只读根文件系统的安全价值                   │
├─────────────────────────────────────────────────┤
│  ✅ 防止恶意软件持久化                          │
│     攻击者无法在容器内写入后门                  │
│                                                  │
│  ✅ 防止配置篡改                                 │
│     应用配置文件无法被修改                      │
│                                                  │
│  ✅ 符合不可变基础设施理念                      │
│     容器应该是一次性的、可替换的                │
│                                                  │
│  ✅ 便于审计                                     │
│     所有变更都在Volume中，易于追踪              │
└─────────────────────────────────────────────────┘
```

**配置只读文件系统：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: readonly-fs-pod
spec:
  containers:
  - name: app
    image: myapp:1.0

    securityContext:
      readOnlyRootFilesystem: true    # 根文件系统只读

    volumeMounts:
    # 为需要写入的目录挂载emptyDir
    - name: tmp
      mountPath: /tmp                 # 临时文件
    - name: cache
      mountPath: /var/cache           # 缓存
    - name: logs
      mountPath: /var/log             # 日志

  volumes:
  - name: tmp
    emptyDir: {}
  - name: cache
    emptyDir: {}
  - name: logs
    emptyDir: {}
```

**测试只读文件系统：**

```bash
# 尝试写入根文件系统 - 应该失败
kubectl exec readonly-fs-pod -- touch /test.txt
# Error: touch: /test.txt: Read-only file system

# 写入允许的目录 - 成功
kubectl exec readonly-fs-pod -- touch /tmp/test.txt
# （成功，无输出）
```

### 6.2.5 Seccomp和AppArmor

#### 6.2.5.1 Seccomp（Secure Computing Mode）

Seccomp用于限制容器可以调用的系统调用（syscalls）。

**Seccomp Profile类型：**

| 类型 | 说明 | 安全性 |
|-----|------|--------|
| **Unconfined** | 不限制系统调用 | 无保护 |
| **RuntimeDefault** | 使用运行时默认配置 | 推荐 |
| **Localhost** | 使用自定义profile | 最安全 |

**配置RuntimeDefault：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: seccomp-default
spec:
  securityContext:
    seccompProfile:
      type: RuntimeDefault    # 使用默认配置

  containers:
  - name: app
    image: nginx:1.21
```

**自定义Seccomp Profile：**

```json
// /var/lib/kubelet/seccomp/nginx-profile.json
{
  "defaultAction": "SCMP_ACT_ERRNO",
  "architectures": [
    "SCMP_ARCH_X86_64",
    "SCMP_ARCH_X86",
    "SCMP_ARCH_X32"
  ],
  "syscalls": [
    {
      "names": [
        "accept4",
        "bind",
        "clone",
        "close",
        "connect",
        "dup",
        "epoll_create1",
        "epoll_ctl",
        "epoll_wait",
        "exit_group",
        "fstat",
        "getpid",
        "listen",
        "mmap",
        "openat",
        "read",
        "rt_sigaction",
        "rt_sigprocmask",
        "setsockopt",
        "socket",
        "write"
      ],
      "action": "SCMP_ACT_ALLOW"
    }
  ]
}
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: seccomp-custom
spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: nginx-profile.json

  containers:
  - name: nginx
    image: nginx:1.21
```

#### 6.2.5.2 AppArmor

AppArmor是Linux安全模块，通过配置文件限制程序的行为。

**检查AppArmor是否启用：**

```bash
# 在节点上检查
cat /sys/module/apparmor/parameters/enabled
# Y 表示已启用
```

**AppArmor Profile示例：**

```
# /etc/apparmor.d/k8s-nginx
#include <tunables/global>

profile k8s-nginx flags=(attach_disconnected,mediate_deleted) {
  #include <abstractions/base>

  # 允许网络
  network inet tcp,
  network inet udp,

  # 允许读取配置文件
  /etc/nginx/** r,
  /usr/share/nginx/** r,

  # 允许写入日志
  /var/log/nginx/** w,

  # 允许读写缓存
  /var/cache/nginx/** rw,

  # 禁止执行命令
  deny /bin/** wrix,
  deny /usr/bin/** wrix,
  deny /sbin/** wrix,
  deny /usr/sbin/** wrix,

  # 禁止访问敏感目录
  deny /root/** rwx,
  deny /home/** rwx,
  deny /etc/shadow r,
}
```

**在Pod中使用AppArmor：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: apparmor-pod
  annotations:
    # 指定容器使用的AppArmor profile
    container.apparmor.security.beta.kubernetes.io/nginx: localhost/k8s-nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.21
```

**验证AppArmor生效：**

```bash
# 查看容器的AppArmor配置
kubectl exec apparmor-pod -- cat /proc/1/attr/current
# 输出：k8s-nginx (enforce)

# 尝试执行被禁止的命令
kubectl exec apparmor-pod -- /bin/bash
# Permission denied (AppArmor阻止)
```

### 6.2.6 实战案例：构建安全的应用

#### 6.2.6.1 场景：部署安全的Web应用

**需求：**
- 非root用户运行
- 只读根文件系统
- 最小Capabilities
- 启用Seccomp
- 符合Restricted标准

**完整配置：**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: secure-web
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  namespace: secure-web
data:
  nginx.conf: |
    user nginx;
    worker_processes auto;
    error_log /var/log/nginx/error.log;
    pid /var/run/nginx.pid;

    events {
      worker_connections 1024;
    }

    http {
      include /etc/nginx/mime.types;
      default_type application/octet-stream;
      access_log /var/log/nginx/access.log;

      server {
        listen 8080;
        server_name _;
        root /usr/share/nginx/html;
        index index.html;

        location / {
          try_files $uri $uri/ =404;
        }
      }
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-nginx
  namespace: secure-web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: secure-nginx
  template:
    metadata:
      labels:
        app: secure-nginx
    spec:
      # Pod级别安全上下文
      securityContext:
        runAsNonRoot: true
        runAsUser: 101          # nginx用户
        runAsGroup: 101
        fsGroup: 101
        seccompProfile:
          type: RuntimeDefault

      containers:
      - name: nginx
        image: nginx:1.21-alpine

        # 容器级别安全上下文
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 101
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
          readOnlyRootFilesystem: true
          seccompProfile:
            type: RuntimeDefault

        ports:
        - containerPort: 8080
          name: http

        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
          readOnly: true
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
        - name: logs
          mountPath: /var/log/nginx

        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5

      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
      - name: logs
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: secure-nginx
  namespace: secure-web
spec:
  selector:
    app: secure-nginx
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP
```

**部署和验证：**

```bash
# 部署应用
kubectl apply -f secure-web-deployment.yaml

# 验证Pod符合Restricted标准
kubectl get pods -n secure-web
kubectl describe pod -n secure-web <pod-name>

# 验证安全配置
kubectl exec -n secure-web <pod-name> -- id
# uid=101(nginx) gid=101(nginx)

# 验证只读文件系统
kubectl exec -n secure-web <pod-name> -- touch /test
# touch: /test: Read-only file system ✅

# 验证无特权
kubectl exec -n secure-web <pod-name> -- cat /proc/1/status | grep Cap
# 应该看到受限的Capabilities
```

本节我们深入学习了Kubernetes的Pod安全机制。通过Pod Security Standards和SecurityContext，可以构建纵深防御体系，有效防范容器安全威胁。在下一节，我们将学习RBAC权限控制，进一步加固集群安全。

---

## 6.3 RBAC权限控制

基于角色的访问控制（Role-Based Access Control，RBAC）是Kubernetes中最核心的安全机制之一。通过RBAC，我们可以精确控制用户、服务账号和应用程序对集群资源的访问权限，实现最小权限原则（Principle of Least Privilege）。

### 6.3.1 为什么需要RBAC

在多租户、多团队的Kubernetes集群中，如果没有权限控制，会面临严重的安全风险：

**1. 权限过度授予的风险**

```yaml
# ❌ 危险示例：给所有ServiceAccount授予cluster-admin权限
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dangerous-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: Group
  name: system:serviceaccounts  # 所有ServiceAccount
  apiGroup: rbac.authorization.k8s.io
```

**后果分析：**
- 任何Pod都能删除集群中的所有资源
- 应用程序可以访问所有Namespace的Secret
- 恶意容器可以创建特权Pod逃逸到宿主机
- 开发人员可能误操作删除生产环境资源

**2. 常见安全事故场景**

| 场景 | 缺乏RBAC的后果 | 真实案例 |
|------|----------------|----------|
| 开发环境误操作 | 开发人员删除了生产环境数据库 | Tesla 2018年生产事故 |
| 服务账号泄露 | 攻击者通过泄露的token控制整个集群 | Docker Hub 2019年数据泄露 |
| 第三方应用过度授权 | 监控工具被植入后门，窃取所有Secret | Capital One 2019年攻击 |
| 多租户隔离失败 | 租户A访问了租户B的敏感数据 | 大量云服务商隔离漏洞 |

**3. RBAC解决的核心问题**

```
┌─────────────────────────────────────────────────────────────┐
│                    RBAC权限控制模型                          │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  主体（Subject）     →    角色（Role）    →    权限（Verbs）│
│  ├── User                 ├── Role              ├── get     │
│  ├── Group                ├── ClusterRole       ├── list    │
│  └── ServiceAccount       └── ...               ├── create  │
│                                                  ├── update  │
│                           资源（Resources）      ├── delete  │
│                           ├── pods               └── ...    │
│                           ├── secrets                       │
│                           ├── configmaps                    │
│                           └── ...                           │
│                                                              │
│  绑定（Binding）：将主体与角色关联                         │
│  ├── RoleBinding（命名空间级别）                           │
│  └── ClusterRoleBinding（集群级别）                        │
└─────────────────────────────────────────────────────────────┘
```

### 6.3.2 RBAC核心概念

Kubernetes RBAC由四种核心资源类型组成：

**1. Role vs ClusterRole**

```yaml
# Role：命名空间级别的权限
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev-team
  name: pod-reader
rules:
- apiGroups: [""]        # "" 表示 core API group
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]
```

```yaml
# ClusterRole：集群级别的权限
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-reader
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["nodes"]
  verbs: ["get", "list"]
```

**Role vs ClusterRole对比：**

| 特性 | Role | ClusterRole |
|------|------|-------------|
| 作用范围 | 单个Namespace | 整个集群 |
| 可管理的资源 | Namespace级资源（Pod、Service等） | 所有资源（包括集群级资源如Node） |
| 绑定方式 | RoleBinding | ClusterRoleBinding |
| 典型用途 | 应用级权限控制 | 集群管理员、监控系统 |
| 示例 | 允许在dev命名空间读取Pod | 允许读取所有节点信息 |

**2. RoleBinding vs ClusterRoleBinding**

```yaml
# RoleBinding：在命名空间内绑定
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: dev-team
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
- kind: ServiceAccount
  name: myapp
  namespace: dev-team
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

```yaml
# ClusterRoleBinding：集群级别绑定
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-nodes-global
subjects:
- kind: Group
  name: monitoring-team
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-reader
  apiGroup: rbac.authorization.k8s.io
```

**3. 主体（Subject）类型**

```yaml
subjects:
# User：人类用户（由外部认证系统提供）
- kind: User
  name: alice@example.com
  apiGroup: rbac.authorization.k8s.io

# Group：用户组
- kind: Group
  name: system:authenticated  # 所有已认证用户
  apiGroup: rbac.authorization.k8s.io

# ServiceAccount：Pod使用的服务账号
- kind: ServiceAccount
  name: my-service-account
  namespace: my-namespace
```

**4. 权限动词（Verbs）详解**

| Verb | 含义 | 典型使用场景 |
|------|------|--------------|
| `get` | 获取单个资源 | 读取特定Pod详情 |
| `list` | 列出资源集合 | 查看所有Pod列表 |
| `watch` | 监听资源变化 | 实时监控Pod状态 |
| `create` | 创建资源 | 部署新的Deployment |
| `update` | 更新资源（整体替换） | 修改ConfigMap内容 |
| `patch` | 部分更新资源 | 修改Pod的某个标签 |
| `delete` | 删除单个资源 | 删除特定Pod |
| `deletecollection` | 批量删除资源 | 删除Namespace下所有Pod |

**特殊权限：**
```yaml
rules:
# 子资源权限
- apiGroups: [""]
  resources: ["pods/exec"]     # Pod执行命令
  verbs: ["create"]

- apiGroups: [""]
  resources: ["pods/portforward"]  # Pod端口转发
  verbs: ["create"]

# 通配符（谨慎使用）
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]  # 完全控制权限
```

### 6.3.3 实战：为不同角色配置权限

**场景1：开发人员权限（Namespace隔离）**

```yaml
# 1. 创建开发命名空间
apiVersion: v1
kind: Namespace
metadata:
  name: dev-team-alpha

---
# 2. 定义开发者角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev-team-alpha
  name: developer
rules:
# 可以管理应用负载
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets", "statefulsets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

# 可以管理Pod和查看日志
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list", "watch", "delete"]

- apiGroups: [""]
  resources: ["pods/exec"]
  verbs: ["create"]  # 允许进入容器调试

# 可以管理配置
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

# 可以查看Service
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

# 可以查看事件
- apiGroups: [""]
  resources: ["events"]
  verbs: ["get", "list", "watch"]

---
# 3. 绑定到开发人员
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developer-binding
  namespace: dev-team-alpha
subjects:
- kind: User
  name: dev-alice
  apiGroup: rbac.authorization.k8s.io
- kind: User
  name: dev-bob
  apiGroup: rbac.authorization.k8s.io
- kind: Group
  name: dev-team-alpha-members
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
```

**场景2：只读用户（适用于审计人员）**

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: read-only-auditor
rules:
# 可以查看所有资源，但不能修改
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["get", "list", "watch"]

# 明确禁止访问敏感资源
- apiGroups: [""]
  resources: ["secrets"]
  verbs: []  # 空列表表示无权限

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: auditor-binding
subjects:
- kind: User
  name: auditor-team
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: read-only-auditor
  apiGroup: rbac.authorization.k8s.io
```

**场景3：应用ServiceAccount权限（最小权限原则）**

```yaml
# 1. 创建ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-app-sa
  namespace: production

---
# 2. 定义应用所需的最小权限
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: app-minimal-permissions
rules:
# 只能读取自己需要的ConfigMap
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["app-config"]  # 限制特定资源
  verbs: ["get"]

# 只能读取自己的Secret
- apiGroups: [""]
  resources: ["secrets"]
  resourceNames: ["app-secret"]
  verbs: ["get"]

# 只能读取Service用于服务发现
- apiGroups: [""]
  resources: ["services", "endpoints"]
  verbs: ["get", "list"]

---
# 3. 绑定权限
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-binding
  namespace: production
subjects:
- kind: ServiceAccount
  name: my-app-sa
  namespace: production
roleRef:
  kind: Role
  name: app-minimal-permissions
  apiGroup: rbac.authorization.k8s.io

---
# 4. 在Deployment中使用ServiceAccount
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      serviceAccountName: my-app-sa  # 使用专用SA
      automountServiceAccountToken: true
      containers:
      - name: app
        image: my-app:v1.0
        env:
        - name: CONFIG_PATH
          value: /var/run/secrets/kubernetes.io/serviceaccount
```

**场景4：CI/CD流水线权限**

```yaml
# 1. 创建部署机器人ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: deploy-bot
  namespace: ci-cd

---
# 2. ClusterRole：可以在多个命名空间部署
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: deployer
rules:
# 可以管理Deployment
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "create", "update", "patch"]

# 可以查看Deployment状态
- apiGroups: ["apps"]
  resources: ["deployments/status"]
  verbs: ["get"]

# 可以查看ReplicaSet（Deployment创建的）
- apiGroups: ["apps"]
  resources: ["replicasets"]
  verbs: ["get", "list"]

# 可以管理ConfigMap和Secret
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "create", "update", "patch"]

# 可以查看Pod状态（验证部署）
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]

- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]

---
# 3. 在允许的命名空间绑定权限
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: deploy-bot-dev
  namespace: dev
subjects:
- kind: ServiceAccount
  name: deploy-bot
  namespace: ci-cd
roleRef:
  kind: ClusterRole
  name: deployer
  apiGroup: rbac.authorization.k8s.io

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: deploy-bot-staging
  namespace: staging
subjects:
- kind: ServiceAccount
  name: deploy-bot
  namespace: ci-cd
roleRef:
  kind: ClusterRole
  name: deployer
  apiGroup: rbac.authorization.k8s.io

# 注意：不在production绑定，需要人工审批
```

### 6.3.4 聚合ClusterRole（Aggregated ClusterRole）

Kubernetes支持通过标签选择器组合多个ClusterRole，实现权限的模块化管理：

```yaml
# 1. 基础只读权限
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: base-read
  labels:
    rbac.example.com/aggregate-to-custom: "true"
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list"]

---
# 2. 扩展监控权限
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring-extension
  labels:
    rbac.example.com/aggregate-to-custom: "true"
rules:
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list"]

---
# 3. 聚合ClusterRole（自动组合上述权限）
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: custom-viewer
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.example.com/aggregate-to-custom: "true"
rules: []  # 规则会自动从匹配的ClusterRole聚合
```

**Kubernetes内置聚合角色：**

```yaml
# 查看系统内置的聚合角色
kubectl get clusterrole admin -o yaml
# 可以看到：
# aggregationRule:
#   clusterRoleSelectors:
#   - matchLabels:
#       rbac.authorization.k8s.io/aggregate-to-admin: "true"
```

**扩展内置角色的最佳实践：**

```yaml
# 为admin角色添加自定义资源权限
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: custom-resource-admin
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"  # 自动加入admin角色
rules:
- apiGroups: ["example.com"]
  resources: ["myresources"]
  verbs: ["*"]
```

### 6.3.5 RBAC最佳实践

**1. 最小权限原则**

```yaml
# ❌ 错误：授予过多权限
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]

# ✅ 正确：精确指定需要的权限
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["app-config"]  # 进一步限制到特定资源
  verbs: ["get"]
```

**2. 使用resourceNames限制访问**

```yaml
# 只允许访问特定名称的Secret
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: specific-secret-reader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  resourceNames:
  - "database-credentials"
  - "api-token"
  verbs: ["get"]
```

**注意：** `resourceNames`只对`get`、`update`、`patch`、`delete`生效，对`list`和`watch`无效。

**3. 避免使用通配符**

```yaml
# ❌ 危险
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]

# ✅ 推荐
- apiGroups: ["apps", "batch"]
  resources: ["deployments", "jobs"]
  verbs: ["get", "list", "watch"]
```

**4. ServiceAccount自动挂载控制**

```yaml
# 如果Pod不需要访问K8s API，禁用自动挂载
apiVersion: v1
kind: Pod
metadata:
  name: no-api-access
spec:
  automountServiceAccountToken: false  # 不挂载token
  containers:
  - name: app
    image: nginx
```

**5. 定期审计权限**

```bash
# 查看用户的权限
kubectl auth can-i list pods --as=dev-alice --namespace=dev-team

# 查看ServiceAccount的权限
kubectl auth can-i create deployments \
  --as=system:serviceaccount:production:my-app-sa \
  --namespace=production

# 查看所有ClusterRoleBinding
kubectl get clusterrolebinding -o wide

# 查找cluster-admin绑定（高风险）
kubectl get clusterrolebinding -o json | \
  jq '.items[] | select(.roleRef.name=="cluster-admin") | .metadata.name'
```

**6. 使用命名空间隔离**

```yaml
# 为不同团队创建隔离的命名空间
---
apiVersion: v1
kind: Namespace
metadata:
  name: team-a
  labels:
    team: alpha

---
apiVersion: v1
kind: Namespace
metadata:
  name: team-b
  labels:
    team: beta

---
# Team A只能访问自己的命名空间
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: team-a-admin
  namespace: team-a
subjects:
- kind: Group
  name: team-a-members
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: admin  # 使用内置admin角色
  apiGroup: rbac.authorization.k8s.io
```

### 6.3.6 常见问题排查

**问题1：权限拒绝（Forbidden）**

```bash
# 错误信息
Error from server (Forbidden): pods is forbidden:
User "dev-alice" cannot list resource "pods" in API group "" in the namespace "production"

# 排查步骤
# 1. 检查用户是否有权限
kubectl auth can-i list pods --as=dev-alice --namespace=production
# no

# 2. 查看用户的RoleBinding
kubectl get rolebinding -n production -o yaml | grep dev-alice

# 3. 查看Role的详细规则
kubectl get role <role-name> -n production -o yaml

# 4. 检查ClusterRoleBinding
kubectl get clusterrolebinding -o yaml | grep dev-alice
```

**问题2：ServiceAccount权限不生效**

```bash
# 验证Pod使用的ServiceAccount
kubectl get pod <pod-name> -o jsonpath='{.spec.serviceAccountName}'

# 查看SA的token是否挂载
kubectl exec <pod-name> -- ls /var/run/secrets/kubernetes.io/serviceaccount/
# 应该看到：ca.crt  namespace  token

# 在Pod内测试权限
kubectl exec <pod-name> -- cat /var/run/secrets/kubernetes.io/serviceaccount/token
```

**问题3：意外拥有过高权限**

```bash
# 审计脚本：查找所有cluster-admin绑定
cat <<'EOF' > audit-rbac.sh
#!/bin/bash
echo "=== Cluster Admin Bindings ==="
kubectl get clusterrolebinding -o json | \
  jq -r '.items[] | select(.roleRef.name=="cluster-admin") |
  "\(.metadata.name): \(.subjects)"'

echo -e "\n=== ServiceAccounts with Cluster Admin ==="
kubectl get clusterrolebinding -o json | \
  jq -r '.items[] | select(.roleRef.name=="cluster-admin") |
  .subjects[]? | select(.kind=="ServiceAccount") |
  "\(.namespace)/\(.name)"'

echo -e "\n=== Wildcard Permissions ==="
kubectl get clusterrole -o json | \
  jq -r '.items[] | select(.rules[]? | .verbs[]? == "*") | .metadata.name'
EOF

chmod +x audit-rbac.sh
./audit-rbac.sh
```

### 6.3.7 完整实战案例：多租户平台RBAC配置

**需求：** 构建一个SaaS平台，为客户提供隔离的Kubernetes命名空间，每个客户有admin、developer、viewer三种角色。

```yaml
# ===========================
# 1. 租户命名空间模板
# ===========================
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-acme
  labels:
    tenant: acme
    billing-tier: premium

---
# ===========================
# 2. 租户管理员角色（完整控制命名空间内资源）
# ===========================
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: tenant-acme
  name: tenant-admin
rules:
# 完全控制应用负载
- apiGroups: ["apps", "batch", "extensions"]
  resources: ["*"]
  verbs: ["*"]

# 完全控制核心资源
- apiGroups: [""]
  resources:
  - pods
  - pods/log
  - pods/exec
  - pods/portforward
  - services
  - configmaps
  - secrets
  - persistentvolumeclaims
  - serviceaccounts
  verbs: ["*"]

# 可以管理RBAC（但限制在命名空间内）
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["roles", "rolebindings"]
  verbs: ["*"]

# 可以查看资源使用情况
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods"]
  verbs: ["get", "list"]

---
# ===========================
# 3. 租户开发者角色（可部署但不能删除关键资源）
# ===========================
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: tenant-acme
  name: tenant-developer
rules:
# 可以管理应用
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets", "statefulsets", "daemonsets"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]  # 不包括delete

# 可以管理Pod（但不能删除）
- apiGroups: [""]
  resources: ["pods", "pods/log", "pods/exec"]
  verbs: ["get", "list", "watch", "create"]

# 可以管理配置
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]

# 只能读取Secret（不能创建/修改）
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list"]

# 可以管理Service
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]

# 可以查看事件
- apiGroups: [""]
  resources: ["events"]
  verbs: ["get", "list", "watch"]

---
# ===========================
# 4. 租户查看者角色（只读）
# ===========================
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: tenant-acme
  name: tenant-viewer
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["get", "list", "watch"]

# 明确禁止查看Secret
- apiGroups: [""]
  resources: ["secrets"]
  verbs: []

---
# ===========================
# 5. 角色绑定
# ===========================
# Admin绑定
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-admin-binding
  namespace: tenant-acme
subjects:
- kind: User
  name: alice@acme.com
  apiGroup: rbac.authorization.k8s.io
- kind: ServiceAccount
  name: admin-sa
  namespace: tenant-acme
roleRef:
  kind: Role
  name: tenant-admin
  apiGroup: rbac.authorization.k8s.io

---
# Developer绑定
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-developer-binding
  namespace: tenant-acme
subjects:
- kind: Group
  name: acme-developers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: tenant-developer
  apiGroup: rbac.authorization.k8s.io

---
# Viewer绑定
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-viewer-binding
  namespace: tenant-acme
subjects:
- kind: Group
  name: acme-viewers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: tenant-viewer
  apiGroup: rbac.authorization.k8s.io

---
# ===========================
# 6. 平台级别的ClusterRole（跨命名空间查看自己的资源）
# ===========================
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: tenant-list-namespaces
rules:
# 允许列出命名空间（但通过标签过滤）
- apiGroups: [""]
  resources: ["namespaces"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: acme-list-namespaces
subjects:
- kind: Group
  name: acme-all-users
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: tenant-list-namespaces
  apiGroup: rbac.authorization.k8s.io
```

**验证脚本：**

```bash
#!/bin/bash
# tenant-rbac-test.sh

NAMESPACE="tenant-acme"

echo "=== 测试Admin权限 ==="
kubectl auth can-i delete deployments --as=alice@acme.com -n $NAMESPACE
# yes ✅

echo "=== 测试Developer权限 ==="
kubectl auth can-i create deployments --as=system:group:acme-developers -n $NAMESPACE
# yes ✅

kubectl auth can-i delete deployments --as=system:group:acme-developers -n $NAMESPACE
# no ✅

kubectl auth can-i create secrets --as=system:group:acme-developers -n $NAMESPACE
# no ✅

echo "=== 测试Viewer权限 ==="
kubectl auth can-i get pods --as=system:group:acme-viewers -n $NAMESPACE
# yes ✅

kubectl auth can-i get secrets --as=system:group:acme-viewers -n $NAMESPACE
# no ✅

kubectl auth can-i create pods --as=system:group:acme-viewers -n $NAMESPACE
# no ✅
```

**部署流程：**

```bash
# 1. 创建命名空间和RBAC配置
kubectl apply -f tenant-acme-rbac.yaml

# 2. 创建测试用户的kubeconfig
cat <<EOF > acme-developer-kubeconfig.yaml
apiVersion: v1
kind: Config
clusters:
- cluster:
    server: https://k8s-api-server:6443
    certificate-authority-data: <CA_DATA>
  name: my-cluster
contexts:
- context:
    cluster: my-cluster
    namespace: tenant-acme
    user: developer
  name: acme-context
current-context: acme-context
users:
- name: developer
  user:
    token: <DEVELOPER_TOKEN>
EOF

# 3. 测试开发者权限
export KUBECONFIG=acme-developer-kubeconfig.yaml

kubectl get pods
# 成功 ✅

kubectl delete deployment my-app
# Error from server (Forbidden) ✅

kubectl create secret generic test --from-literal=key=value
# Error from server (Forbidden) ✅
```

### 6.3.8 RBAC安全检查清单

在生产环境部署前，使用此清单验证RBAC配置：

- [ ] **没有不必要的cluster-admin绑定**
  ```bash
  kubectl get clusterrolebinding -o json | \
    jq '.items[] | select(.roleRef.name=="cluster-admin")'
  ```

- [ ] **ServiceAccount遵循最小权限原则**
  ```bash
  # 检查每个SA的权限是否合理
  for sa in $(kubectl get sa -A -o jsonpath='{range .items[*]}{.metadata.namespace}{"\t"}{.metadata.name}{"\n"}{end}'); do
    echo "=== $sa ==="
    kubectl auth can-i --list --as=system:serviceaccount:$sa
  done
  ```

- [ ] **没有使用通配符 (*) 授权（除非确实需要）**
  ```bash
  kubectl get clusterrole,role -A -o json | \
    jq '.items[] | select(.rules[]?.verbs[]? == "*")'
  ```

- [ ] **敏感资源（Secret、Node）访问受限**
  ```bash
  # 查找可以读取Secret的非系统角色
  kubectl get clusterrole,role -A -o json | \
    jq '.items[] | select(.metadata.name | startswith("system:") | not) |
    select(.rules[]? | .resources[]? == "secrets")'
  ```

- [ ] **生产环境禁用了pods/exec权限（或严格限制）**
  ```bash
  kubectl get role,clusterrole -A -o json | \
    jq '.items[] | select(.rules[]?.resources[]? == "pods/exec")'
  ```

- [ ] **定期审计权限变更**
  ```bash
  # 启用审计日志后，监控RBAC变更
  kubectl get events --all-namespaces --field-selector reason=RoleBindingCreated
  ```

本节我们深入学习了Kubernetes的RBAC权限控制机制。通过Role、ClusterRole、RoleBinding和ClusterRoleBinding的组合，可以实现精细化的权限管理，保护集群资源安全。在下一节，我们将学习网络策略（NetworkPolicy），从网络层面进一步加固安全防线。

---

## 6.4 网络策略（NetworkPolicy）

网络策略（NetworkPolicy）是Kubernetes提供的网络层安全机制，通过定义Pod间的网络访问规则，实现微隔离（Micro-segmentation）。在前面的章节中，我们学习了通过RBAC控制"谁可以操作什么资源"，而NetworkPolicy则控制"谁可以与谁通信"。

### 6.4.1 为什么需要网络策略

**默认的Kubernetes网络行为：**

在没有NetworkPolicy的Kubernetes集群中，所有Pod之间都可以自由通信，这带来了严重的安全隐患：

```
┌────────────────────────────────────────────────────────────┐
│        默认网络模型：所有Pod可以互相访问                   │
├────────────────────────────────────────────────────────────┤
│                                                             │
│  Namespace: frontend          Namespace: backend           │
│  ┌─────────────┐              ┌─────────────┐             │
│  │  Web Pod    │─────────────>│ Database Pod│  ❌ 危险！   │
│  └─────────────┘              └─────────────┘             │
│         │                             │                    │
│         │                             │                    │
│         └─────────────────────────────┘                    │
│              任意Pod都能访问数据库                          │
│                                                             │
│  Namespace: test                                           │
│  ┌─────────────┐                                           │
│  │  Test Pod   │────────────> 可以访问生产数据库 ❌         │
│  └─────────────┘                                           │
└────────────────────────────────────────────────────────────┘
```

**真实安全威胁：**

| 威胁场景 | 无NetworkPolicy的后果 | 真实案例 |
|----------|----------------------|----------|
| 横向移动攻击 | 攻击者攻陷一个前端Pod后，可直接访问数据库 | 2020年SolarWinds供应链攻击 |
| 命名空间隔离失败 | test环境Pod可以访问production数据库 | 2019年某金融公司数据泄露 |
| 第三方组件过度访问 | 日志采集器可以访问所有微服务的敏感端口 | Log4Shell漏洞利用 |
| 容器逃逸后扩散 | 攻击者逃逸后可扫描整个集群网络 | 2018年Tesla K8s挖矿事件 |

**NetworkPolicy解决的核心问题：**

1. **零信任网络**：默认拒绝所有流量，显式允许必要通信
2. **最小权限原则**：Pod只能访问完成工作所需的最少网络资源
3. **深度防御**：即使攻击者攻陷一个Pod，也无法横向移动
4. **合规要求**：满足PCI-DSS、HIPAA等安全合规标准

### 6.4.2 NetworkPolicy核心概念

**1. NetworkPolicy基本结构**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: example-policy
  namespace: default
spec:
  # 策略应用到哪些Pod
  podSelector:
    matchLabels:
      role: db

  # 策略类型：Ingress（入站）和/或 Egress（出站）
  policyTypes:
  - Ingress
  - Egress

  # 入站规则：允许哪些来源访问选中的Pod
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 3306

  # 出站规则：允许选中的Pod访问哪些目标
  egress:
  - to:
    - podSelector:
        matchLabels:
          role: backend
    ports:
    - protocol: TCP
      port: 8080
```

**2. 流量方向：Ingress vs Egress**

```
┌──────────────────────────────────────────────────────────┐
│                   NetworkPolicy流量控制                   │
├──────────────────────────────────────────────────────────┤
│                                                           │
│                      ┌──────────┐                        │
│                      │          │                        │
│      Ingress ───────>│ 目标Pod  │                        │
│      (入站流量)       │  role:db │                        │
│                      │          │                        │
│                      └─────┬────┘                        │
│                            │                             │
│                            │ Egress                      │
│                            │ (出站流量)                   │
│                            ▼                             │
│                      ┌──────────┐                        │
│                      │后端服务   │                        │
│                      └──────────┘                        │
│                                                           │
│  Ingress规则：控制谁可以访问这个Pod                      │
│  Egress规则：控制这个Pod可以访问谁                       │
└──────────────────────────────────────────────────────────┘
```

**3. 选择器类型**

NetworkPolicy支持三种选择器来定义流量源/目标：

```yaml
ingress:
- from:
  # 选择器1：Pod选择器（同一命名空间）
  - podSelector:
      matchLabels:
        app: frontend

  # 选择器2：命名空间选择器
  - namespaceSelector:
      matchLabels:
        team: platform

  # 选择器3：IP块（CIDR）
  - ipBlock:
      cidr: 10.0.0.0/24
      except:
      - 10.0.0.1/32  # 排除特定IP
```

**选择器组合逻辑：**

```yaml
# 情况1：OR逻辑（数组元素之间）
ingress:
- from:
  - podSelector:          # 满足条件1
      matchLabels:
        app: web
  - namespaceSelector:    # 或 满足条件2
      matchLabels:
        env: prod

# 情况2：AND逻辑（同一个元素内）
ingress:
- from:
  - podSelector:          # 同时满足两个条件
      matchLabels:
        app: web
    namespaceSelector:
      matchLabels:
        env: prod
```

**4. 默认行为**

```yaml
# 场景1：没有NetworkPolicy
# 结果：所有流量都允许（默认Allow All）

# 场景2：有NetworkPolicy但podSelector为空
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}  # 空选择器 = 选中所有Pod
  policyTypes:
  - Ingress
  - Egress
# 结果：命名空间内所有Pod的入站和出站都被拒绝

# 场景3：有匹配的NetworkPolicy
# 结果：只允许策略中明确定义的流量（默认Deny）
```

### 6.4.3 常见网络策略模式

**模式1：默认拒绝所有入站流量**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: production
spec:
  podSelector: {}  # 应用到命名空间内所有Pod
  policyTypes:
  - Ingress
  # 没有ingress规则 = 拒绝所有入站
```

**模式2：默认拒绝所有出站流量**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Egress
  # 没有egress规则 = 拒绝所有出站
```

**模式3：默认拒绝所有流量（入站+出站）**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```

**模式4：允许同命名空间内通信**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: {}  # 同命名空间的所有Pod
```

**模式5：允许特定命名空间访问**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-monitoring
  namespace: production
spec:
  podSelector:
    matchLabels:
      expose-metrics: "true"
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring  # 只允许monitoring命名空间访问
    ports:
    - protocol: TCP
      port: 9090  # Prometheus metrics端口
```

**模式6：允许外部访问（Ingress Controller）**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-controller
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    - podSelector:
        matchLabels:
          app: nginx-ingress
    ports:
    - protocol: TCP
      port: 8080
```

### 6.4.4 实战：三层Web架构的网络隔离

**场景：** 构建一个包含前端、后端API、数据库的三层应用，实现严格的网络隔离。

**架构图：**

```
┌────────────────────────────────────────────────────────────┐
│                      网络隔离架构                           │
├────────────────────────────────────────────────────────────┤
│                                                             │
│  Internet                                                  │
│     │                                                       │
│     ▼                                                       │
│  ┌──────────────┐                                          │
│  │Ingress       │ ← 允许外部HTTP/HTTPS                     │
│  │Controller    │                                          │
│  └──────┬───────┘                                          │
│         │                                                   │
│         ▼                                                   │
│  ┌──────────────┐                                          │
│  │ Frontend Pod │ ← 只允许Ingress访问                      │
│  │ (Nginx)      │                                          │
│  └──────┬───────┘                                          │
│         │ 只能访问Backend:8080                             │
│         ▼                                                   │
│  ┌──────────────┐                                          │
│  │ Backend Pod  │ ← 只允许Frontend访问                     │
│  │ (API Server) │                                          │
│  └──────┬───────┘                                          │
│         │ 只能访问Database:5432                            │
│         ▼                                                   │
│  ┌──────────────┐                                          │
│  │Database Pod  │ ← 只允许Backend访问                      │
│  │ (PostgreSQL) │                                          │
│  └──────────────┘                                          │
│         │                                                   │
│         └──> 禁止所有其他流量 ❌                           │
└────────────────────────────────────────────────────────────┘
```

**完整配置：**

```yaml
# ===========================
# 1. 创建应用命名空间
# ===========================
apiVersion: v1
kind: Namespace
metadata:
  name: three-tier-app
  labels:
    name: three-tier-app

---
# ===========================
# 2. 默认拒绝所有流量（安全基线）
# ===========================
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: three-tier-app
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# ===========================
# 3. 允许DNS查询（所有Pod都需要）
# ===========================
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: three-tier-app
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 53

---
# ===========================
# 4. Frontend网络策略
# ===========================
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-policy
  namespace: three-tier-app
spec:
  podSelector:
    matchLabels:
      tier: frontend
  policyTypes:
  - Ingress
  - Egress

  # 入站：只允许Ingress Controller访问
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 80

  # 出站：只允许访问Backend
  egress:
  - to:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 8080

---
# ===========================
# 5. Backend网络策略
# ===========================
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-policy
  namespace: three-tier-app
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Ingress
  - Egress

  # 入站：只允许Frontend访问
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: frontend
    ports:
    - protocol: TCP
      port: 8080

  # 出站：只允许访问Database
  egress:
  - to:
    - podSelector:
        matchLabels:
          tier: database
    ports:
    - protocol: TCP
      port: 5432

---
# ===========================
# 6. Database网络策略
# ===========================
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-policy
  namespace: three-tier-app
spec:
  podSelector:
    matchLabels:
      tier: database
  policyTypes:
  - Ingress
  - Egress

  # 入站：只允许Backend访问
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 5432

  # 出站：禁止所有出站（数据库不需要主动连接外部）
  # 但允许DNS（由allow-dns策略提供）
  egress: []

---
# ===========================
# 7. 部署Frontend
# ===========================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: three-tier-app
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80

---
# ===========================
# 8. 部署Backend
# ===========================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: three-tier-app
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: backend
  template:
    metadata:
      labels:
        tier: backend
    spec:
      containers:
      - name: api
        image: my-api:v1.0
        ports:
        - containerPort: 8080

---
# ===========================
# 9. 部署Database
# ===========================
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
  namespace: three-tier-app
spec:
  serviceName: database
  replicas: 1
  selector:
    matchLabels:
      tier: database
  template:
    metadata:
      labels:
        tier: database
    spec:
      containers:
      - name: postgres
        image: postgres:14
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password

---
# ===========================
# 10. 创建Service
# ===========================
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: three-tier-app
spec:
  selector:
    tier: frontend
  ports:
  - port: 80
    targetPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: three-tier-app
spec:
  selector:
    tier: backend
  ports:
  - port: 8080
    targetPort: 8080

---
apiVersion: v1
kind: Service
metadata:
  name: database
  namespace: three-tier-app
spec:
  selector:
    tier: database
  clusterIP: None  # Headless Service for StatefulSet
  ports:
  - port: 5432
    targetPort: 5432
```

**验证网络策略：**

```bash
# 1. 部署应用
kubectl apply -f three-tier-app.yaml

# 2. 测试Frontend可以访问Backend
kubectl exec -n three-tier-app deployment/frontend -it -- \
  curl http://backend:8080/health
# 应该成功 ✅

# 3. 测试Frontend不能直接访问Database
kubectl exec -n three-tier-app deployment/frontend -it -- \
  nc -zv database 5432
# 应该超时失败 ✅

# 4. 测试Backend可以访问Database
kubectl exec -n three-tier-app deployment/backend -it -- \
  nc -zv database 5432
# 应该成功 ✅

# 5. 测试外部Pod不能访问
kubectl run test-pod --image=busybox --rm -it -- \
  nc -zv backend.three-tier-app 8080
# 应该超时失败 ✅
```

### 6.4.5 高级场景：多租户隔离

**场景：** 在同一集群中，为不同团队提供完全隔离的命名空间。

```yaml
# ===========================
# 租户A的命名空间
# ===========================
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-a
  labels:
    tenant: team-alpha

---
# 默认拒绝所有流量
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: tenant-a
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# 只允许同租户内通信
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-tenant
  namespace: tenant-a
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector: {}  # 同命名空间的Pod
  egress:
  - to:
    - podSelector: {}  # 同命名空间的Pod
  - to:  # 允许DNS
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53

---
# 允许访问共享服务（如监控）
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-monitoring
  namespace: tenant-a
spec:
  podSelector:
    matchLabels:
      expose-metrics: "true"
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 9090

---
# ===========================
# 租户B的命名空间（相同配置）
# ===========================
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-b
  labels:
    tenant: team-beta

# ... 类似的NetworkPolicy配置
```

**验证租户隔离：**

```bash
# 部署测试Pod
kubectl run pod-a --image=nginx -n tenant-a
kubectl run pod-b --image=nginx -n tenant-b

# 测试租户A不能访问租户B
kubectl exec -n tenant-a pod-a -- curl http://pod-b.tenant-b
# 应该超时失败 ✅

# 测试租户B不能访问租户A
kubectl exec -n tenant-b pod-b -- curl http://pod-a.tenant-a
# 应该超时失败 ✅
```

### 6.4.6 Egress网络策略：限制外部访问

**场景：** 限制Pod只能访问特定的外部API，防止数据泄露。

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: restrict-egress
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: payment-service
  policyTypes:
  - Egress
  egress:
  # 1. 允许访问内部服务
  - to:
    - podSelector:
        matchLabels:
          app: user-service
    ports:
    - protocol: TCP
      port: 8080

  # 2. 允许访问特定外部API（通过IP）
  - to:
    - ipBlock:
        cidr: 203.0.113.0/24  # 第三方支付API
    ports:
    - protocol: TCP
      port: 443

  # 3. 允许DNS查询
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53

  # 4. 允许访问Kubernetes API（用于服务发现）
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          component: apiserver
    ports:
    - protocol: TCP
      port: 443
```

**基于FQDN的出站控制（需要CNI支持）：**

某些CNI插件（如Cilium）支持基于域名的策略：

```yaml
# Cilium CiliumNetworkPolicy示例
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-specific-domains
  namespace: production
spec:
  endpointSelector:
    matchLabels:
      app: web
  egress:
  - toFQDNs:
    - matchName: "api.example.com"
    - matchPattern: "*.googleapis.com"
  - toPorts:
    - ports:
      - port: "443"
        protocol: TCP
```

### 6.4.7 常见问题排查

**问题1：NetworkPolicy不生效**

```bash
# 原因1：CNI插件不支持NetworkPolicy
# 检查CNI插件
kubectl get pods -n kube-system | grep -E 'calico|cilium|weave'

# 支持NetworkPolicy的CNI：
# - Calico
# - Cilium
# - Weave Net
# - Antrea

# 不支持的CNI：
# - Flannel (需要配合Calico)

# 原因2：标签选择器错误
# 验证Pod标签
kubectl get pods --show-labels -n production

# 验证策略是否匹配
kubectl get networkpolicy -n production -o yaml

# 原因3：策略冲突
# 查看所有应用到Pod的策略
kubectl get networkpolicy -n production --field-selector spec.podSelector.matchLabels.app=myapp
```

**问题2：Pod无法访问外部网络**

```bash
# 检查是否有阻止Egress的策略
kubectl get networkpolicy -n production -o json | \
  jq '.items[] | select(.spec.policyTypes[]? == "Egress")'

# 添加允许DNS的策略
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
EOF
```

**问题3：调试NetworkPolicy**

```bash
# 1. 查看策略详情
kubectl describe networkpolicy <policy-name> -n <namespace>

# 2. 测试连通性
kubectl run test-pod --image=busybox --rm -it -n production -- \
  nc -zv target-service 8080

# 3. 查看CNI日志（以Calico为例）
kubectl logs -n kube-system -l k8s-app=calico-node

# 4. 使用专门的调试工具
# 安装netshoot调试容器
kubectl run netshoot --image=nicolaka/netshoot --rm -it -n production -- bash

# 在容器内测试
# nc -zv database 5432
# nslookup database
# traceroute database
```

### 6.4.8 NetworkPolicy最佳实践

**1. 默认拒绝策略（零信任模型）**

```yaml
# 在每个命名空间都部署
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```

**2. 显式允许必要流量**

```yaml
# 然后为每个应用创建具体的允许策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-app-specific
spec:
  podSelector:
    matchLabels:
      app: myapp
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080
```

**3. 始终允许DNS**

```yaml
# DNS是基础设施，所有Pod都需要
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
```

**4. 使用命名空间标签**

```yaml
# 为命名空间打标签
kubectl label namespace production name=production

# 在策略中引用
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-prod
spec:
  podSelector:
    matchLabels:
      app: shared-service
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: production  # 使用命名空间标签
```

**5. 分层策略管理**

```yaml
# 第1层：默认拒绝（安全基线）
# default-deny-all

# 第2层：基础设施访问（DNS、监控）
# allow-dns
# allow-monitoring

# 第3层：应用间通信
# frontend-to-backend
# backend-to-database

# 第4层：外部访问
# allow-ingress-controller
# allow-external-api
```

**6. 文档化策略**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-policy
  annotations:
    description: "Allows frontend to access backend API"
    owner: "platform-team@example.com"
    last-reviewed: "2024-01-15"
spec:
  # ...
```

### 6.4.9 NetworkPolicy vs 服务网格

**对比：**

| 特性 | NetworkPolicy | 服务网格（如Istio） |
|------|---------------|---------------------|
| 层级 | L3/L4（IP/端口） | L7（HTTP/gRPC） |
| 粒度 | Pod级别 | 请求级别 |
| 性能开销 | 低 | 中等（Sidecar代理） |
| 功能 | 基础网络隔离 | 流量管理、熔断、重试、监控 |
| 复杂度 | 简单 | 复杂 |
| 适用场景 | 基础安全隔离 | 微服务治理 |

**结合使用：**

```yaml
# NetworkPolicy：提供基础网络隔离
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend
spec:
  podSelector:
    matchLabels:
      app: backend
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend

---
# Istio AuthorizationPolicy：提供L7细粒度控制
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: backend-policy
spec:
  selector:
    matchLabels:
      app: backend
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/default/sa/frontend"]
    to:
    - operation:
        methods: ["GET", "POST"]
        paths: ["/api/v1/*"]
```

**推荐架构：**

```
┌────────────────────────────────────────────────────────┐
│              分层安全防御策略                           │
├────────────────────────────────────────────────────────┤
│                                                         │
│  第1层：NetworkPolicy (L3/L4)                          │
│  ├─ 命名空间隔离                                        │
│  ├─ 基础网络访问控制                                    │
│  └─ 防止横向移动                                        │
│                                                         │
│  第2层：服务网格 (L7)                                   │
│  ├─ mTLS加密                                           │
│  ├─ 细粒度授权（HTTP方法、路径）                        │
│  ├─ 流量管理（重试、超时、熔断）                        │
│  └─ 可观测性（Metrics、Traces）                        │
│                                                         │
│  第3层：应用层安全                                      │
│  ├─ JWT验证                                            │
│  ├─ RBAC授权                                           │
│  └─ 业务逻辑验证                                        │
└────────────────────────────────────────────────────────┘
```

### 6.4.10 NetworkPolicy安全检查清单

在生产环境部署前验证：

- [ ] **每个命名空间都有默认拒绝策略**
  ```bash
  kubectl get networkpolicy --all-namespaces | grep default-deny
  ```

- [ ] **所有应用都有显式的NetworkPolicy**
  ```bash
  # 查找没有NetworkPolicy保护的Pod
  kubectl get pods --all-namespaces -o json | \
    jq -r '.items[] | select(.metadata.labels | length > 0) |
    "\(.metadata.namespace)/\(.metadata.name)"' | \
    while read pod; do
      ns=$(echo $pod | cut -d/ -f1)
      kubectl get networkpolicy -n $ns &>/dev/null || echo "No policy in $ns"
    done
  ```

- [ ] **DNS访问已允许**
  ```bash
  kubectl get networkpolicy --all-namespaces -o yaml | grep -A5 "port: 53"
  ```

- [ ] **数据库Pod禁止Egress**
  ```bash
  kubectl get networkpolicy -l tier=database -o yaml | grep -A10 "policyTypes"
  ```

- [ ] **测试关键路径连通性**
  ```bash
  # 编写自动化测试脚本
  ./test-network-connectivity.sh
  ```

- [ ] **CNI插件支持NetworkPolicy**
  ```bash
  kubectl get pods -n kube-system | grep -E 'calico|cilium|weave'
  ```

本节我们深入学习了Kubernetes的NetworkPolicy网络策略机制。通过定义Pod间的网络访问规则，可以实现微隔离和零信任网络架构，从网络层面加固集群安全。在下一节，我们将学习审计与安全扫描，完善整体安全监控体系。

---

## 6.5 审计与安全扫描

在前面的章节中，我们学习了如何通过RBAC、Pod安全策略和网络策略来主动防御安全威胁。本节将聚焦于被动防御和持续监控，通过审计日志记录所有操作，并使用安全扫描工具发现潜在漏洞。

### 6.5.1 Kubernetes审计日志

**什么是审计日志？**

Kubernetes审计（Audit）功能提供了与安全相关的、按时间顺序排列的记录集，记录了集群中的操作序列。审计日志回答以下关键问题：

- **谁**（What）：哪个用户或ServiceAccount执行了操作？
- **做了什么**（What）：执行了什么操作（创建、删除、更新）？
- **在哪里**（Where）：操作了哪个资源（Pod、Secret、ConfigMap）？
- **什么时候**（When）：操作发生的时间？
- **结果如何**（Result）：操作成功还是失败？

**审计日志的价值：**

```
┌────────────────────────────────────────────────────────────┐
│                    审计日志应用场景                         │
├────────────────────────────────────────────────────────────┤
│                                                             │
│  1. 安全事件调查                                            │
│     ├─ 谁删除了生产环境的Deployment？                      │
│     ├─ 哪个Pod访问了敏感的Secret？                         │
│     └─ 异常的API调用来自哪里？                             │
│                                                             │
│  2. 合规审计                                                │
│     ├─ 满足SOC2、ISO27001合规要求                          │
│     ├─ 提供操作审计轨迹                                     │
│     └─ 证明访问控制有效性                                   │
│                                                             │
│  3. 异常检测                                                │
│     ├─ 检测未授权的资源访问                                 │
│     ├─ 发现权限提升尝试                                     │
│     └─ 识别异常的API调用模式                               │
│                                                             │
│  4. 性能分析                                                │
│     ├─ 分析API Server负载                                  │
│     ├─ 优化控制器行为                                       │
│     └─ 发现频繁的资源更新                                   │
└────────────────────────────────────────────────────────────┘
```

**审计策略（Audit Policy）**

审计策略定义了哪些事件应该被记录，以及记录的详细程度。

**审计级别（Audit Level）：**

| 级别 | 描述 | 记录内容 | 典型用途 |
|------|------|----------|----------|
| `None` | 不记录 | 无 | 排除不重要的事件 |
| `Metadata` | 记录元数据 | 请求的用户、时间、资源、动作 | 日常审计（推荐） |
| `Request` | 记录请求体 | 元数据 + 请求体（不含响应） | 调试特定问题 |
| `RequestResponse` | 记录完整请求和响应 | 元数据 + 请求体 + 响应体 | 深度调查（日志量大） |

**审计阶段（Audit Stage）：**

- `RequestReceived`：请求到达API Server后立即记录
- `ResponseStarted`：响应头发送后记录（用于长时间运行的请求，如watch）
- `ResponseComplete`：响应完成后记录
- `Panic`：请求处理中发生panic时记录

**完整的审计策略示例：**

```yaml
apiVersion: audit.k8s.io/v1
kind: Policy
# 排除不记录的阶段（避免重复）
omitStages:
  - "RequestReceived"

rules:
# 规则1：不记录只读操作和系统组件
- level: None
  verbs: ["get", "list", "watch"]
  userGroups: ["system:nodes", "system:serviceaccounts:kube-system"]

# 规则2：不记录某些资源的元数据变更
- level: None
  resources:
  - group: ""
    resources: ["events", "nodes/status", "pods/status"]

# 规则3：记录Secret的访问（不记录内容，只记录元数据）
- level: Metadata
  resources:
  - group: ""
    resources: ["secrets", "configmaps"]
  omitStages:
  - "RequestReceived"

# 规则4：详细记录关键资源的变更
- level: RequestResponse
  verbs: ["create", "update", "patch", "delete"]
  resources:
  - group: ""
    resources: ["pods", "services", "persistentvolumeclaims"]
  - group: "apps"
    resources: ["deployments", "daemonsets", "statefulsets"]
  - group: "rbac.authorization.k8s.io"
    resources: ["roles", "rolebindings", "clusterroles", "clusterrolebindings"]

# 规则5：记录认证和授权失败
- level: Request
  omitStages:
  - "RequestReceived"
  users: ["system:anonymous"]

# 规则6：记录所有admin权限操作
- level: RequestResponse
  userGroups: ["system:masters"]

# 规则7：默认规则 - 记录所有其他操作的元数据
- level: Metadata
  omitStages:
  - "RequestReceived"
```

**启用审计日志：**

编辑kube-apiserver配置（通常在`/etc/kubernetes/manifests/kube-apiserver.yaml`）：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    # 审计策略文件
    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml
    # 审计日志路径
    - --audit-log-path=/var/log/kubernetes/audit.log
    # 日志最大保留天数
    - --audit-log-maxage=30
    # 单个日志文件最大大小（MB）
    - --audit-log-maxsize=100
    # 保留的日志文件数量
    - --audit-log-maxbackup=10
    # 审计日志格式（json或legacy）
    - --audit-log-format=json

    volumeMounts:
    # 挂载审计策略文件
    - mountPath: /etc/kubernetes/audit-policy.yaml
      name: audit-policy
      readOnly: true
    # 挂载日志目录
    - mountPath: /var/log/kubernetes
      name: audit-log

  volumes:
  - name: audit-policy
    hostPath:
      path: /etc/kubernetes/audit-policy.yaml
      type: File
  - name: audit-log
    hostPath:
      path: /var/log/kubernetes
      type: DirectoryOrCreate
```

**审计日志样例：**

```json
{
  "kind": "Event",
  "apiVersion": "audit.k8s.io/v1",
  "level": "Metadata",
  "auditID": "6c5f5c8d-8e1a-4c9b-9f3e-2a1b3c4d5e6f",
  "stage": "ResponseComplete",
  "requestURI": "/api/v1/namespaces/production/secrets/db-password",
  "verb": "get",
  "user": {
    "username": "alice@example.com",
    "groups": ["developers", "system:authenticated"]
  },
  "sourceIPs": ["10.0.1.15"],
  "userAgent": "kubectl/v1.28.0",
  "objectRef": {
    "resource": "secrets",
    "namespace": "production",
    "name": "db-password",
    "apiVersion": "v1"
  },
  "responseStatus": {
    "metadata": {},
    "code": 200
  },
  "requestReceivedTimestamp": "2024-01-15T10:30:45.123456Z",
  "stageTimestamp": "2024-01-15T10:30:45.234567Z",
  "annotations": {
    "authorization.k8s.io/decision": "allow",
    "authorization.k8s.io/reason": "RBAC: allowed by RoleBinding"
  }
}
```

**审计日志分析：**

```bash
# 查询最近1小时删除操作
jq 'select(.verb == "delete" and .stageTimestamp > (now - 3600 | strftime("%Y-%m-%dT%H:%M:%SZ")))' \
  /var/log/kubernetes/audit.log

# 查询特定用户的操作
jq 'select(.user.username == "alice@example.com")' \
  /var/log/kubernetes/audit.log

# 查询失败的操作
jq 'select(.responseStatus.code >= 400)' \
  /var/log/kubernetes/audit.log

# 统计每个用户的操作次数
jq -r '.user.username' /var/log/kubernetes/audit.log | sort | uniq -c | sort -rn

# 查询对Secret的访问
jq 'select(.objectRef.resource == "secrets")' \
  /var/log/kubernetes/audit.log

# 查询权限拒绝事件
jq 'select(.responseStatus.code == 403)' \
  /var/log/kubernetes/audit.log
```

### 6.5.2 审计日志集中化（Fluentd + Elasticsearch）

**架构：**

```
┌────────────────────────────────────────────────────────────┐
│              审计日志集中化架构                             │
├────────────────────────────────────────────────────────────┤
│                                                             │
│  API Server                                                │
│      │                                                      │
│      │ 生成审计日志                                         │
│      ▼                                                      │
│  /var/log/kubernetes/audit.log                             │
│      │                                                      │
│      │ 收集                                                 │
│      ▼                                                      │
│  Fluentd DaemonSet ────┐                                   │
│      │                  │                                   │
│      │                  │ 转发                              │
│      │                  │                                   │
│      ▼                  ▼                                   │
│  Elasticsearch      Kafka (可选)                           │
│      │                                                      │
│      │ 存储和索引                                           │
│      ▼                                                      │
│  Kibana Dashboard                                          │
│  ├─ 实时监控                                                │
│  ├─ 安全告警                                                │
│  └─ 审计报告                                                │
└────────────────────────────────────────────────────────────┘
```

**部署Fluentd收集审计日志：**

```yaml
# fluentd-audit-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-audit-config
  namespace: kube-system
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/kubernetes/audit.log
      pos_file /var/log/fluentd-audit.log.pos
      tag kubernetes.audit
      <parse>
        @type json
        time_key requestReceivedTimestamp
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    # 过滤：提取关键字段
    <filter kubernetes.audit>
      @type record_transformer
      <record>
        user ${record["user"]["username"]}
        verb ${record["verb"]}
        resource ${record["objectRef"]["resource"]}
        namespace ${record["objectRef"]["namespace"]}
        name ${record["objectRef"]["name"]}
        status_code ${record["responseStatus"]["code"]}
      </record>
    </filter>

    # 输出到Elasticsearch
    <match kubernetes.audit>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      logstash_format true
      logstash_prefix k8s-audit
      include_tag_key true
      type_name audit_log
      tag_key @log_name
      flush_interval 10s
    </match>

---
# fluentd-audit-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-audit
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: fluentd-audit
  template:
    metadata:
      labels:
        app: fluentd-audit
    spec:
      serviceAccountName: fluentd
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          subPath: fluent.conf
      nodeSelector:
        node-role.kubernetes.io/control-plane: ""
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: config
        configMap:
          name: fluentd-audit-config
```

### 6.5.3 容器镜像安全扫描

**为什么需要镜像扫描？**

容器镜像可能包含已知的安全漏洞（CVE），使用未扫描的镜像会带来严重风险：

- 已知的操作系统漏洞（如Heartbleed、Shellshock）
- 应用依赖的漏洞（如Log4Shell、Spring4Shell）
- 恶意软件或后门
- 不安全的配置（如硬编码密码）

**主流镜像扫描工具对比：**

| 工具 | 类型 | 优点 | 缺点 | 适用场景 |
|------|------|------|------|----------|
| **Trivy** | 开源 | 快速、全面、易用 | 无集中管理 | CI/CD、本地扫描 |
| **Clair** | 开源 | 轻量、可扩展 | 配置复杂 | 镜像仓库集成 |
| **Anchore** | 开源/商业 | 策略引擎强大 | 资源消耗大 | 企业级扫描 |
| **Aqua** | 商业 | 功能全面、运行时防护 | 成本高 | 企业安全平台 |
| **Snyk** | 商业 | 开发者友好、修复建议 | 需联网 | DevSecOps |

**Trivy实战：最流行的开源扫描工具**

```bash
# 安装Trivy
# macOS
brew install aquasecurity/trivy/trivy

# Linux
wget https://github.com/aquasecurity/trivy/releases/download/v0.48.0/trivy_0.48.0_Linux-64bit.tar.gz
tar zxvf trivy_0.48.0_Linux-64bit.tar.gz
sudo mv trivy /usr/local/bin/

# 扫描镜像
trivy image nginx:1.21

# 输出示例
# nginx:1.21 (debian 11.6)
# =========================
# Total: 145 (UNKNOWN: 0, LOW: 89, MEDIUM: 35, HIGH: 18, CRITICAL: 3)
#
# ┌───────────────┬────────────────┬──────────┬───────────────────┬───────────────┬────────────────────────────┐
# │    Library    │ Vulnerability  │ Severity │ Installed Version │ Fixed Version │           Title            │
# ├───────────────┼────────────────┼──────────┼───────────────────┼───────────────┼────────────────────────────┤
# │ libssl1.1     │ CVE-2023-0286  │ CRITICAL │ 1.1.1n-0+deb11u3  │ 1.1.1n-0+deb11u4 │ OpenSSL: X.400 address   │
# │               │                │          │                   │               │ type confusion             │
# └───────────────┴────────────────┴──────────┴───────────────────┴───────────────┴────────────────────────────┘

# 只显示高危和严重漏洞
trivy image --severity HIGH,CRITICAL nginx:1.21

# 扫描并输出JSON格式
trivy image -f json -o results.json nginx:1.21

# 设置退出码（用于CI/CD）
trivy image --exit-code 1 --severity CRITICAL nginx:1.21
# 如果发现CRITICAL漏洞，返回码为1，阻止部署

# 扫描tar文件
docker save nginx:1.21 -o nginx.tar
trivy image --input nginx.tar

# 扫描私有镜像仓库
trivy image --username admin --password secret registry.example.com/myapp:latest

# 忽略未修复的漏洞
trivy image --ignore-unfixed nginx:1.21
```

**集成到CI/CD流水线：**

```yaml
# GitLab CI示例
stages:
  - build
  - scan
  - deploy

build:
  stage: build
  script:
    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .
    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA

trivy-scan:
  stage: scan
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 0 --severity LOW,MEDIUM $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
    - trivy image --exit-code 1 --severity HIGH,CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  artifacts:
    reports:
      container_scanning: trivy-report.json

deploy:
  stage: deploy
  script:
    - kubectl set image deployment/myapp myapp=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  only:
    - main
```

**Kubernetes准入控制器集成（自动扫描）：**

```yaml
# 使用OPA Gatekeeper阻止有漏洞的镜像
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8strivy
spec:
  crd:
    spec:
      names:
        kind: K8sTrivy
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8strivy

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          # 调用外部Trivy API进行扫描
          scan_result := http.send({
            "method": "POST",
            "url": "http://trivy-server/scan",
            "body": {"image": container.image}
          })
          scan_result.body.vulnerabilities[_].severity == "CRITICAL"
          msg := sprintf("Image %v has CRITICAL vulnerabilities", [container.image])
        }
```

### 6.5.4 运行时安全监控（Falco）

**Falco简介：**

Falco是CNCF的运行时安全项目，通过监控系统调用来检测异常行为。

**Falco检测的威胁：**

```
┌────────────────────────────────────────────────────────────┐
│                Falco运行时威胁检测                          │
├────────────────────────────────────────────────────────────┤
│                                                             │
│  容器逃逸尝试                                               │
│  ├─ 挂载敏感目录（/proc、/var/run/docker.sock）            │
│  ├─ 创建特权容器                                            │
│  └─ 修改内核模块                                            │
│                                                             │
│  异常进程行为                                               │
│  ├─ 容器内执行shell（bash、sh）                            │
│  ├─ 创建反向shell                                          │
│  ├─ 修改系统二进制文件                                      │
│  └─ 意外的网络连接                                          │
│                                                             │
│  文件系统篡改                                               │
│  ├─ 修改/etc/passwd、/etc/shadow                           │
│  ├─ 写入/tmp目录（潜在恶意软件）                            │
│  └─ 修改应用配置文件                                        │
│                                                             │
│  凭证访问                                                   │
│  ├─ 读取Kubernetes Secret                                 │
│  ├─ 访问云凭证文件（~/.aws/credentials）                   │
│  └─ 读取私钥文件                                            │
└────────────────────────────────────────────────────────────┘
```

**安装Falco：**

```bash
# 使用Helm安装
helm repo add falcosecurity https://falcosecurity.github.io/charts
helm repo update

helm install falco falcosecurity/falco \
  --namespace falco \
  --create-namespace \
  --set falco.grpc.enabled=true \
  --set falco.grpcOutput.enabled=true
```

**Falco规则示例：**

```yaml
# /etc/falco/falco_rules.local.yaml
- rule: Terminal shell in container
  desc: 检测容器内启动shell
  condition: >
    spawned_process and
    container and
    proc.name in (bash, sh, zsh, csh, ksh, tcsh)
  output: >
    Shell spawned in container
    (user=%user.name container_id=%container.id
    container_name=%container.name image=%container.image.repository
    proc=%proc.cmdline)
  priority: WARNING
  tags: [container, shell, mitre_execution]

- rule: Write below root
  desc: 检测容器内修改根文件系统
  condition: >
    write_etc_common and
    container and
    not proc.name in (apt, dpkg, yum, rpm)
  output: >
    File below / or /root opened for writing
    (user=%user.name command=%proc.cmdline file=%fd.name
    container_id=%container.id image=%container.image.repository)
  priority: ERROR
  tags: [filesystem, mitre_persistence]

- rule: Read sensitive file
  desc: 检测读取敏感文件
  condition: >
    open_read and
    container and
    fd.name in (/etc/shadow, /etc/passwd, /etc/sudoers)
  output: >
    Sensitive file opened for reading
    (user=%user.name command=%proc.cmdline file=%fd.name
    container_id=%container.id)
  priority: WARNING

- rule: Outbound Connection to C2 Servers
  desc: 检测连接到已知C2服务器
  condition: >
    outbound and
    fd.sip in (known_c2_ips)
  output: >
    Outbound connection to C2 server
    (command=%proc.cmdline connection=%fd.name
    container_id=%container.id)
  priority: CRITICAL
```

**Falco告警输出：**

```json
{
  "output": "Shell spawned in container (user=root container_id=1234abcd container_name=web-app image=nginx:1.21 proc=bash)",
  "priority": "Warning",
  "rule": "Terminal shell in container",
  "time": "2024-01-15T10:30:45.123456789Z",
  "output_fields": {
    "container.id": "1234abcd",
    "container.name": "web-app",
    "container.image.repository": "nginx",
    "proc.cmdline": "bash",
    "user.name": "root"
  }
}
```

**集成Falco告警到Slack/PagerDuty：**

```yaml
# falcosidekick配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: falcosidekick-config
  namespace: falco
data:
  config.yaml: |
    slack:
      webhookurl: "https://hooks.slack.com/services/XXX/YYY/ZZZ"
      minimumpriority: "warning"
      messageformat: "long"

    pagerduty:
      routingkey: "YOUR_ROUTING_KEY"
      minimumpriority: "error"

    elasticsearch:
      hostport: "http://elasticsearch:9200"
      index: "falco"
      type: "event"
      minimumpriority: "debug"

---
# 部署Falcosidekick
apiVersion: apps/v1
kind: Deployment
metadata:
  name: falcosidekick
  namespace: falco
spec:
  replicas: 1
  selector:
    matchLabels:
      app: falcosidekick
  template:
    metadata:
      labels:
        app: falcosidekick
    spec:
      containers:
      - name: falcosidekick
        image: falcosecurity/falcosidekick:latest
        ports:
        - containerPort: 2801
        volumeMounts:
        - name: config
          mountPath: /etc/falcosidekick
      volumes:
      - name: config
        configMap:
          name: falcosidekick-config
```

### 6.5.5 Kubernetes配置审计（kube-bench & kube-hunter）

**kube-bench：CIS基准测试**

kube-bench根据CIS Kubernetes基准测试检查集群配置。

```bash
# 运行kube-bench（作为Job）
kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml

# 查看结果
kubectl logs -f job/kube-bench

# 输出示例
# [INFO] 1 Control Plane Security Configuration
# [INFO] 1.2 API Server
# [PASS] 1.2.1 Ensure that the --anonymous-auth argument is set to false
# [FAIL] 1.2.2 Ensure that the --basic-auth-file argument is not set
# [PASS] 1.2.3 Ensure that the --token-auth-file parameter is not set
# [WARN] 1.2.4 Ensure that the --kubelet-https argument is set to true
#
# == Remediations ==
# 1.2.2 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml
# and remove the --basic-auth-file=<filename> parameter.
#
# == Summary ==
# 45 checks PASS
# 12 checks FAIL
# 8 checks WARN
```

**kube-hunter：主动漏洞扫描**

kube-hunter主动扫描Kubernetes集群中的已知漏洞。

```bash
# 安装kube-hunter
pip3 install kube-hunter

# 扫描远程集群
kube-hunter --remote <cluster-ip>

# 在集群内运行（Pod模式）
kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: kube-hunter
spec:
  template:
    spec:
      containers:
      - name: kube-hunter
        image: aquasec/kube-hunter:latest
        command: ["kube-hunter"]
        args: ["--pod"]
      restartPolicy: Never
EOF

# 输出示例
# Nodes
# +-------------+----------------+
# | Type        | Location       |
# +-------------+----------------+
# | Node/Master | 10.0.1.10      |
# | Node/Master | control-plane  |
# +-------------+----------------+
#
# Detected Services
# +------------------------+----------------+----------------------+
# | Service                | Location       | Description          |
# +------------------------+----------------+----------------------+
# | API Server             | 10.0.1.10:6443 | The Kubernetes API   |
# | Kubelet API (readonly) | 10.0.1.11:10255| Unauthenticated      |
# +------------------------+----------------+----------------------+
#
# Vulnerabilities
# +----------------------------------------------+----------------------+
# | ID: KHV002                                   | Severity: high       |
# +----------------------------------------------+----------------------+
# | Kubelet API (readonly) allows listing pods without authentication  |
# | Fix: Disable --read-only-port on Kubelet                          |
# +----------------------------------------------+----------------------+
```

### 6.5.6 策略即代码（Open Policy Agent）

**OPA（Open Policy Agent）简介：**

OPA是一个通用的策略引擎，可以在Kubernetes中作为准入控制器使用，实现细粒度的策略控制。

**OPA Gatekeeper安装：**

```bash
kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml
```

**策略示例1：强制要求所有容器设置资源限制**

```yaml
# ConstraintTemplate
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredresources
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredResources
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredresources

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.limits.cpu
          msg := sprintf("Container %v must have CPU limit", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.limits.memory
          msg := sprintf("Container %v must have memory limit", [container.name])
        }

---
# Constraint
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredResources
metadata:
  name: must-have-resources
spec:
  match:
    kinds:
    - apiGroups: [""]
      kinds: ["Pod"]
    namespaces:
    - "production"
```

**策略示例2：禁止使用latest标签**

```yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8sblocklatestimage
spec:
  crd:
    spec:
      names:
        kind: K8sBlockLatestImage
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sblocklatestimage

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          endswith(container.image, ":latest")
          msg := sprintf("Image %v uses latest tag, which is not allowed", [container.image])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not contains(container.image, ":")
          msg := sprintf("Image %v has no tag (defaults to latest), which is not allowed", [container.image])
        }

---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sBlockLatestImage
metadata:
  name: no-latest-tag
spec:
  match:
    kinds:
    - apiGroups: ["apps"]
      kinds: ["Deployment", "StatefulSet", "DaemonSet"]
```

**测试策略：**

```bash
# 尝试部署违反策略的Pod
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: production
spec:
  containers:
  - name: nginx
    image: nginx:latest  # ❌ 违反策略
EOF

# 输出
# Error from server ([no-latest-tag] Image nginx:latest uses latest tag, which is not allowed
# [must-have-resources] Container nginx must have CPU limit
# [must-have-resources] Container nginx must have memory limit):
# error when creating "STDIN": admission webhook "validation.gatekeeper.sh" denied the request
```

### 6.5.7 安全监控仪表板

**集成Prometheus + Grafana监控安全指标：**

```yaml
# prometheus-rules.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-security-rules
  namespace: monitoring
data:
  security.rules: |
    groups:
    - name: security
      interval: 30s
      rules:
      # 检测Privileged容器
      - alert: PrivilegedContainerDetected
        expr: |
          kube_pod_container_status_running{pod=~".*"} * on(pod, namespace)
          group_left(container)
          kube_pod_container_info{container!="", image!=""}
          * on(pod, namespace, container)
          group_left()
          (kube_pod_spec_security_context_privileged == 1)
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Privileged container detected"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has privileged container"

      # 检测未设置资源限制的容器
      - alert: ContainerWithoutResourceLimits
        expr: |
          kube_pod_container_resource_limits{resource="memory"} == 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Container without resource limits"
          description: "Container {{ $labels.container }} in {{ $labels.namespace }}/{{ $labels.pod }} has no memory limit"

      # 检测过多的Pod重启
      - alert: HighPodRestartRate
        expr: |
          rate(kube_pod_container_status_restarts_total[15m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High pod restart rate"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} restarting frequently"

      # 检测失败的认证尝试（来自审计日志）
      - alert: FailedAuthenticationAttempts
        expr: |
          sum(rate(apiserver_audit_event_total{response_code="401"}[5m])) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High rate of failed authentication"
          description: "{{ $value }} failed auth attempts per second"
```

**Grafana仪表板（JSON配置片段）：**

```json
{
  "dashboard": {
    "title": "Kubernetes Security Overview",
    "panels": [
      {
        "title": "Failed API Requests",
        "targets": [{
          "expr": "sum(rate(apiserver_request_total{code=~\"4..|5..\"}[5m])) by (verb, resource)"
        }]
      },
      {
        "title": "RBAC Denials",
        "targets": [{
          "expr": "sum(rate(apiserver_audit_event_total{response_code=\"403\"}[5m]))"
        }]
      },
      {
        "title": "Privileged Containers",
        "targets": [{
          "expr": "count(kube_pod_spec_security_context_privileged == 1)"
        }]
      },
      {
        "title": "Containers Without Limits",
        "targets": [{
          "expr": "count(kube_pod_container_resource_limits{resource=\"memory\"} == 0)"
        }]
      }
    ]
  }
}
```

### 6.5.8 安全最佳实践清单

**审计与监控：**

- [ ] **启用Kubernetes审计日志**
  - 配置审计策略覆盖关键操作
  - 设置合理的日志级别（Metadata为主）
  - 配置日志轮转和归档

- [ ] **集中化日志管理**
  - 使用Fluentd/Filebeat收集审计日志
  - 存储到Elasticsearch或S3
  - 保留至少90天的审计记录

- [ ] **容器镜像扫描**
  - CI/CD中集成Trivy或Clair
  - 阻止包含高危漏洞的镜像部署
  - 定期重新扫描已部署的镜像

- [ ] **运行时安全监控**
  - 部署Falco检测异常行为
  - 配置告警到Slack/PagerDuty
  - 定期审查Falco规则

- [ ] **配置基准测试**
  - 每月运行kube-bench
  - 修复FAIL和WARN项
  - 文档化无法修复的项

- [ ] **策略即代码**
  - 使用OPA Gatekeeper强制安全策略
  - 禁止latest标签
  - 强制资源限制
  - 验证SecurityContext配置

- [ ] **安全监控仪表板**
  - Prometheus收集安全指标
  - Grafana可视化
  - 配置告警规则

本节我们深入学习了Kubernetes的审计与安全扫描机制。通过审计日志记录所有操作、使用扫描工具发现漏洞、部署运行时监控，可以构建完整的安全监控体系，实现持续的安全防护。在下一节，我们将通过一个综合实战项目，整合本章所有知识点。

---

## 6.6 实战项目：构建企业级安全加固方案

在前面的章节中，我们分别学习了资源配额、Pod安全、RBAC、网络策略和审计扫描。本节将通过一个完整的实战项目，整合所有安全机制，为一个电商平台构建企业级的安全加固方案。

### 6.6.1 项目背景与需求

**项目场景：**

某电商公司计划将传统应用迁移到Kubernetes集群，应用架构包含：
- **前端服务**（Web UI）
- **API网关**（对外暴露接口）
- **订单服务**（核心业务逻辑）
- **支付服务**（敏感操作，需最高安全等级）
- **Redis缓存**
- **MySQL数据库**

**安全需求：**

```
┌────────────────────────────────────────────────────────────┐
│                    安全需求分层架构                         │
├────────────────────────────────────────────────────────────┤
│                                                             │
│  第1层：基础安全（Infrastructure Security）                │
│  ├─ 所有Pod必须设置资源限制                                 │
│  ├─ 禁止使用Privileged容器                                 │
│  ├─ 强制使用非root用户运行                                  │
│  └─ 所有镜像必须通过安全扫描                                │
│                                                             │
│  第2层：访问控制（Access Control）                         │
│  ├─ 开发/测试/生产环境完全隔离                              │
│  ├─ 每个服务使用专用ServiceAccount                         │
│  ├─ RBAC最小权限原则                                        │
│  └─ 支付服务禁止被其他服务直接访问                          │
│                                                             │
│  第3层：网络隔离（Network Isolation）                      │
│  ├─ 默认拒绝所有流量                                        │
│  ├─ 只允许必要的服务间通信                                  │
│  ├─ 数据库只能被后端服务访问                                │
│  └─ 限制Pod的外部网络访问                                   │
│                                                             │
│  第4层：审计与监控（Audit & Monitoring）                   │
│  ├─ 记录所有API操作                                         │
│  ├─ 实时检测异常行为                                        │
│  ├─ 安全事件告警                                            │
│  └─ 合规审计报告                                            │
└────────────────────────────────────────────────────────────┘
```

**合规要求：**

- 满足PCI-DSS（支付卡行业数据安全标准）
- 通过SOC2审计
- 数据保留90天以上的审计日志

### 6.6.2 环境准备

**创建命名空间：**

```yaml
# namespaces.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    name: production
    environment: prod
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

---
apiVersion: v1
kind: Namespace
metadata:
  name: staging
  labels:
    name: staging
    environment: staging
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/warn: baseline

---
apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    name: development
    environment: dev
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/warn: baseline
```

### 6.6.3 第1层：基础安全配置

**1. 资源配额（防止资源耗尽）**

```yaml
# resource-quotas.yaml
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    # 计算资源限制
    requests.cpu: "50"
    requests.memory: 100Gi
    limits.cpu: "100"
    limits.memory: 200Gi

    # 对象数量限制
    pods: "100"
    services: "30"
    persistentvolumeclaims: "20"
    secrets: "50"
    configmaps: "50"

---
apiVersion: v1
kind: LimitRange
metadata:
  name: production-limits
  namespace: production
spec:
  limits:
  # Container级别限制
  - type: Container
    max:
      cpu: "4"
      memory: 8Gi
    min:
      cpu: 100m
      memory: 128Mi
    default:
      cpu: 500m
      memory: 512Mi
    defaultRequest:
      cpu: 200m
      memory: 256Mi

  # Pod级别限制
  - type: Pod
    max:
      cpu: "8"
      memory: 16Gi
```

**2. OPA Gatekeeper策略**

```yaml
# gatekeeper-policies.yaml
---
# 策略1：强制资源限制
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredresources
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredResources
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredresources

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.limits.cpu
          msg := sprintf("Container %v must have CPU limit", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.limits.memory
          msg := sprintf("Container %v must have memory limit", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.requests.cpu
          msg := sprintf("Container %v must have CPU request", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.requests.memory
          msg := sprintf("Container %v must have memory request", [container.name])
        }

---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredResources
metadata:
  name: must-have-resources
spec:
  match:
    kinds:
    - apiGroups: ["apps"]
      kinds: ["Deployment", "StatefulSet", "DaemonSet"]
    namespaces:
    - production
    - staging

---
# 策略2：禁止latest标签
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8sblocklatestimage
spec:
  crd:
    spec:
      names:
        kind: K8sBlockLatestImage
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sblocklatestimage

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          endswith(container.image, ":latest")
          msg := sprintf("Image %v uses latest tag", [container.image])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not contains(container.image, ":")
          msg := sprintf("Image %v has no tag (defaults to latest)", [container.image])
        }

---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sBlockLatestImage
metadata:
  name: no-latest-tag
spec:
  match:
    kinds:
    - apiGroups: ["apps"]
      kinds: ["Deployment", "StatefulSet", "DaemonSet"]

---
# 策略3：强制SecurityContext配置
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredsecuritycontext
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredSecurityContext
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredsecuritycontext

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.securityContext.runAsNonRoot
          msg := sprintf("Container %v must set runAsNonRoot=true", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          container.securityContext.allowPrivilegeEscalation != false
          msg := sprintf("Container %v must set allowPrivilegeEscalation=false", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.securityContext.readOnlyRootFilesystem
          msg := sprintf("Container %v should use readOnlyRootFilesystem", [container.name])
        }

---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredSecurityContext
metadata:
  name: required-security-context
spec:
  match:
    kinds:
    - apiGroups: ["apps"]
      kinds: ["Deployment", "StatefulSet"]
    namespaces:
    - production
```

### 6.6.4 第2层：RBAC权限配置

```yaml
# rbac-config.yaml
---
# 1. 为每个服务创建专用ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: frontend-sa
  namespace: production

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: order-service-sa
  namespace: production

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: payment-service-sa
  namespace: production

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: database-sa
  namespace: production

---
# 2. 订单服务角色（需要访问ConfigMap和Secret）
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: order-service-role
  namespace: production
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["order-config"]
  verbs: ["get"]

- apiGroups: [""]
  resources: ["secrets"]
  resourceNames: ["order-db-credentials"]
  verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: order-service-binding
  namespace: production
subjects:
- kind: ServiceAccount
  name: order-service-sa
  namespace: production
roleRef:
  kind: Role
  name: order-service-role
  apiGroup: rbac.authorization.k8s.io

---
# 3. 支付服务角色（最高权限控制）
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: payment-service-role
  namespace: production
rules:
- apiGroups: [""]
  resources: ["secrets"]
  resourceNames: ["payment-api-key", "payment-encryption-key"]
  verbs: ["get"]

- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["payment-config"]
  verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: payment-service-binding
  namespace: production
subjects:
- kind: ServiceAccount
  name: payment-service-sa
  namespace: production
roleRef:
  kind: Role
  name: payment-service-role
  apiGroup: rbac.authorization.k8s.io

---
# 4. 前端服务（不需要访问Kubernetes API）
# 无需创建Role，使用默认的ServiceAccount即可
```

### 6.6.5 第3层：网络策略配置

```yaml
# network-policies.yaml
---
# 1. 默认拒绝所有流量
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# 2. 允许DNS查询
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53

---
# 3. 前端服务网络策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Ingress
  - Egress

  # 入站：允许Ingress Controller访问
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 80

  # 出站：只允许访问API Gateway
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: api-gateway
    ports:
    - protocol: TCP
      port: 8080

---
# 4. API Gateway网络策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-gateway-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: api-gateway
  policyTypes:
  - Ingress
  - Egress

  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080

  egress:
  # 允许访问订单服务
  - to:
    - podSelector:
        matchLabels:
          app: order-service
    ports:
    - protocol: TCP
      port: 8080

  # 允许访问支付服务
  - to:
    - podSelector:
        matchLabels:
          app: payment-service
    ports:
    - protocol: TCP
      port: 8080

---
# 5. 订单服务网络策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: order-service-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: order-service
  policyTypes:
  - Ingress
  - Egress

  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api-gateway
    ports:
    - protocol: TCP
      port: 8080

  egress:
  # 允许访问MySQL
  - to:
    - podSelector:
        matchLabels:
          app: mysql
    ports:
    - protocol: TCP
      port: 3306

  # 允许访问Redis
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379

---
# 6. 支付服务网络策略（最严格）
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: payment-service-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: payment-service
  policyTypes:
  - Ingress
  - Egress

  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api-gateway
    ports:
    - protocol: TCP
      port: 8080

  egress:
  # 允许访问MySQL
  - to:
    - podSelector:
        matchLabels:
          app: mysql
    ports:
    - protocol: TCP
      port: 3306

  # 允许访问外部支付网关（特定IP）
  - to:
    - ipBlock:
        cidr: 203.0.113.0/24  # 支付网关IP
    ports:
    - protocol: TCP
      port: 443

---
# 7. MySQL网络策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: mysql-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: mysql
  policyTypes:
  - Ingress
  - Egress

  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: order-service
    - podSelector:
        matchLabels:
          app: payment-service
    ports:
    - protocol: TCP
      port: 3306

  # 禁止所有出站（数据库不需要主动连接外部）
  egress: []

---
# 8. Redis网络策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: redis-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: redis
  policyTypes:
  - Ingress
  - Egress

  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: order-service
    ports:
    - protocol: TCP
      port: 6379

  egress: []
```

### 6.6.6 第4层：应用部署（安全加固）

```yaml
# deployments.yaml
---
# 1. 前端服务
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      serviceAccountName: frontend-sa
      automountServiceAccountToken: false  # 不需要访问K8s API

      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault

      containers:
      - name: nginx
        image: nginx:1.24.0-alpine  # 使用具体版本号
        ports:
        - containerPort: 80

        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi

        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 1000
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
          readOnlyRootFilesystem: true

        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run

        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5

      volumes:
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}

---
# 2. 订单服务
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
  namespace: production
spec:
  replicas: 5
  selector:
    matchLabels:
      app: order-service
  template:
    metadata:
      labels:
        app: order-service
    spec:
      serviceAccountName: order-service-sa

      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        fsGroup: 1001
        seccompProfile:
          type: RuntimeDefault

      containers:
      - name: order-service
        image: registry.example.com/order-service:v2.3.1
        ports:
        - containerPort: 8080

        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi

        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 1001
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true

        env:
        - name: DB_HOST
          value: mysql-service
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: order-db-credentials
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: order-db-credentials
              key: password
        - name: REDIS_HOST
          value: redis-service

        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: config
          mountPath: /app/config
          readOnly: true

        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 5

      volumes:
      - name: tmp
        emptyDir: {}
      - name: config
        configMap:
          name: order-config

---
# 3. 支付服务（最高安全等级）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-service
  namespace: production
  annotations:
    security-level: "critical"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: payment-service
  template:
    metadata:
      labels:
        app: payment-service
        security-level: critical
    spec:
      serviceAccountName: payment-service-sa

      # 使用专用节点
      nodeSelector:
        workload-type: payment

      # 使用反亲和性确保Pod分布在不同节点
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - payment-service
            topologyKey: kubernetes.io/hostname

      securityContext:
        runAsNonRoot: true
        runAsUser: 2000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault

      containers:
      - name: payment-service
        image: registry.example.com/payment-service:v1.5.2
        ports:
        - containerPort: 8080

        resources:
          requests:
            cpu: 1000m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 4Gi

        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 2000
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true

        env:
        - name: DB_HOST
          value: mysql-service
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: payment-db-credentials
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: payment-db-credentials
              key: password
        - name: PAYMENT_API_KEY
          valueFrom:
            secretKeyRef:
              name: payment-api-key
              key: api-key
        - name: ENCRYPTION_KEY
          valueFrom:
            secretKeyRef:
              name: payment-encryption-key
              key: key

        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: config
          mountPath: /app/config
          readOnly: true

        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 90
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 5

      volumes:
      - name: tmp
        emptyDir: {}
      - name: config
        configMap:
          name: payment-config

---
# 4. MySQL StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: production
spec:
  serviceName: mysql-service
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      serviceAccountName: database-sa

      securityContext:
        runAsNonRoot: true
        runAsUser: 999
        fsGroup: 999
        seccompProfile:
          type: RuntimeDefault

      containers:
      - name: mysql
        image: mysql:8.0.35
        ports:
        - containerPort: 3306

        resources:
          requests:
            cpu: 2000m
            memory: 4Gi
          limits:
            cpu: 4000m
            memory: 8Gi

        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 999
          capabilities:
            drop:
            - ALL
            add:
            - CHOWN
            - SETGID
            - SETUID

        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-root-password
              key: password

        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql

        livenessProbe:
          exec:
            command:
            - mysqladmin
            - ping
            - -h
            - localhost
          initialDelaySeconds: 30
          periodSeconds: 10

        readinessProbe:
          exec:
            command:
            - mysql
            - -h
            - 127.0.0.1
            - -e
            - SELECT 1
          initialDelaySeconds: 10
          periodSeconds: 5

  volumeClaimTemplates:
  - metadata:
      name: mysql-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
```

### 6.6.7 第5层：审计与监控部署

```yaml
# monitoring-setup.yaml
---
# 1. 部署Falco（运行时安全监控）
apiVersion: v1
kind: Namespace
metadata:
  name: falco-system

---
# 使用Helm安装Falco
# helm install falco falcosecurity/falco \
#   --namespace falco-system \
#   --set falco.grpc.enabled=true \
#   --set falco.grpcOutput.enabled=true

---
# 2. 自定义Falco规则
apiVersion: v1
kind: ConfigMap
metadata:
  name: falco-custom-rules
  namespace: falco-system
data:
  custom-rules.yaml: |
    - rule: Detect Payment Service Shell
      desc: 检测支付服务容器内启动shell
      condition: >
        spawned_process and
        container and
        container.image.repository contains "payment-service" and
        proc.name in (bash, sh, zsh)
      output: >
        CRITICAL: Shell spawned in payment service container
        (user=%user.name container=%container.name command=%proc.cmdline)
      priority: CRITICAL
      tags: [container, payment, shell]

    - rule: Unauthorized Database Access
      desc: 检测非授权访问数据库
      condition: >
        open and
        container and
        fd.name contains "mysql" and
        not container.image.repository in (order-service, payment-service)
      output: >
        Unauthorized database access attempt
        (container=%container.name user=%user.name file=%fd.name)
      priority: ERROR

    - rule: Payment Service Network Anomaly
      desc: 检测支付服务异常网络连接
      condition: >
        outbound and
        container.image.repository contains "payment-service" and
        not fd.sip in (mysql-service-ip, payment-gateway-ip)
      output: >
        Anomalous network connection from payment service
        (container=%container.name destination=%fd.sip)
      priority: WARNING

---
# 3. Prometheus监控规则
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-security-alerts
  namespace: monitoring
data:
  security-alerts.yaml: |
    groups:
    - name: security_alerts
      interval: 30s
      rules:
      - alert: PaymentServiceDown
        expr: up{job="payment-service"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Payment service is down"
          description: "Critical payment service unavailable"

      - alert: HighAuthFailureRate
        expr: >
          sum(rate(apiserver_audit_event_total{
            response_code="403",
            namespace="production"
          }[5m])) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High authentication failure rate"

      - alert: SuspiciousSecretAccess
        expr: >
          sum(rate(apiserver_audit_event_total{
            verb="get",
            objectRef_resource="secrets",
            objectRef_namespace="production"
          }[10m])) > 50
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Unusual secret access pattern detected"

      - alert: PaymentServiceHighMemory
        expr: >
          container_memory_usage_bytes{
            pod=~"payment-service.*",
            namespace="production"
          } / container_spec_memory_limit_bytes > 0.9
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Payment service memory usage critical"
```

### 6.6.8 部署验证与测试

**验证脚本：**

```bash
#!/bin/bash
# deploy-and-verify.sh

set -e

echo "=== 第1步：部署基础设施 ==="
kubectl apply -f namespaces.yaml
kubectl apply -f resource-quotas.yaml

echo "=== 第2步：部署OPA Gatekeeper策略 ==="
kubectl apply -f gatekeeper-policies.yaml

# 等待Gatekeeper准备就绪
kubectl wait --for=condition=ready pod -l control-plane=controller-manager \
  -n gatekeeper-system --timeout=300s

echo "=== 第3步：配置RBAC ==="
kubectl apply -f rbac-config.yaml

echo "=== 第4步：配置网络策略 ==="
kubectl apply -f network-policies.yaml

echo "=== 第5步：创建Secrets和ConfigMaps ==="
# 创建数据库凭证
kubectl create secret generic order-db-credentials \
  -n production \
  --from-literal=username=order_user \
  --from-literal=password=$(openssl rand -base64 32)

kubectl create secret generic payment-db-credentials \
  -n production \
  --from-literal=username=payment_user \
  --from-literal=password=$(openssl rand -base64 32)

kubectl create secret generic mysql-root-password \
  -n production \
  --from-literal=password=$(openssl rand -base64 32)

# 创建支付服务密钥
kubectl create secret generic payment-api-key \
  -n production \
  --from-literal=api-key=$(openssl rand -hex 32)

kubectl create secret generic payment-encryption-key \
  -n production \
  --from-literal=key=$(openssl rand -hex 32)

echo "=== 第6步：部署应用 ==="
kubectl apply -f deployments.yaml

echo "=== 第7步：验证部署 ==="
echo "检查Pod状态..."
kubectl get pods -n production

echo "检查网络策略..."
kubectl get networkpolicy -n production

echo "检查资源配额..."
kubectl get resourcequota -n production

echo "=== 第8步：安全测试 ==="

# 测试1：尝试部署违反策略的Pod
echo "测试1：尝试部署没有资源限制的Pod（应该失败）"
cat <<EOF | kubectl apply -f - 2>&1 | grep "denied" && echo "✅ 策略生效" || echo "❌ 策略未生效"
apiVersion: v1
kind: Pod
metadata:
  name: bad-pod
  namespace: production
spec:
  containers:
  - name: nginx
    image: nginx:latest
EOF

# 测试2：验证网络隔离
echo "测试2：验证前端无法直接访问数据库"
kubectl run test-frontend --image=busybox --rm -it -n production \
  --labels="app=frontend" -- timeout 5 nc -zv mysql-service 3306 2>&1 | \
  grep "Connection timed out" && echo "✅ 网络隔离生效" || echo "❌ 网络隔离未生效"

# 测试3：验证RBAC
echo "测试3：验证订单服务只能访问授权的Secret"
kubectl auth can-i get secret/order-db-credentials \
  --as=system:serviceaccount:production:order-service-sa \
  -n production && echo "✅ 授权访问成功"

kubectl auth can-i get secret/payment-api-key \
  --as=system:serviceaccount:production:order-service-sa \
  -n production && echo "❌ 权限过度" || echo "✅ 权限隔离正确"

echo "=== 部署完成 ==="
```

### 6.6.9 安全事件响应流程

```yaml
# incident-response-playbook.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: incident-response-playbook
  namespace: production
data:
  playbook.md: |
    # 安全事件响应手册

    ## 事件分类

    ### P0 - 严重安全事件
    - 支付服务被攻陷
    - 数据库凭证泄露
    - 检测到容器逃逸

    **响应时间：** 立即（15分钟内）

    **响应步骤：**
    1. 隔离受影响的Pod
       ```bash
       kubectl delete pod <compromised-pod> -n production
       kubectl scale deployment <deployment> --replicas=0 -n production
       ```

    2. 启用网络隔离
       ```bash
       kubectl apply -f emergency-network-lockdown.yaml
       ```

    3. 收集证据
       ```bash
       kubectl logs <pod> -n production --previous > incident-logs.txt
       kubectl describe pod <pod> -n production > pod-details.txt
       ```

    4. 通知安全团队和管理层

    5. 启动事后分析

    ### P1 - 高优先级事件
    - 异常的Secret访问
    - Falco检测到shell活动
    - 未授权的API调用

    **响应时间：** 1小时内

    ### P2 - 中优先级事件
    - OPA策略违规
    - 资源使用异常

    **响应时间：** 4小时内

    ## 常见场景处理

    ### 场景1：检测到支付服务容器内执行shell

    ```bash
    # 1. 立即隔离Pod
    kubectl cordon <node>
    kubectl drain <node> --pod-selector=app=payment-service

    # 2. 获取详细日志
    kubectl logs payment-service-xxx -n production --tail=1000 > attack-logs.txt

    # 3. 导出审计日志
    jq 'select(.objectRef.namespace=="production" and
        .objectRef.name | startswith("payment-service"))' \
      /var/log/kubernetes/audit.log > payment-audit.json

    # 4. 回滚到已知良好版本
    kubectl rollout undo deployment/payment-service -n production

    # 5. 启动取证分析
    ```

    ### 场景2：数据库凭证可能泄露

    ```bash
    # 1. 立即轮换凭证
    kubectl create secret generic order-db-credentials-new \
      -n production \
      --from-literal=username=order_user \
      --from-literal=password=$(openssl rand -base64 32)

    # 2. 更新使用该凭证的所有Deployment
    kubectl set env deployment/order-service \
      DB_PASSWORD_SECRET=order-db-credentials-new -n production

    # 3. 删除旧凭证
    kubectl delete secret order-db-credentials -n production

    # 4. 在数据库端修改密码
    mysql -e "ALTER USER 'order_user'@'%' IDENTIFIED BY 'new_password';"
    ```
```

### 6.6.10 持续改进与监控

```yaml
# continuous-monitoring.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: security-audit-scan
  namespace: production
spec:
  schedule: "0 2 * * *"  # 每天凌晨2点运行
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: security-scanner
          restartPolicy: OnFailure
          containers:
          - name: trivy-scan
            image: aquasec/trivy:latest
            command:
            - /bin/sh
            - -c
            - |
              # 扫描所有运行的镜像
              for image in $(kubectl get pods -n production -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\n' | sort -u); do
                echo "Scanning $image"
                trivy image --severity HIGH,CRITICAL $image
              done

          - name: kube-bench
            image: aquasec/kube-bench:latest
            command:
            - kube-bench
            - run
            - --targets=master,node
            volumeMounts:
            - name: var-lib-etcd
              mountPath: /var/lib/etcd
              readOnly: true
            - name: etc-kubernetes
              mountPath: /etc/kubernetes
              readOnly: true

          volumes:
          - name: var-lib-etcd
            hostPath:
              path: /var/lib/etcd
          - name: etc-kubernetes
            hostPath:
              path: /etc/kubernetes

---
# 安全指标收集
apiVersion: v1
kind: ConfigMap
metadata:
  name: security-metrics-dashboard
  namespace: monitoring
data:
  dashboard.json: |
    {
      "dashboard": {
        "title": "生产环境安全概览",
        "panels": [
          {
            "title": "活跃Pod数量",
            "targets": [{
              "expr": "count(kube_pod_info{namespace='production'})"
            }]
          },
          {
            "title": "特权容器数量（应为0）",
            "targets": [{
              "expr": "count(kube_pod_container_status_running{namespace='production'}) * on(pod, namespace) group_left(container) kube_pod_container_info{container!=''} * on(pod, namespace, container) group_left() (kube_pod_spec_security_context_privileged == 1)"
            }]
          },
          {
            "title": "未设置资源限制的容器",
            "targets": [{
              "expr": "count(kube_pod_container_resource_limits{namespace='production', resource='memory'} == 0)"
            }]
          },
          {
            "title": "Falco告警（最近1小时）",
            "targets": [{
              "expr": "increase(falco_events{priority=~'Error|Critical'}[1h])"
            }]
          },
          {
            "title": "支付服务可用性",
            "targets": [{
              "expr": "avg(up{job='payment-service', namespace='production'})"
            }]
          },
          {
            "title": "API认证失败率",
            "targets": [{
              "expr": "sum(rate(apiserver_audit_event_total{response_code='403', namespace='production'}[5m]))"
            }]
          }
        ]
      }
    }
```

本节我们通过一个完整的电商平台安全加固项目，整合了本章所有知识点：资源配额、Pod安全、RBAC、网络策略、审计监控。这个实战项目展示了如何在生产环境中构建多层安全防御体系，实现纵深防御和持续监控。在下一节的本章小结中，我们将回顾整个第6章的核心知识点。

---

## 6.7 本章小结

经过本章的深入学习，我们全面掌握了Kubernetes的配置与安全管理体系。从资源配额到Pod安全，从RBAC权限到网络策略，再到审计监控和完整的实战项目。让我们系统回顾本章的核心知识点。

### 6.7.1 知识体系总览

本章构建了一个完整的Kubernetes安全管理知识体系：

```
┌─────────────────────────────────────────────────────────────┐
│              第6章：配置与安全管理知识体系                   │
├─────────────────────────────────────────────────────────────┤
│  第1层：资源管理（6.1）                                      │
│    ├─ ResourceQuota（资源配额）                             │
│    ├─ LimitRange（限制范围）                                │
│    ├─ 计算资源配额（CPU、内存）                             │
│    ├─ 对象数量配额（Pod、Service、PVC）                     │
│    └─ 多环境资源隔离                                        │
│                                                              │
│  第2层：Pod安全（6.2）                                       │
│    ├─ Pod Security Standards（安全标准）                    │
│    ├─ Pod Security Admission（准入控制）                    │
│    ├─ SecurityContext（安全上下文）                         │
│    ├─ Seccomp和AppArmor（系统调用过滤）                     │
│    └─ 非特权容器与rootless                                  │
│                                                              │
│  第3层：权限控制（6.3）                                      │
│    ├─ RBAC核心概念（Role、ClusterRole）                     │
│    ├─ 主体绑定（RoleBinding、ClusterRoleBinding）           │
│    ├─ ServiceAccount（服务账号）                            │
│    ├─ 聚合ClusterRole                                       │
│    └─ 多租户权限隔离                                        │
│                                                              │
│  第4层：网络安全（6.4）                                      │
│    ├─ NetworkPolicy核心概念                                 │
│    ├─ Ingress策略（入站流量控制）                           │
│    ├─ Egress策略（出站流量控制）                            │
│    ├─ 命名空间隔离                                          │
│    └─ 三层架构网络隔离                                      │
│                                                              │
│  第5层：审计监控（6.5）                                      │
│    ├─ Kubernetes审计日志                                    │
│    ├─ 容器镜像安全扫描（Trivy）                             │
│    ├─ 运行时安全监控（Falco）                               │
│    ├─ 配置审计（kube-bench、kube-hunter）                   │
│    ├─ 策略即代码（OPA）                                     │
│    └─ 安全监控仪表板                                        │
│                                                              │
│  第6层：生产实战（6.6）                                      │
│    ├─ 企业级电商平台安全加固                                │
│    ├─ 五层安全防护体系                                      │
│    ├─ 纵深防御策略                                          │
│    ├─ 安全事件响应流程                                      │
│    └─ 持续改进与监控                                        │
└─────────────────────────────────────────────────────────────┘
```

### 6.7.2 核心概念汇总

#### 6.7.2.1 资源配额与限制

| 资源类型 | 作用域 | 主要功能 | 典型场景 |
|---------|--------|---------|---------|
| **ResourceQuota** | Namespace级别 | 限制资源总量 | 多租户环境资源隔离 |
| **LimitRange** | Namespace级别 | 限制单个对象资源 | 防止单个Pod过度消耗 |

**ResourceQuota核心配额项：**

```yaml
# 计算资源配额
requests.cpu: "10"           # CPU请求总量
requests.memory: "20Gi"      # 内存请求总量
limits.cpu: "20"             # CPU限制总量
limits.memory: "40Gi"        # 内存限制总量

# 对象数量配额
pods: "50"                   # 最多50个Pod
services: "10"               # 最多10个Service
persistentvolumeclaims: "5"  # 最多5个PVC
configmaps: "10"             # 最多10个ConfigMap
secrets: "10"                # 最多10个Secret

# 存储配额
requests.storage: "100Gi"    # 存储请求总量
<storageclass>.storageclass.storage.k8s.io/requests.storage: "50Gi"  # 按StorageClass配额
```

**LimitRange核心限制：**

| 限制类型 | 字段 | 作用 |
|---------|------|------|
| **Container** | defaultRequest | 默认资源请求 |
| **Container** | default | 默认资源限制 |
| **Container** | min/max | 最小/最大资源限制 |
| **Container** | maxLimitRequestRatio | limits/requests比例上限 |
| **Pod** | min/max | Pod级别资源限制 |
| **PersistentVolumeClaim** | min/max | PVC容量限制 |

#### 6.7.2.2 Pod安全标准

**Pod Security Standards三层安全级别：**

```
┌────────────────┬────────────────────┬─────────────────────┐
│  安全级别      │   限制内容         │   适用场景          │
├────────────────┼────────────────────┼─────────────────────┤
│ Privileged     │ 无限制             │ 系统组件            │
│ (特权模式)     │ 允许特权容器       │ (kube-proxy, CNI)   │
│                │ 允许hostNetwork等  │                     │
├────────────────┼────────────────────┼─────────────────────┤
│ Baseline       │ 禁止特权容器       │ 大部分应用          │
│ (基线模式)     │ 禁止hostNetwork    │ (Web、API、数据库)  │
│                │ 允许常见卷类型     │                     │
├────────────────┼────────────────────┼─────────────────────┤
│ Restricted     │ 非root运行         │ 高安全要求应用      │
│ (受限模式)     │ 只读根文件系统     │ (金融、支付、认证)  │
│                │ 禁用权限提升       │                     │
│                │ 限制Capabilities   │                     │
└────────────────┴────────────────────┴─────────────────────┘
```

**Pod Security Admission执行模式：**

| 模式 | 行为 | 使用场景 |
|-----|------|---------|
| **enforce** | 拒绝违规Pod | 生产环境 |
| **audit** | 允许但记录日志 | 审计阶段 |
| **warn** | 允许但显示警告 | 迁移过渡期 |

**SecurityContext关键配置：**

```yaml
# Pod级别SecurityContext
securityContext:
  runAsNonRoot: true        # 强制非root运行
  runAsUser: 1000           # 指定UID
  fsGroup: 2000             # 文件系统组ID
  seccompProfile:
    type: RuntimeDefault    # 启用Seccomp

# Container级别SecurityContext
securityContext:
  allowPrivilegeEscalation: false  # 禁止权限提升
  readOnlyRootFilesystem: true     # 只读根文件系统
  capabilities:
    drop:
    - ALL                            # 移除所有Capabilities
  runAsNonRoot: true
  runAsUser: 1000
```

#### 6.7.2.3 RBAC权限控制

**RBAC核心资源：**

```
┌──────────────────┬─────────────┬──────────────────────┐
│   资源类型       │   作用域    │    主要用途          │
├──────────────────┼─────────────┼──────────────────────┤
│ Role             │ Namespace   │ 命名空间级别权限定义 │
│ ClusterRole      │ Cluster     │ 集群级别权限定义     │
│ RoleBinding      │ Namespace   │ 绑定Role到主体       │
│ ClusterRoleBinding│ Cluster    │ 绑定ClusterRole到主体│
│ ServiceAccount   │ Namespace   │ Pod身份标识          │
└──────────────────┴─────────────┴──────────────────────┘
```

**权限规则（Rule）结构：**

```yaml
rules:
- apiGroups: [""]           # 核心API组（Pod、Service等）
  resources: ["pods"]       # 资源类型
  verbs: ["get", "list"]    # 允许的操作

- apiGroups: ["apps"]       # apps API组
  resources: ["deployments"]
  verbs: ["*"]              # 所有操作

- apiGroups: [""]
  resources: ["pods/log"]   # 子资源
  verbs: ["get"]

- apiGroups: [""]
  resources: ["pods"]
  resourceNames: ["my-pod"] # 特定资源名称
  verbs: ["delete"]
```

**常见角色权限映射：**

| 角色 | 权限范围 | 典型权限 |
|-----|---------|---------|
| **集群管理员** | cluster-admin | 所有资源的所有操作 |
| **命名空间管理员** | admin | 命名空间内所有操作（除ResourceQuota/LimitRange） |
| **开发者** | edit | 创建/修改应用资源（Pod、Deployment、Service等） |
| **只读用户** | view | 只读访问，不能修改资源 |
| **CI/CD** | 自定义 | Deployment更新、镜像拉取、Secret读取 |

#### 6.7.2.4 网络策略（NetworkPolicy）

**NetworkPolicy核心概念：**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: example-policy
  namespace: production
spec:
  podSelector:              # 策略应用到哪些Pod
    matchLabels:
      app: web
  policyTypes:              # 策略类型
  - Ingress                 # 入站流量控制
  - Egress                  # 出站流量控制
  
  ingress:                  # 入站规则
  - from:                   # 允许来源
    - namespaceSelector:    # 来自特定命名空间
        matchLabels:
          env: production
    - podSelector:          # 来自特定Pod
        matchLabels:
          app: frontend
    - ipBlock:              # 来自特定IP段
        cidr: 10.0.0.0/24
        except:
        - 10.0.0.1/32
    ports:                  # 允许端口
    - protocol: TCP
      port: 80
  
  egress:                   # 出站规则
  - to:
    - podSelector:
        matchLabels:
          app: database
    ports:
    - protocol: TCP
      port: 3306
```

**常见网络策略模式：**

| 模式 | 描述 | 使用场景 |
|-----|------|---------|
| **默认拒绝所有入站** | ingress: [] | 高安全环境初始化 |
| **默认拒绝所有出站** | egress: [] | 限制外部访问 |
| **允许同命名空间通信** | namespaceSelector: {} | 多租户隔离 |
| **三层架构隔离** | 前端→后端→数据库 | 微服务安全 |
| **白名单模式** | 明确允许列表 | 生产环境推荐 |

#### 6.7.2.5 审计与监控工具

| 工具 | 类型 | 主要功能 | 输出 |
|-----|------|---------|------|
| **审计日志** | 内置 | 记录API请求 | JSON日志 |
| **Trivy** | 镜像扫描 | CVE漏洞检测 | 漏洞报告 |
| **Falco** | 运行时监控 | 异常行为检测 | 实时告警 |
| **kube-bench** | 配置审计 | CIS基准检查 | 合规报告 |
| **kube-hunter** | 渗透测试 | 集群安全扫描 | 漏洞列表 |
| **OPA** | 策略引擎 | 自定义策略 | 准入决策 |

### 6.7.3 安全防护体系对比

**纵深防御五层模型：**

```
┌─────────────────────────────────────────────────────────┐
│                    安全防护层次                          │
├─────────────────────────────────────────────────────────┤
│  第1层：基础安全配置                                     │
│    ├─ ResourceQuota（资源隔离）                         │
│    ├─ LimitRange（资源限制）                            │
│    ├─ Pod Security Admission（Pod安全）                 │
│    └─ SecurityContext（容器安全上下文）                 │
│                                                          │
│  第2层：身份与权限                                       │
│    ├─ ServiceAccount（Pod身份）                         │
│    ├─ RBAC（细粒度权限控制）                            │
│    ├─ Secret管理（敏感信息保护）                        │
│    └─ 最小权限原则                                      │
│                                                          │
│  第3层：网络隔离                                         │
│    ├─ NetworkPolicy（流量控制）                         │
│    ├─ 命名空间隔离                                      │
│    ├─ 微服务间访问控制                                  │
│    └─ Egress出站限制                                    │
│                                                          │
│  第4层：镜像与运行时安全                                 │
│    ├─ 镜像扫描（Trivy）                                 │
│    ├─ 私有镜像仓库                                      │
│    ├─ 镜像签名验证                                      │
│    └─ 运行时监控（Falco）                               │
│                                                          │
│  第5层：审计与监控                                       │
│    ├─ Kubernetes审计日志                                │
│    ├─ 配置审计（kube-bench）                            │
│    ├─ 安全扫描（kube-hunter）                           │
│    ├─ 策略即代码（OPA）                                 │
│    └─ 实时监控告警                                      │
└─────────────────────────────────────────────────────────┘
```

### 6.7.4 最佳实践总结

#### 6.7.4.1 资源配额最佳实践

**✅ 必须做的：**
- 生产环境所有Namespace必须配置ResourceQuota
- 为每个环境（开发/测试/生产）配置不同的配额
- 同时配置requests和limits配额
- 配置对象数量配额（Pod、Service、PVC等）
- 定期审查和调整配额

**❌ 避免的陷阱：**
- 不配置LimitRange导致单个Pod资源失控
- 配额过小导致应用无法部署
- 忘记配置存储配额导致磁盘耗尽
- 不监控配额使用率

**推荐配额模板：**

| 环境 | CPU Requests | Memory Requests | Pods | PVC |
|-----|--------------|-----------------|------|-----|
| **开发** | 5核 | 10Gi | 30 | 5 |
| **测试** | 10核 | 20Gi | 50 | 10 |
| **生产** | 50核 | 100Gi | 100 | 20 |

#### 6.7.4.2 Pod安全最佳实践

**✅ 强制执行（Restricted级别）：**
```yaml
# 生产环境推荐配置
securityContext:
  # Pod级别
  runAsNonRoot: true              # 强制非root
  runAsUser: 1000                 # 指定UID
  fsGroup: 2000                   # 文件系统组
  seccompProfile:
    type: RuntimeDefault          # 启用Seccomp
  
  # Container级别
  allowPrivilegeEscalation: false # 禁止权限提升
  readOnlyRootFilesystem: true    # 只读根文件系统
  capabilities:
    drop:
    - ALL                         # 移除所有Capabilities
```

**❌ 禁止的行为：**
- ❌ 使用特权容器（privileged: true）
- ❌ 挂载Docker Socket（/var/run/docker.sock）
- ❌ 使用hostNetwork、hostPID、hostIPC
- ❌ 以root用户运行应用
- ❌ 添加不必要的Capabilities

**安全加固清单：**
- [ ] 所有容器以非root用户运行
- [ ] 启用只读根文件系统
- [ ] 禁用权限提升
- [ ] 移除所有Capabilities
- [ ] 启用Seccomp或AppArmor
- [ ] 不挂载敏感目录
- [ ] 使用Pod Security Admission强制执行

#### 6.7.4.3 RBAC最佳实践

**✅ 权限最小化原则：**

| 原则 | 说明 | 示例 |
|-----|------|------|
| **最小权限** | 只授予必需的权限 | 只读用户仅授予get/list权限 |
| **精确范围** | 使用Role而非ClusterRole | 开发者只在所属Namespace有权限 |
| **避免通配符** | 明确指定资源和动词 | 使用["pods"]而非["*"] |
| **定期审查** | 删除不再使用的权限 | 季度审查权限绑定 |
| **禁用默认SA** | 禁用default ServiceAccount | automountServiceAccountToken: false |

**❌ 常见错误：**
- ❌ 直接使用cluster-admin角色
- ❌ 使用通配符（verbs: ["*"]）
- ❌ ClusterRoleBinding绑定到default ServiceAccount
- ❌ 不删除离职人员的权限
- ❌ 应用Pod使用过高权限

**角色分配示例：**

```yaml
# ✅ 正确：精确权限
rules:
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "update", "patch"]

# ❌ 错误：过度授权
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
```

#### 6.7.4.4 网络策略最佳实践

**✅ 推荐策略：**

1. **默认拒绝所有入站流量**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

2. **默认拒绝所有出站流量**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
spec:
  podSelector: {}
  policyTypes:
  - Egress
```

3. **白名单模式：明确允许**
```yaml
# 允许前端访问后端
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-to-backend
spec:
  podSelector:
    matchLabels:
      app: backend
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080
```

**❌ 避免的陷阱：**
- 不配置NetworkPolicy（默认允许所有流量）
- 忘记配置Egress策略（DNS解析失败）
- 使用过于宽松的选择器（{}）
- 不测试策略就应用到生产

**三层架构网络隔离：**

```
┌────────────┐         ┌────────────┐         ┌────────────┐
│  Frontend  │  ────>  │  Backend   │  ────>  │  Database  │
│  (Web)     │  HTTP   │  (API)     │  MySQL  │  (MySQL)   │
└────────────┘         └────────────┘         └────────────┘
     ↓                      ↓                      ↓
 允许80/443           允许来自Frontend       允许来自Backend
 拒绝其他             拒绝其他访问          拒绝其他访问
```

#### 6.7.4.5 审计监控最佳实践

**✅ 必须监控的指标：**

| 类别 | 监控内容 | 告警阈值 |
|-----|---------|---------|
| **资源配额** | 配额使用率 | >80%告警 |
| **Pod安全** | 特权容器数量 | >0告警 |
| **RBAC** | 权限拒绝事件 | 异常增长 |
| **网络策略** | 被拒绝的连接 | 频繁拒绝 |
| **镜像安全** | 高危漏洞数量 | >0告警 |
| **运行时安全** | Falco告警 | Critical级别 |
| **审计日志** | 敏感操作 | 实时告警 |

**审计日志必须记录：**
- ✅ 所有认证事件（成功/失败）
- ✅ 权限拒绝事件
- ✅ Secret/ConfigMap访问
- ✅ RBAC变更
- ✅ Pod创建/删除
- ✅ 特权操作

**日志保留策略：**
- 审计日志：至少保留90天
- Falco告警：至少保留30天
- 合规环境：1年以上

### 6.7.5 常用命令速查表

#### 6.7.5.1 资源配额相关

```bash
# 查看ResourceQuota
kubectl get resourcequota -n <namespace>
kubectl describe resourcequota <name> -n <namespace>

# 查看配额使用情况
kubectl get resourcequota -n <namespace> -o yaml

# 查看LimitRange
kubectl get limitrange -n <namespace>
kubectl describe limitrange <name> -n <namespace>

# 查看命名空间资源使用
kubectl top pods -n <namespace>
kubectl top nodes
```

#### 6.7.5.2 Pod安全相关

```bash
# 查看Pod安全策略
kubectl get psa -A
kubectl label namespace <namespace> pod-security.kubernetes.io/enforce=restricted

# 查看Pod SecurityContext
kubectl get pod <name> -o jsonpath='{.spec.securityContext}'
kubectl get pod <name> -o jsonpath='{.spec.containers[*].securityContext}'

# 检查特权容器
kubectl get pods -A -o json | jq '.items[] | select(.spec.containers[].securityContext.privileged==true) | .metadata.name'

# 检查root用户运行的容器
kubectl get pods -A -o json | jq '.items[] | select(.spec.securityContext.runAsUser==0 or .spec.containers[].securityContext.runAsUser==0) | .metadata.name'
```

#### 6.7.5.3 RBAC相关

```bash
# 查看角色和绑定
kubectl get roles,rolebindings -n <namespace>
kubectl get clusterroles,clusterrolebindings

# 查看ServiceAccount
kubectl get serviceaccounts -n <namespace>
kubectl describe sa <name> -n <namespace>

# 检查用户权限
kubectl auth can-i <verb> <resource> --as=<user>
kubectl auth can-i create pods --as=system:serviceaccount:default:my-sa

# 查看当前用户所有权限
kubectl auth can-i --list

# 查看某个角色的权限
kubectl describe clusterrole <name>
kubectl describe role <name> -n <namespace>

# 查看某个ServiceAccount的权限
kubectl get rolebindings,clusterrolebindings --all-namespaces -o json | \
  jq '.items[] | select(.subjects[]?.name=="<sa-name>") | {name: .metadata.name, namespace: .metadata.namespace, role: .roleRef.name}'
```

#### 6.7.5.4 网络策略相关

```bash
# 查看NetworkPolicy
kubectl get networkpolicies -A
kubectl describe networkpolicy <name> -n <namespace>

# 测试网络连通性（从Pod A访问Pod B）
kubectl exec -it <pod-a> -- wget -qO- http://<pod-b-service>:80

# 测试外部访问（Egress）
kubectl exec -it <pod> -- wget -qO- http://www.google.com

# 查看CNI插件是否支持NetworkPolicy
kubectl get pods -n kube-system | grep -E "calico|cilium|weave"

# 验证NetworkPolicy是否生效
kubectl exec -it <pod> -- nc -zv <target-ip> <port>
```

#### 6.7.5.5 审计监控相关

```bash
# 查看审计日志（需要配置审计后端）
tail -f /var/log/kubernetes/audit.log

# Trivy镜像扫描
trivy image <image-name>
trivy image --severity HIGH,CRITICAL <image-name>

# Falco日志
kubectl logs -n falco -l app=falco -f

# kube-bench检查
kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml
kubectl logs -f job/kube-bench

# kube-hunter扫描
kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-hunter/main/job.yaml
kubectl logs -f job/kube-hunter

# 查看OPA策略
kubectl get constrainttemplates
kubectl get constraints
kubectl describe constraint <name>
```

### 6.7.6 故障排查决策树

#### 6.7.6.1 资源配额问题

```
问题：Pod无法创建，报exceeded quota错误
   │
   ├─> 1. 检查ResourceQuota
   │     kubectl describe resourcequota -n <namespace>
   │     → 查看Used vs Hard
   │
   ├─> 2. 检查当前资源使用
   │     kubectl get pods -n <namespace>
   │     kubectl top pods -n <namespace>
   │
   ├─> 3. 清理不需要的资源
   │     kubectl delete pod <unused-pod> -n <namespace>
   │
   └─> 4. 调整配额（如确实需要）
         kubectl edit resourcequota <name> -n <namespace>
```

#### 6.7.6.2 Pod安全问题

```
问题：Pod被拒绝，报Pod Security violation错误
   │
   ├─> 1. 检查命名空间PSA标签
   │     kubectl get ns <namespace> --show-labels
   │
   ├─> 2. 查看具体违规内容
   │     kubectl describe pod <name> -n <namespace>
   │
   ├─> 3. 修复SecurityContext
   │     - 添加runAsNonRoot: true
   │     - 添加allowPrivilegeEscalation: false
   │     - 添加readOnlyRootFilesystem: true
   │     - 移除特权配置
   │
   └─> 4. 降低安全级别（仅测试环境）
         kubectl label namespace <namespace> pod-security.kubernetes.io/enforce=baseline --overwrite
```

#### 6.7.6.3 RBAC权限问题

```
问题：操作被拒绝，报Forbidden错误
   │
   ├─> 1. 确认当前身份
   │     kubectl auth whoami
   │
   ├─> 2. 检查权限
   │     kubectl auth can-i <verb> <resource> --as=<user>
   │
   ├─> 3. 查看相关RoleBinding
   │     kubectl get rolebindings,clusterrolebindings -A | grep <user>
   │
   ├─> 4. 检查ServiceAccount
   │     kubectl get sa <name> -n <namespace>
   │     kubectl describe sa <name> -n <namespace>
   │
   └─> 5. 创建或修改权限
         kubectl create rolebinding <name> --role=<role> --serviceaccount=<ns>:<sa> -n <namespace>
```

#### 6.7.6.4 网络策略问题

```
问题：Pod之间无法通信
   │
   ├─> 1. 检查是否有NetworkPolicy
   │     kubectl get networkpolicy -n <namespace>
   │
   ├─> 2. 查看策略详情
   │     kubectl describe networkpolicy <name> -n <namespace>
   │
   ├─> 3. 验证标签选择器
   │     kubectl get pods --show-labels -n <namespace>
   │
   ├─> 4. 测试连通性
   │     kubectl exec -it <pod-a> -- nc -zv <pod-b-ip> <port>
   │
   ├─> 5. 检查DNS（Egress策略可能阻止）
   │     kubectl exec -it <pod> -- nslookup kubernetes.default
   │
   └─> 6. 临时移除策略测试
         kubectl delete networkpolicy <name> -n <namespace>
```

### 6.7.7 安全检查清单

#### 6.7.7.1 集群级别安全清单

**□ 基础安全**
- [ ] API Server启用认证和授权
- [ ] 启用RBAC（--authorization-mode=RBAC）
- [ ] 禁用匿名认证（--anonymous-auth=false）
- [ ] 启用审计日志
- [ ] etcd启用TLS加密
- [ ] etcd数据静态加密

**□ 网络安全**
- [ ] 安装CNI插件支持NetworkPolicy
- [ ] 配置默认拒绝所有的NetworkPolicy
- [ ] 隔离kube-system命名空间
- [ ] 限制NodePort范围
- [ ] 配置防火墙规则

**□ 准入控制**
- [ ] 启用PodSecurityAdmission
- [ ] 配置ResourceQuota准入插件
- [ ] 配置LimitRanger准入插件
- [ ] 考虑部署OPA Gatekeeper

**□ 镜像安全**
- [ ] 配置私有镜像仓库
- [ ] 启用镜像签名验证
- [ ] 集成镜像扫描（Trivy）
- [ ] 使用ImagePullSecrets

#### 6.7.7.2 命名空间级别安全清单

**□ 资源管理**
- [ ] 配置ResourceQuota
- [ ] 配置LimitRange
- [ ] 监控资源使用率
- [ ] 定期审查和调整配额

**□ Pod安全**
- [ ] 设置Pod Security Admission标签
- [ ] 强制非root运行
- [ ] 禁用特权容器
- [ ] 启用Seccomp/AppArmor

**□ 权限控制**
- [ ] 为每个应用创建独立ServiceAccount
- [ ] 禁用default ServiceAccount自动挂载
- [ ] 配置最小权限RBAC
- [ ] 定期审查权限绑定

**□ 网络隔离**
- [ ] 配置默认拒绝NetworkPolicy
- [ ] 配置白名单Ingress策略
- [ ] 配置Egress策略（包括DNS）
- [ ] 测试网络隔离效果

#### 6.7.7.3 应用级别安全清单

**□ 容器配置**
- [ ] 使用非root用户
- [ ] 设置只读根文件系统
- [ ] 禁止权限提升
- [ ] 移除不必要的Capabilities
- [ ] 配置资源requests/limits

**□ 配置管理**
- [ ] 敏感信息存储在Secret
- [ ] Secret启用加密（EncryptionConfiguration）
- [ ] 不在镜像中硬编码密钥
- [ ] 使用外部密钥管理（Vault）

**□ 运行时安全**
- [ ] 部署Falco监控
- [ ] 配置审计日志
- [ ] 监控异常行为
- [ ] 定期扫描运行中的容器

### 6.7.8 学习路线与进阶方向

#### 6.7.8.1 本章学习成果

通过本章学习，你已经掌握：

**✅ 理论知识：**
- 资源配额和限制机制
- Pod安全标准和准入控制
- RBAC权限模型
- 网络策略原理
- 审计监控体系

**✅ 实践技能：**
- 配置ResourceQuota和LimitRange
- 实施Pod安全策略
- 设计RBAC权限方案
- 配置网络隔离
- 部署安全监控工具

**✅ 生产经验：**
- 企业级安全加固方案
- 五层安全防护体系
- 安全事件响应流程
- 持续改进方法

#### 6.7.8.2 进阶学习方向

**1. 高级安全特性：**
- Kubernetes Secrets加密（KMS集成）
- 外部密钥管理（HashiCorp Vault、AWS Secrets Manager）
- 服务网格安全（Istio mTLS）
- 零信任网络架构
- eBPF安全监控（Cilium Hubble）

**2. 合规性与审计：**
- PCI-DSS合规配置
- HIPAA合规要求
- SOC 2审计准备
- GDPR数据保护
- 等保2.0要求

**3. 安全自动化：**
- GitOps安全（Flux、ArgoCD）
- 策略即代码（OPA Rego）
- 自动化安全扫描流水线
- 漏洞自动修复
- 合规自动检查

**4. 渗透测试与红队：**
- Kubernetes攻击面分析
- 常见攻击向量
- 权限提升技术
- 横向移动检测
- 攻防演练

**5. 多租户架构：**
- 虚拟集群（vCluster）
- 多租户隔离方案
- 命名空间即服务
- 租户资源配额
- 租户计费

**6. 云原生安全工具链：**
- Falco（运行时安全）
- Trivy（镜像扫描）
- Kubesec（配置扫描）
- Polaris（最佳实践检查）
- Tracee（eBPF运行时追踪）
- Tetragon（eBPF安全可观测性）

#### 6.7.8.3 实战项目建议

**初级项目：**
1. 配置多租户资源配额
2. 实施Pod安全准入控制
3. 创建基础RBAC角色体系
4. 配置简单NetworkPolicy

**中级项目：**
1. 构建三层架构网络隔离
2. 集成Trivy镜像扫描到CI/CD
3. 部署Falco运行时监控
4. 实现审计日志集中化

**高级项目：**
1. 设计多租户安全隔离方案
2. 实施零信任网络架构
3. 集成OPA策略引擎
4. 构建安全可观测平台
5. 实现自动化安全合规检查

### 6.7.9 知识图谱

```
Kubernetes配置与安全管理知识图谱
│
├─ 资源管理
│  ├─ ResourceQuota（资源配额）
│  │  ├─ 计算资源（CPU、内存）
│  │  ├─ 对象数量（Pod、Service、PVC等）
│  │  ├─ 存储资源
│  │  └─ 扩展资源
│  │
│  └─ LimitRange（限制范围）
│     ├─ Container限制
│     ├─ Pod限制
│     └─ PVC限制
│
├─ Pod安全
│  ├─ Pod Security Standards
│  │  ├─ Privileged（特权）
│  │  ├─ Baseline（基线）
│  │  └─ Restricted（受限）
│  │
│  ├─ Pod Security Admission
│  │  ├─ enforce（强制）
│  │  ├─ audit（审计）
│  │  └─ warn（警告）
│  │
│  ├─ SecurityContext
│  │  ├─ runAsNonRoot
│  │  ├─ allowPrivilegeEscalation
│  │  ├─ readOnlyRootFilesystem
│  │  └─ Capabilities
│  │
│  └─ 系统调用过滤
│     ├─ Seccomp
│     └─ AppArmor
│
├─ 权限控制（RBAC）
│  ├─ 角色定义
│  │  ├─ Role（命名空间级别）
│  │  └─ ClusterRole（集群级别）
│  │
│  ├─ 角色绑定
│  │  ├─ RoleBinding
│  │  └─ ClusterRoleBinding
│  │
│  ├─ 主体（Subject）
│  │  ├─ User
│  │  ├─ Group
│  │  └─ ServiceAccount
│  │
│  └─ 权限规则（Rule）
│     ├─ apiGroups
│     ├─ resources
│     ├─ verbs
│     └─ resourceNames
│
├─ 网络安全
│  ├─ NetworkPolicy
│  │  ├─ podSelector（Pod选择器）
│  │  ├─ policyTypes（策略类型）
│  │  ├─ Ingress（入站规则）
│  │  └─ Egress（出站规则）
│  │
│  ├─ 流量控制
│  │  ├─ namespaceSelector
│  │  ├─ podSelector
│  │  ├─ ipBlock
│  │  └─ ports
│  │
│  └─ 隔离模式
│     ├─ 默认拒绝
│     ├─ 白名单模式
│     └─ 三层架构隔离
│
└─ 审计监控
   ├─ 审计日志
   │  ├─ 审计策略
   │  ├─ 审计后端
   │  └─ 日志分析
   │
   ├─ 安全扫描
   │  ├─ 镜像扫描（Trivy）
   │  ├─ 配置审计（kube-bench）
   │  └─ 渗透测试（kube-hunter）
   │
   ├─ 运行时监控
   │  ├─ Falco（异常检测）
   │  └─ 实时告警
   │
   └─ 策略引擎
      ├─ OPA（准入控制）
      └─ 自定义策略
```

### 6.7.10 学习检查清单

在进入下一章之前，请确认你已掌握以下知识点：

**□ 资源管理（6.1）**
- [ ] 理解ResourceQuota和LimitRange的区别
- [ ] 能配置计算资源、对象数量、存储配额
- [ ] 理解配额使用率和超限处理
- [ ] 掌握多环境资源隔离

**□ Pod安全（6.2）**
- [ ] 理解三层Pod Security Standards
- [ ] 能配置Pod Security Admission
- [ ] 掌握SecurityContext配置
- [ ] 了解Seccomp和AppArmor

**□ RBAC权限（6.3）**
- [ ] 理解Role和ClusterRole
- [ ] 能创建RoleBinding和ClusterRoleBinding
- [ ] 掌握ServiceAccount使用
- [ ] 能排查权限问题

**□ 网络策略（6.4）**
- [ ] 理解NetworkPolicy工作原理
- [ ] 能配置Ingress和Egress策略
- [ ] 掌握三层架构网络隔离
- [ ] 能测试和调试网络策略

**□ 审计监控（6.5）**
- [ ] 理解Kubernetes审计日志
- [ ] 能使用Trivy扫描镜像
- [ ] 掌握Falco运行时监控
- [ ] 了解OPA策略引擎

**□ 实战项目（6.6）**
- [ ] 能构建五层安全防护体系
- [ ] 掌握安全加固最佳实践
- [ ] 能实施安全事件响应
- [ ] 掌握持续改进方法

### 6.7.11 总结与展望

#### 6.7.11.1 本章核心收获

**安全是Kubernetes生产部署的基石。**本章通过系统化的学习路径，我们完整掌握了从基础到实战的全部安全知识：

1. **理论基础扎实**：深入理解了资源配额、Pod安全、RBAC、网络策略、审计监控等核心概念
2. **实践能力提升**：通过大量YAML示例和实战项目，掌握了实际操作技能
3. **生产经验积累**：企业级安全加固项目展示了真实生产环境的最佳实践
4. **安全意识建立**：理解了纵深防御和零信任原则

**关键要点回顾：**

```
┌─────────────────────────────────────────────────────┐
│       Kubernetes安全管理核心要点                     │
├─────────────────────────────────────────────────────┤
│  1. 资源隔离：ResourceQuota + LimitRange             │
│  2. Pod安全：Restricted级别 + SecurityContext        │
│  3. 权限控制：RBAC最小权限原则                       │
│  4. 网络隔离：默认拒绝 + 白名单模式                  │
│  5. 审计监控：日志 + 扫描 + 运行时监控               │
│  6. 纵深防御：五层安全防护体系                       │
└─────────────────────────────────────────────────────┘
```

#### 6.7.11.2 与其他章节的关联

本章是整个Kubernetes知识体系的安全核心：

**前置知识：**
- 第1章：Kubernetes架构（理解组件安全）
- 第2章：Pod核心概念（理解容器和SecurityContext）
- 第3章：工作负载控制器（理解应用部署）
- 第4章：Service与网络（理解网络基础）
- 第5章：存储管理（理解Secret和ConfigMap）

**后续应用：**
- 第7章：调度与资源管理（资源配额与调度）
- 第8章：监控与日志（安全监控指标）
- 第9章：有状态应用（数据库安全加固）
- 第10章：CI/CD（安全扫描集成）

#### 6.7.11.3 下一步学习建议

**立即实践：**
1. 在测试环境应用本章所有安全配置
2. 使用kube-bench扫描集群
3. 部署Falco监控运行时行为
4. 实施多租户资源隔离

**深入研究：**
1. 阅读Kubernetes安全最佳实践文档
2. 研究CIS Kubernetes基准
3. 学习常见攻击向量和防御方法
4. 参与安全社区和漏洞披露

**准备认证：**
- CKS（Certified Kubernetes Security Specialist）
- CKA安全部分
- CKAD安全最佳实践

#### 6.7.11.4 安全黄金法则

**记住这些原则：**

```
┌─────────────────────────────────────────────────┐
│           Kubernetes安全黄金法则                 │
├─────────────────────────────────────────────────┤
│  🔒 最小权限原则                                 │
│     只授予完成任务所需的最小权限                │
│                                                  │
│  🛡️ 纵深防御                                    │
│     多层安全防护，单点失败不影响整体            │
│                                                  │
│  🚫 默认拒绝                                     │
│     网络和权限默认拒绝，明确白名单              │
│                                                  │
│  👁️ 持续监控                                    │
│     实时监控、审计日志、异常告警                │
│                                                  │
│  🔄 定期审查                                     │
│     定期审查权限、策略、漏洞扫描                │
│                                                  │
│  📝 策略即代码                                   │
│     安全配置版本化、自动化、可审计              │
└─────────────────────────────────────────────────┘
```

#### 6.7.11.5 结语

安全不是一次性工作，而是持续的过程。在Kubernetes环境中，安全贯穿于设计、开发、部署、运维的全生命周期。

**安全三要素：**
- **预防**：通过ResourceQuota、Pod Security、RBAC、NetworkPolicy等机制预防安全问题
- **检测**：通过审计日志、安全扫描、运行时监控及时发现安全事件
- **响应**：通过应急流程、自动化修复快速响应安全事件

**最后的建议：**

> 安全没有银弹，需要多层防护和持续改进。从最小权限开始，逐步构建纵深防御体系。定期审查和更新安全策略，保持对新威胁的敏感度。安全是团队的责任，不是某个人的工作。

**安全不是阻碍创新，而是为创新保驾护航。**通过合理的安全配置，我们既能保护系统安全，又不影响开发效率。

恭喜你完成第6章的学习！现在你已经掌握了Kubernetes安全管理的核心知识，可以自信地在生产环境中构建安全可靠的系统了。

**让我们继续前进，在第7章中学习Kubernetes的调度与资源管理！** 🚀

---

**第6章结束 | 总行数: ~8100行 | 核心概念: ResourceQuota、Pod Security、RBAC、NetworkPolicy、审计监控、安全加固**
