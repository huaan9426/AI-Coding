# 第6章：配置与安全管理

> 在第5章中，我们系统学习了Kubernetes的存储管理，包括ConfigMap和Secret的基础用法。本章将在此基础上，深入探讨Kubernetes的配置管理和安全机制，构建生产级的安全防护体系。

**本章学习目标：**
- 掌握资源配额（ResourceQuota）和限制范围（LimitRange）
- 理解Pod安全策略（Pod Security）
- 深入学习RBAC权限控制体系
- 掌握网络策略（NetworkPolicy）
- 了解审计日志和安全扫描
- 构建完整的安全防护体系

---

## 6.1 资源配额与限制

在多租户Kubernetes集群中，资源管理是一个核心问题。如何防止某个应用或租户过度消耗资源？如何确保集群资源的公平分配？本节将深入学习Kubernetes的资源配额和限制机制。

### 6.1.1 为什么需要资源配额

#### 6.1.1.1 多租户环境的挑战

在实际生产环境中，一个Kubernetes集群通常会被多个团队或应用共享：

```
┌─────────────────────────────────────────────────┐
│          多租户集群资源竞争场景                  │
├─────────────────────────────────────────────────┤
│  团队A：电商应用（高峰期流量大）                │
│  团队B：数据处理（计算密集型）                  │
│  团队C：开发测试（资源需求不稳定）              │
│                                                  │
│  问题：                                          │
│  - 团队A在促销期间疯狂扩容，占用80%资源         │
│  - 团队B的批处理任务导致其他服务响应变慢       │
│  - 团队C的测试Pod泄漏，持续消耗资源            │
│                                                  │
│  后果：                                          │
│  ❌ 资源争抢导致服务不稳定                      │
│  ❌ 关键应用无法获得足够资源                    │
│  ❌ 成本失控，账单暴增                          │
└─────────────────────────────────────────────────┘
```

#### 6.1.1.2 资源管理的痛点

**没有资源配额的情况：**

| 问题 | 场景 | 影响 |
|-----|------|------|
| **资源霸占** | 单个应用申请过多资源 | 其他应用无法部署 |
| **成本失控** | 无限制创建资源 | 云账单暴增 |
| **稳定性问题** | 资源耗尽导致集群不稳定 | 服务大面积故障 |
| **公平性缺失** | 先到先得的资源分配 | 重要应用资源不足 |
| **容量规划困难** | 无法预测资源使用情况 | 扩容滞后 |

**真实案例：**

```yaml
# ❌ 不受控制的资源申请
apiVersion: apps/v1
kind: Deployment
metadata:
  name: greedy-app
  namespace: team-a
spec:
  replicas: 100        # 过度扩容
  template:
    spec:
      containers:
      - name: app
        image: myapp:1.0
        resources:
          requests:
            memory: "8Gi"      # 每个Pod申请8Gi内存
            cpu: "4"           # 每个Pod申请4核CPU
        # 总需求：800Gi内存 + 400核CPU
        # 如果集群只有1000Gi内存，其他应用将无法运行！
```

#### 6.1.1.3 资源配额的价值

**引入资源配额后：**

```
┌─────────────────────────────────────────────────┐
│          资源配额带来的好处                      │
├─────────────────────────────────────────────────┤
│  ✅ 资源隔离                                     │
│     每个Namespace有明确的资源上限                │
│                                                  │
│  ✅ 成本控制                                     │
│     防止资源滥用，控制云成本                    │
│                                                  │
│  ✅ 公平性                                       │
│     确保资源公平分配给各团队                    │
│                                                  │
│  ✅ 稳定性                                       │
│     防止单点故障影响整个集群                    │
│                                                  │
│  ✅ 容量规划                                     │
│     清晰的资源使用预期，便于扩容决策            │
└─────────────────────────────────────────────────┘
```

### 6.1.2 ResourceQuota资源配额

#### 6.1.2.1 ResourceQuota核心概念

ResourceQuota是Namespace级别的资源，用于限制该Namespace下所有资源的总量。

**基本结构：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota-example
  namespace: team-a
spec:
  hard:
    # 计算资源配额
    requests.cpu: "10"           # CPU请求总量：10核
    requests.memory: "20Gi"      # 内存请求总量：20Gi
    limits.cpu: "20"             # CPU限制总量：20核
    limits.memory: "40Gi"        # 内存限制总量：40Gi

    # 对象数量配额
    pods: "50"                   # 最多50个Pod
    services: "10"               # 最多10个Service
    persistentvolumeclaims: "5"  # 最多5个PVC

    # 存储配额
    requests.storage: "100Gi"    # 存储请求总量：100Gi
```

**工作原理：**

```
┌─────────────────────────────────────────────────┐
│          ResourceQuota工作流程                   │
├─────────────────────────────────────────────────┤
│                                                  │
│  1. 用户提交Pod/Deployment                      │
│           ↓                                      │
│  2. API Server检查ResourceQuota                 │
│           ↓                                      │
│  3. 计算当前Namespace资源使用量                 │
│           ↓                                      │
│  4. 判断：新资源 + 已用资源 <= 配额？           │
│           ↓                 ↓                    │
│         YES               NO                     │
│           ↓                 ↓                    │
│      允许创建          拒绝创建                 │
│           ↓                 ↓                    │
│      资源计数+1    返回错误：exceeded quota      │
└─────────────────────────────────────────────────┘
```

#### 6.1.2.2 计算资源配额

**示例1：限制CPU和内存**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: development
spec:
  hard:
    # CPU配额
    requests.cpu: "4"      # 该Namespace下所有Pod的CPU请求总和不超过4核
    limits.cpu: "8"        # 该Namespace下所有Pod的CPU限制总和不超过8核

    # 内存配额
    requests.memory: "8Gi"   # 内存请求总和不超过8Gi
    limits.memory: "16Gi"    # 内存限制总和不超过16Gi
```

**应用到Namespace：**

```bash
kubectl apply -f compute-quota.yaml
kubectl get resourcequota -n development
```

**测试配额：**

```yaml
# 尝试创建Pod
apiVersion: v1
kind: Pod
metadata:
  name: test-pod-1
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "4Gi"
        cpu: "2"
```

```bash
# 创建第1个Pod - 成功（使用：CPU 1核，内存2Gi）
kubectl apply -f test-pod-1.yaml

# 创建第2个Pod - 成功（累计：CPU 2核，内存4Gi）
kubectl apply -f test-pod-2.yaml

# 创建第3个Pod - 成功（累计：CPU 3核，内存6Gi）
kubectl apply -f test-pod-3.yaml

# 创建第4个Pod - 成功（累计：CPU 4核，内存8Gi，达到requests上限）
kubectl apply -f test-pod-4.yaml

# 创建第5个Pod - 失败！
kubectl apply -f test-pod-5.yaml
# Error: exceeded quota: compute-quota,
# requested: requests.cpu=1,requests.memory=2Gi,
# used: requests.cpu=4,requests.memory=8Gi,
# limited: requests.cpu=4,requests.memory=8Gi
```

**查看配额使用情况：**

```bash
kubectl describe resourcequota compute-quota -n development
```

输出：
```
Name:            compute-quota
Namespace:       development
Resource         Used   Hard
--------         ----   ----
limits.cpu       8      8
limits.memory    16Gi   16Gi
requests.cpu     4      4
requests.memory  8Gi    8Gi
```

#### 6.1.2.3 对象数量配额

**示例2：限制对象数量**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota
  namespace: development
spec:
  hard:
    # Pod数量
    pods: "20"

    # Service数量
    services: "5"
    services.loadbalancers: "2"      # LoadBalancer类型Service
    services.nodeports: "3"          # NodePort类型Service

    # PVC数量
    persistentvolumeclaims: "10"

    # ConfigMap和Secret数量
    configmaps: "20"
    secrets: "20"

    # ReplicationController数量
    replicationcontrollers: "10"
```

**按存储类限制PVC：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: development
spec:
  hard:
    # 总存储配额
    requests.storage: "100Gi"
    persistentvolumeclaims: "10"

    # 按StorageClass限制
    nfs-client.storageclass.storage.k8s.io/requests.storage: "50Gi"
    nfs-client.storageclass.storage.k8s.io/persistentvolumeclaims: "5"

    local-storage.storageclass.storage.k8s.io/requests.storage: "30Gi"
    local-storage.storageclass.storage.k8s.io/persistentvolumeclaims: "3"
```

#### 6.1.2.4 作用域选择器（Scope Selector）

ResourceQuota可以根据Pod的优先级或QoS类别进行限制。

**按优先级限制：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: high-priority-quota
  namespace: production
spec:
  hard:
    pods: "100"
    requests.cpu: "50"
    requests.memory: "100Gi"
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["high-priority"]   # 只对high-priority优先级的Pod生效
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: low-priority-quota
  namespace: production
spec:
  hard:
    pods: "20"
    requests.cpu: "10"
    requests.memory: "20Gi"
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["low-priority"]    # 只对low-priority优先级的Pod生效
```

**按QoS类别限制：**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-quota
  namespace: development
spec:
  hard:
    pods: "10"            # BestEffort类Pod最多10个
  scopes:
  - BestEffort            # 只对BestEffort QoS的Pod生效
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: notbesteffort-quota
  namespace: development
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
  scopes:
  - NotBestEffort         # 对Burstable和Guaranteed QoS的Pod生效
```

**QoS类别说明：**

| QoS类别 | 条件 | 特点 |
|--------|------|------|
| **Guaranteed** | requests = limits（且所有容器都设置） | 最高优先级，不会被OOM Kill |
| **Burstable** | 设置了requests，但requests < limits | 中等优先级，资源不足时可能被Kill |
| **BestEffort** | 未设置requests和limits | 最低优先级，优先被Kill |

### 6.1.3 LimitRange限制范围

#### 6.1.3.1 LimitRange核心概念

LimitRange是Namespace级别的资源，用于限制单个资源对象的大小。

**ResourceQuota vs LimitRange：**

```
┌──────────────────┬─────────────────┬─────────────────┐
│    特性          │  ResourceQuota  │   LimitRange    │
├──────────────────┼─────────────────┼─────────────────┤
│  作用范围        │  Namespace总量  │  单个对象       │
│  限制内容        │  总资源上限     │  单个资源上下限 │
│  典型用途        │  多租户资源隔离 │  防止单点过大   │
│  示例            │  总CPU不超过10核│  单Pod不超过2核 │
└──────────────────┴─────────────────┴─────────────────┘
```

**组合使用场景：**

```yaml
# ResourceQuota: 限制总量
# development namespace总共最多使用10核CPU

# LimitRange: 限制单个对象
# 每个Pod最多使用2核CPU
# 这样可以确保至少能运行5个Pod
```

#### 6.1.3.2 限制Pod和容器资源

**示例1：限制容器资源范围**

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: container-limit-range
  namespace: development
spec:
  limits:
  - type: Container
    # 默认值（如果未指定）
    default:
      cpu: "500m"        # 默认limit
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"        # 默认request
      memory: "256Mi"

    # 最大值
    max:
      cpu: "2"           # 单个容器最多2核
      memory: "4Gi"      # 单个容器最多4Gi内存

    # 最小值
    min:
      cpu: "100m"        # 单个容器至少100m CPU
      memory: "128Mi"    # 单个容器至少128Mi内存

    # 最大/最小比例
    maxLimitRequestRatio:
      cpu: "4"           # limit最多是request的4倍
      memory: "4"
```

**效果演示：**

```yaml
# 案例1：未指定资源 - 自动应用默认值
apiVersion: v1
kind: Pod
metadata:
  name: pod-default
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    # 未指定resources，自动应用：
    # requests: cpu=250m, memory=256Mi
    # limits: cpu=500m, memory=512Mi
```

```yaml
# 案例2：超过最大值 - 拒绝创建
apiVersion: v1
kind: Pod
metadata:
  name: pod-too-large
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "3"      # ❌ 超过max（2核）
# Error: [spec.containers[0].resources.requests.cpu: Invalid value: "3": must be less than or equal to cpu limit of 2]
```

```yaml
# 案例3：低于最小值 - 拒绝创建
apiVersion: v1
kind: Pod
metadata:
  name: pod-too-small
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "50m"    # ❌ 低于min（100m）
# Error: minimum cpu usage per Container is 100m
```

```yaml
# 案例4：比例不合理 - 拒绝创建
apiVersion: v1
kind: Pod
metadata:
  name: pod-bad-ratio
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "200m"
      limits:
        cpu: "1"       # ❌ 比例5倍，超过maxLimitRequestRatio（4倍）
# Error: cpu max limit to request ratio per Container is 4
```

**示例2：限制Pod资源范围**

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: pod-limit-range
  namespace: development
spec:
  limits:
  - type: Pod
    max:
      cpu: "4"           # 单个Pod（所有容器总和）最多4核
      memory: "8Gi"      # 单个Pod（所有容器总和）最多8Gi
    min:
      cpu: "200m"
      memory: "256Mi"
```

**多容器Pod示例：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "1"
        memory: "2Gi"

  - name: sidecar
    image: busybox
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"

  # Pod总资源：cpu=1.5, memory=3Gi
  # 符合LimitRange: max pod cpu=4, memory=8Gi ✅
```

#### 6.1.3.3 限制PVC存储

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: storage-limit-range
  namespace: development
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: "50Gi"    # 单个PVC最大50Gi
    min:
      storage: "1Gi"     # 单个PVC最小1Gi
```

**测试：**

```yaml
# ✅ 符合范围
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-valid
  namespace: development
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi    # 在1Gi-50Gi范围内

# ❌ 超过最大值
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-too-large
  namespace: development
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi   # 超过50Gi上限
# Error: maximum storage usage per PersistentVolumeClaim is 50Gi
```

### 6.1.4 实战案例：多环境资源配额

#### 6.1.4.1 场景设计

为一家公司的三个环境（Dev、Test、Prod）配置不同的资源配额：

| 环境 | Pod数量 | CPU总量 | 内存总量 | 存储总量 | 优先级 |
|-----|--------|---------|---------|---------|--------|
| **Dev** | 50 | 10核 | 20Gi | 100Gi | 低 |
| **Test** | 30 | 20核 | 40Gi | 200Gi | 中 |
| **Prod** | 100 | 50核 | 100Gi | 500Gi | 高 |

#### 6.1.4.2 Dev环境配置

**dev-namespace.yaml：**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    environment: development
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-resource-quota
  namespace: dev
spec:
  hard:
    # 计算资源
    requests.cpu: "10"
    requests.memory: "20Gi"
    limits.cpu: "15"
    limits.memory: "30Gi"

    # 对象数量
    pods: "50"
    services: "10"
    persistentvolumeclaims: "10"
    configmaps: "30"
    secrets: "30"

    # 存储
    requests.storage: "100Gi"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limit-range
  namespace: dev
spec:
  limits:
  # 容器限制
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "200m"
      memory: "256Mi"
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "50m"
      memory: "64Mi"
    maxLimitRequestRatio:
      cpu: "4"
      memory: "4"

  # Pod限制
  - type: Pod
    max:
      cpu: "4"
      memory: "8Gi"

  # PVC限制
  - type: PersistentVolumeClaim
    max:
      storage: "20Gi"
    min:
      storage: "1Gi"
```

#### 6.1.4.3 Prod环境配置

**prod-namespace.yaml：**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: prod
  labels:
    environment: production
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: prod-resource-quota
  namespace: prod
spec:
  hard:
    # 计算资源（更高配额）
    requests.cpu: "50"
    requests.memory: "100Gi"
    limits.cpu: "80"
    limits.memory: "150Gi"

    # 对象数量
    pods: "100"
    services: "20"
    persistentvolumeclaims: "30"
    configmaps: "50"
    secrets: "50"

    # 存储
    requests.storage: "500Gi"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: prod-limit-range
  namespace: prod
spec:
  limits:
  # 容器限制（更宽松）
  - type: Container
    default:
      cpu: "1"
      memory: "2Gi"
    defaultRequest:
      cpu: "500m"
      memory: "1Gi"
    max:
      cpu: "8"          # 生产环境允许更大的容器
      memory: "16Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    maxLimitRequestRatio:
      cpu: "4"
      memory: "4"

  # Pod限制
  - type: Pod
    max:
      cpu: "16"
      memory: "32Gi"

  # PVC限制
  - type: PersistentVolumeClaim
    max:
      storage: "100Gi"
    min:
      storage: "5Gi"
```

#### 6.1.4.4 部署和验证

```bash
# 部署环境配置
kubectl apply -f dev-namespace.yaml
kubectl apply -f test-namespace.yaml
kubectl apply -f prod-namespace.yaml

# 查看配额
kubectl get resourcequota --all-namespaces
kubectl get limitrange --all-namespaces

# 查看Dev环境详情
kubectl describe resourcequota dev-resource-quota -n dev
kubectl describe limitrange dev-limit-range -n dev

# 查看Prod环境详情
kubectl describe resourcequota prod-resource-quota -n prod
kubectl describe limitrange prod-limit-range -n prod
```

**部署测试应用：**

```yaml
# dev-test-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-app
  namespace: dev
spec:
  replicas: 3
  selector:
    matchLabels:
      app: test-app
  template:
    metadata:
      labels:
        app: test-app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        # 未指定资源，自动应用LimitRange默认值：
        # requests: cpu=200m, memory=256Mi
        # limits: cpu=500m, memory=512Mi
```

```bash
kubectl apply -f dev-test-deployment.yaml

# 查看资源使用情况
kubectl describe resourcequota dev-resource-quota -n dev
# 输出：
# Resource         Used    Hard
# --------         ----    ----
# pods             3       50
# requests.cpu     600m    10
# requests.memory  768Mi   20Gi
# ...
```

### 6.1.5 最佳实践

#### 6.1.5.1 资源配额设计原则

**1. 合理规划配额：**

```yaml
# ✅ 推荐：基于实际监控数据设置
# 监控1个月后发现：
# - 平均CPU使用：8核
# - 峰值CPU使用：12核
# - 设置配额：15核（留有余量）

apiVersion: v1
kind: ResourceQuota
metadata:
  name: data-driven-quota
spec:
  hard:
    requests.cpu: "15"    # 峰值 + 25% buffer
    requests.memory: "30Gi"
```

**2. 分层配额策略：**

```
┌─────────────────────────────────────────┐
│         分层配额架构                     │
├─────────────────────────────────────────┤
│  第1层：集群级别（总资源池）             │
│    └─ 总CPU: 100核, 总内存: 200Gi      │
│                                          │
│  第2层：环境级别（Namespace配额）        │
│    ├─ Prod: 50核, 100Gi (50%)          │
│    ├─ Test: 30核, 60Gi (30%)           │
│    └─ Dev: 20核, 40Gi (20%)            │
│                                          │
│  第3层：应用级别（LimitRange限制）       │
│    └─ 单Pod: ≤4核, ≤8Gi                │
└─────────────────────────────────────────┘
```

**3. 强制要求资源设置：**

```yaml
# 部署ResourceQuota后，必须为所有Pod设置资源
# 否则Pod无法创建

# ❌ 没有ResourceQuota的情况
apiVersion: v1
kind: Pod
metadata:
  name: pod-no-resources
spec:
  containers:
  - name: app
    image: nginx:1.21
    # 未设置resources - 可以创建

# ✅ 有ResourceQuota的情况
# 必须设置resources或配置LimitRange默认值
```

**配置LimitRange提供默认值：**

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
spec:
  limits:
  - type: Container
    defaultRequest:    # 为未指定的Pod提供默认request
      cpu: "100m"
      memory: "128Mi"
    default:           # 为未指定的Pod提供默认limit
      cpu: "200m"
      memory: "256Mi"
```

#### 6.1.5.2 监控和告警

**查看配额使用情况：**

```bash
# 查看所有Namespace的配额
kubectl get resourcequota --all-namespaces

# 详细查看某个配额
kubectl describe resourcequota <name> -n <namespace>

# 查看配额使用率（自定义脚本）
kubectl get resourcequota dev-resource-quota -n dev -o json | \
  jq '.status.used, .status.hard'
```

**配置Prometheus告警：**

```yaml
# prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: resourcequota-alerts
spec:
  groups:
  - name: resourcequota
    interval: 30s
    rules:
    # CPU配额使用率超过80%
    - alert: NamespaceCPUQuotaExceeding
      expr: |
        sum(kube_resourcequota{resource="requests.cpu", type="used"}) by (namespace)
        /
        sum(kube_resourcequota{resource="requests.cpu", type="hard"}) by (namespace)
        > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Namespace {{ $labels.namespace }} CPU quota exceeding 80%"

    # 内存配额使用率超过80%
    - alert: NamespaceMemoryQuotaExceeding
      expr: |
        sum(kube_resourcequota{resource="requests.memory", type="used"}) by (namespace)
        /
        sum(kube_resourcequota{resource="requests.memory", type="hard"}) by (namespace)
        > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Namespace {{ $labels.namespace }} memory quota exceeding 80%"
```

#### 6.1.5.3 常见问题排查

**问题1：Pod无法创建，提示exceeded quota**

```bash
# 错误信息
Error from server (Forbidden): error when creating "pod.yaml":
pods "myapp" is forbidden: exceeded quota: compute-quota,
requested: requests.cpu=2,requests.memory=4Gi,
used: requests.cpu=9,requests.memory=18Gi,
limited: requests.cpu=10,requests.memory=20Gi

# 排查步骤
# 1. 查看当前配额使用情况
kubectl describe resourcequota compute-quota -n <namespace>

# 2. 找出资源消耗大的Pod
kubectl top pods -n <namespace> --sort-by=cpu
kubectl top pods -n <namespace> --sort-by=memory

# 3. 解决方案
# 方案A：删除不必要的Pod
kubectl delete pod <unused-pod> -n <namespace>

# 方案B：减少新Pod的资源请求
# 修改Deployment的resources.requests

# 方案C：增加配额（需要管理员权限）
kubectl edit resourcequota compute-quota -n <namespace>
```

**问题2：Pod一直Pending，提示Insufficient CPU/Memory**

```bash
# 查看Pod事件
kubectl describe pod <pod-name> -n <namespace>
# Events:
#   Warning  FailedScheduling  Pod didn't fit: Insufficient CPU

# 原因分析：
# 1. ResourceQuota限制了总量
# 2. LimitRange限制了单个Pod大小
# 3. 节点实际资源不足

# 检查LimitRange
kubectl describe limitrange -n <namespace>

# 检查节点资源
kubectl top nodes
kubectl describe node <node-name>
```

**问题3：ResourceQuota未生效**

```bash
# 可能原因1：Pod未设置resources
# 解决：配置LimitRange提供默认值

# 可能原因2：ResourceQuota配置错误
kubectl get resourcequota <name> -n <namespace> -o yaml
# 检查spec.hard字段是否正确

# 可能原因3：权限问题
kubectl auth can-i create resourcequotas --as=system:serviceaccount:<namespace>:<serviceaccount>
```

本节我们深入学习了Kubernetes的资源配额和限制机制。通过ResourceQuota可以限制Namespace的总资源，通过LimitRange可以限制单个对象的资源范围。合理使用这些机制，可以实现多租户环境的资源隔离、成本控制和稳定性保障。

接下来，我们将学习Pod安全策略，进一步加固集群的安全防护。

---

