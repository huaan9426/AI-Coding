# 第8章 存储管理

在前面的章节中，我们深入学习了Kubernetes的调度与资源管理机制，掌握了如何高效地将Pod调度到合适的节点上，并通过优先级、亲和性、资源配额等手段实现企业级的资源管理。然而，真实的生产环境中，大部分应用都是**有状态应用**（Stateful Application），它们需要持久化存储来保存数据：

- **数据库**：MySQL、PostgreSQL、MongoDB需要持久化数据库文件
- **消息队列**：Kafka、RabbitMQ需要持久化消息数据
- **文件存储**：MinIO、Ceph需要持久化对象存储
- **日志系统**：Elasticsearch需要持久化日志索引
- **配置中心**：etcd、Consul需要持久化配置数据

容器的本质是**进程**，当Pod被删除或重启时，容器内的数据会随之消失。这对于无状态应用（如Web服务器）不是问题，但对于有状态应用却是致命的。Kubernetes通过强大的**存储管理体系**解决了这一难题，提供了从临时存储到企业级分布式存储的完整解决方案。

本章将系统学习Kubernetes的存储管理机制，从基础概念到高级特性，再到企业级实战，构建完整的存储知识体系。

---

## 本章结构

本章共分为8节，逐步深入Kubernetes存储管理的核心技术：

- **8.1 存储基础概念与架构**：理解Kubernetes存储体系的整体架构，掌握Volume、PV、PVC的核心概念
- **8.2 Volume详解**：深入学习emptyDir、hostPath、configMap、secret等基础Volume类型
- **8.3 持久卷（PersistentVolume）**：掌握PV的生命周期、访问模式、回收策略
- **8.4 持久卷声明（PersistentVolumeClaim）**：理解PVC的绑定机制、存储类选择
- **8.5 存储类（StorageClass）**：学习动态存储供应、参数配置、回收策略
- **8.6 StatefulSet与有状态应用**：掌握StatefulSet的存储管理机制、volumeClaimTemplates
- **8.7 CSI存储插件**：理解Container Storage Interface标准，学习云存储集成
- **8.8 实战项目与本章小结**：通过企业级存储方案实战，总结存储管理最佳实践

---

## 8.1 存储基础概念与架构

在深入具体技术之前，我们需要先建立对Kubernetes存储体系的整体认知。本节将回答以下核心问题：

1. **为什么容器需要持久化存储？** 容器数据的生命周期与存储需求
2. **Kubernetes提供了哪些存储方案？** 临时存储、持久化存储、配置存储的分类
3. **PV、PVC、StorageClass的关系是什么？** 存储抽象的三层架构
4. **存储如何与Pod绑定？** Volume挂载的完整流程

---

### 8.1.1 容器存储的挑战

**问题1：容器数据的临时性**

容器的文件系统是基于镜像的分层文件系统（Union FS），容器内的所有写操作都发生在**可写层**（Writable Layer）：

```
容器文件系统层级结构
┌─────────────────────────┐
│  可写层（Container Layer）  │ ← 容器运行时的所有写操作
├─────────────────────────┤
│  镜像层4（ADD app.jar）    │
├─────────────────────────┤
│  镜像层3（RUN apt update） │
├─────────────────────────┤
│  镜像层2（COPY . /app）    │
├─────────────────────────┤
│  镜像层1（FROM ubuntu）    │ ← 只读基础镜像
└─────────────────────────┘
```

**核心问题：**
- ❌ 容器删除时，可写层数据随之消失
- ❌ 容器重启时，数据重置到镜像初始状态
- ❌ 同一Pod内的多个容器无法共享数据
- ❌ Pod迁移到其他节点时，数据无法跟随

**典型场景的数据丢失：**

```bash
# 场景1：Pod重启导致数据丢失
$ kubectl exec mysql-pod -- mysql -e "CREATE DATABASE test;"
$ kubectl delete pod mysql-pod  # Pod被删除
$ kubectl get pod               # 新Pod被ReplicaSet重建
$ kubectl exec mysql-pod -- mysql -e "SHOW DATABASES;"  # test数据库消失！

# 场景2：容器崩溃导致日志丢失
$ kubectl exec nginx-pod -- sh -c "echo 'important log' >> /var/log/access.log"
$ kubectl exec nginx-pod -- kill 1  # 容器崩溃重启
$ kubectl exec nginx-pod -- cat /var/log/access.log  # 日志文件不存在！
```

---

**问题2：有状态应用的复杂需求**

真实的生产环境中，有状态应用有更复杂的存储需求：

| 应用类型 | 存储需求 | 挑战 |
|---------|---------|------|
| **MySQL主从** | - 主库需要独立持久卷<br>- 从库需要只读访问<br>- 数据目录需要固定路径 | 如何为每个实例分配独立存储？<br>如何保证存储的访问模式正确？ |
| **Elasticsearch集群** | - 每个节点需要独立数据目录<br>- 存储需要高IOPS（SSD）<br>- 需要动态扩容 | 如何根据性能需求选择存储类型？<br>如何支持在线扩容？ |
| **Kafka** | - 日志段需要顺序写入<br>- 消费者offset需要持久化<br>- 存储容量需要动态调整 | 如何保证写入性能？<br>如何处理存储容量不足？ |
| **共享文件服务** | - 多个Pod同时读写<br>- 需要文件锁机制<br>- 跨节点访问 | 如何支持ReadWriteMany模式？<br>如何选择合适的网络存储？ |

---

**问题3：传统存储与容器编排的鸿沟**

在容器化之前，存储管理是运维团队的职责：

```
传统模式（手动管理）
运维团队：创建LUN → 格式化 → 挂载到服务器 → 配置权限
开发团队：在固定路径/data/mysql使用存储
```

容器化后，Pod可能在任意节点启动，传统的手动挂载方式失效：

```
容器化挑战
Pod在node1启动 → 需要自动挂载存储A
Pod被调度到node2 → 需要自动卸载node1并挂载到node2
Pod扩容到3副本 → 需要自动创建3个独立存储
```

**Kubernetes存储体系的设计目标：**
- ✅ **解耦**：应用开发者无需关心底层存储细节（NFS/Ceph/云盘）
- ✅ **自动化**：存储的创建、挂载、卸载、删除全自动
- ✅ **可移植**：同一份YAML可以在不同环境（AWS/阿里云/本地）运行
- ✅ **动态供应**：根据应用需求自动创建存储，无需提前准备

---

### 8.1.2 Kubernetes存储体系架构

Kubernetes通过**三层抽象**解决容器存储难题：

```
┌─────────────────────────────────────────────────────────────┐
│  应用层（Pod）                                                 │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ containers:                                           │  │
│  │   - name: mysql                                       │  │
│  │     volumeMounts:                                     │  │
│  │       - name: data                                    │  │
│  │         mountPath: /var/lib/mysql  ← 应用只关心挂载路径  │  │
│  └──────────────────────────────────────────────────────┘  │
└────────────────────────┬────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│  抽象层（PVC - 存储需求声明）                                    │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ kind: PersistentVolumeClaim                           │  │
│  │ spec:                                                 │  │
│  │   accessModes: [ReadWriteOnce]                        │  │
│  │   resources:                                          │  │
│  │     requests:                                         │  │
│  │       storage: 10Gi           ← 我需要10GB的RWO存储    │  │
│  │   storageClassName: fast-ssd  ← 我需要SSD类型         │  │
│  └──────────────────────────────────────────────────────┘  │
└────────────────────────┬────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│  供应层（StorageClass - 存储供应策略）                           │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ kind: StorageClass                                    │  │
│  │ provisioner: kubernetes.io/aws-ebs                    │  │
│  │ parameters:                                           │  │
│  │   type: gp3                   ← 使用AWS GP3 SSD       │  │
│  │   iopsPerGB: "50"             ← 性能参数              │  │
│  └──────────────────────────────────────────────────────┘  │
└────────────────────────┬────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│  实现层（PV - 真实存储资源）                                     │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ kind: PersistentVolume                                │  │
│  │ spec:                                                 │  │
│  │   capacity:                                           │  │
│  │     storage: 10Gi                                     │  │
│  │   awsElasticBlockStore:                               │  │
│  │     volumeID: vol-0a1b2c3d4e5f  ← 真实的AWS EBS卷ID   │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

**核心概念解析：**

**1. Volume（存储卷）**
- **定义**：Pod中定义的存储抽象，是容器挂载存储的基础单元
- **生命周期**：与Pod绑定，Pod删除时Volume行为取决于类型
- **分类**：
  - **临时卷**：emptyDir（随Pod删除）
  - **节点卷**：hostPath（节点本地路径）
  - **配置卷**：configMap、secret（配置数据）
  - **持久卷**：通过PVC引用PV（独立生命周期）

**2. PersistentVolume（PV，持久卷）**
- **定义**：集群级别的存储资源，由管理员创建或动态供应
- **特点**：
  - 独立于Pod的生命周期（Pod删除后PV依然存在）
  - 包含真实存储的细节（NFS服务器地址、云盘ID等）
  - 有容量、访问模式、回收策略等属性
- **类比**：PV就像数据中心的"存储货架"，是真实的物理资源

**3. PersistentVolumeClaim（PVC，持久卷声明）**
- **定义**：用户对存储的请求声明，描述需要什么样的存储
- **特点**：
  - 命名空间级别（与Pod在同一命名空间）
  - 只描述需求（容量、访问模式、存储类），不关心实现细节
  - 通过绑定机制与PV关联
- **类比**：PVC就像"采购订单"，描述需要多大、什么性能的存储

**4. StorageClass（SC，存储类）**
- **定义**：存储的"配置模板"，定义如何动态创建PV
- **特点**：
  - 包含provisioner（存储供应商插件）
  - 包含parameters（存储参数，如磁盘类型、IOPS）
  - 支持动态供应（PVC创建时自动创建PV）
- **类比**：StorageClass就像"采购合同"，定义了从哪里、如何采购存储

---

### 8.1.3 存储类型分类

Kubernetes支持丰富的存储类型，根据使用场景可以分为以下几类：

**分类1：按生命周期分类**

| 类型 | 生命周期 | 典型场景 | 代表类型 |
|------|---------|---------|---------|
| **临时存储** | 与Pod绑定<br>Pod删除时数据丢失 | - 缓存数据<br>- 临时文件<br>- 容器间共享 | emptyDir |
| **节点存储** | 与节点绑定<br>Pod迁移时数据丢失 | - 节点日志采集<br>- 主机路径访问<br>- DaemonSet数据 | hostPath |
| **持久存储** | 独立生命周期<br>Pod删除后数据保留 | - 数据库数据<br>- 用户上传文件<br>- 持久化队列 | PVC/PV |

**分类2：按访问模式分类**

| 访问模式 | 缩写 | 含义 | 典型场景 | 支持的存储后端 |
|---------|------|------|---------|---------------|
| **ReadWriteOnce** | RWO | 单节点读写 | - 数据库（MySQL/PostgreSQL）<br>- 块存储应用 | AWS EBS、Azure Disk<br>GCE PD、本地磁盘 |
| **ReadOnlyMany** | ROX | 多节点只读 | - 静态资源分发<br>- 共享配置文件 | NFS、CephFS |
| **ReadWriteMany** | RWX | 多节点读写 | - 共享文件服务<br>- 多副本应用共享数据 | NFS、GlusterFS<br>CephFS、Azure File |
| **ReadWriteOncePod** | RWOP | 单Pod读写（K8s 1.22+） | - 独占访问保证<br>- 避免脑裂 | CSI驱动支持 |

**重要提示：**
- ⚠️ **RWO不是指"单个Pod"，而是"单个节点"**：同一节点的多个Pod可以同时挂载RWO卷
- ⚠️ **RWX需要网络存储**：本地磁盘、云盘（EBS/Azure Disk）不支持RWX

**分类3：按存储后端分类**

```
存储后端技术栈
├── 本地存储
│   ├── emptyDir（节点临时目录）
│   ├── hostPath（节点路径）
│   └── local（本地持久卷，1.14+）
├── 网络存储
│   ├── NFS（Network File System）
│   ├── iSCSI（块存储协议）
│   ├── GlusterFS（分布式文件系统）
│   └── CephFS/RBD（Ceph分布式存储）
├── 云存储
│   ├── AWS EBS（Elastic Block Store）
│   ├── Azure Disk（托管磁盘）
│   ├── GCE PD（Persistent Disk）
│   └── 阿里云盘、腾讯云盘
└── 特殊存储
    ├── ConfigMap（配置数据）
    ├── Secret（敏感数据）
    ├── Projected（投射卷，组合多个源）
    └── CSI（Container Storage Interface，统一接口）
```

---

### 8.1.4 PV/PVC绑定机制

**核心流程：**

```
┌──────────────────────────────────────────────────────────────┐
│  步骤1：用户创建PVC（我需要10GB RWO存储）                         │
└────────────────┬─────────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────────┐
│  步骤2：Controller匹配PV（查找满足条件的PV）                      │
│  匹配条件：                                                     │
│  ✓ 容量 >= PVC请求（10GB）                                     │
│  ✓ 访问模式匹配（RWO）                                          │
│  ✓ StorageClass匹配（或都为空）                                │
│  ✓ Selector标签匹配（如果PVC指定了selector）                    │
└────────────────┬─────────────────────────────────────────────┘
                 │
      ┌──────────┴──────────┐
      │                     │
      ▼                     ▼
┌────────────┐        ┌────────────┐
│ 找到匹配PV  │        │ 未找到PV    │
└─────┬──────┘        └─────┬──────┘
      │                     │
      ▼                     ▼
┌────────────┐        ┌────────────┐
│ 绑定PVC到PV │        │ 触发动态供应 │
│ (Bound状态) │        │ (Pending)   │
└─────┬──────┘        └─────┬──────┘
      │                     │
      │                     ▼
      │              ┌────────────┐
      │              │ SC创建PV   │
      │              └─────┬──────┘
      │                     │
      └──────────┬──────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────────┐
│  步骤3：Pod引用PVC（volumeMounts挂载到容器）                      │
└────────────────┬─────────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────────┐
│  步骤4：Kubelet挂载存储（调用Volume Plugin或CSI驱动）             │
│  - Attach阶段：将存储卷附加到节点（如AWS EBS attach）             │
│  - Mount阶段：将存储卷挂载到容器路径                              │
└──────────────────────────────────────────────────────────────┘
```

**绑定规则详解：**

**规则1：容量匹配**
```yaml
# PVC请求10GB
spec:
  resources:
    requests:
      storage: 10Gi

# 可以绑定到15GB的PV（容量 >= 请求即可）
# 但会浪费5GB空间
# ✅ 推荐：PV容量精确等于PVC请求
```

**规则2：访问模式匹配**
```yaml
# PVC请求RWO
accessModes: [ReadWriteOnce]

# ❌ 无法绑定到只支持ROX的PV
# ✅ 可以绑定到同时支持RWO和RWX的PV
# PV的accessModes必须包含PVC请求的所有模式
```

**规则3：StorageClass匹配**
```yaml
# 场景1：静态绑定（都不指定SC）
PVC: storageClassName: ""  # 明确指定为空
PV:  storageClassName: ""  # 不指定SC
结果: ✅ 可以绑定

# 场景2：动态供应（PVC指定SC）
PVC: storageClassName: "fast-ssd"
结果: 触发StorageClass创建PV

# 场景3：类名匹配
PVC: storageClassName: "fast-ssd"
PV:  storageClassName: "fast-ssd"
结果: ✅ 可以绑定

# 场景4：类名不匹配
PVC: storageClassName: "fast-ssd"
PV:  storageClassName: "standard-hdd"
结果: ❌ 无法绑定
```

**规则4：Selector标签匹配**
```yaml
# PVC通过selector精确选择PV
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 10Gi
  selector:
    matchLabels:
      environment: production  # 只绑定带此标签的PV
      tier: database
```

---

### 8.1.5 Volume基础类型详解

在深入PV/PVC之前,我们先了解Pod中直接使用的基础Volume类型。

**类型1：emptyDir（临时目录）**

**定义：** Pod创建时自动创建的空目录，Pod删除时数据随之删除。

**典型场景：**
1. 容器间共享数据（同一Pod内的多个容器）
2. 临时缓存（如编译产物、下载文件）
3. 检查点文件（Checkpoint）

**示例：容器间共享日志文件**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: log-sharing-pod
spec:
  containers:
  # 容器1：生成日志
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx  # Nginx日志写入此目录

  # 容器2：采集日志
  - name: log-collector
    image: busybox:1.35
    command: ['sh', '-c', 'tail -f /logs/access.log']
    volumeMounts:
    - name: shared-logs
      mountPath: /logs  # 读取同一目录的日志

  volumes:
  - name: shared-logs
    emptyDir: {}  # Pod级别的临时存储
```

**高级特性：基于内存的emptyDir**

```yaml
volumes:
- name: cache
  emptyDir:
    medium: Memory  # 使用内存而非磁盘
    sizeLimit: 1Gi  # 限制最大使用1GB内存
```

**使用场景：**
- ✅ 高性能缓存（读写速度快）
- ⚠️ 注意内存限制（会占用Pod的内存资源）

---

**类型2：hostPath（主机路径）**

**定义：** 将宿主机的文件或目录挂载到Pod中。

**典型场景：**
1. 访问宿主机的Docker socket（/var/run/docker.sock）
2. 节点日志采集（/var/log）
3. 时区同步（/etc/localtime）

**⚠️ 安全警告：** hostPath允许Pod访问节点文件系统，具有较大安全风险，生产环境应谨慎使用。

**示例1：Docker-in-Docker（访问Docker守护进程）**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: docker-cli-pod
spec:
  containers:
  - name: docker-cli
    image: docker:20.10
    command: ['sh', '-c', 'docker ps && sleep 3600']
    volumeMounts:
    - name: docker-sock
      mountPath: /var/run/docker.sock  # 容器内访问宿主机Docker

  volumes:
  - name: docker-sock
    hostPath:
      path: /var/run/docker.sock  # 宿主机Docker socket
      type: Socket  # 类型校验：必须是socket文件
```

**示例2：节点日志采集（DaemonSet典型场景）**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-logger
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd:v1.14
        volumeMounts:
        - name: varlog
          mountPath: /var/log  # 读取节点日志
          readOnly: true  # 只读挂载，安全最佳实践

      volumes:
      - name: varlog
        hostPath:
          path: /var/log  # 节点的日志目录
          type: Directory  # 类型校验：必须是已存在的目录
```

**hostPath类型校验（type字段）：**

| type值 | 含义 | 行为 |
|--------|------|------|
| `""` | 默认（不检查） | 无论路径是否存在都挂载 |
| `DirectoryOrCreate` | 目录或创建 | 目录不存在时自动创建 |
| `Directory` | 必须是目录 | 目录不存在时Pod启动失败 |
| `FileOrCreate` | 文件或创建 | 文件不存在时自动创建 |
| `File` | 必须是文件 | 文件不存在时Pod启动失败 |
| `Socket` | 必须是Socket | 不是socket文件时启动失败 |
| `CharDevice` | 字符设备 | 用于设备文件（如/dev/xxx） |
| `BlockDevice` | 块设备 | 用于块设备文件 |

**最佳实践：**
- ✅ 始终指定`type`字段进行类型校验
- ✅ 尽量使用`readOnly: true`只读挂载
- ✅ 避免挂载敏感路径（如/etc/shadow）
- ⚠️ 注意Pod迁移到其他节点时，hostPath路径可能不存在

---

**类型3：configMap（配置数据）**

**定义：** 将ConfigMap的数据以文件形式挂载到Pod中。

**示例：Nginx配置文件注入**

```yaml
# 第一步：创建ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    server {
        listen 80;
        server_name example.com;

        location / {
            root /usr/share/nginx/html;
            index index.html;
        }

        location /api {
            proxy_pass http://backend:8080;
        }
    }
  index.html: |
    <html>
      <body><h1>Hello from ConfigMap!</h1></body>
    </html>

---
# 第二步：Pod挂载ConfigMap
apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-config
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    volumeMounts:
    - name: config-volume
      mountPath: /etc/nginx/conf.d  # 配置文件目录
    - name: html-volume
      mountPath: /usr/share/nginx/html  # 静态文件目录

  volumes:
  - name: config-volume
    configMap:
      name: nginx-config
      items:  # 选择性挂载
      - key: nginx.conf
        path: default.conf  # 挂载为default.conf文件

  - name: html-volume
    configMap:
      name: nginx-config
      items:
      - key: index.html
        path: index.html
```

**挂载后的文件结构：**
```bash
# 容器内查看
$ kubectl exec nginx-with-config -- ls -la /etc/nginx/conf.d
total 4
drwxrwxrwx 3 root root  80 Jan 13 10:00 .
drwxr-xr-x 1 root root  41 Jan 13 10:00 ..
drwxr-xr-x 2 root root  28 Jan 13 10:00 ..2024_01_13_10_00_00.123456789
lrwxrwxrwx 1 root root  32 Jan 13 10:00 ..data -> ..2024_01_13_10_00_00.123456789
lrwxrwxrwx 1 root root  19 Jan 13 10:00 default.conf -> ..data/default.conf

# ConfigMap更新时，Kubernetes会自动更新挂载的文件（有约60秒延迟）
```

---

**类型4：secret（敏感数据）**

**定义：** 与ConfigMap类似，但用于存储敏感信息（密码、证书、Token），数据以Base64编码存储。

**示例：MySQL密码注入**

```yaml
# 第一步：创建Secret
apiVersion: v1
kind: Secret
metadata:
  name: mysql-credentials
type: Opaque
data:
  # Base64编码后的值
  username: cm9vdA==  # root
  password: bXlzcWwxMjM0NTY=  # mysql123456

---
# 第二步：Pod使用Secret
apiVersion: v1
kind: Pod
metadata:
  name: mysql-pod
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    env:
    # 方式1：通过环境变量注入
    - name: MYSQL_ROOT_PASSWORD
      valueFrom:
        secretKeyRef:
          name: mysql-credentials
          key: password

    # 方式2：通过文件挂载（更安全）
    volumeMounts:
    - name: credentials
      mountPath: /etc/mysql/conf.d
      readOnly: true  # 只读挂载，防止容器篡改

  volumes:
  - name: credentials
    secret:
      secretName: mysql-credentials
      defaultMode: 0400  # 文件权限：仅所有者可读
```

**Secret vs ConfigMap：**

| 特性 | Secret | ConfigMap |
|------|--------|-----------|
| **数据编码** | Base64编码 | 明文 |
| **API权限** | 更严格（RBAC） | 宽松 |
| **加密存储** | 支持etcd加密（需开启） | 不支持 |
| **文件权限** | 默认0644，可配置 | 默认0644 |
| **典型用途** | 密码、证书、Token | 配置文件、环境变量 |

⚠️ **重要提示：** Base64编码≠加密，Secret在etcd中默认是明文存储，需要配置[etcd加密](https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)才能真正加密。

---

### 8.1.6 PV/PVC核心概念深入

现在我们深入学习持久化存储的核心：PV和PVC。

**PersistentVolume（PV）核心属性：**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-example
  labels:
    type: local
    environment: production
spec:
  # ========== 容量 ==========
  capacity:
    storage: 10Gi  # PV的总容量

  # ========== 访问模式 ==========
  accessModes:
    - ReadWriteOnce  # 单节点读写
    # - ReadOnlyMany   # 多节点只读
    # - ReadWriteMany  # 多节点读写

  # ========== 回收策略 ==========
  persistentVolumeReclaimPolicy: Retain  # PVC删除后的行为
    # Retain：保留数据，手动回收
    # Delete：自动删除PV和底层存储
    # Recycle（废弃）：擦除数据后重新可用

  # ========== 存储类 ==========
  storageClassName: manual  # 关联的StorageClass名称

  # ========== 挂载选项 ==========
  mountOptions:
    - hard
    - nfsvers=4.1

  # ========== 节点亲和性（Local PV必需） ==========
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1  # 此PV只能在node1上使用

  # ========== 具体存储实现（选择其一） ==========
  nfs:  # NFS存储
    server: 192.168.1.100
    path: /data/pv-example

  # 或者其他类型：
  # hostPath:  # 本地路径
  #   path: /mnt/data
  # awsElasticBlockStore:  # AWS EBS
  #   volumeID: vol-0a1b2c3d4e5f
  #   fsType: ext4
  # cephfs:  # CephFS
  #   monitors: [...]
```

**PersistentVolumeClaim（PVC）核心属性：**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-example
  namespace: default  # PVC是命名空间级别资源
spec:
  # ========== 访问模式（必需） ==========
  accessModes:
    - ReadWriteOnce  # 必须与PV的accessModes匹配

  # ========== 资源请求（必需） ==========
  resources:
    requests:
      storage: 8Gi  # 请求8GB存储（PV容量需 >= 8Gi）

  # ========== 存储类（可选） ==========
  storageClassName: manual  # 指定使用哪个StorageClass
    # "" - 明确禁用动态供应，只绑定无StorageClass的PV
    # <不指定> - 使用默认StorageClass（如果集群有默认SC）
    # <类名> - 使用指定的StorageClass

  # ========== 选择器（可选） ==========
  selector:
    matchLabels:
      type: local  # 只绑定带此标签的PV
      environment: production
    # matchExpressions:  # 更复杂的选择逻辑
    #   - key: tier
    #     operator: In
    #     values: [database, cache]

  # ========== 卷模式（可选，1.13+） ==========
  volumeMode: Filesystem  # 文件系统模式（默认）
    # Filesystem - 挂载为文件系统（需要格式化）
    # Block - 块设备模式（直接访问裸设备）
```

---

### 8.1.7 完整实战案例：MySQL持久化存储

让我们通过一个完整的案例，将上述概念串联起来。

**场景：** 部署一个MySQL数据库，数据持久化到NFS存储，即使Pod删除数据也不会丢失。

**前提条件：** 已有NFS服务器（192.168.1.100），共享目录/data/mysql

**步骤1：创建PV（管理员操作）**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv
  labels:
    app: mysql
    environment: production
spec:
  capacity:
    storage: 20Gi  # 提供20GB存储
  accessModes:
    - ReadWriteOnce  # MySQL需要RWO模式
  persistentVolumeReclaimPolicy: Retain  # 删除PVC后保留数据
  storageClassName: nfs-storage  # 存储类名称
  mountOptions:
    - hard  # NFS硬挂载（推荐）
    - nfsvers=4.1  # NFS版本
  nfs:
    server: 192.168.1.100
    path: /data/mysql  # NFS共享路径
```

```bash
# 创建PV
$ kubectl apply -f mysql-pv.yaml
persistentvolume/mysql-pv created

# 查看PV状态
$ kubectl get pv mysql-pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   AGE
mysql-pv   20Gi       RWO            Retain           Available           nfs-storage    5s
# STATUS=Available 表示PV已就绪，等待PVC绑定
```

**步骤2：创建PVC（开发者操作）**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce  # 与PV的accessModes匹配
  resources:
    requests:
      storage: 15Gi  # 请求15GB（小于PV的20GB）
  storageClassName: nfs-storage  # 与PV的storageClassName匹配
  selector:
    matchLabels:
      app: mysql  # 精确选择带mysql标签的PV
      environment: production
```

```bash
# 创建PVC
$ kubectl apply -f mysql-pvc.yaml
persistentvolumeclaim/mysql-pvc created

# 查看PVC状态
$ kubectl get pvc mysql-pvc
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mysql-pvc   Bound    mysql-pv   20Gi       RWO            nfs-storage    3s
# STATUS=Bound 表示PVC已成功绑定到mysql-pv

# 再次查看PV状态
$ kubectl get pv mysql-pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   AGE
mysql-pv   20Gi       RWO            Retain           Bound    default/mysql-pvc   nfs-storage    2m
# STATUS从Available变为Bound
# CLAIM显示绑定到default/mysql-pvc
```

**步骤3：部署MySQL Pod**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    env:
    - name: MYSQL_ROOT_PASSWORD
      value: "mysql123456"  # 生产环境应使用Secret
    ports:
    - containerPort: 3306
      name: mysql
    volumeMounts:
    - name: mysql-storage
      mountPath: /var/lib/mysql  # MySQL数据目录

  volumes:
  - name: mysql-storage
    persistentVolumeClaim:
      claimName: mysql-pvc  # 引用PVC
```

```bash
# 部署MySQL
$ kubectl apply -f mysql-pod.yaml
pod/mysql created

# 等待Pod运行
$ kubectl get pod mysql
NAME    READY   STATUS    RESTARTS   AGE
mysql   1/1     Running   0          30s

# 进入容器验证挂载
$ kubectl exec -it mysql -- df -h /var/lib/mysql
Filesystem                Size  Used Avail Use% Mounted on
192.168.1.100:/data/mysql  20G  1.2G   18G   6% /var/lib/mysql
# 可以看到NFS存储已成功挂载
```

**步骤4：验证数据持久化**

```bash
# 创建测试数据
$ kubectl exec -it mysql -- mysql -uroot -pmysql123456 -e "
CREATE DATABASE testdb;
USE testdb;
CREATE TABLE users (id INT, name VARCHAR(50));
INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob');
SELECT * FROM users;
"
+------+-------+
| id   | name  |
+------+-------+
|    1 | Alice |
|    2 | Bob   |
+------+-------+

# 删除Pod
$ kubectl delete pod mysql
pod "mysql" deleted

# 重新创建Pod（使用相同的PVC）
$ kubectl apply -f mysql-pod.yaml
pod/mysql created

# 等待Pod运行后验证数据
$ kubectl exec -it mysql -- mysql -uroot -pmysql123456 -e "
USE testdb;
SELECT * FROM users;
"
+------+-------+
| id   | name  |
+------+-------+
|    1 | Alice |
|    2 | Bob   |
+------+-------+
# ✅ 数据完整保留！Pod删除重建后数据依然存在
```

**步骤5：清理资源（注意顺序）**

```bash
# 第一步：删除Pod
$ kubectl delete pod mysql
pod "mysql" deleted

# 第二步：删除PVC
$ kubectl delete pvc mysql-pvc
persistentvolumeclaim "mysql-pvc" deleted

# 查看PV状态
$ kubectl get pv mysql-pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM               STORAGECLASS   AGE
mysql-pv   20Gi       RWO            Retain           Released   default/mysql-pvc   nfs-storage    10m
# STATUS变为Released（已释放，但数据仍保留）

# 第三步：根据需要手动删除PV
$ kubectl delete pv mysql-pv  # 删除PV对象
$ # NFS服务器上的/data/mysql目录数据依然存在（Retain策略）
```

**回收策略的影响：**

| 回收策略 | PVC删除后的行为 | NFS服务器数据 | PV状态 |
|---------|---------------|--------------|--------|
| **Retain** | PV变为Released状态<br>需手动删除PV | 保留 | Released |
| **Delete** | 自动删除PV<br>自动删除底层存储数据 | 删除 | （PV已删除） |
| **Recycle**（废弃） | 执行`rm -rf /volume/*`<br>PV变回Available | 清空 | Available |

---

### 8.1.8 常见问题与排查

**问题1：PVC一直处于Pending状态**

```bash
$ kubectl get pvc
NAME        STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mysql-pvc   Pending                                      nfs-storage    5m
```

**排查步骤：**

```bash
# 1. 查看PVC详细事件
$ kubectl describe pvc mysql-pvc
Events:
  Type     Reason              Message
  ----     ------              -------
  Warning  ProvisioningFailed  Failed to provision volume with StorageClass "nfs-storage": storageclass.storage.k8s.io "nfs-storage" not found

# 可能原因：
# ✓ StorageClass不存在
# ✓ 没有匹配的PV（静态供应场景）
# ✓ PV容量不足
# ✓ 访问模式不匹配
# ✓ 标签选择器不匹配

# 2. 检查是否有可用PV
$ kubectl get pv
# 如果没有PV，需要创建或检查StorageClass是否支持动态供应

# 3. 检查StorageClass是否存在
$ kubectl get storageclass nfs-storage
```

**解决方案：**
- 创建匹配的PV（静态供应）
- 创建/修复StorageClass（动态供应）
- 调整PVC的请求容量或访问模式

---

**问题2：Pod无法挂载PVC**

```bash
$ kubectl get pod mysql
NAME    READY   STATUS              RESTARTS   AGE
mysql   0/1     ContainerCreating   0          2m
```

```bash
$ kubectl describe pod mysql
Events:
  Warning  FailedMount  Unable to attach or mount volumes: failed to attach volume "mysql-pv": rpc error: code = Internal desc = Could not mount "192.168.1.100:/data/mysql"
```

**可能原因：**
1. **NFS服务器不可达**
   ```bash
   # 在节点上测试NFS连接
   $ showmount -e 192.168.1.100
   clnt_create: RPC: Port mapper failure - Unable to receive: errno 113 (No route to host)
   ```
   解决：检查网络连通性、防火墙规则

2. **NFS目录不存在**
   ```bash
   # NFS服务器上检查
   $ ls -la /data/mysql
   ls: cannot access '/data/mysql': No such file or directory
   ```
   解决：创建目录并配置NFS导出

3. **节点缺少NFS客户端工具**
   ```bash
   # 节点上安装NFS客户端
   $ apt-get install -y nfs-common  # Debian/Ubuntu
   $ yum install -y nfs-utils       # CentOS/RHEL
   ```

---

**问题3：多个Pod竞争同一RWO PVC**

```yaml
# Deployment尝试创建2个副本，都使用同一个RWO PVC
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
spec:
  replicas: 2  # ❌ 错误：RWO PVC只能被单节点挂载
  template:
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: mysql-pvc  # RWO PVC
```

**现象：** 第二个Pod无法启动，报错"Multi-Attach error"

```bash
$ kubectl describe pod mysql-xxx
Events:
  Warning  FailedAttachVolume  Multi-Attach error for volume "pvc-xxx": Volume is already exclusively attached to one node and can't be attached to another
```

**解决方案：**
- ✅ 使用StatefulSet代替Deployment（下一节详解）
- ✅ 使用RWX模式的PVC（需要支持的存储后端如NFS）
- ✅ 将replicas设置为1

---

### 8.1.9 最佳实践总结

**1. Volume类型选择决策树**

```
需要持久化数据吗？
├─ 否 → emptyDir（容器间共享）或不使用Volume
└─ 是
   ├─ 数据只在本节点使用？
   │  └─ 是 → hostPath（谨慎使用）或local PV
   └─ 否
      ├─ 需要多节点同时读写（RWX）？
      │  ├─ 是 → NFS、CephFS、GlusterFS
      │  └─ 否 → AWS EBS、Azure Disk、GCE PD（RWO）
      └─ 数据是配置/密钥？
         ├─ 敏感数据 → Secret
         └─ 普通配置 → ConfigMap
```

**2. PV/PVC使用规范**

| 规范 | 说明 | 示例 |
|------|------|------|
| **PV命名** | 描述性名称，包含存储类型和用途 | `mysql-prod-nfs-pv`<br>`redis-cache-local-pv` |
| **PVC命名** | 与应用关联，描述用途 | `mysql-data-pvc`<br>`nginx-logs-pvc` |
| **容量规划** | PV容量略大于PVC请求（10-20%缓冲） | PVC请求10Gi，PV提供12Gi |
| **标签管理** | 使用标签标识环境、应用、团队 | `environment: production`<br>`app: mysql`<br>`team: platform` |
| **回收策略** | 生产环境使用Retain<br>开发环境可用Delete | `persistentVolumeReclaimPolicy: Retain` |
| **访问模式** | 根据应用需求精确选择 | 数据库用RWO<br>共享文件用RWX |

**3. 安全最佳实践**

```yaml
# ✅ 推荐配置
spec:
  containers:
  - name: app
    volumeMounts:
    - name: config
      mountPath: /etc/config
      readOnly: true  # 只读挂载配置

  volumes:
  - name: config
    secret:
      secretName: app-secret
      defaultMode: 0400  # 限制文件权限为只读
```

**4. 性能优化建议**

- ✅ 数据库使用SSD后端（设置StorageClass的type参数）
- ✅ 日志数据使用HDD后端（成本优化）
- ✅ 避免在emptyDir中存储大量数据（占用节点空间）
- ✅ 对于高IOPS需求，使用本地SSD（local PV）

---

### 8.1.10 下一节预告

在本节中，我们建立了Kubernetes存储体系的整体认知：

- ✅ 理解了容器存储的挑战和Kubernetes的解决方案
- ✅ 掌握了PV、PVC、StorageClass的三层架构
- ✅ 学习了emptyDir、hostPath、ConfigMap、Secret等基础Volume类型
- ✅ 深入理解了PV/PVC的绑定机制和生命周期
- ✅ 通过MySQL案例完整实践了静态存储供应流程

然而,我们还有很多细节需要深入：

- **emptyDir的高级特性**：内存模式、大小限制
- **hostPath的安全隐患**：如何在生产环境安全使用
- **projected卷**：如何组合多个ConfigMap和Secret
- **downwardAPI卷**：如何将Pod元数据注入容器

**在下一节（8.2 Volume详解）中**，我们将深入学习各种Volume类型的高级特性、使用场景和最佳实践，为理解PV/PVC和StorageClass打下坚实基础。

---

**本节完**

*下一节预告：8.2节《Volume详解》- 深入emptyDir、hostPath、projected、downwardAPI等Volume类型的高级特性。*
## 8.2 Volume详解

在上一节中，我们建立了Kubernetes存储体系的整体认知，初步接触了emptyDir、hostPath、ConfigMap、Secret等基础Volume类型。本节将深入这些Volume的高级特性，探索更多实用的Volume类型，并通过丰富的实战案例掌握它们在生产环境中的应用。

---

### 8.2.1 emptyDir高级特性

emptyDir是Kubernetes中最简单但也最实用的Volume类型。在8.1节中我们了解了它的基本用法，现在深入其高级特性。

**核心特性回顾：**
- **生命周期**：与Pod绑定，Pod删除时数据随之删除
- **存储位置**：默认存储在节点的`/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~empty-dir/<volume-name>`
- **典型用途**：临时缓存、容器间数据共享、检查点文件

---

#### 特性1：内存模式（Memory-backed emptyDir）

**配置方式：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: memory-cache-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      limits:
        memory: "512Mi"  # 重要：内存emptyDir会占用容器内存配额
    volumeMounts:
    - name: cache
      mountPath: /cache

  volumes:
  - name: cache
    emptyDir:
      medium: Memory  # 关键配置：使用内存而非磁盘
      sizeLimit: 256Mi  # 限制最大使用256MB内存
```

**工作原理：**

```
传统emptyDir（磁盘模式）
Pod容器 → /cache → emptyDir → 节点磁盘 /var/lib/kubelet/...
                              ↓
                          写入速度：~100-500 MB/s（取决于磁盘类型）

内存emptyDir（medium: Memory）
Pod容器 → /cache → emptyDir → 节点内存 tmpfs
                              ↓
                          写入速度：~5-10 GB/s（内存速度）
```

**性能对比测试：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: emptydir-benchmark
spec:
  containers:
  - name: benchmark
    image: ubuntu:20.04
    command:
    - bash
    - -c
    - |
      echo "=== 测试磁盘emptyDir性能 ==="
      dd if=/dev/zero of=/disk-cache/test.dat bs=1M count=100
      
      echo ""
      echo "=== 测试内存emptyDir性能 ==="
      dd if=/dev/zero of=/memory-cache/test.dat bs=1M count=100
      
      echo ""
      echo "测试完成，保持运行..."
      sleep 3600
    volumeMounts:
    - name: disk-cache
      mountPath: /disk-cache
    - name: memory-cache
      mountPath: /memory-cache

  volumes:
  - name: disk-cache
    emptyDir: {}  # 默认磁盘模式

  - name: memory-cache
    emptyDir:
      medium: Memory
      sizeLimit: 200Mi
```

**执行测试：**

```bash
# 创建测试Pod
$ kubectl apply -f emptydir-benchmark.yaml
pod/emptydir-benchmark created

# 等待Pod运行
$ kubectl wait --for=condition=Ready pod/emptydir-benchmark --timeout=60s

# 查看测试结果
$ kubectl logs emptydir-benchmark
=== 测试磁盘emptyDir性能 ===
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.524876 s, 200 MB/s

=== 测试内存emptyDir性能 ===
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.0153821 s, 6.8 GB/s

# 内存模式速度是磁盘模式的 34 倍！
```

**内存模式的注意事项：**

1. **占用Pod内存配额**
   ```yaml
   spec:
     containers:
     - name: app
       resources:
         limits:
           memory: "512Mi"  # emptyDir的256Mi会计入此限制
       volumeMounts:
       - name: cache
         mountPath: /cache
     volumes:
     - name: cache
       emptyDir:
         medium: Memory
         sizeLimit: 256Mi  # 实际Pod可用内存 = 512Mi - 256Mi = 256Mi
   ```

2. **节点内存压力**
   ```bash
   # 如果节点内存不足，内存emptyDir的数据可能被回收
   $ kubectl describe node node1
   Conditions:
     Type             Status  Reason
     ----             ------  ------
     MemoryPressure   True    KubeletHasInsufficientMemory
   
   # 此时内存emptyDir可能被清空，导致数据丢失
   ```

3. **数据丢失风险**
   - ❌ **不要存储关键数据**：内存断电即丢失
   - ❌ **不要超过sizeLimit**：超出限制Pod可能被驱逐
   - ✅ **适用场景**：高频读写的临时缓存、编译产物

**典型应用场景：**

```yaml
# 场景1：Redis缓存加速
apiVersion: v1
kind: Pod
metadata:
  name: redis-with-memory-cache
spec:
  containers:
  - name: redis
    image: redis:7.0
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    # 禁用持久化，纯内存缓存
    volumeMounts:
    - name: redis-data
      mountPath: /data

  volumes:
  - name: redis-data
    emptyDir:
      medium: Memory
      sizeLimit: 1Gi
```

---

#### 特性2：容量限制（sizeLimit）

**作用：** 限制emptyDir的最大使用空间，防止磁盘/内存被耗尽。

**磁盘模式的sizeLimit：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: disk-limit-test
spec:
  containers:
  - name: writer
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      # 尝试写入150MB数据（超过100Mi限制）
      dd if=/dev/zero of=/data/large-file bs=1M count=150 || echo "写入失败"
      df -h /data
      sleep 3600
    volumeMounts:
    - name: limited-volume
      mountPath: /data

  volumes:
  - name: limited-volume
    emptyDir:
      sizeLimit: 100Mi  # 限制最大100MB
```

**测试结果：**

```bash
$ kubectl logs disk-limit-test
dd: error writing '/data/large-file': No space left on device
写入失败
Filesystem      Size  Used Avail Use% Mounted on
tmpfs           100M  100M     0 100% /data
```

**重要机制：**

1. **Kubelet驱逐机制**
   ```bash
   # 当emptyDir使用超过sizeLimit时
   $ kubectl get pod disk-limit-test
   NAME              READY   STATUS    RESTARTS   AGE
   disk-limit-test   0/1     Evicted   0          2m
   
   $ kubectl describe pod disk-limit-test
   Status:  Failed
   Reason:  Evicted
   Message: Pod ephemeral local storage usage exceeds the total limit of containers 100Mi
   ```

2. **监控emptyDir使用量**
   ```bash
   # 进入Pod查看实际使用量
   $ kubectl exec -it <pod> -- df -h /cache
   Filesystem      Size  Used Avail Use% Mounted on
   overlay         100M   45M   55M  45% /cache
   ```

**最佳实践：**

```yaml
# ✅ 推荐配置
volumes:
- name: build-cache
  emptyDir:
    sizeLimit: 5Gi  # 明确设置限制
    # medium: Memory  # 只有需要极致性能时才用内存模式

# ❌ 不推荐
volumes:
- name: build-cache
  emptyDir: {}  # 没有sizeLimit，可能耗尽节点磁盘
```

---

#### 特性3：容器间数据共享

**场景：** 一个Pod中的多个容器需要共享数据（如日志采集、初始化数据）。

**示例1：Sidecar日志采集模式**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: logging-sidecar
spec:
  containers:
  # 主容器：生成日志
  - name: app
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      while true; do
        echo "$(date) - Application log entry" >> /var/log/app.log
        sleep 1
      done
    volumeMounts:
    - name: logs
      mountPath: /var/log

  # Sidecar容器：采集日志
  - name: log-collector
    image: fluent/fluentd:v1.14
    volumeMounts:
    - name: logs
      mountPath: /var/log
      readOnly: true  # 只读挂载，防止误修改

  volumes:
  - name: logs
    emptyDir:
      sizeLimit: 500Mi
```

**示例2：Init容器准备数据**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: init-container-demo
spec:
  # Init容器：下载配置文件
  initContainers:
  - name: config-downloader
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      echo "Downloading config files..."
      echo "server { listen 80; }" > /config/nginx.conf
      echo "Config downloaded successfully"
    volumeMounts:
    - name: config
      mountPath: /config

  # 主容器：使用配置文件
  containers:
  - name: nginx
    image: nginx:1.21
    volumeMounts:
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true

  volumes:
  - name: config
    emptyDir: {}
```

**验证数据共享：**

```bash
# 创建Pod
$ kubectl apply -f init-container-demo.yaml

# 查看Init容器日志
$ kubectl logs init-container-demo -c config-downloader
Downloading config files...
Config downloaded successfully

# 验证主容器是否能访问配置
$ kubectl exec init-container-demo -- cat /etc/nginx/conf.d/nginx.conf
server { listen 80; }
```

---

### 8.2.2 hostPath安全使用指南

hostPath允许Pod访问节点的文件系统，功能强大但风险极大。生产环境必须谨慎使用。

---

#### 安全风险分析

**风险1：容器逃逸**

```yaml
# ❌ 危险示例：挂载Docker socket
apiVersion: v1
kind: Pod
metadata:
  name: dangerous-pod
spec:
  containers:
  - name: hacker
    image: docker:20.10
    command:
    - sh
    - -c
    - |
      # 攻击者可以通过Docker socket控制宿主机
      docker run -v /:/host --privileged alpine chroot /host bash
      # 现在拥有宿主机root权限！
    volumeMounts:
    - name: docker-sock
      mountPath: /var/run/docker.sock

  volumes:
  - name: docker-sock
    hostPath:
      path: /var/run/docker.sock
      type: Socket
```

**风险2：敏感数据泄露**

```yaml
# ❌ 危险示例：挂载系统敏感目录
volumes:
- name: etc-passwd
  hostPath:
    path: /etc  # 暴露整个/etc目录
    type: Directory

# 攻击者可以读取：
# /etc/shadow   - 用户密码哈希
# /etc/ssh/*    - SSH密钥
# /etc/kubernetes/* - K8s证书和配置
```

**风险3：节点资源耗尽**

```yaml
# ❌ 危险示例：写入大量数据到节点磁盘
volumes:
- name: node-disk
  hostPath:
    path: /var/log/pods  # 写入大量日志
    type: DirectoryOrCreate

# 可能导致节点磁盘被填满，影响所有Pod
```

---

#### 安全使用规范

**规范1：最小权限原则**

```yaml
# ✅ 推荐：只读挂载
apiVersion: v1
kind: Pod
metadata:
  name: safe-hostpath-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: timezone
      mountPath: /etc/localtime
      readOnly: true  # 关键：只读挂载

  volumes:
  - name: timezone
    hostPath:
      path: /etc/localtime
      type: File  # 类型校验：必须是文件
```

**规范2：路径白名单**

```yaml
# ✅ 允许的路径（相对安全）
允许：
  - /etc/localtime           # 时区同步
  - /etc/timezone            # 时区配置
  - /var/log                 # 日志采集（只读）
  - /sys/fs/cgroup           # cgroup信息（只读）

# ❌ 禁止的路径（高危）
禁止：
  - /                        # 根目录
  - /etc                     # 系统配置目录
  - /root                    # root用户目录
  - /var/run/docker.sock     # Docker socket
  - /etc/kubernetes          # K8s配置目录
  - /var/lib/kubelet         # Kubelet数据目录
```

**规范3：使用PodSecurityPolicy限制（K8s 1.21前）**

```yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted-hostpath
spec:
  # 允许的hostPath路径
  allowedHostPaths:
  - pathPrefix: /etc/localtime
    readOnly: true
  - pathPrefix: /var/log
    readOnly: true

  # 禁止挂载Docker socket
  volumes:
  - configMap
  - emptyDir
  - secret
  - hostPath  # 虽然允许hostPath，但通过allowedHostPaths限制路径

  # 禁止特权模式
  privileged: false
  
  # 禁止访问主机网络
  hostNetwork: false
  hostPID: false
  hostIPC: false
```

**规范4：使用Pod Security Standards（K8s 1.23+）**

```yaml
# Namespace级别限制
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

# restricted策略会自动拒绝hostPath（除非特别配置）
```

---

#### 典型安全场景

**场景1：时区同步（安全）**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-with-timezone
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:1.0
        volumeMounts:
        - name: localtime
          mountPath: /etc/localtime
          readOnly: true  # ✅ 只读

      volumes:
      - name: localtime
        hostPath:
          path: /etc/localtime
          type: File  # ✅ 类型校验
```

**场景2：节点日志采集（DaemonSet专用）**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: log-collector
  namespace: kube-system  # ✅ 系统命名空间，受严格RBAC控制
spec:
  selector:
    matchLabels:
      app: log-collector
  template:
    metadata:
      labels:
        app: log-collector
    spec:
      serviceAccountName: log-collector  # ✅ 专用ServiceAccount
      
      containers:
      - name: fluentd
        image: fluent/fluentd:v1.14
        volumeMounts:
        - name: varlog
          mountPath: /var/log
          readOnly: true  # ✅ 只读
        - name: containers
          mountPath: /var/lib/docker/containers
          readOnly: true  # ✅ 只读

      volumes:
      - name: varlog
        hostPath:
          path: /var/log
          type: Directory
      - name: containers
        hostPath:
          path: /var/lib/docker/containers
          type: DirectoryOrCreate

      # ✅ 节点选择器（可选）
      nodeSelector:
        logging: enabled
```

**场景3：监控节点指标（安全）**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: node-exporter
spec:
  hostNetwork: true  # 需要访问主机网络
  hostPID: true      # 需要访问主机进程
  
  containers:
  - name: node-exporter
    image: prom/node-exporter:v1.3.1
    args:
    - --path.procfs=/host/proc
    - --path.sysfs=/host/sys
    - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
    
    volumeMounts:
    - name: proc
      mountPath: /host/proc
      readOnly: true  # ✅ 只读
    - name: sys
      mountPath: /host/sys
      readOnly: true  # ✅ 只读

  volumes:
  - name: proc
    hostPath:
      path: /proc
      type: Directory
  - name: sys
    hostPath:
      path: /sys
      type: Directory
```

---

### 8.2.3 projected卷：组合多个数据源

**定义：** projected卷可以将多个Volume源（ConfigMap、Secret、DownwardAPI、ServiceAccountToken）组合到同一个目录。

**优势：**
- ✅ 统一挂载点，简化配置
- ✅ 避免目录冲突
- ✅ 支持权限统一设置

---

#### 基础语法

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: projected-volume-demo
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: all-in-one
      mountPath: /projected-volume
      readOnly: true

  volumes:
  - name: all-in-one
    projected:
      defaultMode: 0644  # 统一设置文件权限
      sources:
      # 数据源1：ConfigMap
      - configMap:
          name: app-config
          items:
          - key: app.properties
            path: config/app.properties

      # 数据源2：Secret
      - secret:
          name: db-credentials
          items:
          - key: username
            path: secrets/db-user
          - key: password
            path: secrets/db-pass

      # 数据源3：DownwardAPI
      - downwardAPI:
          items:
          - path: labels
            fieldRef:
              fieldPath: metadata.labels
          - path: namespace
            fieldRef:
              fieldPath: metadata.namespace

      # 数据源4：ServiceAccountToken
      - serviceAccountToken:
          path: token
          expirationSeconds: 3600
          audience: my-service
```

**挂载后的目录结构：**

```bash
$ kubectl exec projected-volume-demo -- tree /projected-volume
/projected-volume
├── config
│   └── app.properties       # 来自ConfigMap
├── secrets
│   ├── db-user              # 来自Secret
│   └── db-pass              # 来自Secret
├── labels                   # 来自DownwardAPI
├── namespace                # 来自DownwardAPI
└── token                    # ServiceAccountToken

2 directories, 5 files
```

---

#### 实战案例：应用配置集中管理

**场景：** 一个Web应用需要：
1. 应用配置文件（ConfigMap）
2. 数据库密码（Secret）
3. Pod元数据（DownwardAPI）
4. 服务间认证Token（ServiceAccountToken）

**步骤1：创建ConfigMap和Secret**

```yaml
# 应用配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config
data:
  application.yaml: |
    server:
      port: 8080
    logging:
      level: INFO
    feature-flags:
      new-ui: true

---
# 数据库密码
apiVersion: v1
kind: Secret
metadata:
  name: webapp-secret
type: Opaque
data:
  db-password: bXlzcWwxMjM0NTY=  # mysql123456
  api-key: YWJjZDEyMzQ1Njc4OTA=   # abcd1234567890
```

**步骤2：创建使用projected卷的Pod**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
    version: v1.0
    environment: production
spec:
  serviceAccountName: webapp-sa  # 专用ServiceAccount
  
  containers:
  - name: app
    image: mywebapp:1.0
    env:
    # 从projected卷读取密码（通过环境变量）
    - name: DB_PASSWORD_FILE
      value: /app-config/secrets/db-password
    - name: API_KEY_FILE
      value: /app-config/secrets/api-key
    
    volumeMounts:
    - name: app-config
      mountPath: /app-config
      readOnly: true

  volumes:
  - name: app-config
    projected:
      defaultMode: 0400  # 只读权限（重要：保护敏感数据）
      sources:
      # 1. 应用配置
      - configMap:
          name: webapp-config
          items:
          - key: application.yaml
            path: config/application.yaml

      # 2. 敏感信息
      - secret:
          name: webapp-secret
          items:
          - key: db-password
            path: secrets/db-password
          - key: api-key
            path: secrets/api-key

      # 3. Pod元数据
      - downwardAPI:
          items:
          - path: metadata/pod-name
            fieldRef:
              fieldPath: metadata.name
          - path: metadata/pod-namespace
            fieldRef:
              fieldPath: metadata.namespace
          - path: metadata/pod-ip
            fieldRef:
              fieldPath: status.podIP
          - path: metadata/labels
            fieldRef:
              fieldPath: metadata.labels
          - path: metadata/annotations
            fieldRef:
              fieldPath: metadata.annotations

      # 4. 服务账户Token（用于调用其他K8s服务）
      - serviceAccountToken:
          path: tokens/api-token
          expirationSeconds: 7200  # 2小时过期
          audience: kubernetes.default.svc
```

**步骤3：应用内读取配置**

```python
# Python应用示例
import os
import yaml

# 读取配置文件
with open('/app-config/config/application.yaml') as f:
    config = yaml.safe_load(f)
    print(f"Server port: {config['server']['port']}")

# 读取Secret
with open('/app-config/secrets/db-password') as f:
    db_password = f.read().strip()
    print(f"DB Password loaded: {'*' * len(db_password)}")

# 读取Pod元数据
with open('/app-config/metadata/pod-name') as f:
    pod_name = f.read().strip()
    print(f"Running in Pod: {pod_name}")

with open('/app-config/metadata/pod-ip') as f:
    pod_ip = f.read().strip()
    print(f"Pod IP: {pod_ip}")

# 读取ServiceAccount Token
with open('/app-config/tokens/api-token') as f:
    token = f.read().strip()
    print(f"Token loaded: {token[:20]}...")
```

**验证：**

```bash
# 创建资源
$ kubectl apply -f webapp-config.yaml
$ kubectl apply -f webapp.yaml

# 查看挂载的文件
$ kubectl exec webapp -- ls -la /app-config
total 0
drwxr-xr-x 5 root root  80 Jan 13 15:30 .
drwxr-xr-x 1 root root  20 Jan 13 15:30 ..
drwxr-xr-x 2 root root  40 Jan 13 15:30 config
drwxr-xr-x 2 root root  40 Jan 13 15:30 metadata
drwxr-xr-x 2 root root  40 Jan 13 15:30 secrets
drwxr-xr-x 2 root root  20 Jan 13 15:30 tokens

# 查看配置文件
$ kubectl exec webapp -- cat /app-config/config/application.yaml
server:
  port: 8080
logging:
  level: INFO
feature-flags:
  new-ui: true

# 查看Pod元数据
$ kubectl exec webapp -- cat /app-config/metadata/pod-name
webapp

$ kubectl exec webapp -- cat /app-config/metadata/labels
app="webapp"
environment="production"
version="v1.0"
```

---

### 8.2.4 downwardAPI卷：Pod元数据注入

**定义：** downwardAPI卷允许将Pod的元数据（labels、annotations、资源限制等）以文件形式暴露给容器。

**支持的字段：**

| 字段类别 | fieldRef可用字段 | resourceFieldRef可用字段 |
|---------|----------------|------------------------|
| **基本信息** | `metadata.name`<br>`metadata.namespace`<br>`metadata.uid` | - |
| **标签和注解** | `metadata.labels`<br>`metadata.annotations` | - |
| **网络信息** | `status.podIP`<br>`status.hostIP` | - |
| **服务账户** | `spec.serviceAccountName` | - |
| **节点信息** | `spec.nodeName` | - |
| **资源信息** | - | `limits.cpu`<br>`limits.memory`<br>`requests.cpu`<br>`requests.memory` |

---

#### 基础示例

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: downwardapi-demo
  labels:
    app: myapp
    tier: frontend
  annotations:
    version: "1.0.0"
    build-id: "20240113-abc123"
spec:
  containers:
  - name: app
    image: busybox:1.35
    command: ['sh', '-c', 'sleep 3600']
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "200m"
        memory: "256Mi"
    volumeMounts:
    - name: podinfo
      mountPath: /etc/podinfo

  volumes:
  - name: podinfo
    downwardAPI:
      items:
      # Pod基本信息
      - path: pod-name
        fieldRef:
          fieldPath: metadata.name
      - path: pod-namespace
        fieldRef:
          fieldPath: metadata.namespace
      - path: pod-ip
        fieldRef:
          fieldPath: status.podIP

      # 标签和注解（以键值对形式）
      - path: labels
        fieldRef:
          fieldPath: metadata.labels
      - path: annotations
        fieldRef:
          fieldPath: metadata.annotations

      # 资源限制
      - path: cpu-limit
        resourceFieldRef:
          containerName: app
          resource: limits.cpu
      - path: memory-limit
        resourceFieldRef:
          containerName: app
          resource: limits.memory
```

**查看注入的数据：**

```bash
# 查看Pod名称
$ kubectl exec downwardapi-demo -- cat /etc/podinfo/pod-name
downwardapi-demo

# 查看Pod IP
$ kubectl exec downwardapi-demo -- cat /etc/podinfo/pod-ip
10.244.1.15

# 查看标签
$ kubectl exec downwardapi-demo -- cat /etc/podinfo/labels
app="myapp"
tier="frontend"

# 查看注解
$ kubectl exec downwardapi-demo -- cat /etc/podinfo/annotations
version="1.0.0"
build-id="20240113-abc123"

# 查看资源限制
$ kubectl exec downwardapi-demo -- cat /etc/podinfo/cpu-limit
1  # 200m转换为CPU核心数 = 0.2（显示为整数1是因为单位转换）

$ kubectl exec downwardapi-demo -- cat /etc/podinfo/memory-limit
268435456  # 256Mi = 256 * 1024 * 1024 字节
```

---

#### 实战案例：应用自适应配置

**场景：** 应用需要根据自身的资源配额自动调整参数（如JVM堆大小、线程池大小）。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: java-app-autotune
  labels:
    app: java-app
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
spec:
  containers:
  - name: app
    image: openjdk:11-jre
    command:
    - bash
    - -c
    - |
      # 读取内存限制
      MEMORY_LIMIT=$(cat /etc/podinfo/memory-limit)
      MEMORY_LIMIT_MB=$((MEMORY_LIMIT / 1024 / 1024))
      
      # 自动计算JVM堆大小（80%的Pod内存限制）
      HEAP_SIZE=$((MEMORY_LIMIT_MB * 80 / 100))
      
      # 读取CPU限制
      CPU_LIMIT=$(cat /etc/podinfo/cpu-limit)
      
      echo "Pod Memory Limit: ${MEMORY_LIMIT_MB}MB"
      echo "JVM Heap Size: ${HEAP_SIZE}MB"
      echo "CPU Limit: ${CPU_LIMIT} cores"
      
      # 启动Java应用
      java -Xmx${HEAP_SIZE}m \
           -XX:+UseG1GC \
           -XX:ParallelGCThreads=${CPU_LIMIT} \
           -jar /app/application.jar

    resources:
      requests:
        cpu: "500m"
        memory: "512Mi"
      limits:
        cpu: "1000m"
        memory: "1Gi"

    volumeMounts:
    - name: podinfo
      mountPath: /etc/podinfo
      readOnly: true

  volumes:
  - name: podinfo
    downwardAPI:
      items:
      - path: memory-limit
        resourceFieldRef:
          containerName: app
          resource: limits.memory
          divisor: "1"  # 返回字节数
      - path: cpu-limit
        resourceFieldRef:
          containerName: app
          resource: limits.cpu
          divisor: "1m"  # 返回毫核数
```

**运行结果：**

```bash
$ kubectl logs java-app-autotune
Pod Memory Limit: 1024MB
JVM Heap Size: 819MB
CPU Limit: 1000 cores
```

---

### 8.2.5 subPath和subPathExpr

**问题：** 默认情况下，Volume会覆盖挂载目标目录的所有内容。

```yaml
# ❌ 问题示例
containers:
- name: app
  image: nginx:1.21
  volumeMounts:
  - name: config
    mountPath: /etc/nginx  # 整个/etc/nginx目录被覆盖，原有文件丢失
volumes:
- name: config
  configMap:
    name: nginx-config
```

**解决方案：** 使用`subPath`只挂载Volume中的单个文件。

---

#### subPath基础用法

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  custom.conf: |
    server {
        listen 8080;
        server_name example.com;
    }

---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-subpath
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    volumeMounts:
    - name: config
      mountPath: /etc/nginx/conf.d/custom.conf  # 挂载到具体文件
      subPath: custom.conf  # 只挂载ConfigMap中的custom.conf

  volumes:
  - name: config
    configMap:
      name: nginx-config
```

**验证：**

```bash
# 查看/etc/nginx目录
$ kubectl exec nginx-subpath -- ls -la /etc/nginx
total 40
drwxr-xr-x 1 root root 4096 Jan 13 16:00 .
drwxr-xr-x 1 root root 4096 Jan 13 16:00 ..
drwxr-xr-x 2 root root 4096 Jan 13 16:00 conf.d  # 原有目录保留
-rw-r--r-- 1 root root 1007 Jan  1 12:00 fastcgi_params  # 原有文件保留
-rw-r--r-- 1 root root  648 Jan  1 12:00 mime.types
-rw-r--r-- 1 root root  636 Jan  1 12:00 nginx.conf

# 查看挂载的文件
$ kubectl exec nginx-subpath -- cat /etc/nginx/conf.d/custom.conf
server {
    listen 8080;
    server_name example.com;
}
```

---

#### subPathExpr动态路径

**场景：** 需要根据Pod信息动态生成挂载路径（如按Pod名称隔离日志目录）。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: logger-pod-001
spec:
  containers:
  - name: app
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      echo "Logging from $(hostname)" >> /logs/app.log
      tail -f /logs/app.log
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    volumeMounts:
    - name: logs
      mountPath: /logs
      subPathExpr: $(POD_NAME)  # 动态路径：/var/log/pods/logger-pod-001

  volumes:
  - name: logs
    hostPath:
      path: /var/log/pods
      type: DirectoryOrCreate
```

**验证：**

```bash
# 创建多个Pod
$ kubectl apply -f logger-pod.yaml
$ sed 's/logger-pod-001/logger-pod-002/g' logger-pod.yaml | kubectl apply -f -

# 查看节点目录结构
$ kubectl get pod -o wide  # 找到Pod所在节点
$ ssh node1
$ tree /var/log/pods
/var/log/pods
├── logger-pod-001
│   └── app.log
└── logger-pod-002
    └── app.log

# 每个Pod的日志隔离在独立目录
```

---

### 8.2.6 临时卷（Ephemeral Volumes）

Kubernetes 1.23+引入了更多临时卷类型。

---

#### Generic Ephemeral Volume（通用临时卷）

**定义：** 支持任何StorageClass的临时PVC，Pod删除时自动清理。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-volume-demo
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: scratch
      mountPath: /scratch

  volumes:
  - name: scratch
    ephemeral:
      volumeClaimTemplate:
        metadata:
          labels:
            type: scratch-space
        spec:
          accessModes: [ "ReadWriteOnce" ]
          storageClassName: "fast-ssd"  # 使用SSD StorageClass
          resources:
            requests:
              storage: 1Gi
```

**特点：**
- ✅ 自动创建PVC（命名格式：`<pod-name>-<volume-name>`）
- ✅ Pod删除时自动删除PVC和底层存储
- ✅ 支持所有StorageClass特性（快照、克隆、扩容等）

---

### 8.2.7 完整实战案例：多层Volume组合

**场景：** 部署一个完整的微服务应用，综合使用多种Volume类型。

**应用架构：**
- **应用容器**：Node.js Web服务
- **Sidecar容器**：Nginx反向代理
- **Init容器**：下载静态资源

**存储需求：**
1. 应用配置（ConfigMap）
2. TLS证书（Secret）
3. 临时缓存（emptyDir内存模式）
4. 静态文件（emptyDir共享）
5. 日志存储（hostPath，DaemonSet采集）
6. Pod元数据（downwardAPI）

**完整配置：**

```yaml
# 第一步：创建ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  app.json: |
    {
      "port": 3000,
      "logLevel": "info",
      "cacheEnabled": true
    }
  nginx.conf: |
    upstream app {
        server 127.0.0.1:3000;
    }
    server {
        listen 80;
        location / {
            proxy_pass http://app;
        }
    }

---
# 第二步：创建Secret
apiVersion: v1
kind: Secret
metadata:
  name: tls-secret
type: kubernetes.io/tls
data:
  tls.crt: LS0tLS1CRUdJTi...  # Base64编码的证书
  tls.key: LS0tLS1CRUdJTi...  # Base64编码的私钥

---
# 第三步：部署Pod
apiVersion: v1
kind: Pod
metadata:
  name: microservice-app
  labels:
    app: microservice
    tier: backend
  annotations:
    version: "2.0.0"
    maintainer: "devops@example.com"
spec:
  # Init容器：下载静态资源
  initContainers:
  - name: assets-downloader
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      echo "Downloading assets..."
      mkdir -p /static/css /static/js
      echo "body { color: blue; }" > /static/css/style.css
      echo "console.log('App loaded');" > /static/js/app.js
      echo "Assets downloaded successfully"
    volumeMounts:
    - name: static-files
      mountPath: /static

  # 主容器1：Node.js应用
  containers:
  - name: app
    image: node:16-alpine
    command:
    - node
    - -e
    - |
      const http = require('http');
      const fs = require('fs');
      
      // 读取配置
      const config = JSON.parse(fs.readFileSync('/config/app.json'));
      
      // 读取Pod元数据
      const podName = fs.readFileSync('/podinfo/pod-name', 'utf8');
      const podIP = fs.readFileSync('/podinfo/pod-ip', 'utf8');
      
      const server = http.createServer((req, res) => {
        const log = `${new Date().toISOString()} - ${req.method} ${req.url}\n`;
        fs.appendFileSync('/logs/access.log', log);
        
        res.writeHead(200);
        res.end(`Hello from ${podName} (${podIP})\n`);
      });
      
      server.listen(config.port, () => {
        console.log(`Server running on port ${config.port}`);
      });
    
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "200m"
        memory: "256Mi"
    
    volumeMounts:
    # 配置文件
    - name: app-config
      mountPath: /config
      readOnly: true
    # 临时缓存（内存）
    - name: cache
      mountPath: /cache
    # 静态文件（与Init容器共享）
    - name: static-files
      mountPath: /static
      readOnly: true
    # 日志
    - name: logs
      mountPath: /logs
    # Pod元数据
    - name: podinfo
      mountPath: /podinfo
      readOnly: true

  # 主容器2：Nginx Sidecar
  - name: nginx
    image: nginx:1.21
    ports:
    - containerPort: 80
    volumeMounts:
    # Nginx配置
    - name: nginx-config
      mountPath: /etc/nginx/conf.d/default.conf
      subPath: nginx.conf
    # TLS证书
    - name: tls
      mountPath: /etc/nginx/ssl
      readOnly: true
    # 静态文件（与App容器共享）
    - name: static-files
      mountPath: /usr/share/nginx/html/static
      readOnly: true
    # 日志（与App容器共享）
    - name: logs
      mountPath: /var/log/nginx

  # Volume定义
  volumes:
  # 1. ConfigMap：应用配置
  - name: app-config
    configMap:
      name: app-config
      items:
      - key: app.json
        path: app.json

  # 2. ConfigMap：Nginx配置
  - name: nginx-config
    configMap:
      name: app-config

  # 3. Secret：TLS证书
  - name: tls
    secret:
      secretName: tls-secret
      defaultMode: 0400

  # 4. emptyDir：内存缓存
  - name: cache
    emptyDir:
      medium: Memory
      sizeLimit: 100Mi

  # 5. emptyDir：静态文件共享
  - name: static-files
    emptyDir:
      sizeLimit: 500Mi

  # 6. hostPath：日志持久化
  - name: logs
    hostPath:
      path: /var/log/pods/microservice-app
      type: DirectoryOrCreate

  # 7. downwardAPI：Pod元数据
  - name: podinfo
    downwardAPI:
      items:
      - path: pod-name
        fieldRef:
          fieldPath: metadata.name
      - path: pod-ip
        fieldRef:
          fieldPath: status.podIP
      - path: labels
        fieldRef:
          fieldPath: metadata.labels
```

**部署和验证：**

```bash
# 创建资源
$ kubectl apply -f app-config.yaml
$ kubectl apply -f tls-secret.yaml
$ kubectl apply -f microservice-app.yaml

# 等待Pod运行
$ kubectl wait --for=condition=Ready pod/microservice-app --timeout=60s

# 查看Init容器日志
$ kubectl logs microservice-app -c assets-downloader
Downloading assets...
Assets downloaded successfully

# 查看应用日志
$ kubectl logs microservice-app -c app
Server running on port 3000

# 测试应用
$ kubectl exec microservice-app -c nginx -- curl localhost
Hello from microservice-app (10.244.1.25)

# 验证Volume挂载
$ kubectl exec microservice-app -c app -- ls -la /config
-rw-r--r-- 1 root root  65 Jan 13 17:00 app.json

$ kubectl exec microservice-app -c app -- ls -la /static
drwxr-xr-x 2 root root   40 Jan 13 17:00 css
drwxr-xr-x 2 root root   40 Jan 13 17:00 js

$ kubectl exec microservice-app -c app -- cat /podinfo/labels
app="microservice"
tier="backend"

# 查看日志文件
$ kubectl exec microservice-app -c app -- cat /logs/access.log
2024-01-13T17:05:23.123Z - GET /
2024-01-13T17:05:24.456Z - GET /health
```

---

### 8.2.8 Volume使用最佳实践总结

**1. Volume类型选择决策**

```
需求场景                    → 推荐Volume类型
─────────────────────────────────────────
容器间临时数据共享           → emptyDir
高性能临时缓存               → emptyDir (medium: Memory)
配置文件注入                 → ConfigMap
敏感信息（密码/证书）        → Secret
Pod元数据访问               → downwardAPI
多数据源组合                 → projected
节点文件访问（谨慎）         → hostPath (readOnly: true)
持久化数据                   → PVC (下一节详解)
```

**2. 性能优化建议**

| 场景 | 配置 | 性能提升 |
|------|------|---------|
| **编译缓存** | `emptyDir` + `medium: Memory` | 10-50倍 |
| **静态资源** | `emptyDir` + CDN分发 | 减少网络I/O |
| **日志缓冲** | `emptyDir` + 异步刷盘 | 降低磁盘压力 |
| **大文件读写** | PVC + SSD StorageClass | 2-5倍 |

**3. 安全加固清单**

```yaml
# ✅ 安全配置模板
volumes:
- name: sensitive-config
  secret:
    secretName: app-secret
    defaultMode: 0400  # 只有所有者可读
    
volumeMounts:
- name: sensitive-config
  mountPath: /etc/secrets
  readOnly: true  # 只读挂载

# ✅ hostPath严格限制
- name: logs
  hostPath:
    path: /var/log/app  # 白名单路径
    type: Directory     # 类型校验
volumeMounts:
- name: logs
  mountPath: /logs
  readOnly: true  # 只读（日志采集场景）
```

**4. 资源配额建议**

```yaml
# emptyDir容量规划
volumes:
- name: build-cache
  emptyDir:
    sizeLimit: 5Gi  # ✅ 始终设置限制

containers:
- name: app
  resources:
    limits:
      memory: "1Gi"  # ✅ 内存emptyDir计入此限制
  volumeMounts:
  - name: cache
    mountPath: /cache
    
volumes:
- name: cache
  emptyDir:
    medium: Memory
    sizeLimit: 256Mi  # 实际可用内存 = 1Gi - 256Mi
```

---

### 8.2.9 下一节预告

在本节中，我们深入学习了Volume的高级特性：

- ✅ emptyDir的内存模式和容量限制
- ✅ hostPath的安全风险和使用规范
- ✅ projected卷的多数据源组合
- ✅ downwardAPI卷的Pod元数据注入
- ✅ subPath和subPathExpr的文件级挂载
- ✅ 完整微服务应用的多层Volume组合实战

这些Volume类型适用于临时数据、配置管理和元数据访问。然而，对于需要持久化的数据（如数据库、用户上传文件），我们需要使用**PersistentVolume（PV）和PersistentVolumeClaim（PVC）**。

**在下一节（8.3 持久卷PersistentVolume）中**，我们将深入学习：
- PV的生命周期管理（Available → Bound → Released → Failed）
- 访问模式的深入理解（RWO/RWX/ROX的选择和限制）
- 回收策略的实际影响（Retain/Delete/Recycle）
- 静态供应的完整实战（NFS/iSCSI/Local PV）
- PV容量回收和状态恢复

---

**本节完**

*下一节预告：8.3节《持久卷PersistentVolume》- 深入PV的生命周期、访问模式、回收策略和静态供应实战。*
## 8.3 持久卷（PersistentVolume）

在前两节中，我们学习了Kubernetes存储体系的整体架构和各种Volume类型的使用。emptyDir、hostPath等Volume适用于临时数据和配置管理，但它们的生命周期与Pod紧密绑定，无法满足数据持久化的需求。本节将深入学习**PersistentVolume（PV）**，它是Kubernetes中真正实现数据持久化的核心机制。

---

### 8.3.1 PV生命周期管理

PersistentVolume是集群级别的存储资源，独立于Pod的生命周期存在。理解PV的生命周期是掌握持久化存储的关键。

---

#### PV的五种状态

```
PV生命周期状态机
┌──────────────┐
│  Available   │ ← 初始状态：PV已创建，等待PVC绑定
└──────┬───────┘
       │ PVC创建并匹配
       ▼
┌──────────────┐
│    Bound     │ ← PV与PVC成功绑定，正在被Pod使用
└──────┬───────┘
       │ PVC被删除
       ▼
┌──────────────┐
│   Released   │ ← PV已释放，但数据未清理（Retain策略）
└──────┬───────┘
       │ 手动清理 / 自动回收
       ▼
┌──────────────┐
│   Available  │ ← 重新可用（Recycle策略，已废弃）
│      或       │
│    Deleted   │ ← 自动删除（Delete策略）
└──────┬───────┘
       │ 出现错误
       ▼
┌──────────────┐
│    Failed    │ ← PV回收失败，需要手动干预
└──────────────┘
```

**状态详解：**

| 状态 | 含义 | 可用性 | 典型场景 |
|------|------|--------|---------|
| **Available** | PV空闲可用 | ✅ 可以被PVC绑定 | 刚创建的PV<br>Recycle后的PV |
| **Bound** | 已绑定到PVC | ❌ 不可被其他PVC绑定 | 正在被Pod使用 |
| **Released** | PVC已删除，数据未清理 | ❌ 不可被新PVC绑定 | PVC删除后<br>（Retain策略） |
| **Failed** | 回收失败 | ❌ 需要手动修复 | 自动回收出错<br>存储后端故障 |

---

#### 完整生命周期演示

**场景：** 创建PV → 绑定PVC → 使用 → 删除PVC → 状态变化观察

**步骤1：创建Available状态的PV**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-lifecycle-demo
  labels:
    type: nfs
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain  # 关键：使用Retain观察Released状态
  storageClassName: manual
  nfs:
    server: 192.168.1.100
    path: /data/pv-lifecycle
```

```bash
# 创建PV
$ kubectl apply -f pv-lifecycle-demo.yaml
persistentvolume/pv-lifecycle-demo created

# 查看状态：Available
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Available           manual         5s
```

**步骤2：创建PVC触发绑定 → Bound状态**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-lifecycle-demo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi  # 小于PV的5Gi
  storageClassName: manual
```

```bash
# 创建PVC
$ kubectl apply -f pvc-lifecycle-demo.yaml
persistentvolumeclaim/pvc-lifecycle-demo created

# 查看PVC状态：Bound
$ kubectl get pvc pvc-lifecycle-demo
NAME                 STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle-demo   Bound    pv-lifecycle-demo   5Gi        RWO            manual         3s

# 再次查看PV状态：从Available变为Bound
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                        STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Bound    default/pvc-lifecycle-demo   manual         2m
#                                                               ^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^^
#                                                               状态变化  显示绑定的PVC
```

**步骤3：Pod使用PVC**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-pvc
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: data
      mountPath: /usr/share/nginx/html
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-lifecycle-demo
```

```bash
# 创建Pod
$ kubectl apply -f pod-using-pvc.yaml
pod/pod-using-pvc created

# 写入测试数据
$ kubectl exec pod-using-pvc -- sh -c 'echo "Hello from PV" > /usr/share/nginx/html/index.html'

# 验证数据
$ kubectl exec pod-using-pvc -- cat /usr/share/nginx/html/index.html
Hello from PV
```

**步骤4：删除Pod（PV仍为Bound）**

```bash
# 删除Pod
$ kubectl delete pod pod-using-pvc
pod "pod-using-pvc" deleted

# PV状态依然是Bound（因为PVC还存在）
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                        STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Bound    default/pvc-lifecycle-demo   manual         5m
```

**步骤5：删除PVC → Released状态**

```bash
# 删除PVC
$ kubectl delete pvc pvc-lifecycle-demo
persistentvolumeclaim "pvc-lifecycle-demo" deleted

# PV状态变为Released
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                        STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Released   default/pvc-lifecycle-demo   manual         6m
#                                                               ^^^^^^^^
#                                                               注意：从Bound变为Released

# 查看详细信息
$ kubectl describe pv pv-lifecycle-demo
Name:            pv-lifecycle-demo
Status:          Released
Claim:           default/pvc-lifecycle-demo  # 仍然记录之前的PVC
Reclaim Policy:  Retain
Access Modes:    RWO
Message:         # 可能有消息说明为什么不能重新绑定
```

**步骤6：验证数据是否保留**

```bash
# 在NFS服务器上检查数据
$ ssh 192.168.1.100
$ cat /data/pv-lifecycle/index.html
Hello from PV
# ✅ 数据完整保留！
```

**步骤7：尝试重新绑定（失败）**

```bash
# 尝试创建新的PVC
$ kubectl apply -f pvc-lifecycle-demo.yaml
persistentvolumeclaim/pvc-lifecycle-demo created

# 新PVC无法绑定到Released状态的PV
$ kubectl get pvc pvc-lifecycle-demo
NAME                 STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle-demo   Pending                                      manual         10s
#                    ^^^^^^^
#                    一直处于Pending状态

# PV状态依然是Released
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM   STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Released           manual         10m
```

**步骤8：手动回收PV → 重新Available**

```bash
# 方法1：删除PV并重新创建（简单粗暴）
$ kubectl delete pv pv-lifecycle-demo
$ kubectl apply -f pv-lifecycle-demo.yaml

# 方法2：编辑PV，移除claimRef字段（推荐）
$ kubectl edit pv pv-lifecycle-demo
# 删除以下部分：
#   claimRef:
#     apiVersion: v1
#     kind: PersistentVolumeClaim
#     name: pvc-lifecycle-demo
#     namespace: default
#     resourceVersion: "123456"
#     uid: abcd-1234-5678-efgh

# 保存后，PV状态变为Available
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Available           manual         12m
#                                                               ^^^^^^^^^

# 现在新PVC可以成功绑定
$ kubectl get pvc pvc-lifecycle-demo
NAME                 STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle-demo   Bound    pv-lifecycle-demo   5Gi        RWO            manual         2m
```

---

### 8.3.2 访问模式深入理解

访问模式（Access Modes）定义了PV如何被挂载到节点上。理解访问模式的限制是选择存储方案的关键。

---

#### 三种访问模式详解

**1. ReadWriteOnce（RWO）- 单节点读写**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-rwo
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce  # 单节点读写
  awsElasticBlockStore:
    volumeID: vol-0a1b2c3d4e5f
    fsType: ext4
```

**特点：**
- ✅ **同一节点的多个Pod可以同时挂载**
- ❌ **不同节点的Pod无法同时挂载**
- ✅ **适用于块存储**：AWS EBS、Azure Disk、GCE PD、本地磁盘

**验证实验：**

```yaml
# 实验1：同一节点的2个Pod同时挂载RWO PVC
apiVersion: v1
kind: Pod
metadata:
  name: pod1-rwo
spec:
  nodeSelector:
    kubernetes.io/hostname: node1  # 强制调度到node1
  containers:
  - name: app
    image: busybox:1.35
    command: ['sh', '-c', 'echo "Pod1" >> /data/test.txt && sleep 3600']
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-rwo

---
apiVersion: v1
kind: Pod
metadata:
  name: pod2-rwo
spec:
  nodeSelector:
    kubernetes.io/hostname: node1  # 同样调度到node1
  containers:
  - name: app
    image: busybox:1.35
    command: ['sh', '-c', 'echo "Pod2" >> /data/test.txt && sleep 3600']
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-rwo
```

```bash
# 创建Pod
$ kubectl apply -f pod1-rwo.yaml
$ kubectl apply -f pod2-rwo.yaml

# 两个Pod都成功运行（因为在同一节点）
$ kubectl get pod -o wide
NAME        READY   STATUS    NODE    
pod1-rwo    1/1     Running   node1   
pod2-rwo    1/1     Running   node1   

# 验证数据共享
$ kubectl exec pod1-rwo -- cat /data/test.txt
Pod1
Pod2
# ✅ 同一节点的多个Pod可以同时读写
```

```yaml
# 实验2：不同节点的Pod尝试挂载RWO PVC
apiVersion: v1
kind: Pod
metadata:
  name: pod3-rwo
spec:
  nodeSelector:
    kubernetes.io/hostname: node2  # 调度到不同节点node2
  containers:
  - name: app
    image: busybox:1.35
    command: ['sh', '-c', 'sleep 3600']
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-rwo  # 同一个PVC
```

```bash
# 创建Pod
$ kubectl apply -f pod3-rwo.yaml

# Pod无法启动
$ kubectl get pod pod3-rwo
NAME       READY   STATUS              RESTARTS   AGE
pod3-rwo   0/1     ContainerCreating   0          2m

# 查看详细事件
$ kubectl describe pod pod3-rwo
Events:
  Warning  FailedAttachVolume  Multi-Attach error for volume "pvc-xxx": Volume is already exclusively attached to one node and can't be attached to another
# ❌ RWO模式不允许跨节点挂载
```

---

**2. ReadWriteMany（RWX）- 多节点读写**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-rwx
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteMany  # 多节点读写
  nfs:
    server: 192.168.1.100
    path: /data/shared
```

**特点：**
- ✅ **多个节点的Pod可以同时挂载**
- ✅ **所有Pod可以并发读写**
- ✅ **适用于网络存储**：NFS、GlusterFS、CephFS、Azure Files
- ❌ **不支持块存储**：AWS EBS、Azure Disk不支持RWX

**验证实验：**

```yaml
# 3个Pod分别调度到不同节点，都挂载RWX PVC
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-rwx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      affinity:
        podAntiAffinity:  # 强制分散到不同节点
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: nginx
            topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: nginx:1.21
        volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
      volumes:
      - name: html
        persistentVolumeClaim:
          claimName: pvc-rwx  # RWX模式的PVC
```

```bash
# 创建Deployment
$ kubectl apply -f nginx-rwx.yaml

# 所有Pod都成功运行在不同节点
$ kubectl get pod -o wide
NAME                         READY   STATUS    NODE
nginx-rwx-5d7f8b9c4-abc12    1/1     Running   node1
nginx-rwx-5d7f8b9c4-def34    1/1     Running   node2
nginx-rwx-5d7f8b9c4-ghi56    1/1     Running   node3

# 在一个Pod中写入数据
$ kubectl exec nginx-rwx-5d7f8b9c4-abc12 -- sh -c 'echo "Shared data" > /usr/share/nginx/html/index.html'

# 在其他Pod中读取数据
$ kubectl exec nginx-rwx-5d7f8b9c4-def34 -- cat /usr/share/nginx/html/index.html
Shared data

$ kubectl exec nginx-rwx-5d7f8b9c4-ghi56 -- cat /usr/share/nginx/html/index.html
Shared data
# ✅ 所有节点的Pod都能读写共享数据
```

---

**3. ReadOnlyMany（ROX）- 多节点只读**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-rox
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadOnlyMany  # 多节点只读
  nfs:
    server: 192.168.1.100
    path: /data/static-content
```

**特点：**
- ✅ **多个节点的Pod可以同时挂载**
- ✅ **所有Pod只能读取，不能写入**
- ✅ **典型场景**：静态资源分发、配置文件共享

**验证实验：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-rox
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: static
      mountPath: /usr/share/nginx/html
      readOnly: true  # 强制只读
  volumes:
  - name: static
    persistentVolumeClaim:
      claimName: pvc-rox  # ROX模式的PVC
```

```bash
# 尝试写入数据
$ kubectl exec pod-rox -- sh -c 'echo "test" > /usr/share/nginx/html/test.txt'
sh: can't create /usr/share/nginx/html/test.txt: Read-only file system
# ❌ 只读文件系统，无法写入

# 可以正常读取
$ kubectl exec pod-rox -- cat /usr/share/nginx/html/index.html
<html>...</html>
# ✅ 读取成功
```

---

#### 访问模式与存储后端兼容性

| 存储后端 | RWO | RWX | ROX | 说明 |
|---------|-----|-----|-----|------|
| **AWS EBS** | ✅ | ❌ | ❌ | 块存储，仅支持单节点 |
| **Azure Disk** | ✅ | ❌ | ❌ | 块存储，仅支持单节点 |
| **GCE PD** | ✅ | ❌ | ✅ | 块存储+只读多节点 |
| **NFS** | ✅ | ✅ | ✅ | 网络文件系统，全支持 |
| **CephFS** | ✅ | ✅ | ✅ | 分布式文件系统，全支持 |
| **GlusterFS** | ✅ | ✅ | ✅ | 分布式文件系统，全支持 |
| **iSCSI** | ✅ | ❌ | ❌ | 块存储，仅支持单节点 |
| **Local PV** | ✅ | ❌ | ❌ | 本地磁盘，仅支持单节点 |
| **Azure Files** | ✅ | ✅ | ✅ | SMB协议，全支持 |

**重要提示：**
- ⚠️ **PV的accessModes必须包含PVC请求的所有模式**
  ```yaml
  # ✅ 可以绑定
  PV: accessModes: [ReadWriteOnce, ReadWriteMany]
  PVC: accessModes: [ReadWriteOnce]
  
  # ❌ 无法绑定
  PV: accessModes: [ReadWriteOnce]
  PVC: accessModes: [ReadWriteMany]
  ```

---

### 8.3.3 回收策略详解

回收策略（Reclaim Policy）决定了PVC删除后，PV和底层存储数据的处理方式。

---

#### 三种回收策略对比

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-example
spec:
  persistentVolumeReclaimPolicy: Retain  # 回收策略
  # 可选值：
  # - Retain：保留（默认静态PV）
  # - Delete：删除（默认动态PV）
  # - Recycle：回收（已废弃）
```

| 策略 | PVC删除后的行为 | PV状态 | 底层存储 | 适用场景 |
|------|---------------|--------|---------|---------|
| **Retain** | PV变为Released<br>需手动清理 | Released | 数据保留 | 生产环境数据库<br>重要数据备份 |
| **Delete** | 自动删除PV<br>自动删除存储数据 | （PV已删除） | 数据删除 | 临时数据<br>可重建数据 |
| **Recycle**<br>（已废弃） | 执行`rm -rf /volume/*`<br>PV重新可用 | Available | 数据清空 | 不推荐使用 |

---

#### Retain策略实战

**场景：** 数据库数据需要在PVC删除后保留，用于备份或迁移。

```yaml
# PV配置
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv-retain
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain  # 保留策略
  storageClassName: manual
  hostPath:
    path: /data/mysql-retain
    type: DirectoryOrCreate

---
# PVC配置
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: manual

---
# MySQL Pod
apiVersion: v1
kind: Pod
metadata:
  name: mysql
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    env:
    - name: MYSQL_ROOT_PASSWORD
      value: "mysql123"
    volumeMounts:
    - name: data
      mountPath: /var/lib/mysql
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: mysql-pvc
```

**操作流程：**

```bash
# 1. 创建资源
$ kubectl apply -f mysql-pv-retain.yaml
$ kubectl apply -f mysql-pvc.yaml
$ kubectl apply -f mysql-pod.yaml

# 2. 写入测试数据
$ kubectl exec mysql -- mysql -uroot -pmysql123 -e "
CREATE DATABASE testdb;
USE testdb;
CREATE TABLE users (id INT, name VARCHAR(50));
INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob');
"

# 3. 删除Pod和PVC
$ kubectl delete pod mysql
$ kubectl delete pvc mysql-pvc

# 4. 查看PV状态
$ kubectl get pv mysql-pv-retain
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM               STORAGECLASS   AGE
mysql-pv-retain    20Gi       RWO            Retain           Released   default/mysql-pvc   manual         5m
#                                            ^^^^^^           ^^^^^^^^
#                                            Retain策略        Released状态

# 5. 验证数据是否保留
$ ls -la /data/mysql-retain/
total 180M
drwxr-x--- 6 999  999   4.0K Jan 14 10:00 testdb/      # ✅ 数据库文件完整保留
drwxr-x--- 2 999  999   4.0K Jan 14 10:00 mysql/
-rw-r----- 1 999  999    56M Jan 14 10:00 ib_logfile0
-rw-r----- 1 999  999    12M Jan 14 10:00 ibdata1

# 6. 手动恢复数据（创建新PVC前，先清理PV的claimRef）
$ kubectl patch pv mysql-pv-retain -p '{"spec":{"claimRef": null}}'

# 7. 创建新PVC绑定到同一PV
$ kubectl apply -f mysql-pvc.yaml

# 8. 创建新Pod验证数据
$ kubectl apply -f mysql-pod.yaml
$ kubectl exec mysql -- mysql -uroot -pmysql123 -e "USE testdb; SELECT * FROM users;"
+------+-------+
| id   | name  |
+------+-------+
|    1 | Alice |
|    2 | Bob   |
+------+-------+
# ✅ 数据完整恢复！
```

---

#### Delete策略实战

**场景：** 动态供应的临时数据，PVC删除后自动清理存储。

```yaml
# StorageClass配置（动态供应）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-delete
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
reclaimPolicy: Delete  # StorageClass级别的回收策略
volumeBindingMode: Immediate

---
# PVC配置（动态供应）
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: temp-data-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: fast-delete  # 使用Delete策略的StorageClass
```

**操作流程：**

```bash
# 1. 创建PVC（自动创建PV）
$ kubectl apply -f temp-data-pvc.yaml

# 2. 查看自动创建的PV
$ kubectl get pv
NAME                                       CAPACITY   RECLAIM POLICY   STATUS   CLAIM
pvc-abc123-def456-ghi789                   10Gi       Delete           Bound    default/temp-data-pvc
#                                                     ^^^^^^
#                                                     自动设置为Delete

# 3. 使用PVC
$ kubectl run test-pod --image=nginx --overrides='
{
  "spec": {
    "volumes": [{
      "name": "data",
      "persistentVolumeClaim": {"claimName": "temp-data-pvc"}
    }],
    "containers": [{
      "name": "nginx",
      "image": "nginx:1.21",
      "volumeMounts": [{
        "name": "data",
        "mountPath": "/data"
      }]
    }]
  }
}'

# 4. 写入数据
$ kubectl exec test-pod -- sh -c 'echo "Temporary data" > /data/test.txt'

# 5. 删除PVC
$ kubectl delete pvc temp-data-pvc
persistentvolumeclaim "temp-data-pvc" deleted

# 6. PV自动删除
$ kubectl get pv
No resources found
# ✅ PV已被自动删除

# 7. 底层AWS EBS卷也被删除
$ aws ec2 describe-volumes --volume-ids vol-abc123
An error occurred (InvalidVolume.NotFound)
# ✅ 底层存储也被自动删除
```

---

#### Recycle策略（已废弃）

**定义：** 执行`rm -rf /volume/*`清空数据，PV重新变为Available。

**为什么废弃：**
- ❌ **不安全**：简单的`rm -rf`无法彻底清除敏感数据
- ❌ **不可靠**：某些文件系统可能清理失败
- ❌ **不灵活**：无法自定义清理逻辑

**替代方案：** 使用Dynamic Provisioning（动态供应）+ Delete策略。

---

### 8.3.4 静态供应完整实战

静态供应（Static Provisioning）是指管理员手动创建PV，开发者创建PVC进行绑定。适用于已有存储资源或需要精细控制的场景。

---

#### 实战1：NFS静态供应

**场景：** 企业内部有NFS服务器，提供共享存储给Kubernetes集群。

**前提条件：**
```bash
# NFS服务器配置（192.168.1.100）
$ sudo apt-get install -y nfs-kernel-server

# 创建共享目录
$ sudo mkdir -p /data/k8s-nfs/{pv1,pv2,pv3}
$ sudo chmod 777 /data/k8s-nfs/*

# 配置NFS导出
$ sudo tee /etc/exports <<EOF
/data/k8s-nfs/pv1  *(rw,sync,no_subtree_check,no_root_squash)
/data/k8s-nfs/pv2  *(rw,sync,no_subtree_check,no_root_squash)
/data/k8s-nfs/pv3  *(rw,sync,no_subtree_check,no_root_squash)
EOF

# 应用配置
$ sudo exportfs -ra
$ sudo systemctl restart nfs-kernel-server

# 验证导出
$ showmount -e localhost
Export list for localhost:
/data/k8s-nfs/pv1 *
/data/k8s-nfs/pv2 *
/data/k8s-nfs/pv3 *
```

**步骤1：创建多个NFS PV**

```yaml
# nfs-pvs.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-1
  labels:
    tier: gold
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    server: 192.168.1.100
    path: /data/k8s-nfs/pv1

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-2
  labels:
    tier: silver
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  nfs:
    server: 192.168.1.100
    path: /data/k8s-nfs/pv2

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-3
  labels:
    tier: bronze
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce  # 注意：NFS支持RWX，但这里设置为RWO演示访问模式匹配
  persistentVolumeReclaimPolicy: Delete
  storageClassName: nfs
  nfs:
    server: 192.168.1.100
    path: /data/k8s-nfs/pv3
```

```bash
# 创建PV
$ kubectl apply -f nfs-pvs.yaml
persistentvolume/nfs-pv-1 created
persistentvolume/nfs-pv-2 created
persistentvolume/nfs-pv-3 created

# 查看PV
$ kubectl get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      STORAGECLASS   AGE
nfs-pv-1   10Gi       RWX            Retain           Available   nfs            10s
nfs-pv-2   5Gi        RWX            Retain           Available   nfs            10s
nfs-pv-3   2Gi        RWO            Delete           Available   nfs            10s
```

**步骤2：创建PVC并验证绑定规则**

```yaml
# pvc-tier-gold.yaml（使用标签选择器）
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-gold
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 8Gi  # 小于10Gi
  storageClassName: nfs
  selector:
    matchLabels:
      tier: gold  # 精确选择gold标签的PV
```

```bash
# 创建PVC
$ kubectl apply -f pvc-tier-gold.yaml

# 验证绑定到nfs-pv-1
$ kubectl get pvc pvc-gold
NAME       STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-gold   Bound    nfs-pv-1   10Gi       RWX            nfs            5s

$ kubectl get pv nfs-pv-1
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   AGE
nfs-pv-1   10Gi       RWX            Retain           Bound    default/pvc-gold   nfs            2m
```

**步骤3：部署应用使用NFS存储**

```yaml
# wordpress-nfs.yaml
apiVersion: v1
kind: Service
metadata:
  name: wordpress
spec:
  selector:
    app: wordpress
  ports:
  - port: 80
  type: LoadBalancer

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
spec:
  replicas: 3  # 多副本共享NFS存储
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:6.0-apache
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql:3306
        - name: WORDPRESS_DB_PASSWORD
          value: "wordpress123"
        ports:
        - containerPort: 80
        volumeMounts:
        - name: wordpress-data
          mountPath: /var/www/html
      volumes:
      - name: wordpress-data
        persistentVolumeClaim:
          claimName: pvc-gold  # 使用NFS RWX PVC
```

```bash
# 部署WordPress
$ kubectl apply -f wordpress-nfs.yaml

# 验证3个Pod都运行在不同节点
$ kubectl get pod -o wide
NAME                         READY   STATUS    NODE
wordpress-7d8f9b6c5-abc12    1/1     Running   node1
wordpress-7d8f9b6c5-def34    1/1     Running   node2
wordpress-7d8f9b6c5-ghi56    1/1     Running   node3

# 访问WordPress初始化
$ kubectl get svc wordpress
NAME        TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)        AGE
wordpress   LoadBalancer   10.96.100.200   203.0.113.10    80:30080/TCP   2m

# 浏览器访问 http://203.0.113.10 完成WordPress安装

# 验证NFS服务器上的数据
$ ssh 192.168.1.100
$ ls -la /data/k8s-nfs/pv1/
total 15M
drwxr-xr-x 5 www-data www-data 4.0K Jan 14 11:00 wp-admin/
drwxr-xr-x 9 www-data www-data 4.0K Jan 14 11:00 wp-content/
drwxr-xr-x 2 www-data www-data 4.0K Jan 14 11:00 wp-includes/
-rw-r--r-- 1 www-data www-data 405  Jan 14 11:00 index.php
# ✅ WordPress文件成功写入NFS共享存储
```

---

#### 实战2：Local PV静态供应

**场景：** 需要高性能本地SSD，用于数据库或缓存应用。

**重要特性：**
- ✅ **性能最佳**：本地SSD，无网络延迟
- ⚠️ **节点亲和性**：Pod必须调度到PV所在节点
- ❌ **无法跨节点迁移**：节点故障时数据不可用

**步骤1：准备节点本地磁盘**

```bash
# 在每个节点创建本地目录
$ ssh node1
$ sudo mkdir -p /mnt/disks/ssd1
$ sudo chmod 777 /mnt/disks/ssd1

$ ssh node2
$ sudo mkdir -p /mnt/disks/ssd2
$ sudo chmod 777 /mnt/disks/ssd2
```

**步骤2：创建Local PV**

```yaml
# local-pv-node1.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-node1
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1  # 本地路径
  nodeAffinity:  # 关键：必须设置节点亲和性
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1  # 此PV只能在node1使用

---
# local-pv-node2.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-node2
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd2
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node2
```

```bash
# 创建Local PV
$ kubectl apply -f local-pv-node1.yaml
$ kubectl apply -f local-pv-node2.yaml

# 查看PV（注意NODE AFFINITY列）
$ kubectl get pv
NAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      STORAGECLASS    VOLUMEATTRIBUTESCLASS   AGE
local-pv-node1    100Gi      RWO            Retain           Available   local-storage   <unset>                 10s
local-pv-node2    100Gi      RWO            Retain           Available   local-storage   <unset>                 10s

$ kubectl describe pv local-pv-node1 | grep -A 5 "Node Affinity"
Node Affinity:
  Required Terms:
    Term 0:  kubernetes.io/hostname in [node1]
```

**步骤3：部署StatefulSet使用Local PV**

```yaml
# redis-statefulset-local.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis
spec:
  clusterIP: None  # Headless Service
  selector:
    app: redis
  ports:
  - port: 6379

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
spec:
  serviceName: redis
  replicas: 2
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7.0
        ports:
        - containerPort: 6379
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: local-storage
      resources:
        requests:
          storage: 50Gi
```

```bash
# 部署StatefulSet
$ kubectl apply -f redis-statefulset-local.yaml

# 查看Pod调度情况
$ kubectl get pod -o wide
NAME      READY   STATUS    NODE    PV
redis-0   1/1     Running   node1   local-pv-node1
redis-1   1/1     Running   node2   local-pv-node2
# ✅ Pod根据PV的nodeAffinity自动调度到正确节点

# 查看PVC绑定
$ kubectl get pvc
NAME           STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS    AGE
data-redis-0   Bound    local-pv-node1   100Gi      RWO            local-storage   2m
data-redis-1   Bound    local-pv-node2   100Gi      RWO            local-storage   2m

# 测试性能
$ kubectl exec redis-0 -- redis-benchmark -t set,get -n 100000 -q
SET: 125000.00 requests per second
GET: 142857.14 requests per second
# ✅ 本地SSD性能远超网络存储
```

---

### 8.3.5 PV容量管理和扩容

**容量匹配规则：**
- PV容量 >= PVC请求容量
- PVC实际获得的容量 = PV容量（不是PVC请求容量）

**示例：**

```yaml
# PV: 100Gi
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-large
spec:
  capacity:
    storage: 100Gi
  # ...

---
# PVC: 请求10Gi
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-small
spec:
  resources:
    requests:
      storage: 10Gi
  # ...
```

```bash
# PVC绑定后，容量显示为100Gi（而不是10Gi）
$ kubectl get pvc pvc-small
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-small   Bound    pv-large   100Gi      RWO            manual         10s
#                               ^^^^^
#                               PV的容量

# 浪费了90Gi空间！
```

**最佳实践：** PV容量应精确匹配PVC请求，或使用动态供应自动匹配。

---

### 8.3.6 PV状态恢复和故障排查

#### 场景1：Released状态的PV无法重新绑定

**问题：**

```bash
$ kubectl get pv
NAME     CAPACITY   STATUS     CLAIM             STORAGECLASS   AGE
my-pv    10Gi       Released   default/old-pvc   manual         10m

$ kubectl get pvc new-pvc
NAME      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
new-pvc   Pending                                      manual         2m
# PVC无法绑定到Released状态的PV
```

**原因：** Released状态的PV仍然保留着之前PVC的`claimRef`引用。

**解决方案1：编辑PV，删除claimRef**

```bash
$ kubectl edit pv my-pv
# 删除以下部分：
#   claimRef:
#     apiVersion: v1
#     kind: PersistentVolumeClaim
#     name: old-pvc
#     namespace: default
#     resourceVersion: "123456"
#     uid: abcd-1234-5678

# 或使用patch命令
$ kubectl patch pv my-pv -p '{"spec":{"claimRef": null}}'

# PV状态变为Available
$ kubectl get pv my-pv
NAME    CAPACITY   STATUS      CLAIM   STORAGECLASS   AGE
my-pv   10Gi       Available           manual         11m

# 新PVC成功绑定
$ kubectl get pvc new-pvc
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
new-pvc   Bound    my-pv    10Gi       RWO            manual         3m
```

**解决方案2：删除PV并重新创建**

```bash
# 备份数据（如果需要）
$ cp -r /data/pv-path /backup/

# 删除PV
$ kubectl delete pv my-pv

# 重新创建PV
$ kubectl apply -f my-pv.yaml

# PVC自动绑定
```

---

#### 场景2：PV一直处于Pending状态

**可能原因：**

1. **没有匹配的PV**
   ```bash
   $ kubectl get pv
   No resources found  # 没有PV
   
   $ kubectl get pvc
   NAME      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
   my-pvc    Pending                                      manual         5m
   ```
   
   **解决：** 创建匹配的PV

2. **容量不足**
   ```bash
   $ kubectl get pv
   NAME    CAPACITY   STATUS      CLAIM   STORAGECLASS   AGE
   small   5Gi        Available           manual         1m
   
   $ kubectl get pvc
   NAME      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
   large     Pending                                      manual         1m
   
   $ kubectl describe pvc large
   Spec:
     Resources:
       Requests:
         storage:  10Gi  # 请求10Gi，但只有5Gi的PV
   ```
   
   **解决：** 创建更大容量的PV

3. **访问模式不匹配**
   ```bash
   $ kubectl describe pvc my-pvc
   Spec:
     Access Modes:
       ReadWriteMany  # 请求RWX
   
   $ kubectl describe pv my-pv
   Spec:
     Access Modes:
       ReadWriteOnce  # 只支持RWO
   ```
   
   **解决：** 使用支持RWX的存储后端（如NFS）

4. **StorageClass不匹配**
   ```bash
   $ kubectl describe pvc my-pvc
   Spec:
     Storage Class Name:  fast-ssd
   
   $ kubectl get pv
   NAME    CAPACITY   STORAGECLASS   STATUS
   my-pv   10Gi       slow-hdd       Available
   ```
   
   **解决：** 创建匹配StorageClass的PV

---

### 8.3.7 PV最佳实践总结

**1. 生命周期管理**
```yaml
# ✅ 生产环境推荐配置
apiVersion: v1
kind: PersistentVolume
metadata:
  name: prod-db-pv
  labels:
    environment: production
    app: database
    backup: required
spec:
  persistentVolumeReclaimPolicy: Retain  # 生产数据使用Retain
  # ...
```

**2. 访问模式选择**
```
应用类型              → 推荐访问模式
───────────────────────────────────
单实例数据库          → RWO (块存储)
多副本只读服务        → ROX (NFS)
多副本读写服务        → RWX (NFS/CephFS)
StatefulSet          → RWO (每个Pod独立PV)
```

**3. 容量规划**
```yaml
# ✅ 精确匹配容量
PV:  capacity: storage: 100Gi
PVC: requests: storage: 100Gi

# ❌ 避免浪费
PV:  capacity: storage: 100Gi
PVC: requests: storage: 10Gi  # 浪费90Gi
```

**4. 标签管理**
```yaml
metadata:
  labels:
    environment: production  # 环境标识
    tier: database           # 应用层级
    performance: high        # 性能等级
    backup-policy: daily     # 备份策略
```

---

### 8.3.8 下一节预告

在本节中，我们深入学习了PersistentVolume的核心知识：

- ✅ PV生命周期的五种状态（Available/Bound/Released/Failed）
- ✅ 访问模式的深入理解（RWO/RWX/ROX及存储后端兼容性）
- ✅ 回收策略的实际影响（Retain/Delete/Recycle）
- ✅ NFS和Local PV的完整静态供应实战
- ✅ PV容量管理和状态恢复

然而，静态供应需要管理员手动创建PV，当集群规模扩大、PVC数量增多时，手动管理变得不可行。Kubernetes通过**PersistentVolumeClaim（PVC）**和**动态供应**解决了这一难题。

**在下一节（8.4 持久卷声明PersistentVolumeClaim）中**，我们将深入学习：
- PVC的绑定机制和选择器
- PVC的状态管理和生命周期
- PVC的扩容操作（Volume Expansion）
- PVC的克隆和快照（CSI Snapshot）
- PVC与Pod的绑定关系

---

**本节完**

*下一节预告：8.4节《持久卷声明PersistentVolumeClaim》- 深入PVC绑定机制、扩容、克隆和快照操作。*
## 8.4 持久卷声明（PersistentVolumeClaim）

在上一节中，我们深入学习了PersistentVolume的生命周期、访问模式和静态供应。PV是集群管理员创建的存储资源，而**PersistentVolumeClaim（PVC）**则是用户对存储的请求声明。PVC将用户与底层存储实现解耦，使开发者无需关心具体使用的是NFS、Ceph还是云盘，只需声明"我需要10GB RWO存储"即可。

本节将深入学习PVC的核心机制，掌握如何高效使用和管理持久化存储。

---

### 8.4.1 PVC绑定机制详解

PVC的核心功能是与PV建立绑定关系。理解绑定机制是掌握PVC的关键。

---

#### 绑定过程详解

```
PVC绑定流程
┌─────────────────────────────────────────────────────────────┐
│  第1步：用户创建PVC                                            │
│  apiVersion: v1                                              │
│  kind: PersistentVolumeClaim                                 │
│  spec:                                                       │
│    accessModes: [ReadWriteOnce]                              │
│    resources:                                                │
│      requests:                                               │
│        storage: 10Gi                                         │
│    storageClassName: fast-ssd                                │
└────────────────────┬────────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────────┐
│  第2步：PV Controller扫描可用PV                                │
│  筛选条件：                                                    │
│  ✓ storageClassName匹配（或都为空）                            │
│  ✓ 访问模式兼容（PV包含PVC请求的所有模式）                       │
│  ✓ 容量满足（PV >= PVC请求）                                   │
│  ✓ 选择器匹配（如果PVC指定了selector）                          │
│  ✓ PV状态为Available                                          │
└────────────────────┬────────────────────────────────────────┘
                     │
      ┌──────────────┴──────────────┐
      │                             │
      ▼                             ▼
┌────────────┐              ┌────────────────┐
│ 找到匹配PV  │              │ 未找到匹配PV     │
└─────┬──────┘              └─────┬──────────┘
      │                           │
      ▼                           ▼
┌────────────┐              ┌────────────────┐
│ 第3步：     │              │ StorageClass   │
│ 建立绑定    │              │ 存在？         │
│            │              └─────┬──────────┘
│ PVC.Status │                    │
│ = Bound    │        ┌───────────┴──────────┐
│            │        │                      │
│ PV.Status  │        ▼                      ▼
│ = Bound    │   ┌─────────┐          ┌──────────┐
└─────┬──────┘   │ 触发动态 │          │ PVC保持  │
      │          │ 供应     │          │ Pending  │
      │          └─────┬───┘          └──────────┘
      │                │
      │                ▼
      │          ┌─────────────┐
      │          │ Provisioner │
      │          │ 创建PV      │
      │          └─────┬───────┘
      │                │
      └────────────────┴────────────────┐
                                        │
                                        ▼
                              ┌──────────────────┐
                              │ 第4步：绑定完成   │
                              │ PVC可被Pod使用   │
                              └──────────────────┘
```

---

#### 绑定规则优先级

**规则1：精确匹配优先**

```yaml
# 场景：有3个PV
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-10gi
spec:
  capacity:
    storage: 10Gi
  accessModes: [ReadWriteOnce]
  storageClassName: standard

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-20gi
spec:
  capacity:
    storage: 20Gi
  accessModes: [ReadWriteOnce]
  storageClassName: standard

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-50gi
spec:
  capacity:
    storage: 50Gi
  accessModes: [ReadWriteOnce]
  storageClassName: standard
```

```yaml
# PVC请求10Gi
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-test
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard
```

```bash
# PVC会绑定到pv-10gi（最小满足容量的PV）
$ kubectl get pvc pvc-test
NAME       STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-test   Bound    pv-10gi   10Gi       RWO            standard       5s
#                   ^^^^^^^
#                   精确匹配，避免浪费
```

---

**规则2：选择器优先**

```yaml
# PV带有标签
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-ssd-prod
  labels:
    type: ssd
    environment: production
spec:
  capacity:
    storage: 100Gi
  accessModes: [ReadWriteOnce]
  storageClassName: premium

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hdd-dev
  labels:
    type: hdd
    environment: development
spec:
  capacity:
    storage: 100Gi
  accessModes: [ReadWriteOnce]
  storageClassName: premium
```

```yaml
# PVC使用选择器精确匹配
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: db-pvc
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 50Gi
  storageClassName: premium
  selector:
    matchLabels:
      type: ssd              # 必须匹配
      environment: production
```

```bash
# PVC只会绑定到pv-ssd-prod
$ kubectl get pvc db-pvc
NAME     STATUS   VOLUME         CAPACITY   ACCESS MODES   STORAGECLASS   AGE
db-pvc   Bound    pv-ssd-prod    100Gi      RWO            premium        3s

# pv-hdd-dev虽然容量足够，但标签不匹配，不会绑定
$ kubectl get pv pv-hdd-dev
NAME         CAPACITY   STATUS      CLAIM   STORAGECLASS   AGE
pv-hdd-dev   100Gi      Available           premium        1m
```

---

**规则3：访问模式匹配**

```yaml
# 场景：PV支持RWX
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 50Gi
  accessModes:
    - ReadWriteMany
    - ReadWriteOnce  # 同时支持RWX和RWO
  nfs:
    server: 192.168.1.100
    path: /data/nfs
```

```yaml
# PVC请求RWO
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-rwo
spec:
  accessModes:
    - ReadWriteOnce  # 只请求RWO
  resources:
    requests:
      storage: 10Gi
```

```bash
# ✅ 可以绑定（PV支持RWO）
$ kubectl get pvc pvc-rwo
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   AGE
pvc-rwo   Bound    nfs-pv   50Gi       RWO,RWX        5s
#                                      ^^^^^^^^
#                                      PV实际支持的模式
```

```yaml
# PVC请求RWX
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-rwx
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
```

```bash
# ❌ 如果只有RWO的PV，无法绑定
# 如果有支持RWX的PV，可以绑定
```

---

#### 选择器高级用法

**matchExpressions表达式**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: advanced-pvc
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 20Gi
  selector:
    matchExpressions:
    # 表达式1：type必须是ssd或nvme
    - key: type
      operator: In
      values:
      - ssd
      - nvme
    
    # 表达式2：environment不能是testing
    - key: environment
      operator: NotIn
      values:
      - testing
    
    # 表达式3：必须存在backup标签
    - key: backup
      operator: Exists
    
    # 表达式4：不能存在deprecated标签
    - key: deprecated
      operator: DoesNotExist
```

**选择器操作符详解：**

| 操作符 | 含义 | 示例 |
|--------|------|------|
| `In` | 值在列表中 | `type In [ssd, nvme]` |
| `NotIn` | 值不在列表中 | `env NotIn [testing]` |
| `Exists` | 键存在（值任意） | `backup Exists` |
| `DoesNotExist` | 键不存在 | `deprecated DoesNotExist` |

---

### 8.4.2 PVC状态管理和生命周期

PVC有三种主要状态，理解状态转换是排查问题的关键。

---

#### PVC的三种状态

```
PVC状态机
┌──────────────┐
│   Pending    │ ← 初始状态：等待绑定到PV
└──────┬───────┘
       │ 找到匹配PV或动态供应完成
       ▼
┌──────────────┐
│    Bound     │ ← 绑定成功，可被Pod使用
└──────┬───────┘
       │ PVC被删除
       ▼
┌──────────────┐
│   Deleted    │ ← PVC已删除，PV根据回收策略处理
└──────────────┘
```

| 状态 | 含义 | 原因 | 解决方案 |
|------|------|------|---------|
| **Pending** | 等待绑定 | - 没有匹配的PV<br>- 动态供应中<br>- 容量/模式不匹配 | - 创建匹配的PV<br>- 检查StorageClass<br>- 调整PVC请求 |
| **Bound** | 已绑定 | 成功绑定到PV | 正常状态 |
| **Lost** | 绑定丢失 | PV被意外删除 | 重新创建PV或PVC |

---

#### PVC生命周期演示

**步骤1：创建PVC（Pending状态）**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-lifecycle
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: manual  # 静态供应
```

```bash
# 创建PVC（此时没有匹配的PV）
$ kubectl apply -f pvc-lifecycle.yaml
persistentvolumeclaim/pvc-lifecycle created

# 查看状态：Pending
$ kubectl get pvc pvc-lifecycle
NAME            STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle   Pending                                      manual         10s

# 查看详细事件
$ kubectl describe pvc pvc-lifecycle
Events:
  Type     Reason              Message
  ----     ------              -------
  Warning  ProvisioningFailed  no persistent volumes available for this claim and no storage class is set
```

**步骤2：创建匹配的PV（Bound状态）**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-for-lifecycle
spec:
  capacity:
    storage: 10Gi  # 大于PVC请求的5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: /data/pv-lifecycle
    type: DirectoryOrCreate
```

```bash
# 创建PV
$ kubectl apply -f pv-for-lifecycle.yaml
persistentvolume/pv-for-lifecycle created

# PVC自动绑定到PV
$ kubectl get pvc pvc-lifecycle
NAME            STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle   Bound    pv-for-lifecycle   10Gi       RWO            manual         2m
#               ^^^^^
#               从Pending变为Bound

# PV也显示绑定信息
$ kubectl get pv pv-for-lifecycle
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS   AGE
pv-for-lifecycle   10Gi       RWO            Retain           Bound    default/pvc-lifecycle   manual         30s
#                                                             ^^^^^    ^^^^^^^^^^^^^^^^^^^
#                                                             Bound    绑定到这个PVC
```

**步骤3：Pod使用PVC**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-pvc
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: data
      mountPath: /usr/share/nginx/html
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-lifecycle  # 引用PVC
```

```bash
# 创建Pod
$ kubectl apply -f pod-using-pvc.yaml
pod/pod-using-pvc created

# 查看Pod状态
$ kubectl get pod pod-using-pvc
NAME            READY   STATUS    RESTARTS   AGE
pod-using-pvc   1/1     Running   0          15s

# 写入数据
$ kubectl exec pod-using-pvc -- sh -c 'echo "Hello from PVC" > /usr/share/nginx/html/index.html'

# 验证数据
$ kubectl exec pod-using-pvc -- cat /usr/share/nginx/html/index.html
Hello from PVC
```

**步骤4：删除Pod（PVC仍为Bound）**

```bash
# 删除Pod
$ kubectl delete pod pod-using-pvc
pod "pod-using-pvc" deleted

# PVC状态不变
$ kubectl get pvc pvc-lifecycle
NAME            STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle   Bound    pv-for-lifecycle   10Gi       RWO            manual         5m
#               ^^^^^
#               仍然是Bound
```

**步骤5：删除PVC（根据回收策略处理）**

```bash
# 删除PVC
$ kubectl delete pvc pvc-lifecycle
persistentvolumeclaim "pvc-lifecycle" deleted

# PV状态变为Released（因为回收策略是Retain）
$ kubectl get pv pv-for-lifecycle
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                   STORAGECLASS   AGE
pv-for-lifecycle   10Gi       RWO            Retain           Released   default/pvc-lifecycle   manual         6m
#                                                             ^^^^^^^^
#                                                             从Bound变为Released
```

---

### 8.4.3 PVC扩容操作（Volume Expansion）

Kubernetes 1.11+支持在线扩容PVC，无需重建Pod。

---

#### 扩容前提条件

1. **StorageClass支持扩容**
   ```yaml
   apiVersion: storage.k8s.io/v1
   kind: StorageClass
   metadata:
     name: expandable-sc
   provisioner: kubernetes.io/aws-ebs
   parameters:
     type: gp3
   allowVolumeExpansion: true  # ← 必须开启
   ```

2. **存储后端支持扩容**
   
   | 存储类型 | 支持扩容 | 说明 |
   |---------|---------|------|
   | AWS EBS | ✅ | 支持在线扩容 |
   | GCE PD | ✅ | 支持在线扩容 |
   | Azure Disk | ✅ | 支持在线扩容 |
   | Ceph RBD | ✅ | 支持在线扩容 |
   | NFS | ❌ | 通常不支持自动扩容 |
   | HostPath | ❌ | 不支持扩容 |

3. **PVC必须是Bound状态**

---

#### 在线扩容完整演示

**步骤1：创建支持扩容的StorageClass**

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-expandable
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iopsPerGB: "50"
allowVolumeExpansion: true  # 关键配置
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

**步骤2：创建PVC（初始10Gi）**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc-expand
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi  # 初始容量10Gi
  storageClassName: fast-expandable
```

```bash
# 创建PVC
$ kubectl apply -f mysql-pvc-expand.yaml
persistentvolumeclaim/mysql-pvc-expand created

# 查看状态
$ kubectl get pvc mysql-pvc-expand
NAME               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
mysql-pvc-expand   Bound    pvc-abc123-def456-ghi789                   10Gi       RWO            fast-expandable    30s
```

**步骤3：部署MySQL Pod**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysql-expand-demo
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    env:
    - name: MYSQL_ROOT_PASSWORD
      value: "mysql123"
    volumeMounts:
    - name: data
      mountPath: /var/lib/mysql
    resources:
      requests:
        memory: "512Mi"
        cpu: "500m"
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: mysql-pvc-expand
```

```bash
# 部署MySQL
$ kubectl apply -f mysql-expand-demo.yaml

# 写入一些数据
$ kubectl exec mysql-expand-demo -- mysql -uroot -pmysql123 -e "
CREATE DATABASE testdb;
USE testdb;
CREATE TABLE users (id INT, name VARCHAR(50));
INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob');
"

# 查看当前容量
$ kubectl exec mysql-expand-demo -- df -h /var/lib/mysql
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvdf       9.8G  200M  9.1G   3% /var/lib/mysql
#               ^^^^
#               10Gi (9.8G实际可用)
```

**步骤4：在线扩容到20Gi**

```bash
# 方法1：kubectl edit
$ kubectl edit pvc mysql-pvc-expand
# 修改：
#   resources:
#     requests:
#       storage: 20Gi  # 从10Gi改为20Gi

# 方法2：kubectl patch（推荐）
$ kubectl patch pvc mysql-pvc-expand -p '{"spec":{"resources":{"requests":{"storage":"20Gi"}}}}'
persistentvolumeclaim/mysql-pvc-expand patched

# 查看扩容进度
$ kubectl get pvc mysql-pvc-expand
NAME               STATUS   VOLUME                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
mysql-pvc-expand   Bound    pvc-abc123-def456-ghi789   10Gi       RWO            fast-expandable    5m
#                                                       ^^^^
#                                                       还是10Gi，扩容中...

# 查看详细事件
$ kubectl describe pvc mysql-pvc-expand
Events:
  Type     Reason                      Message
  ----     ------                      -------
  Normal   Resizing                    External resizer is resizing volume pvc-abc123-def456-ghi789
  Normal   FileSystemResizeRequired    Require file system resize of volume on node
```

**步骤5：等待扩容完成**

```bash
# 等待一段时间后（通常30秒-2分钟）
$ kubectl get pvc mysql-pvc-expand
NAME               STATUS   VOLUME                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
mysql-pvc-expand   Bound    pvc-abc123-def456-ghi789   20Gi       RWO            fast-expandable    7m
#                                                       ^^^^
#                                                       扩容完成！

# 在Pod内验证
$ kubectl exec mysql-expand-demo -- df -h /var/lib/mysql
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvdf        20G  200M   19G   2% /var/lib/mysql
#                ^^^
#                扩容成功！

# 验证数据完整性
$ kubectl exec mysql-expand-demo -- mysql -uroot -pmysql123 -e "USE testdb; SELECT * FROM users;"
+------+-------+
| id   | name  |
+------+-------+
|    1 | Alice |
|    2 | Bob   |
+------+-------+
# ✅ 数据完整，无需重启Pod！
```

---

#### 文件系统扩容说明

**自动扩容（大部分情况）：**
- Kubernetes 1.15+支持自动文件系统扩容
- 无需手动执行`resize2fs`或`xfs_growfs`

**手动扩容（旧版本）：**

```bash
# 如果自动扩容失败，需要手动操作
$ kubectl exec mysql-expand-demo -- sh -c '
# 对于ext4文件系统
resize2fs /dev/xvdf

# 对于xfs文件系统
xfs_growfs /var/lib/mysql
'
```

---

#### 扩容限制和注意事项

**限制1：只能扩容，不能缩容**

```bash
# ❌ 尝试缩容（从20Gi改为10Gi）
$ kubectl patch pvc mysql-pvc-expand -p '{"spec":{"resources":{"requests":{"storage":"10Gi"}}}}'
Error: spec.resources.requests.storage: Forbidden: field can not be less than previous value
# 不允许缩小容量
```

**限制2：某些存储需要离线扩容**

```yaml
# 查看StorageClass的扩容能力
$ kubectl get sc fast-expandable -o yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
allowVolumeExpansion: true
# 如果没有此字段或为false，则不支持扩容
```

**限制3：扩容失败处理**

```bash
# 查看PVC状态
$ kubectl describe pvc mysql-pvc-expand
Events:
  Type     Reason                Message
  ----     ------                -------
  Warning  VolumeResizeFailed    resize volume failed: rpc error: code = Internal desc = Could not resize volume

# 可能原因：
# 1. 底层存储配额不足（云盘配额用完）
# 2. PV已达到最大容量限制
# 3. 存储后端不支持在线扩容

# 解决方案：
# - 检查云服务商配额
# - 确认存储后端支持扩容
# - 必要时重建PVC和Pod
```

---

### 8.4.4 PVC克隆和快照（CSI Snapshot）

Kubernetes 1.17+通过CSI（Container Storage Interface）支持卷快照和克隆。

---

#### 卷快照（Volume Snapshot）

**前提条件：**
1. 使用支持快照的CSI驱动（如AWS EBS CSI、GCE PD CSI）
2. 创建VolumeSnapshotClass

**步骤1：创建VolumeSnapshotClass**

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: aws-ebs-snapshot-class
driver: ebs.csi.aws.com  # CSI驱动名称
deletionPolicy: Delete   # 快照删除策略
parameters:
  tagSpecification_1: "Name=MySnapshot"
  tagSpecification_2: "Environment=Production"
```

**步骤2：创建数据源PVC**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-data-original
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: aws-ebs-gp3
```

```bash
# 部署MySQL并写入数据
$ kubectl apply -f mysql-pod.yaml

$ kubectl exec mysql-pod -- mysql -uroot -pmysql123 -e "
CREATE DATABASE proddb;
USE proddb;
CREATE TABLE orders (id INT, total DECIMAL(10,2));
INSERT INTO orders VALUES (1, 99.99), (2, 199.99);
"
```

**步骤3：创建快照**

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: mysql-snapshot-20240114
spec:
  volumeSnapshotClassName: aws-ebs-snapshot-class
  source:
    persistentVolumeClaimName: mysql-data-original  # 源PVC
```

```bash
# 创建快照
$ kubectl apply -f mysql-snapshot.yaml
volumesnapshot.snapshot.storage.k8s.io/mysql-snapshot-20240114 created

# 查看快照状态
$ kubectl get volumesnapshot
NAME                      READYTOUSE   SOURCEPVC             SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS             AGE
mysql-snapshot-20240114   true         mysql-data-original                           50Gi          aws-ebs-snapshot-class    30s
#                         ^^^^
#                         快照已就绪

# 查看详细信息
$ kubectl describe volumesnapshot mysql-snapshot-20240114
Status:
  Bound Volume Snapshot Content Name:  snapcontent-abc123-def456
  Creation Time:                       2024-01-14T10:30:00Z
  Ready To Use:                        true
  Restore Size:                        50Gi
```

**步骤4：从快照恢复数据**

```yaml
# 创建新PVC，从快照恢复
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-data-restored
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: aws-ebs-gp3
  dataSource:
    name: mysql-snapshot-20240114  # 从快照恢复
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
```

```bash
# 创建PVC
$ kubectl apply -f mysql-pvc-restored.yaml
persistentvolumeclaim/mysql-data-restored created

# 查看PVC
$ kubectl get pvc mysql-data-restored
NAME                  STATUS   VOLUME                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mysql-data-restored   Bound    pvc-restored-xyz789        50Gi       RWO            aws-ebs-gp3    20s

# 部署新Pod使用恢复的PVC
$ kubectl apply -f mysql-pod-restored.yaml

# 验证数据
$ kubectl exec mysql-pod-restored -- mysql -uroot -pmysql123 -e "USE proddb; SELECT * FROM orders;"
+------+--------+
| id   | total  |
+------+--------+
|    1 |  99.99 |
|    2 | 199.99 |
+------+--------+
# ✅ 数据成功恢复！
```

---

#### PVC克隆（Volume Cloning）

**定义：** 直接从现有PVC创建新PVC，无需先创建快照。

```yaml
# 源PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: source-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: fast-ssd

---
# 克隆PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cloned-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi  # 必须 >= 源PVC容量
  storageClassName: fast-ssd
  dataSource:
    name: source-pvc  # 直接引用源PVC
    kind: PersistentVolumeClaim
```

```bash
# 创建克隆PVC
$ kubectl apply -f cloned-pvc.yaml
persistentvolumeclaim/cloned-pvc created

# 查看克隆状态
$ kubectl get pvc cloned-pvc
NAME         STATUS   VOLUME                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
cloned-pvc   Bound    pvc-cloned-abc123          20Gi       RWO            fast-ssd       15s

# 验证数据完整性
$ kubectl run test-pod --image=busybox --rm -it --restart=Never -- \
  --overrides='{"spec":{"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"cloned-pvc"}}],"containers":[{"name":"busybox","image":"busybox","command":["ls","-la","/data"],"volumeMounts":[{"name":"data","mountPath":"/data"}]}]}}'
# 输出源PVC的所有文件
```

---

#### 快照 vs 克隆对比

| 特性 | 快照（Snapshot） | 克隆（Clone） |
|------|----------------|--------------|
| **创建方式** | PVC → Snapshot → 新PVC | PVC → 新PVC（直接） |
| **中间产物** | 生成VolumeSnapshot对象 | 无 |
| **用途** | 备份、多次恢复 | 一次性复制 |
| **性能** | 两步操作，较慢 | 一步操作，较快 |
| **存储占用** | 快照独立存储 | 新PVC独立存储 |
| **保留时间** | 可长期保留 | 通常临时使用 |
| **典型场景** | 生产备份、灾难恢复 | 开发测试、数据迁移 |

---

### 8.4.5 PVC与Pod的绑定关系

理解PVC与Pod的绑定机制，对于排查存储问题至关重要。

---

#### 绑定关系详解

```
Pod → Volume → PVC → PV → 底层存储

示例：
┌─────────────────────┐
│  Pod: mysql         │
│  containers:        │
│    volumeMounts:    │
│      - name: data   │ ←─┐
│        mountPath:   │   │
│        /var/lib/...│   │
└─────────────────────┘   │
                          │ 引用
┌─────────────────────┐   │
│  volumes:           │   │
│    - name: data     │ ──┘
│      persistentVol..│ ←─┐
│        claimName:   │   │
│        mysql-pvc    │   │
└─────────────────────┘   │
                          │ 引用
┌─────────────────────┐   │
│  PVC: mysql-pvc     │ ──┘
│  status:            │
│    phase: Bound     │
│    volume: pv-001   │ ←─┐
└─────────────────────┘   │
                          │ 绑定
┌─────────────────────┐   │
│  PV: pv-001         │ ──┘
│  spec:              │
│    awsElasticBlock..│ ←─┐
│      volumeID:      │   │
│      vol-abc123     │   │
└─────────────────────┘   │
                          │ 引用
┌─────────────────────┐   │
│  AWS EBS:           │ ──┘
│  vol-abc123         │
│  /dev/xvdf          │
└─────────────────────┘
```

---

#### 多Pod共享PVC

**场景1：RWO模式，同一节点多Pod**

```yaml
# Pod1和Pod2在同一节点，共享RWO PVC
apiVersion: v1
kind: Pod
metadata:
  name: pod1-rwo
spec:
  nodeSelector:
    kubernetes.io/hostname: node1  # 强制同节点
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: shared
      mountPath: /data
  volumes:
  - name: shared
    persistentVolumeClaim:
      claimName: rwo-pvc

---
apiVersion: v1
kind: Pod
metadata:
  name: pod2-rwo
spec:
  nodeSelector:
    kubernetes.io/hostname: node1  # 同一节点
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: shared
      mountPath: /data
  volumes:
  - name: shared
    persistentVolumeClaim:
      claimName: rwo-pvc  # 同一个PVC
```

```bash
# 两个Pod都成功运行
$ kubectl get pod -o wide
NAME       READY   STATUS    NODE
pod1-rwo   1/1     Running   node1
pod2-rwo   1/1     Running   node1

# Pod1写入数据
$ kubectl exec pod1-rwo -- sh -c 'echo "from pod1" > /data/test.txt'

# Pod2读取数据
$ kubectl exec pod2-rwo -- cat /data/test.txt
from pod1
# ✅ RWO模式下，同节点多Pod可共享
```

---

**场景2：RWX模式，跨节点多Pod**

```yaml
# Deployment 3副本，共享RWX PVC
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shared-storage-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: shared-app
  template:
    metadata:
      labels:
        app: shared-app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        volumeMounts:
        - name: shared-html
          mountPath: /usr/share/nginx/html
      volumes:
      - name: shared-html
        persistentVolumeClaim:
          claimName: nfs-rwx-pvc  # RWX模式的NFS PVC
```

```bash
# 3个Pod分布在不同节点
$ kubectl get pod -o wide
NAME                                 READY   STATUS    NODE
shared-storage-app-5d7f8b9c4-abc12   1/1     Running   node1
shared-storage-app-5d7f8b9c4-def34   1/1     Running   node2
shared-storage-app-5d7f8b9c4-ghi56   1/1     Running   node3

# 任意Pod写入数据
$ kubectl exec shared-storage-app-5d7f8b9c4-abc12 -- sh -c 'echo "Shared content" > /usr/share/nginx/html/index.html'

# 其他Pod都能读取
$ kubectl exec shared-storage-app-5d7f8b9c4-def34 -- cat /usr/share/nginx/html/index.html
Shared content

$ kubectl exec shared-storage-app-5d7f8b9c4-ghi56 -- cat /usr/share/nginx/html/index.html
Shared content
# ✅ RWX模式下，跨节点多Pod可共享
```

---

#### PVC保护机制

**1. 使用中保护（Storage Object in Use Protection）**

```bash
# 创建PVC和Pod
$ kubectl apply -f pvc.yaml
$ kubectl apply -f pod.yaml

# 尝试删除正在使用的PVC
$ kubectl delete pvc my-pvc
persistentvolumeclaim "my-pvc" deleted
# 命令立即返回，但PVC实际未删除

# 查看PVC状态
$ kubectl get pvc my-pvc
NAME     STATUS        VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
my-pvc   Terminating   pv-001   10Gi       RWO            standard       5m
#        ^^^^^^^^^^^
#        处于Terminating状态，等待Pod删除

$ kubectl describe pvc my-pvc
Finalizers:  [kubernetes.io/pvc-protection]
# PVC被finalizer保护

# 删除Pod后，PVC才真正删除
$ kubectl delete pod my-pod
pod "my-pod" deleted

$ kubectl get pvc my-pvc
Error from server (NotFound): persistentvolumeclaims "my-pvc" not found
# ✅ PVC已删除
```

---

### 8.4.6 完整实战案例：WordPress高可用存储方案

**场景：** 部署高可用WordPress，使用NFS共享存储，支持扩容和快照备份。

**架构：**
```
┌─────────────────────────────────────────────────┐
│  3副本WordPress Pod（跨节点）                      │
│  ├── Pod1 (node1) ──┐                           │
│  ├── Pod2 (node2) ──┼── 共享NFS RWX PVC        │
│  └── Pod3 (node3) ──┘                           │
└─────────────────────────────────────────────────┘
              │
              ▼
┌─────────────────────────────────────────────────┐
│  PVC: wordpress-pvc (RWX, 20Gi → 扩容到50Gi)    │
└─────────────────────────────────────────────────┘
              │
              ▼
┌─────────────────────────────────────────────────┐
│  PV: NFS (192.168.1.100:/data/wordpress)        │
└─────────────────────────────────────────────────┘
```

**步骤1：创建NFS PV**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: wordpress-nfs-pv
spec:
  capacity:
    storage: 50Gi  # 提前准备足够容量
  accessModes:
    - ReadWriteMany  # 支持多节点读写
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs-storage
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    server: 192.168.1.100
    path: /data/wordpress
```

**步骤2：创建PVC（初始20Gi）**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wordpress-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 20Gi  # 初始容量
  storageClassName: nfs-storage
```

**步骤3：部署MySQL（使用独立RWO存储）**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: wordpress-mysql
spec:
  selector:
    app: wordpress-mysql
  ports:
  - port: 3306
  clusterIP: None  # Headless Service

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
    - ReadWriteOnce  # MySQL使用RWO
  resources:
    requests:
      storage: 20Gi
  storageClassName: local-storage  # 高性能本地存储

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wordpress-mysql
  template:
    metadata:
      labels:
        app: wordpress-mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "wordpress123"
        - name: MYSQL_DATABASE
          value: "wordpress"
        - name: MYSQL_USER
          value: "wpuser"
        - name: MYSQL_PASSWORD
          value: "wppass123"
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-data
        persistentVolumeClaim:
          claimName: mysql-pvc
```

**步骤4：部署WordPress（3副本共享NFS）**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: wordpress
spec:
  selector:
    app: wordpress
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
spec:
  replicas: 3  # 3副本高可用
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      affinity:
        podAntiAffinity:  # 分散到不同节点
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: wordpress
            topologyKey: kubernetes.io/hostname
      
      containers:
      - name: wordpress
        image: wordpress:6.0-apache
        env:
        - name: WORDPRESS_DB_HOST
          value: "wordpress-mysql:3306"
        - name: WORDPRESS_DB_USER
          value: "wpuser"
        - name: WORDPRESS_DB_PASSWORD
          value: "wppass123"
        - name: WORDPRESS_DB_NAME
          value: "wordpress"
        ports:
        - containerPort: 80
        volumeMounts:
        - name: wordpress-data
          mountPath: /var/www/html
      
      volumes:
      - name: wordpress-data
        persistentVolumeClaim:
          claimName: wordpress-pvc  # 共享NFS存储
```

**步骤5：部署和验证**

```bash
# 创建所有资源
$ kubectl apply -f wordpress-nfs-pv.yaml
$ kubectl apply -f wordpress-pvc.yaml
$ kubectl apply -f wordpress-mysql.yaml
$ kubectl apply -f wordpress.yaml

# 查看Pod分布
$ kubectl get pod -o wide
NAME                               READY   STATUS    NODE
wordpress-mysql-7d8f9b6c5-abc12    1/1     Running   node1
wordpress-5d7f8b9c4-def34          1/1     Running   node1
wordpress-5d7f8b9c4-ghi56          1/1     Running   node2
wordpress-5d7f8b9c4-jkl78          1/1     Running   node3
# ✅ WordPress 3副本分散在不同节点

# 查看PVC
$ kubectl get pvc
NAME            STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS    AGE
wordpress-pvc   Bound    wordpress-nfs-pv    50Gi       RWX            nfs-storage     2m
mysql-pvc       Bound    local-pv-node1      20Gi       RWO            local-storage   2m

# 访问WordPress
$ kubectl get svc wordpress
NAME        TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)        AGE
wordpress   LoadBalancer   10.96.100.200   203.0.113.10    80:30080/TCP   3m

# 浏览器访问 http://203.0.113.10 完成WordPress安装
```

**步骤6：模拟扩容（20Gi → 50Gi）**

```bash
# 编辑PVC扩容
$ kubectl patch pvc wordpress-pvc -p '{"spec":{"resources":{"requests":{"storage":"50Gi"}}}}'

# 查看扩容结果
$ kubectl get pvc wordpress-pvc
NAME            STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
wordpress-pvc   Bound    wordpress-nfs-pv    50Gi       RWX            nfs-storage    10m

# 验证WordPress仍正常运行
$ kubectl get pod -l app=wordpress
NAME                        READY   STATUS    RESTARTS   AGE
wordpress-5d7f8b9c4-def34   1/1     Running   0          11m
wordpress-5d7f8b9c4-ghi56   1/1     Running   0          11m
wordpress-5d7f8b9c4-jkl78   1/1     Running   0          11m
# ✅ 扩容完成，无需重启Pod
```

---

### 8.4.7 PVC最佳实践总结

**1. 容量规划**
```yaml
# ✅ 预留充足空间
resources:
  requests:
    storage: 20Gi  # 数据库初始数据5Gi，预留4倍空间

# ✅ 使用支持扩容的StorageClass
storageClassName: expandable-sc
```

**2. 访问模式选择**
```
应用场景              → PVC访问模式    → 存储后端
─────────────────────────────────────────────────
单实例数据库          → RWO           → Local PV/EBS
多副本只读Web        → ROX           → NFS
多副本读写文件服务    → RWX           → NFS/CephFS
StatefulSet         → RWO（每Pod独立） → EBS/Azure Disk
```

**3. 命名规范**
```yaml
# ✅ 描述性命名
metadata:
  name: mysql-prod-data-pvc
  # 格式：<应用>-<环境>-<用途>-pvc
```

**4. 标签管理**
```yaml
metadata:
  labels:
    app: wordpress
    component: storage
    environment: production
    backup-required: "true"
```

---

### 8.4.8 下一节预告

在本节中，我们深入学习了PersistentVolumeClaim的核心知识：

- ✅ PVC绑定机制和选择器（matchLabels/matchExpressions）
- ✅ PVC状态管理和生命周期（Pending → Bound → Deleted）
- ✅ PVC在线扩容操作（10Gi → 20Gi无需重启）
- ✅ PVC快照和克隆（CSI Snapshot/Volume Cloning）
- ✅ PVC与Pod的绑定关系（RWO/RWX共享机制）
- ✅ WordPress高可用完整实战

到目前为止，我们学习的都是静态供应或手动创建的PVC。在生产环境中，当集群规模扩大时，手动创建PV变得不可行。Kubernetes通过**StorageClass**和**动态供应**彻底解决了这一难题。

**在下一节（8.5 存储类StorageClass）中**，我们将深入学习：
- StorageClass的工作原理和Provisioner
- 动态供应的完整流程
- StorageClass参数配置（不同存储后端）
- VolumeBindingMode（延迟绑定 vs 立即绑定）
- 默认StorageClass的设置和使用

---

**本节完**

*下一节预告：8.5节《存储类StorageClass》- 深入动态供应、Provisioner配置和VolumeBindingMode机制。*

## 8.5 存储类（StorageClass）

在上一节中，我们深入学习了PersistentVolumeClaim（PVC）的绑定机制、扩容、克隆和快照等高级特性。然而，在实际生产环境中，手动创建PV（静态供应）存在诸多问题：集群管理员需要提前预估存储需求、手动创建大量PV、容量浪费、运维成本高。Kubernetes通过**StorageClass**实现了**动态供应（Dynamic Provisioning）**，彻底解放了管理员，让存储资源像云服务一样按需分配。

本节将深入探讨StorageClass的核心机制、Provisioner配置、各云厂商的存储类实战、以及VolumeBindingMode等关键特性。

---

### 8.5.1 动态供应机制详解

#### 静态供应 vs 动态供应对比

**静态供应（Static Provisioning）的痛点**：

```
管理员视角：
┌─────────────────────────────────┐
│ 1. 评估未来3个月存储需求         │
│ 2. 手动创建100个PV（10Gi各100个）│
│ 3. 开发团队申请PVC               │
│ 4. 50个PV被绑定，50个闲置        │
│ 5. 突然需要50Gi大盘，没有预留！  │
│ 6. 紧急手动创建新PV...           │
└─────────────────────────────────┘

问题汇总：
❌ 容量浪费（预创建但未使用的PV）
❌ 容量不足（未预料到的大容量需求）
❌ 运维成本高（手动创建、标签管理、生命周期管理）
❌ 无法灵活调整（不同StorageClass需求）
```

**动态供应（Dynamic Provisioning）的优势**：

```
用户视角：
┌─────────────────────────────────┐
│ 1. 创建PVC，指定StorageClass     │
│ 2. Kubernetes自动创建PV          │
│ 3. 自动绑定PVC ↔ PV              │
│ 4. 删除PVC，自动回收PV           │
└─────────────────────────────────┘

优势汇总：
✅ 按需分配（需要多少创建多少）
✅ 零容量浪费（动态创建，用完即删）
✅ 自动化管理（无需人工干预）
✅ 多层次存储（fast-ssd、standard-hdd等）
```

#### 动态供应完整工作流程

让我们通过一个完整的时序图理解动态供应的工作原理：

```
用户                PVC Controller       Provisioner         存储后端
 │                      │                    │                  │
 │  1. 创建PVC          │                    │                  │
 │  storageClassName:   │                    │                  │
 │  fast-ssd            │                    │                  │
 ├─────────────────────>│                    │                  │
 │                      │                    │                  │
 │                      │  2. 检测新PVC       │                  │
 │                      │  没有匹配的PV       │                  │
 │                      │                    │                  │
 │                      │  3. 查找StorageClass│                  │
 │                      │  名为"fast-ssd"     │                  │
 │                      │                    │                  │
 │                      │  4. 调用Provisioner │                  │
 │                      │  创建PV请求         │                  │
 │                      ├───────────────────>│                  │
 │                      │                    │                  │
 │                      │                    │  5. 调用存储API   │
 │                      │                    │  创建实际存储卷   │
 │                      │                    ├─────────────────>│
 │                      │                    │                  │
 │                      │                    │  6. 返回卷信息    │
 │                      │                    │  (volumeID等)     │
 │                      │                    │<─────────────────┤
 │                      │                    │                  │
 │                      │  7. 创建PV对象      │                  │
 │                      │  绑定到PVC          │                  │
 │                      │<───────────────────┤                  │
 │                      │                    │                  │
 │  8. PVC状态：Bound   │                    │                  │
 │<─────────────────────┤                    │                  │
 │                      │                    │                  │
```

**关键步骤解析**：

1. **Step 1-2**：用户创建PVC，指定`storageClassName: fast-ssd`，PVC Controller检测到新PVC且没有现成的PV可以绑定
2. **Step 3**：Controller查找名为`fast-ssd`的StorageClass定义
3. **Step 4-5**：调用该StorageClass配置的Provisioner（如AWS EBS CSI Driver），Provisioner调用云厂商API创建实际存储卷
4. **Step 6-7**：存储卷创建成功后，Provisioner自动创建对应的PV对象，并标记为与该PVC绑定
5. **Step 8**：PVC状态从Pending变为Bound，用户可以使用

#### 第一个动态供应实验

让我们通过一个简单的本地实验理解动态供应的魔力：

**实验环境准备**：使用Kubernetes内置的`kubernetes.io/no-provisioner`（需要手动创建底层存储，但演示流程）

```yaml
# storage-class-local.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner  # 本地存储需要手动预创建卷
volumeBindingMode: WaitForFirstConsumer    # 延迟绑定，等待Pod调度
```

```bash
# 创建StorageClass
$ kubectl apply -f storage-class-local.yaml
storageclass.storage.k8s.io/local-storage created

# 查看StorageClass
$ kubectl get storageclass
NAME            PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION
local-storage   kubernetes.io/no-provisioner   Delete          WaitForFirstConsumer   false
```

**创建使用该StorageClass的PVC**：

```yaml
# pvc-dynamic.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-dynamic-demo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: local-storage  # 指定StorageClass
```

```bash
# 创建PVC
$ kubectl apply -f pvc-dynamic.yaml
persistentvolumeclaim/pvc-dynamic-demo created

# 查看PVC状态（注意：状态为Pending，等待Pod使用）
$ kubectl get pvc pvc-dynamic-demo
NAME               STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
pvc-dynamic-demo   Pending                                      local-storage   10s

# 查看详细事件（解释为什么是Pending）
$ kubectl describe pvc pvc-dynamic-demo
Events:
  Type    Reason                Age   From                         Message
  ----    ------                ----  ----                         -------
  Normal  WaitForFirstConsumer  15s   persistentvolume-controller  waiting for first consumer to be created before binding
```

**关键观察**：
- ✅ PVC成功创建，但状态为`Pending`
- ✅ 原因：`volumeBindingMode: WaitForFirstConsumer`（延迟绑定，稍后详解）
- ✅ 事件显示：等待第一个消费者（Pod）创建后再绑定

我们将在8.5.4节详细解析延迟绑定机制。现在先理解：**StorageClass是PVC和存储后端之间的桥梁**。

---

### 8.5.2 StorageClass核心字段详解

一个完整的StorageClass定义包含多个关键字段，让我们逐一解析：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd                          # StorageClass名称
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"  # 设置为默认StorageClass
provisioner: kubernetes.io/aws-ebs        # Provisioner插件
parameters:                               # 传递给Provisioner的参数
  type: gp3                               # AWS EBS卷类型
  iopsPerGB: "10"                         # IOPS配置
  encrypted: "true"                       # 启用加密
  kmsKeyId: arn:aws:kms:...               # KMS密钥
reclaimPolicy: Delete                     # 回收策略
allowVolumeExpansion: true                # 允许扩容
volumeBindingMode: Immediate              # 绑定模式
mountOptions:                             # 挂载选项
  - debug
  - noatime
```

#### 1. Provisioner（供应器）

**Provisioner** 是实际执行存储卷创建/删除操作的插件。Kubernetes支持两类Provisioner：

**内置Provisioner（In-Tree）**：
```yaml
# AWS EBS
provisioner: kubernetes.io/aws-ebs

# GCE Persistent Disk
provisioner: kubernetes.io/gce-pd

# Azure Disk
provisioner: kubernetes.io/azure-disk

# Azure File
provisioner: kubernetes.io/azure-file

# Cinder (OpenStack)
provisioner: kubernetes.io/cinder

# vSphere
provisioner: kubernetes.io/vsphere-volume
```

**外置Provisioner（Out-of-Tree，通过CSI）**：
```yaml
# AWS EBS CSI Driver（推荐，替代In-Tree）
provisioner: ebs.csi.aws.com

# Ceph RBD CSI
provisioner: rbd.csi.ceph.com

# NFS Subdir External Provisioner
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner

# Longhorn（CNCF项目）
provisioner: driver.longhorn.io
```

**In-Tree vs CSI对比**：

| 特性           | In-Tree                     | CSI（外置）                    |
|----------------|-----------------------------|---------------------------------|
| **维护性**     | 与K8s核心代码耦合，升级困难 | 独立插件，可独立升级            |
| **功能性**     | 功能受限                    | 支持快照、克隆、拓扑等高级特性 |
| **未来趋势**   | 逐步废弃（K8s 1.17+）       | 官方推荐，成为主流              |
| **典型代表**   | kubernetes.io/aws-ebs       | ebs.csi.aws.com                 |

**最佳实践**：
- ✅ 新集群优先使用CSI Provisioner
- ⚠️ 旧集群逐步迁移到CSI（参考云厂商文档）

#### 2. Parameters（参数）

Parameters将特定于存储后端的配置传递给Provisioner。不同Provisioner支持的参数完全不同：

**AWS EBS示例**：
```yaml
provisioner: ebs.csi.aws.com
parameters:
  type: gp3                   # 卷类型：gp3（通用SSD）、io2（高性能SSD）、st1（HDD）
  iops: "3000"                # gp3/io2的IOPS
  throughput: "125"           # gp3的吞吐量（MiB/s）
  encrypted: "true"           # 启用EBS加密
  kmsKeyId: "arn:aws:kms:..." # KMS密钥ARN
  fsType: ext4                # 文件系统类型
```

**GCE PD示例**：
```yaml
provisioner: pd.csi.storage.gke.io
parameters:
  type: pd-ssd                # pd-standard（HDD）或pd-ssd（SSD）
  replication-type: regional-pd  # 区域复制（高可用）
  disk-encryption-kms-key: "projects/..."  # 加密密钥
```

**Azure Disk示例**：
```yaml
provisioner: disk.csi.azure.com
parameters:
  skuName: Premium_LRS        # Standard_LRS、Premium_LRS、StandardSSD_LRS
  kind: Managed               # Managed或Dedicated
  cachingMode: ReadOnly       # None、ReadOnly、ReadWrite
```

**Ceph RBD示例**：
```yaml
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: "b9127830-b0cc-..."        # Ceph集群ID
  pool: kubernetes                      # RBD池名称
  imageFeatures: layering               # RBD镜像特性
  csi.storage.k8s.io/provisioner-secret-name: ceph-secret
  csi.storage.k8s.io/node-stage-secret-name: ceph-secret
```

#### 3. ReclaimPolicy（回收策略）

决定PVC删除后PV的处理方式，支持两种策略：

| 策略       | 行为描述                                      | 使用场景                       |
|------------|-----------------------------------------------|--------------------------------|
| **Delete** | PVC删除 → PV自动删除 → 底层存储卷删除         | 临时数据、测试环境、成本敏感   |
| **Retain** | PVC删除 → PV变为Released → 底层存储卷保留     | 生产数据、需要手动备份/迁移    |

**对比实验**：

```yaml
# storage-class-delete.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: sc-delete-policy
provisioner: kubernetes.io/no-provisioner
reclaimPolicy: Delete  # 删除策略
volumeBindingMode: Immediate
```

```yaml
# storage-class-retain.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: sc-retain-policy
provisioner: kubernetes.io/no-provisioner
reclaimPolicy: Retain  # 保留策略
volumeBindingMode: Immediate
```

```bash
# 分别创建使用两种策略的PVC
$ kubectl apply -f pvc-with-delete-sc.yaml
$ kubectl apply -f pvc-with-retain-sc.yaml

# 删除PVC
$ kubectl delete pvc pvc-delete-demo
$ kubectl delete pvc pvc-retain-demo

# 观察PV状态
$ kubectl get pv

# Delete策略：PV自动删除（列表中看不到）
# Retain策略：PV状态变为Released，但仍然存在
NAME                       CAPACITY   STATUS     CLAIM
pv-retain-demo             5Gi        Released   default/pvc-retain-demo
```

**最佳实践**：
- ✅ 生产环境：默认使用`Retain`，删除前手动确认
- ✅ 测试环境：使用`Delete`，自动清理节省成本
- ⚠️ 关键数据：即使使用`Retain`，也要定期备份（快照）

#### 4. AllowVolumeExpansion（允许扩容）

控制是否允许PVC在线扩容（修改PVC的`spec.resources.requests.storage`）：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: expandable-storage
provisioner: ebs.csi.aws.com
allowVolumeExpansion: true  # 允许扩容
parameters:
  type: gp3
```

**扩容实验**（接续8.4.3节）：

```bash
# 创建使用可扩容StorageClass的PVC
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-expand-sc-demo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: expandable-storage
EOF

# 扩容到20Gi
$ kubectl patch pvc pvc-expand-sc-demo -p '{"spec":{"resources":{"requests":{"storage":"20Gi"}}}}'

# 观察扩容过程
$ kubectl get pvc pvc-expand-sc-demo --watch
NAME                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS
pvc-expand-sc-demo    Bound    pvc-d4b3a9c1-...                          10Gi       RWO            expandable-storage
pvc-expand-sc-demo    Bound    pvc-d4b3a9c1-...                          20Gi       RWO            expandable-storage
```

**注意事项**：
- ⚠️ 仅支持扩容，不支持缩容（存储系统限制）
- ⚠️ 部分存储后端需要重启Pod才能生效（如旧版本EBS）
- ⚠️ CSI驱动需要支持`EXPAND_VOLUME` capability

#### 5. VolumeBindingMode（卷绑定模式）

这是StorageClass最容易被忽视但极其重要的字段，决定PV何时绑定：

| 模式                      | 绑定时机                              | 适用场景                       |
|---------------------------|---------------------------------------|--------------------------------|
| **Immediate（立即绑定）** | PVC创建后立即绑定PV                  | 网络存储（NFS、Ceph、云盘）    |
| **WaitForFirstConsumer**  | Pod使用PVC时才绑定（考虑Pod调度位置） | 本地存储（Local PV）、拓扑感知 |

**为什么需要延迟绑定？**

场景：假设你有一个3节点集群，每个节点有本地SSD：

```
节点分布：
┌─────────────┬─────────────┬─────────────┐
│   node1     │   node2     │   node3     │
│  Local SSD  │  Local SSD  │  Local SSD  │
│   100Gi     │   100Gi     │   100Gi     │
└─────────────┴─────────────┴─────────────┘
```

**立即绑定的问题**：

```
1. 用户创建PVC（storageClassName: local-ssd）
2. PV Controller立即绑定到node1的Local PV
3. Pod创建，调度器决定调度到node2（因为CPU充足）
4. ❌ Pod无法启动！PV在node1，但Pod在node2
```

**延迟绑定的解决方案**：

```
1. 用户创建PVC（storageClassName: local-ssd, volumeBindingMode: WaitForFirstConsumer）
2. PVC保持Pending状态，不绑定任何PV
3. Pod创建，调度器综合考虑CPU、内存、存储位置，调度到node2
4. PV Controller绑定node2的Local PV
5. ✅ Pod成功启动，数据访问本地SSD
```

**完整实验对比**：

```yaml
# sc-immediate.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-immediate
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: Immediate  # 立即绑定
---
# sc-wait.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-wait
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer  # 延迟绑定
```

```bash
# 创建两个Local PV（分别在node1和node2）
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-node1
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-immediate
  local:
    path: /mnt/disk1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-node2
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-wait
  local:
    path: /mnt/disk2
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node2
EOF

# 创建使用Immediate模式的PVC
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-immediate
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: local-immediate
EOF

# 立即绑定到node1的PV
$ kubectl get pvc pvc-immediate
NAME            STATUS   VOLUME           CAPACITY   STORAGECLASS      AGE
pvc-immediate   Bound    local-pv-node1   10Gi       local-immediate   2s

# 创建使用WaitForFirstConsumer模式的PVC
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-wait
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: local-wait
EOF

# 保持Pending状态
$ kubectl get pvc pvc-wait
NAME       STATUS    VOLUME   CAPACITY   STORAGECLASS   AGE
pvc-wait   Pending                       local-wait     5s

# 创建使用PVC的Pod（调度到node2）
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: pod-wait-demo
spec:
  nodeSelector:
    kubernetes.io/hostname: node2  # 强制调度到node2
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-wait
EOF

# Pod创建后，PVC绑定到node2的PV
$ kubectl get pvc pvc-wait
NAME       STATUS   VOLUME           CAPACITY   STORAGECLASS   AGE
pvc-wait   Bound    local-pv-node2   10Gi       local-wait     30s
```

**最佳实践**：
- ✅ Local PV必须使用`WaitForFirstConsumer`
- ✅ 多可用区部署时，使用`WaitForFirstConsumer`避免跨区域挂载
- ✅ 网络存储（NFS、Ceph）可以使用`Immediate`

#### 6. MountOptions（挂载选项）

传递给底层文件系统的挂载参数，用于性能优化或特定功能：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-optimized
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
mountOptions:
  - nfsvers=4.1         # NFS版本
  - hard                # 硬挂载（网络故障时阻塞而不是报错）
  - timeo=600           # 超时时间（0.1秒单位，600=60秒）
  - retrans=2           # 重传次数
  - noatime             # 不更新访问时间（性能优化）
  - nodiratime          # 不更新目录访问时间
```

**常见挂载选项**：

| 选项         | 作用                                  | 适用存储类型     |
|--------------|---------------------------------------|------------------|
| `noatime`    | 不更新文件访问时间，提升性能         | 所有文件系统     |
| `nodiratime` | 不更新目录访问时间                   | 所有文件系统     |
| `nfsvers=4.1`| 指定NFS协议版本                      | NFS              |
| `hard`       | 硬挂载（网络故障时阻塞）             | NFS              |
| `soft`       | 软挂载（网络故障时报错）             | NFS              |
| `ro`         | 只读挂载                             | 所有             |
| `discard`    | 启用TRIM（SSD优化）                  | 块存储           |

---

### 8.5.3 主流云厂商StorageClass配置实战

不同云厂商提供的存储服务差异巨大，下面是生产级配置示例：

#### AWS EBS StorageClass（推荐CSI）

AWS提供多种EBS卷类型，适合不同性能需求：

```yaml
# 高性能SSD（适合数据库）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: aws-ebs-gp3-fast
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: ebs.csi.aws.com
parameters:
  type: gp3                      # 通用SSD（第3代，推荐）
  iops: "16000"                  # 16000 IOPS（gp3最大）
  throughput: "1000"             # 1000 MiB/s（gp3最大）
  encrypted: "true"              # 启用加密
  kmsKeyId: "arn:aws:kms:us-west-2:123456789012:key/abc123..."  # KMS密钥
  fsType: ext4
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer  # 支持多AZ
---
# 极致性能SSD（适合高IOPS场景）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: aws-ebs-io2-extreme
provisioner: ebs.csi.aws.com
parameters:
  type: io2                      # Provisioned IOPS SSD
  iops: "64000"                  # 最高64000 IOPS
  encrypted: "true"
allowVolumeExpansion: true
reclaimPolicy: Retain            # 高价值数据，保留策略
volumeBindingMode: WaitForFirstConsumer
---
# 经济型HDD（适合日志、备份）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: aws-ebs-st1-cold
provisioner: ebs.csi.aws.com
parameters:
  type: st1                      # Throughput Optimized HDD
  encrypted: "false"
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

**EBS类型对比**：

| 类型  | IOPS范围          | 吞吐量         | 延迟      | 价格   | 使用场景               |
|-------|-------------------|----------------|-----------|--------|------------------------|
| gp3   | 3000-16000        | 125-1000 MiB/s | 个位数ms  | 中等   | 通用工作负载、数据库   |
| io2   | 100-64000         | 1000 MiB/s     | 亚毫秒级  | 高     | 高性能数据库、关键应用 |
| st1   | 500 IOPS          | 500 MiB/s      | 高        | 低     | 大数据、日志           |
| sc1   | 250 IOPS          | 250 MiB/s      | 高        | 极低   | 冷数据、归档           |

**多可用区支持验证**：

```bash
# 创建跨AZ的PVC
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-multi-az
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: aws-ebs-gp3-fast
EOF

# 创建Pod（调度器会考虑PVC的可用区）
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: pod-az-demo
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-multi-az
EOF

# 检查Pod和PV的可用区（应该在同一个AZ）
$ kubectl get pod pod-az-demo -o jsonpath='{.spec.nodeName}' | xargs kubectl get node -o jsonpath='{.metadata.labels.topology\.kubernetes\.io/zone}'
us-west-2a

$ kubectl get pvc pvc-multi-az -o jsonpath='{.metadata.annotations.volume\.kubernetes\.io/selected-node}'
ip-10-0-1-123.us-west-2.compute.internal  # node在us-west-2a

$ kubectl get pv $(kubectl get pvc pvc-multi-az -o jsonpath='{.spec.volumeName}') -o jsonpath='{.spec.nodeAffinity}'
{"required":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-west-2a"]}]}]}}
```

#### GCP Persistent Disk StorageClass

```yaml
# 高性能SSD
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gcp-pd-ssd
provisioner: pd.csi.storage.gke.io
parameters:
  type: pd-ssd                   # SSD类型
  replication-type: none         # 单区域复制
  fstype: ext4
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
---
# 区域高可用SSD（跨区域复制）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gcp-pd-ssd-regional
provisioner: pd.csi.storage.gke.io
parameters:
  type: pd-ssd
  replication-type: regional-pd  # 跨区域复制（高可用）
  fstype: ext4
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
---
# 经济型HDD
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gcp-pd-standard
provisioner: pd.csi.storage.gke.io
parameters:
  type: pd-standard              # HDD类型
  replication-type: none
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

**Regional PD高可用验证**：

```bash
# 创建使用区域复制的PVC
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-regional-pd
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 200Gi
  storageClassName: gcp-pd-ssd-regional
EOF

# 创建StatefulSet验证跨区域迁移
$ kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-regional
spec:
  serviceName: mysql
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "password"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: pvc-regional-pd
EOF

# 模拟节点故障（删除Pod所在节点，GKE会在另一个区域重建）
$ kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

# Pod重建后，数据完整（因为Regional PD跨区域复制）
$ kubectl exec mysql-regional-0 -- mysql -uroot -ppassword -e "SHOW DATABASES;"
# 数据库列表完整保留
```

#### Azure Disk StorageClass

```yaml
# 高性能Premium SSD
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: azure-disk-premium
provisioner: disk.csi.azure.com
parameters:
  skuName: Premium_LRS           # Premium SSD（本地冗余）
  kind: Managed
  cachingMode: ReadOnly          # 缓存模式：None、ReadOnly、ReadWrite
  fsType: ext4
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
---
# 标准HDD
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: azure-disk-standard
provisioner: disk.csi.azure.com
parameters:
  skuName: Standard_LRS          # 标准HDD
  kind: Managed
  cachingMode: None
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
---
# 区域冗余SSD（高可用）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: azure-disk-premium-zrs
provisioner: disk.csi.azure.com
parameters:
  skuName: Premium_ZRS           # 区域冗余SSD
  kind: Managed
  cachingMode: ReadOnly
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
```

**Azure SKU对比**：

| SKU名称         | 冗余类型       | 性能   | 价格   | 可用性          |
|-----------------|----------------|--------|--------|-----------------|
| Standard_LRS    | 本地冗余       | 低     | 低     | 99.5%           |
| StandardSSD_LRS | 本地冗余       | 中     | 中     | 99.5%           |
| Premium_LRS     | 本地冗余       | 高     | 高     | 99.9%           |
| Premium_ZRS     | 区域冗余       | 高     | 极高   | 99.99%（多AZ）  |

#### 阿里云ACK StorageClass

```yaml
# 阿里云ESSD PL3（极致性能）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: alicloud-disk-essd-pl3
provisioner: diskplugin.csi.alibabacloud.com
parameters:
  type: cloud_essd               # ESSD类型
  performanceLevel: PL3          # PL0、PL1、PL2、PL3（性能递增）
  encrypted: "true"
  fsType: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
---
# 阿里云SSD云盘
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: alicloud-disk-ssd
provisioner: diskplugin.csi.alibabacloud.com
parameters:
  type: cloud_ssd
  encrypted: "false"
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate
---
# 阿里云NAS（文件存储，支持RWX）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: alicloud-nas
provisioner: nasplugin.csi.alibabacloud.com
parameters:
  volumeAs: subpath              # subpath或filesystem
  server: "xxxxxx.cn-hangzhou.nas.aliyuncs.com:/"
mountOptions:
  - nolock
  - tcp
  - noresvport
reclaimPolicy: Retain
volumeBindingMode: Immediate
```

**阿里云ESSD性能等级对比**：

| 性能等级 | 最大IOPS | 最大吞吐量   | 延迟      | 适用场景               |
|----------|----------|--------------|-----------|------------------------|
| PL0      | 10000    | 180 MiB/s    | 亚毫秒级  | 测试环境、轻负载       |
| PL1      | 50000    | 350 MiB/s    | 亚毫秒级  | 通用工作负载           |
| PL2      | 100000   | 750 MiB/s    | 亚毫秒级  | 中大型数据库           |
| PL3      | 1000000  | 4000 MiB/s   | 微秒级    | 极致性能数据库、大数据 |

---

### 8.5.4 开源存储方案StorageClass配置

对于自建集群或混合云环境，开源存储方案是更灵活的选择：

#### NFS动态供应（NFS Subdir External Provisioner）

**部署NFS Provisioner**：

```bash
# 添加Helm仓库
$ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/

# 安装Provisioner
$ helm install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
  --set nfs.server=192.168.1.100 \
  --set nfs.path=/data/k8s-nfs \
  --set storageClass.name=nfs-client \
  --set storageClass.defaultClass=true
```

**创建的StorageClass**：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "true"        # 删除时归档而非删除
  pathPattern: "${.PVC.namespace}-${.PVC.name}"  # 子目录命名规则
mountOptions:
  - nfsvers=4.1
  - hard
  - timeo=600
  - retrans=2
reclaimPolicy: Delete
allowVolumeExpansion: false      # NFS不支持自动扩容
volumeBindingMode: Immediate
```

**动态供应实验**：

```bash
# 创建PVC
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-dynamic
  namespace: default
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: nfs-client
EOF

# PVC自动绑定
$ kubectl get pvc pvc-nfs-dynamic
NAME              STATUS   VOLUME                                     CAPACITY   STORAGECLASS
pvc-nfs-dynamic   Bound    pvc-a1b2c3d4-e5f6-7890-abcd-1234567890ab   5Gi        nfs-client

# 查看NFS服务器上的目录（自动创建）
$ ssh 192.168.1.100 "ls -la /data/k8s-nfs/"
drwxrwxrwx 2 root root 4096 Jan 14 10:30 default-pvc-nfs-dynamic-pvc-a1b2c3d4...

# 删除PVC（目录被归档）
$ kubectl delete pvc pvc-nfs-dynamic
$ ssh 192.168.1.100 "ls -la /data/k8s-nfs/"
drwxrwxrwx 2 root root 4096 Jan 14 10:35 archived-default-pvc-nfs-dynamic-pvc-a1b2c3d4...
```

#### Ceph RBD动态供应（Rook Operator）

**部署Rook Ceph**（简化版，完整部署参考Rook文档）：

```bash
# 安装Rook Operator
$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/release-1.12/deploy/examples/crds.yaml
$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/release-1.12/deploy/examples/operator.yaml

# 创建Ceph集群
$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/release-1.12/deploy/examples/cluster.yaml

# 等待Ceph集群Ready
$ kubectl -n rook-ceph get cephcluster
NAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH
rook-ceph   /var/lib/rook     3          5m    Ready   Cluster created successfully   HEALTH_OK
```

**创建RBD StorageClass**：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: replicapool                      # Ceph池名称
  imageFormat: "2"
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/fstype: ext4
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

**创建CephFS StorageClass（支持RWX）**：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  clusterID: rook-ceph
  fsName: myfs                           # CephFS文件系统名称
  pool: myfs-data0
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

**RBD vs CephFS对比**：

| 特性         | Rook Ceph RBD                   | Rook CephFS                     |
|--------------|---------------------------------|---------------------------------|
| **访问模式** | RWO（单节点读写）               | RWX（多节点读写）               |
| **性能**     | 高（块存储）                    | 中（文件系统开销）              |
| **适用场景** | 数据库、单实例应用              | 多副本应用、共享文件            |
| **快照支持** | ✅ 支持                         | ✅ 支持                         |
| **扩容支持** | ✅ 在线扩容                     | ✅ 在线扩容                     |

**Ceph RBD动态供应实验**：

```bash
# 创建PVC
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-rbd-dynamic
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: rook-ceph-block
EOF

# PVC自动绑定（Rook自动创建RBD镜像）
$ kubectl get pvc pvc-rbd-dynamic
NAME              STATUS   VOLUME                                     CAPACITY   STORAGECLASS
pvc-rbd-dynamic   Bound    pvc-b2c3d4e5-f6a7-8901-bcde-2345678901bc   10Gi       rook-ceph-block

# 查看Ceph集群中的RBD镜像
$ kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- rbd ls -p replicapool
csi-vol-b2c3d4e5-f6a7-8901-bcde-2345678901bc

# 查看镜像详情
$ kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- rbd info replicapool/csi-vol-b2c3d4e5...
rbd image 'csi-vol-b2c3d4e5...':
        size 10 GiB in 2560 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        block_name_prefix: rbd_data.abc123
        format: 2
        features: layering
```

#### Longhorn（CNCF云原生分布式存储）

**安装Longhorn**：

```bash
# 使用Helm安装
$ helm repo add longhorn https://charts.longhorn.io
$ helm repo update
$ helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace

# 等待Longhorn组件Ready
$ kubectl -n longhorn-system get pod
NAME                                        READY   STATUS    RESTARTS   AGE
longhorn-manager-xxxxx                      1/1     Running   0          2m
longhorn-driver-deployer-xxxxx              1/1     Running   0          2m
longhorn-ui-xxxxx                           1/1     Running   0          2m
```

**Longhorn自动创建的StorageClass**：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn
provisioner: driver.longhorn.io
parameters:
  numberOfReplicas: "3"                  # 数据副本数（高可用）
  staleReplicaTimeout: "2880"            # 过时副本超时（分钟）
  fromBackup: ""                         # 从备份恢复
  fsType: ext4
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

**自定义Longhorn StorageClass（高性能）**：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-fast
provisioner: driver.longhorn.io
parameters:
  numberOfReplicas: "2"                  # 2副本（平衡性能和可靠性）
  dataLocality: "best-effort"            # 数据本地性（减少网络传输）
  staleReplicaTimeout: "30"              # 快速故障转移
  diskSelector: "ssd"                    # 仅使用SSD节点
  nodeSelector: "storage-node"           # 仅在特定节点创建副本
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
```

**Longhorn动态供应实验**：

```bash
# 创建PVC
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-longhorn-demo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: longhorn
EOF

# PVC自动绑定
$ kubectl get pvc pvc-longhorn-demo
NAME                STATUS   VOLUME                                     CAPACITY   STORAGECLASS
pvc-longhorn-demo   Bound    pvc-c3d4e5f6-a7b8-9012-cdef-3456789012cd   20Gi       longhorn

# 访问Longhorn UI查看卷详情
$ kubectl -n longhorn-system port-forward svc/longhorn-frontend 8080:80
# 浏览器访问 http://localhost:8080

# 查看卷的副本分布
$ kubectl -n longhorn-system get volumes.longhorn.io pvc-c3d4e5f6... -o jsonpath='{.spec.replicas}'
3  # 3个副本分布在不同节点
```

---

### 8.5.5 默认StorageClass设置与管理

#### 设置默认StorageClass

当PVC不指定`storageClassName`时，Kubernetes会使用默认StorageClass：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"  # 标记为默认
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
```

**默认StorageClass行为验证**：

```bash
# 创建不指定storageClassName的PVC
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-use-default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  # 注意：没有storageClassName字段
EOF

# PVC自动使用默认StorageClass
$ kubectl get pvc pvc-use-default
NAME              STATUS   VOLUME                                     CAPACITY   STORAGECLASS
pvc-use-default   Bound    pvc-d4e5f6a7-b8c9-0123-def0-4567890123de   5Gi        standard

# 查看PVC详情确认
$ kubectl get pvc pvc-use-default -o yaml | grep storageClassName
storageClassName: standard  # 自动填充
```

#### 管理多个默认StorageClass（常见错误）

⚠️ **常见错误**：设置多个默认StorageClass导致PVC创建失败

```bash
# 错误场景：设置两个默认StorageClass
$ kubectl get storageclass
NAME                 PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE
fast-ssd (default)   ebs.csi.aws.com        Delete          Immediate
standard (default)   kubernetes.io/gce-pd   Delete          Immediate

# 创建不指定storageClassName的PVC
$ kubectl apply -f pvc-no-sc.yaml

# PVC保持Pending状态
$ kubectl get pvc pvc-no-sc
NAME        STATUS    VOLUME   CAPACITY   STORAGECLASS   AGE
pvc-no-sc   Pending                                      30s

# 查看事件错误
$ kubectl describe pvc pvc-no-sc
Events:
  Type     Reason              Age   From                         Message
  ----     ------              ----  ----                         -------
  Warning  ProvisioningFailed  10s   persistentvolume-controller  storageclass.storage.k8s.io "standard" not found
```

**修复方法**：只保留一个默认StorageClass

```bash
# 移除standard的默认标记
$ kubectl patch storageclass standard -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'

# 确认只有一个默认StorageClass
$ kubectl get storageclass
NAME                 PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE
fast-ssd (default)   ebs.csi.aws.com        Delete          Immediate
standard             kubernetes.io/gce-pd   Delete          Immediate

# 删除并重新创建PVC，成功绑定
$ kubectl delete pvc pvc-no-sc
$ kubectl apply -f pvc-no-sc.yaml
$ kubectl get pvc pvc-no-sc
NAME        STATUS   VOLUME                                     CAPACITY   STORAGECLASS
pvc-no-sc   Bound    pvc-e5f6a7b8-c9d0-1234-ef01-5678901234ef   5Gi        fast-ssd
```

#### 禁用默认StorageClass（显式控制）

某些场景下，你可能希望强制用户显式指定StorageClass：

```bash
# 移除所有默认StorageClass标记
$ kubectl get storageclass -o name | xargs -I {} kubectl patch {} -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'

# 确认没有默认StorageClass
$ kubectl get storageclass
NAME        PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE
fast-ssd    ebs.csi.aws.com        Delete          Immediate
standard    kubernetes.io/gce-pd   Delete          Immediate
# 注意：没有(default)标记

# 创建不指定storageClassName的PVC（失败）
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-no-default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
EOF

# PVC保持Pending状态
$ kubectl get pvc pvc-no-default
NAME             STATUS    VOLUME   CAPACITY   STORAGECLASS   AGE
pvc-no-default   Pending                                      20s

# 查看事件
$ kubectl describe pvc pvc-no-default
Events:
  Type     Reason              Age   From                         Message
  ----     ------              ----  ----                         -------
  Warning  ProvisioningFailed  10s   persistentvolume-controller  no default storage class available

# 修复：显式指定storageClassName
$ kubectl delete pvc pvc-no-default
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-explicit-sc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: fast-ssd  # 显式指定
EOF

# PVC成功绑定
$ kubectl get pvc pvc-explicit-sc
NAME               STATUS   VOLUME                                     CAPACITY   STORAGECLASS
pvc-explicit-sc    Bound    pvc-f6a7b8c9-d0e1-2345-f012-6789012345f0   5Gi        fast-ssd
```

---

### 8.5.6 完整实战案例：多层次存储架构

在生产环境中，通常会根据数据的重要性和性能需求设计多层次存储架构：

```
存储层次架构：
┌─────────────────────────────────────────────────────────────┐
│  Tier 1: 极致性能层（hot-nvme）                              │
│  - 数据库主节点、缓存                                         │
│  - NVMe SSD、高IOPS                                          │
│  - 成本：极高                                                 │
├─────────────────────────────────────────────────────────────┤
│  Tier 2: 高性能层（fast-ssd）                                │
│  - 应用数据、日志、数据库从节点                               │
│  - SSD、中等IOPS                                             │
│  - 成本：高                                                   │
├─────────────────────────────────────────────────────────────┤
│  Tier 3: 标准层（standard-hdd）                              │
│  - 备份、归档、冷数据                                         │
│  - HDD                                                       │
│  - 成本：低                                                   │
└─────────────────────────────────────────────────────────────┘
```

**创建多层次StorageClass**：

```yaml
# Tier 1: 极致性能层（AWS io2 Block Express）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hot-nvme
  labels:
    tier: hot
provisioner: ebs.csi.aws.com
parameters:
  type: io2
  iops: "64000"
  throughput: "1000"
  encrypted: "true"
allowVolumeExpansion: true
reclaimPolicy: Retain          # 关键数据，保留策略
volumeBindingMode: WaitForFirstConsumer
---
# Tier 2: 高性能层（AWS gp3）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
  labels:
    tier: fast
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  iops: "16000"
  throughput: "1000"
  encrypted: "true"
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
---
# Tier 3: 标准层（AWS st1）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard-hdd
  labels:
    tier: cold
provisioner: ebs.csi.aws.com
parameters:
  type: st1
  encrypted: "false"
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

**部署三层存储的MySQL集群**：

```yaml
# MySQL主节点（hot-nvme）
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-primary
spec:
  serviceName: mysql-primary
  replicas: 1
  selector:
    matchLabels:
      app: mysql
      role: primary
  template:
    metadata:
      labels:
        app: mysql
        role: primary
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "password"
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: hot-nvme        # Tier 1存储
      resources:
        requests:
          storage: 100Gi
---
# MySQL从节点（fast-ssd）
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-replica
spec:
  serviceName: mysql-replica
  replicas: 2
  selector:
    matchLabels:
      app: mysql
      role: replica
  template:
    metadata:
      labels:
        app: mysql
        role: replica
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "password"
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd        # Tier 2存储
      resources:
        requests:
          storage: 100Gi
---
# MySQL备份Job（standard-hdd）
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mysql-backup
spec:
  schedule: "0 2 * * *"  # 每天2点执行
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: mysql:8.0
            command:
            - /bin/bash
            - -c
            - |
              mysqldump -h mysql-primary -uroot -ppassword --all-databases > /backup/backup-$(date +%Y%m%d).sql
              # 保留最近7天的备份
              find /backup -name "backup-*.sql" -mtime +7 -delete
            volumeMounts:
            - name: backup
              mountPath: /backup
          restartPolicy: OnFailure
          volumes:
          - name: backup
            persistentVolumeClaim:
              claimName: mysql-backup-pvc
---
# 备份PVC（standard-hdd）
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-backup-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Gi               # 大容量备份空间
  storageClassName: standard-hdd   # Tier 3存储（成本优化）
```

**部署并验证多层次存储**：

```bash
# 部署所有组件
$ kubectl apply -f multi-tier-storage.yaml

# 等待所有Pod就绪
$ kubectl get pod -l app=mysql
NAME               READY   STATUS    RESTARTS   AGE
mysql-primary-0    1/1     Running   0          2m
mysql-replica-0    1/1     Running   0          2m
mysql-replica-1    1/1     Running   0          2m

# 查看PVC和对应的StorageClass
$ kubectl get pvc
NAME                      STATUS   VOLUME                                     CAPACITY   STORAGECLASS    AGE
data-mysql-primary-0      Bound    pvc-aaa111...                              100Gi      hot-nvme        2m
data-mysql-replica-0      Bound    pvc-bbb222...                              100Gi      fast-ssd        2m
data-mysql-replica-1      Bound    pvc-ccc333...                              100Gi      fast-ssd        2m
mysql-backup-pvc          Bound    pvc-ddd444...                              500Gi      standard-hdd    2m

# 性能测试对比
$ kubectl exec mysql-primary-0 -- sysbench --mysql-host=localhost --mysql-user=root --mysql-password=password /usr/share/sysbench/oltp_read_write.lua --tables=10 --table-size=100000 run
# hot-nvme: 5000+ TPS

$ kubectl exec mysql-replica-0 -- sysbench ...
# fast-ssd: 3000+ TPS

# 查看成本对比（假设AWS定价）
# hot-nvme (io2): $0.125/GB/month + $0.065/provisioned IOPS = 100GB * 0.125 + 64000 * 0.065 = $4172.5/month
# fast-ssd (gp3): $0.08/GB/month + $0.005/IOPS (超过3000) = 100GB * 0.08 + 13000 * 0.005 = $73/month
# standard-hdd (st1): $0.045/GB/month = 500GB * 0.045 = $22.5/month
```

**成本优化策略验证**：

```bash
# 场景：将MySQL从节点降级到standard-hdd（读多写少场景）
$ kubectl patch statefulset mysql-replica -p '{"spec":{"volumeClaimTemplates":[{"metadata":{"name":"data"},"spec":{"storageClassName":"standard-hdd"}}]}}'

# 注意：StatefulSet的volumeClaimTemplates不可变，需要重新创建
$ kubectl delete statefulset mysql-replica --cascade=false  # 保留Pod
$ kubectl apply -f mysql-replica-with-hdd.yaml

# 新Pod使用HDD存储
$ kubectl get pvc -l app=mysql,role=replica
NAME                      STATUS   VOLUME          CAPACITY   STORAGECLASS    AGE
data-mysql-replica-0      Bound    pvc-eee555...   100Gi      standard-hdd    30s
data-mysql-replica-1      Bound    pvc-fff666...   100Gi      standard-hdd    30s

# 成本降低：从$73/month降至$4.5/month（节省94%）
```

---

### 8.5.7 StorageClass故障排查指南

#### 问题1：PVC一直处于Pending状态

**现象**：
```bash
$ kubectl get pvc
NAME        STATUS    VOLUME   CAPACITY   STORAGECLASS   AGE
my-pvc      Pending                       fast-ssd       5m
```

**排查步骤**：

```bash
# Step 1: 查看PVC事件
$ kubectl describe pvc my-pvc
Events:
  Type     Reason              Age                From                         Message
  ----     ------              ----               ----                         -------
  Warning  ProvisioningFailed  2m (x5 over 5m)    persistentvolume-controller  storageclass.storage.k8s.io "fast-ssd" not found

# 原因：StorageClass不存在
$ kubectl get storageclass fast-ssd
Error from server (NotFound): storageclasses.storage.k8s.io "fast-ssd" not found

# 修复：创建StorageClass或修改PVC使用现有的
$ kubectl get storageclass
NAME       PROVISIONER            RECLAIMPOLICY
standard   kubernetes.io/gce-pd   Delete

$ kubectl patch pvc my-pvc -p '{"spec":{"storageClassName":"standard"}}'
```

**其他常见Pending原因**：

```bash
# 原因2：Provisioner未安装
$ kubectl describe pvc my-pvc
Events:
  Warning  ProvisioningFailed  persistentvolume-controller  Failed to provision volume with StorageClass "ceph-rbd": failed to get secret "ceph-secret"

# 排查：检查Provisioner Pod
$ kubectl get pod -n kube-system | grep csi
# 如果没有相关Pod，需要安装CSI驱动

# 原因3：配额不足
$ kubectl describe pvc my-pvc
Events:
  Warning  ProvisioningFailed  exceeded quota: storage-quota, requested: requests.storage=100Gi, used: requests.storage=900Gi, limited: requests.storage=1000Gi

# 排查：检查ResourceQuota
$ kubectl get resourcequota
NAME            AGE   REQUEST                                      LIMIT
storage-quota   10d   requests.storage: 900Gi/1000Gi

# 修复：增加配额或减少PVC请求
$ kubectl edit resourcequota storage-quota
```

#### 问题2：Pod无法挂载PVC（VolumeBindingMode问题）

**现象**：
```bash
$ kubectl get pod
NAME      READY   STATUS    RESTARTS   AGE
my-pod    0/1     Pending   0          2m

$ kubectl describe pod my-pod
Events:
  Warning  FailedScheduling  pod has unbound immediate PersistentVolumeClaims
```

**排查**：

```bash
# 检查PVC状态
$ kubectl get pvc
NAME     STATUS    VOLUME   CAPACITY   STORAGECLASS   AGE
my-pvc   Pending                       local-storage  5m

# 检查StorageClass的volumeBindingMode
$ kubectl get storageclass local-storage -o yaml | grep volumeBindingMode
volumeBindingMode: WaitForFirstConsumer

# 原因：延迟绑定模式等待Pod调度，但Pod因其他原因无法调度
$ kubectl describe pod my-pod
Events:
  Warning  FailedScheduling  0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector

# 修复：检查Pod的nodeSelector或nodeAffinity
$ kubectl get pod my-pod -o yaml | grep -A5 nodeSelector
nodeSelector:
  disk: ssd  # 要求节点有disk=ssd标签

$ kubectl get nodes --show-labels | grep disk=ssd
# 没有节点有该标签

# 添加标签
$ kubectl label node node1 disk=ssd
# Pod和PVC同时变为Running/Bound
```

#### 问题3：扩容失败

**现象**：
```bash
$ kubectl patch pvc my-pvc -p '{"spec":{"resources":{"requests":{"storage":"20Gi"}}}}'
$ kubectl get pvc my-pvc
NAME     STATUS   VOLUME                                     CAPACITY   STORAGECLASS
my-pvc   Bound    pvc-abc123...                              10Gi       fast-ssd
# 容量未变化
```

**排查**：

```bash
# 检查StorageClass是否允许扩容
$ kubectl get storageclass fast-ssd -o yaml | grep allowVolumeExpansion
allowVolumeExpansion: false  # 原因：未启用扩容

# 修复：启用扩容
$ kubectl patch storageclass fast-ssd -p '{"allowVolumeExpansion":true}'

# 重新触发扩容
$ kubectl patch pvc my-pvc -p '{"spec":{"resources":{"requests":{"storage":"10Gi"}}}}'  # 先改回原值
$ kubectl patch pvc my-pvc -p '{"spec":{"resources":{"requests":{"storage":"20Gi"}}}}'  # 再扩容

# 检查PVC Conditions
$ kubectl get pvc my-pvc -o yaml | grep -A10 conditions
conditions:
- lastProbeTime: null
  lastTransitionTime: "2024-01-14T10:30:00Z"
  message: Waiting for user to (re-)start a pod to finish file system resize of volume on node.
  status: "True"
  type: FileSystemResizePending

# 需要重启Pod完成文件系统扩容
$ kubectl delete pod my-pod
$ kubectl get pvc my-pvc
NAME     STATUS   VOLUME                                     CAPACITY   STORAGECLASS
my-pvc   Bound    pvc-abc123...                              20Gi       fast-ssd  # 扩容成功
```

---

### 8.5.8 StorageClass最佳实践总结

#### 1. 命名规范

✅ **推荐命名方式**：
```yaml
# 格式：<性能层级>-<存储类型>-<特殊属性>
hot-nvme-encrypted         # 极致性能、加密
fast-ssd-retain            # 高性能、保留策略
standard-hdd-regional      # 标准性能、区域复制
cold-s3-archive            # 冷数据、归档
```

❌ **避免的命名**：
```yaml
sc1                        # 无意义名称
my-storage                 # 过于通用
test                       # 环境不明确
```

#### 2. 标签管理

为StorageClass添加标签便于筛选和管理：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
  labels:
    tier: fast               # 性能层级
    backend: aws-ebs         # 存储后端
    encryption: enabled      # 是否加密
    environment: production  # 环境
```

```bash
# 按标签筛选
$ kubectl get storageclass -l tier=fast
$ kubectl get storageclass -l encryption=enabled
```

#### 3. 默认StorageClass策略

- ✅ 为每个集群设置一个默认StorageClass（最常用的类型）
- ✅ 默认StorageClass应平衡性能和成本（如gp3、pd-ssd）
- ⚠️ 避免设置极致性能或极低性能为默认
- ⚠️ 定期审计默认StorageClass的使用情况

#### 4. 回收策略选择

| 环境       | 推荐策略 | 原因                             |
|------------|----------|----------------------------------|
| 开发/测试  | Delete   | 自动清理，节省成本               |
| 预生产     | Retain   | 保留数据用于问题排查             |
| 生产环境   | Retain   | 防止误删，手动确认后删除         |

#### 5. VolumeBindingMode选择

| 存储类型         | 推荐模式              | 原因                                   |
|------------------|-----------------------|----------------------------------------|
| Local PV         | WaitForFirstConsumer  | 必须保证Pod和存储在同一节点            |
| 多AZ云盘         | WaitForFirstConsumer  | 避免跨AZ挂载失败                       |
| NFS/Ceph/云文件  | Immediate             | 可跨节点访问，立即绑定简化部署         |

#### 6. 安全配置

```yaml
# 生产环境StorageClass安全模板
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: production-secure
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  encrypted: "true"                         # 启用加密
  kmsKeyId: "arn:aws:kms:..."               # 指定KMS密钥
  iops: "16000"
  throughput: "1000"
allowVolumeExpansion: true
reclaimPolicy: Retain                       # 防止误删
volumeBindingMode: WaitForFirstConsumer
```

---

### 8.5.9 下一节预告

在本节中，我们深入学习了StorageClass的动态供应机制、核心字段配置、各云厂商和开源存储方案的实战配置、以及故障排查和最佳实践。我们掌握了：

- ✅ 动态供应 vs 静态供应的优劣对比
- ✅ StorageClass的完整工作流程
- ✅ Provisioner、Parameters、VolumeBindingMode等核心字段
- ✅ AWS EBS、GCP PD、Azure Disk、阿里云等云厂商配置
- ✅ NFS、Ceph RBD、Longhorn等开源方案配置
- ✅ 多层次存储架构设计（hot/fast/cold）
- ✅ 常见问题排查（Pending、扩容失败等）

然而，生产环境中还有更复杂的存储需求：**如何在Kubernetes中使用有状态应用？如何实现数据库、消息队列等服务的持久化存储？如何进行存储容量规划和监控？**

**在下一节（8.6 StatefulSet与持久化存储集成）中**，我们将学习：
- StatefulSet的存储语义和volumeClaimTemplates
- 数据库（MySQL、PostgreSQL、MongoDB）的持久化部署
- 消息队列（Kafka、RabbitMQ）的存储配置
- 有状态应用的备份和恢复策略
- 存储容量监控和告警

---

**本节完**

*下一节预告：8.6节《StatefulSet与持久化存储集成》- 深入有状态应用的持久化部署、数据库存储最佳实践和容量规划。*

## 8.6 StatefulSet与持久化存储集成

在上一节中，我们深入学习了StorageClass的动态供应机制、各云厂商和开源存储方案的配置、以及多层次存储架构设计。然而，在实际生产环境中，最具挑战性的场景是**有状态应用（Stateful Applications）的持久化部署**：数据库需要稳定的网络标识和持久化存储、消息队列需要保证数据不丢失、分布式系统需要多副本之间的数据一致性。

Kubernetes通过**StatefulSet**工作负载控制器专门解决有状态应用的部署难题。本节将深入探讨StatefulSet的存储语义、volumeClaimTemplates机制、数据库和消息队列的持久化部署最佳实践、以及备份恢复策略。

---

### 8.6.1 StatefulSet存储语义深度解析

#### Deployment vs StatefulSet存储对比

在理解StatefulSet之前，让我们先对比Deployment和StatefulSet在存储处理上的根本差异：

**Deployment + PVC的问题**：

```yaml
# deployment-with-pvc.yaml（错误示范）
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: fast-ssd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deployment
spec:
  replicas: 3  # 3个副本
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: shared-data  # 所有Pod共享同一个PVC
```

**部署后的问题**：

```bash
$ kubectl apply -f deployment-with-pvc.yaml

# 查看Pod状态
$ kubectl get pod -l app=mysql
NAME                               READY   STATUS    RESTARTS   AGE
mysql-deployment-abc123-xxxxx      1/1     Running   0          10s
mysql-deployment-abc123-yyyyy      0/1     Pending   0          10s
mysql-deployment-abc123-zzzzz      0/1     Pending   0          10s

# 查看Pending原因
$ kubectl describe pod mysql-deployment-abc123-yyyyy
Events:
  Warning  FailedAttachVolume  Multi-Attach error for volume "pvc-abc123..." Volume is already exclusively attached to one node and can't be attached to another
```

**根本原因**：
- ❌ RWO (ReadWriteOnce) PVC只能被单个节点挂载
- ❌ 3个Pod可能调度到不同节点，无法共享同一个RWO卷
- ❌ 即使都在同一节点，MySQL数据库文件不支持多进程并发写入（数据损坏）

**StatefulSet的解决方案**：

```yaml
# statefulset-with-storage.yaml（正确示范）
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql  # Headless Service名称
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "password"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:  # 关键：每个Pod独立的PVC
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 10Gi
```

**StatefulSet存储特性**：

```
Pod与PVC的一一对应关系：
┌─────────────────────────────────────────────────────┐
│  mysql-0                                            │
│  ├─ 稳定标识: mysql-0.mysql.default.svc.cluster.local│
│  └─ 独立PVC: data-mysql-0 (10Gi)                    │
├─────────────────────────────────────────────────────┤
│  mysql-1                                            │
│  ├─ 稳定标识: mysql-1.mysql.default.svc.cluster.local│
│  └─ 独立PVC: data-mysql-1 (10Gi)                    │
├─────────────────────────────────────────────────────┤
│  mysql-2                                            │
│  ├─ 稳定标识: mysql-2.mysql.default.svc.cluster.local│
│  └─ 独立PVC: data-mysql-2 (10Gi)                    │
└─────────────────────────────────────────────────────┘

关键特性：
✅ 每个Pod有独立的PVC（data-mysql-0、data-mysql-1、data-mysql-2）
✅ Pod重建后，重新绑定到原来的PVC（数据持久化）
✅ 有序创建/删除（mysql-0 → mysql-1 → mysql-2）
✅ 稳定的网络标识（DNS名称固定）
```

#### volumeClaimTemplates工作机制

**volumeClaimTemplates**是StatefulSet的核心存储特性，它为每个Pod副本自动创建独立的PVC：

```yaml
volumeClaimTemplates:
- metadata:
    name: data                    # PVC名称前缀
    labels:
      app: mysql                  # 自定义标签
  spec:
    accessModes: [ "ReadWriteOnce" ]
    storageClassName: fast-ssd
    resources:
      requests:
        storage: 10Gi
```

**自动创建的PVC命名规则**：

```
格式：<volumeClaimTemplate名称>-<StatefulSet名称>-<副本序号>

示例：
- data-mysql-0  （第0个副本）
- data-mysql-1  （第1个副本）
- data-mysql-2  （第2个副本）
```

**完整生命周期实验**：

```bash
# 创建Headless Service（StatefulSet必需）
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  clusterIP: None  # Headless Service
  selector:
    app: mysql
  ports:
  - port: 3306
    name: mysql
EOF

# 创建StatefulSet
$ kubectl apply -f statefulset-with-storage.yaml

# 观察Pod和PVC的有序创建
$ kubectl get pod,pvc -l app=mysql --watch
NAME          READY   STATUS              RESTARTS   AGE
pod/mysql-0   0/1     ContainerCreating   0          5s

NAME                               STATUS    VOLUME   CAPACITY   STORAGECLASS
persistentvolumeclaim/data-mysql-0 Pending                       fast-ssd

# 等待mysql-0完全Running后，才创建mysql-1
NAME          READY   STATUS    RESTARTS   AGE
pod/mysql-0   1/1     Running   0          30s
pod/mysql-1   0/1     Pending   0          2s

persistentvolumeclaim/data-mysql-0 Bound     pvc-abc123...   10Gi       fast-ssd
persistentvolumeclaim/data-mysql-1 Pending                              fast-ssd

# 所有Pod就绪
$ kubectl get pod,pvc -l app=mysql
NAME          READY   STATUS    RESTARTS   AGE
pod/mysql-0   1/1     Running   0          2m
pod/mysql-1   1/1     Running   0          1m30s
pod/mysql-2   1/1     Running   0          1m

NAME                               STATUS   VOLUME                                     CAPACITY   STORAGECLASS
persistentvolumeclaim/data-mysql-0 Bound    pvc-abc123-1111-2222-3333-444444444444     10Gi       fast-ssd
persistentvolumeclaim/data-mysql-1 Bound    pvc-def456-5555-6666-7777-888888888888     10Gi       fast-ssd
persistentvolumeclaim/data-mysql-2 Bound    pvc-ghi789-9999-aaaa-bbbb-cccccccccccc     10Gi       fast-ssd
```

**Pod重建后的PVC绑定验证**：

```bash
# 删除mysql-1 Pod（模拟故障）
$ kubectl delete pod mysql-1
pod "mysql-1" deleted

# Pod自动重建，重新绑定到原PVC
$ kubectl get pod mysql-1 -o jsonpath='{.spec.volumes[0].persistentVolumeClaim.claimName}'
data-mysql-1  # 仍然是同一个PVC

# 验证数据完整性
$ kubectl exec mysql-1 -- mysql -uroot -ppassword -e "SELECT COUNT(*) FROM testdb.users;"
+----------+
| COUNT(*) |
+----------+
|     1000 |
+----------+
# 数据完整保留
```

**扩缩容时的PVC行为**：

```bash
# 扩容到5个副本
$ kubectl scale statefulset mysql --replicas=5

# 新创建data-mysql-3和data-mysql-4
$ kubectl get pvc -l app=mysql
NAME             STATUS   VOLUME          CAPACITY   STORAGECLASS
data-mysql-0     Bound    pvc-abc123...   10Gi       fast-ssd
data-mysql-1     Bound    pvc-def456...   10Gi       fast-ssd
data-mysql-2     Bound    pvc-ghi789...   10Gi       fast-ssd
data-mysql-3     Bound    pvc-jkl012...   10Gi       fast-ssd  # 新创建
data-mysql-4     Bound    pvc-mno345...   10Gi       fast-ssd  # 新创建

# 缩容到2个副本
$ kubectl scale statefulset mysql --replicas=2

# Pod被删除，但PVC保留！
$ kubectl get pod -l app=mysql
NAME      READY   STATUS    RESTARTS   AGE
mysql-0   1/1     Running   0          5m
mysql-1   1/1     Running   0          5m

$ kubectl get pvc -l app=mysql
NAME             STATUS   VOLUME          CAPACITY   STORAGECLASS
data-mysql-0     Bound    pvc-abc123...   10Gi       fast-ssd
data-mysql-1     Bound    pvc-def456...   10Gi       fast-ssd
data-mysql-2     Bound    pvc-ghi789...   10Gi       fast-ssd  # 保留
data-mysql-3     Bound    pvc-jkl012...   10Gi       fast-ssd  # 保留
data-mysql-4     Bound    pvc-mno345...   10Gi       fast-ssd  # 保留

# 再次扩容到5个副本，重用已有PVC
$ kubectl scale statefulset mysql --replicas=5

# mysql-2/3/4重用原PVC，数据完整
$ kubectl exec mysql-2 -- ls /var/lib/mysql
# 数据文件完整保留
```

**关键行为总结**：
- ✅ 扩容：自动创建新PVC
- ✅ 缩容：Pod删除，但PVC保留（防止数据丢失）
- ✅ 再次扩容：重用已有PVC（数据完整恢复）
- ⚠️ 手动删除PVC需谨慎（数据永久丢失）

#### StatefulSet存储的唯一性保证

StatefulSet通过**Pod序号 + PVC绑定**确保存储的唯一性：

**实验：验证Pod与PVC的强绑定关系**

```bash
# 手动删除mysql-1的PVC（模拟存储故障）
$ kubectl delete pvc data-mysql-1
persistentvolumeclaim "data-mysql-1" deleted

# Pod进入CrashLoopBackOff状态
$ kubectl get pod mysql-1
NAME      READY   STATUS             RESTARTS   AGE
mysql-1   0/1     CrashLoopBackOff   5          2m

$ kubectl describe pod mysql-1
Events:
  Warning  FailedMount  MountVolume.SetUp failed for volume "pvc-def456..." : persistentvolumeclaim "data-mysql-1" not found

# 手动重新创建PVC（相同名称）
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-mysql-1
spec:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 10Gi
EOF

# Pod自动恢复（但数据已丢失，因为是新PVC）
$ kubectl get pod mysql-1
NAME      READY   STATUS    RESTARTS   AGE
mysql-1   1/1     Running   6          5m
```

**结论**：
- ✅ Pod强依赖特定名称的PVC（data-mysql-1）
- ❌ PVC删除后，Pod无法启动
- ⚠️ 重新创建同名PVC可恢复Pod，但数据丢失

---

### 8.6.2 数据库持久化部署完整实战

生产环境中最常见的有状态应用是数据库。下面我们将实战部署MySQL主从复制集群、PostgreSQL高可用集群、MongoDB副本集。

#### MySQL主从复制集群（1主2从）

**架构设计**：

```
MySQL主从架构：
┌────────────────────────────────────────────────────┐
│  mysql-0 (Master)                                  │
│  ├─ 读写操作                                        │
│  ├─ binlog复制到Slave                              │
│  └─ PVC: data-mysql-0 (50Gi, fast-ssd)             │
├────────────────────────────────────────────────────┤
│  mysql-1 (Slave)                                   │
│  ├─ 只读操作                                        │
│  ├─ 从Master同步数据                                │
│  └─ PVC: data-mysql-1 (50Gi, fast-ssd)             │
├────────────────────────────────────────────────────┤
│  mysql-2 (Slave)                                   │
│  ├─ 只读操作                                        │
│  ├─ 从Master同步数据                                │
│  └─ PVC: data-mysql-2 (50Gi, fast-ssd)             │
└────────────────────────────────────────────────────┘

客户端访问：
- 写操作 → mysql-0.mysql:3306
- 读操作 → mysql-read Service (负载均衡到mysql-1/2)
```

**完整部署清单**：

```yaml
# mysql-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-config
data:
  master.cnf: |
    [mysqld]
    log-bin=mysql-bin       # 启用binlog
    server-id=1             # Master的server-id
    binlog-format=ROW
    max_connections=1000
    innodb_buffer_pool_size=1G
    
  slave.cnf: |
    [mysqld]
    server-id=100           # Slave的server-id（每个Slave不同）
    relay-log=mysql-relay
    read_only=1             # 只读模式
    max_connections=1000
    innodb_buffer_pool_size=1G
---
# mysql-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
type: Opaque
stringData:
  root-password: "MySecurePassword123!"
  replication-password: "ReplicationPass456!"
---
# mysql-headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  clusterIP: None  # Headless Service
  selector:
    app: mysql
  ports:
  - port: 3306
    name: mysql
---
# mysql-read-service.yaml（只读副本负载均衡）
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  labels:
    app: mysql
spec:
  selector:
    app: mysql
  ports:
  - port: 3306
    name: mysql
  sessionAffinity: ClientIP  # 会话保持
---
# mysql-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
      # Init容器1：根据序号生成配置文件
      - name: init-mysql
        image: mysql:8.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # 根据Pod序号生成server-id
          [[ $(hostname) =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          
          # mysql-0使用master.cnf，其他使用slave.cnf
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      
      # Init容器2：克隆上一个副本的数据（仅Slave）
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          [[ $(hostname) =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0  # mysql-0跳过
          
          # 从上一个副本克隆数据
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      
      containers:
      # 主容器：MySQL
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: root-password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        livenessProbe:
          exec:
            command:
            - mysqladmin
            - ping
            - -uroot
            - -p${MYSQL_ROOT_PASSWORD}
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command:
            - mysql
            - -uroot
            - -p${MYSQL_ROOT_PASSWORD}
            - -e
            - "SELECT 1"
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      
      # Sidecar容器：xtrabackup（数据克隆服务器）
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - containerPort: 3307
          name: xtrabackup
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql
          # 启动xtrabackup服务器，等待下一个副本克隆
          if [[ -f xtrabackup_slave_info && ! -f change_master_to.sql.in ]]; then
            cat xtrabackup_slave_info | sed -e 's/;$//g' > change_master_to.sql.in
            rm -f xtrabackup_slave_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            [[ $(cat xtrabackup_binlog_info) =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm -f xtrabackup_binlog_info xtrabackup_binary_logs
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi
          
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -uroot -p${MYSQL_ROOT_PASSWORD} -e "SELECT 1"; do sleep 1; done
            
            echo "Initializing replication from clone position"
            mysql -uroot -p${MYSQL_ROOT_PASSWORD} <<EOF
          $(<change_master_to.sql.in),
            MASTER_HOST='mysql-0.mysql',
            MASTER_USER='root',
            MASTER_PASSWORD='${MYSQL_ROOT_PASSWORD}',
            MASTER_CONNECT_RETRY=10;
          START SLAVE;
          EOF
          fi
          
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xtar --host=127.0.0.1 --user=root --password=${MYSQL_ROOT_PASSWORD}"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
      
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql-config
  
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 50Gi
```

**部署和验证**：

```bash
# 部署所有资源
$ kubectl apply -f mysql-configmap.yaml
$ kubectl apply -f mysql-secret.yaml
$ kubectl apply -f mysql-headless-service.yaml
$ kubectl apply -f mysql-read-service.yaml
$ kubectl apply -f mysql-statefulset.yaml

# 等待所有Pod就绪
$ kubectl get pod -l app=mysql --watch
NAME      READY   STATUS     RESTARTS   AGE
mysql-0   2/2     Running    0          2m
mysql-1   2/2     Running    0          4m
mysql-2   2/2     Running    0          6m

# 验证主从复制状态
$ kubectl exec mysql-1 -- mysql -uroot -p${MYSQL_ROOT_PASSWORD} -e "SHOW SLAVE STATUS\G" | grep Running
Slave_IO_Running: Yes
Slave_SQL_Running: Yes

# 在Master写入数据
$ kubectl exec mysql-0 -- mysql -uroot -p${MYSQL_ROOT_PASSWORD} <<EOF
CREATE DATABASE testdb;
USE testdb;
CREATE TABLE users (id INT PRIMARY KEY, name VARCHAR(50));
INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob'), (3, 'Charlie');
EOF

# 在Slave读取数据（验证复制）
$ kubectl exec mysql-1 -- mysql -uroot -p${MYSQL_ROOT_PASSWORD} -e "SELECT * FROM testdb.users;"
+----+---------+
| id | name    |
+----+---------+
|  1 | Alice   |
|  2 | Bob     |
|  3 | Charlie |
+----+---------+

# 验证mysql-read Service负载均衡
$ for i in {1..10}; do 
    kubectl run mysql-client-$i --image=mysql:8.0 --rm -it --restart=Never -- \
    mysql -h mysql-read -uroot -p${MYSQL_ROOT_PASSWORD} -e "SELECT @@hostname;"
done
# 输出：mysql-1、mysql-2、mysql-1、mysql-2...（轮询）
```

**数据持久化验证**：

```bash
# 删除mysql-1 Pod
$ kubectl delete pod mysql-1

# 等待Pod重建
$ kubectl get pod mysql-1 --watch
NAME      READY   STATUS              RESTARTS   AGE
mysql-1   0/2     ContainerCreating   0          5s
mysql-1   2/2     Running             0          30s

# 数据完整保留
$ kubectl exec mysql-1 -- mysql -uroot -p${MYSQL_ROOT_PASSWORD} -e "SELECT COUNT(*) FROM testdb.users;"
+----------+
| COUNT(*) |
+----------+
|        3 |
+----------+
```

#### PostgreSQL高可用集群（Patroni + etcd）

**架构设计**：

```
PostgreSQL HA架构（使用Patroni）：
┌────────────────────────────────────────────────────┐
│  postgres-0 (Leader)                               │
│  ├─ Patroni自动选举为Leader                        │
│  ├─ 读写操作                                        │
│  └─ PVC: data-postgres-0 (100Gi)                   │
├────────────────────────────────────────────────────┤
│  postgres-1 (Replica)                              │
│  ├─ 只读副本，自动同步                              │
│  └─ PVC: data-postgres-1 (100Gi)                   │
├────────────────────────────────────────────────────┤
│  postgres-2 (Replica)                              │
│  ├─ 只读副本，自动同步                              │
│  └─ PVC: data-postgres-2 (100Gi)                   │
└────────────────────────────────────────────────────┘

etcd集群（存储Patroni状态）：
- etcd-0.etcd:2379
- etcd-1.etcd:2379
- etcd-2.etcd:2379

特性：
✅ 自动故障转移（Leader宕机，自动选举新Leader）
✅ 流复制（Streaming Replication）
✅ 同步提交（Synchronous Commit）保证数据一致性
```

**完整部署清单**（简化版，完整版使用Helm Chart）：

```yaml
# postgres-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
data:
  patroni.yml: |
    scope: postgres-cluster
    name: ${HOSTNAME}
    
    restapi:
      listen: 0.0.0.0:8008
      connect_address: ${HOSTNAME}.postgres:8008
    
    etcd:
      hosts: etcd-0.etcd:2379,etcd-1.etcd:2379,etcd-2.etcd:2379
    
    bootstrap:
      dcs:
        ttl: 30
        loop_wait: 10
        retry_timeout: 10
        maximum_lag_on_failover: 1048576
        postgresql:
          use_pg_rewind: true
          parameters:
            max_connections: 500
            shared_buffers: 2GB
            effective_cache_size: 6GB
            maintenance_work_mem: 512MB
            checkpoint_completion_target: 0.9
            wal_buffers: 16MB
            default_statistics_target: 100
            random_page_cost: 1.1
            effective_io_concurrency: 200
            work_mem: 10MB
            min_wal_size: 1GB
            max_wal_size: 4GB
      
      initdb:
      - encoding: UTF8
      - data-checksums
      
      pg_hba:
      - host replication replicator 0.0.0.0/0 md5
      - host all all 0.0.0.0/0 md5
      
      users:
        admin:
          password: admin123
          options:
            - createrole
            - createdb
    
    postgresql:
      listen: 0.0.0.0:5432
      connect_address: ${HOSTNAME}.postgres:5432
      data_dir: /var/lib/postgresql/data/pgdata
      authentication:
        replication:
          username: replicator
          password: replicator123
        superuser:
          username: postgres
          password: postgres123
---
# postgres-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: patroni:latest  # 包含Patroni的PostgreSQL镜像
        env:
        - name: PATRONI_SCOPE
          value: postgres-cluster
        - name: PATRONI_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: PATRONI_POSTGRESQL_DATA_DIR
          value: /var/lib/postgresql/data/pgdata
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        ports:
        - containerPort: 5432
          name: postgres
        - containerPort: 8008
          name: patroni
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
        - name: config
          mountPath: /etc/patroni
        resources:
          requests:
            cpu: 1000m
            memory: 2Gi
          limits:
            cpu: 4000m
            memory: 8Gi
        livenessProbe:
          httpGet:
            path: /liveness
            port: 8008
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /readiness
            port: 8008
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: config
        configMap:
          name: postgres-config
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
```

**验证高可用性**：

```bash
# 查看Patroni集群状态
$ kubectl exec postgres-0 -- patronictl -c /etc/patroni/patroni.yml list
+ Cluster: postgres-cluster (7123456789012345678) -----+----+-----------+
| Member     | Host           | Role    | State   | TL | Lag in MB |
+------------+----------------+---------+---------+----+-----------+
| postgres-0 | 10.244.1.10    | Leader  | running |  1 |           |
| postgres-1 | 10.244.2.20    | Replica | running |  1 |         0 |
| postgres-2 | 10.244.3.30    | Replica | running |  1 |         0 |
+------------+----------------+---------+---------+----+-----------+

# 模拟Leader故障（删除postgres-0）
$ kubectl delete pod postgres-0 --force --grace-period=0

# 自动故障转移（postgres-1或postgres-2成为新Leader）
$ kubectl exec postgres-1 -- patronictl -c /etc/patroni/patroni.yml list
+ Cluster: postgres-cluster (7123456789012345678) -----+----+-----------+
| Member     | Host           | Role    | State   | TL | Lag in MB |
+------------+----------------+---------+---------+----+-----------+
| postgres-1 | 10.244.2.20    | Leader  | running |  2 |           |  # 新Leader
| postgres-2 | 10.244.3.30    | Replica | running |  2 |         0 |
+------------+----------------+---------+---------+----+-----------+

# postgres-0恢复后，自动成为Replica
$ kubectl get pod postgres-0 --watch
NAME         READY   STATUS    RESTARTS   AGE
postgres-0   1/1     Running   0          1m

$ kubectl exec postgres-1 -- patronictl -c /etc/patroni/patroni.yml list
+ Cluster: postgres-cluster (7123456789012345678) -----+----+-----------+
| Member     | Host           | Role    | State   | TL | Lag in MB |
+------------+----------------+---------+---------+----+-----------+
| postgres-0 | 10.244.1.10    | Replica | running |  2 |         0 |  # 自动降级为Replica
| postgres-1 | 10.244.2.20    | Leader  | running |  2 |           |
| postgres-2 | 10.244.3.30    | Replica | running |  2 |         0 |
+------------+----------------+---------+---------+----+-----------+
```

#### MongoDB副本集（3节点）

**架构设计**：

```
MongoDB ReplicaSet架构：
┌────────────────────────────────────────────────────┐
│  mongo-0 (Primary)                                 │
│  ├─ 读写操作                                        │
│  ├─ Oplog复制                                      │
│  └─ PVC: data-mongo-0 (200Gi)                      │
├────────────────────────────────────────────────────┤
│  mongo-1 (Secondary)                               │
│  ├─ 只读操作（可选）                                │
│  ├─ 参与选举                                        │
│  └─ PVC: data-mongo-1 (200Gi)                      │
├────────────────────────────────────────────────────┤
│  mongo-2 (Secondary)                               │
│  ├─ 只读操作（可选）                                │
│  ├─ 参与选举                                        │
│  └─ PVC: data-mongo-2 (200Gi)                      │
└────────────────────────────────────────────────────┘

特性：
✅ 自动选举（Primary宕机，Secondary自动选举）
✅ 数据冗余（3副本保证高可用）
✅ 读写分离（Secondary可承担读操作）
```

**完整部署清单**：

```yaml
# mongo-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: mongo-secret
type: Opaque
stringData:
  root-password: "MongoRootPass123!"
  replica-key: "MyReplicaSetKey567890ABCDEF"  # 副本集认证密钥
---
# mongo-headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  clusterIP: None
  selector:
    app: mongo
  ports:
  - port: 27017
    name: mongo
---
# mongo-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
spec:
  serviceName: mongo
  replicas: 3
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongo
        image: mongo:6.0
        command:
        - mongod
        - --replSet
        - rs0
        - --bind_ip_all
        - --keyFile
        - /etc/mongo-key/replica-key
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: "root"
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongo-secret
              key: root-password
        ports:
        - containerPort: 27017
          name: mongo
        volumeMounts:
        - name: data
          mountPath: /data/db
        - name: mongo-key
          mountPath: /etc/mongo-key
          readOnly: true
        resources:
          requests:
            cpu: 1000m
            memory: 2Gi
          limits:
            cpu: 4000m
            memory: 8Gi
        livenessProbe:
          exec:
            command:
            - mongosh
            - --eval
            - "db.adminCommand('ping')"
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - mongosh
            - --eval
            - "db.adminCommand('ping')"
          initialDelaySeconds: 5
          periodSeconds: 5
      
      # Sidecar容器：初始化副本集
      initContainers:
      - name: init-replica
        image: mongo:6.0
        command:
        - bash
        - -c
        - |
          # 仅在mongo-0执行初始化
          if [[ $(hostname) == "mongo-0" ]]; then
            sleep 10  # 等待所有Pod启动
            mongosh --host mongo-0.mongo --username root --password ${MONGO_INITDB_ROOT_PASSWORD} --authenticationDatabase admin <<EOF
            rs.initiate({
              _id: "rs0",
              members: [
                {_id: 0, host: "mongo-0.mongo:27017"},
                {_id: 1, host: "mongo-1.mongo:27017"},
                {_id: 2, host: "mongo-2.mongo:27017"}
              ]
            })
          EOF
          fi
        env:
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongo-secret
              key: root-password
      
      volumes:
      - name: mongo-key
        secret:
          secretName: mongo-secret
          items:
          - key: replica-key
            path: replica-key
          defaultMode: 0400  # 只读权限
  
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 200Gi
```

**验证副本集状态**：

```bash
# 查看副本集状态
$ kubectl exec mongo-0 -- mongosh --username root --password ${MONGO_INITDB_ROOT_PASSWORD} --authenticationDatabase admin --eval "rs.status()" | grep -E "(name|stateStr)"
"name" : "mongo-0.mongo:27017",
"stateStr" : "PRIMARY",
"name" : "mongo-1.mongo:27017",
"stateStr" : "SECONDARY",
"name" : "mongo-2.mongo:27017",
"stateStr" : "SECONDARY",

# 写入数据到Primary
$ kubectl exec mongo-0 -- mongosh --username root --password ${MONGO_INITDB_ROOT_PASSWORD} --authenticationDatabase admin <<EOF
use testdb
db.users.insertMany([
  {name: "Alice", age: 30},
  {name: "Bob", age: 25},
  {name: "Charlie", age: 35}
])
EOF

# 从Secondary读取（需要设置slaveOk）
$ kubectl exec mongo-1 -- mongosh --username root --password ${MONGO_INITDB_ROOT_PASSWORD} --authenticationDatabase admin <<EOF
use testdb
db.getMongo().setReadPref("secondary")
db.users.find()
EOF
# 输出：3条文档

# 模拟Primary故障
$ kubectl delete pod mongo-0 --force --grace-period=0

# 等待选举（mongo-1或mongo-2成为新Primary）
$ kubectl exec mongo-1 -- mongosh --username root --password ${MONGO_INITDB_ROOT_PASSWORD} --authenticationDatabase admin --eval "rs.status()" | grep -E "(name|stateStr)"
"name" : "mongo-1.mongo:27017",
"stateStr" : "PRIMARY",  # 新Primary
"name" : "mongo-2.mongo:27017",
"stateStr" : "SECONDARY",
```

---

### 8.6.3 消息队列持久化存储配置

#### Kafka集群（3 Broker + Zookeeper）

**架构设计**：

```
Kafka集群架构：
┌────────────────────────────────────────────────────┐
│  Zookeeper集群（元数据管理）                        │
│  ├─ zk-0.zk:2181                                   │
│  ├─ zk-1.zk:2181                                   │
│  └─ zk-2.zk:2181                                   │
│  └─ PVC: data-zk-{0,1,2} (20Gi each)               │
├────────────────────────────────────────────────────┤
│  Kafka Broker集群                                  │
│  ├─ kafka-0.kafka:9092                             │
│  │  └─ PVC: data-kafka-0 (500Gi)                   │
│  ├─ kafka-1.kafka:9092                             │
│  │  └─ PVC: data-kafka-1 (500Gi)                   │
│  └─ kafka-2.kafka:9092                             │
│     └─ PVC: data-kafka-2 (500Gi)                   │
└────────────────────────────────────────────────────┘

存储策略：
- Zookeeper: 小容量、高IOPS（fast-ssd）
- Kafka: 大容量、高吞吐量（standard-ssd或Throughput Optimized HDD）
```

**Kafka StatefulSet配置**（使用Strimzi Operator简化，这里展示核心部分）：

```yaml
# kafka-statefulset.yaml（简化版）
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
spec:
  serviceName: kafka
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.5.0
        ports:
        - containerPort: 9092
          name: kafka
        env:
        - name: KAFKA_BROKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: KAFKA_ZOOKEEPER_CONNECT
          value: "zk-0.zk:2181,zk-1.zk:2181,zk-2.zk:2181"
        - name: KAFKA_ADVERTISED_LISTENERS
          value: "PLAINTEXT://$(POD_NAME).kafka:9092"
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "3"
        - name: KAFKA_LOG_DIRS
          value: "/var/lib/kafka/data"
        - name: KAFKA_LOG_RETENTION_HOURS
          value: "168"  # 7天
        - name: KAFKA_LOG_SEGMENT_BYTES
          value: "1073741824"  # 1GB
        volumeMounts:
        - name: data
          mountPath: /var/lib/kafka/data
        resources:
          requests:
            cpu: 2000m
            memory: 4Gi
          limits:
            cpu: 8000m
            memory: 16Gi
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: throughput-optimized  # AWS st1或类似
      resources:
        requests:
          storage: 500Gi
```

**存储容量规划**：

```bash
# Kafka数据量计算公式
每日数据量 = 消息速率 × 消息大小 × 86400秒
保留天数 = 7天（KAFKA_LOG_RETENTION_HOURS）
副本因子 = 3（KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR）

示例计算：
- 消息速率：10,000 msg/s
- 平均消息大小：1 KB
- 每日数据量 = 10,000 × 1KB × 86400 = 864 GB/天
- 7天保留 = 864 × 7 = 6048 GB
- 3副本 = 6048 × 3 = 18144 GB ≈ 18 TB
- 单Broker = 18 TB / 3 = 6 TB

建议：
- 预留50%缓冲 → 单Broker需要9TB存储
- 实际配置：10TB per Broker
```

#### RabbitMQ集群（3节点镜像队列）

**架构设计**：

```
RabbitMQ集群架构：
┌────────────────────────────────────────────────────┐
│  rabbitmq-0                                        │
│  ├─ Disk Node                                      │
│  ├─ 队列镜像                                        │
│  └─ PVC: data-rabbitmq-0 (100Gi)                   │
├────────────────────────────────────────────────────┤
│  rabbitmq-1                                        │
│  ├─ Disk Node                                      │
│  ├─ 队列镜像                                        │
│  └─ PVC: data-rabbitmq-1 (100Gi)                   │
├────────────────────────────────────────────────────┤
│  rabbitmq-2                                        │
│  ├─ Disk Node                                      │
│  ├─ 队列镜像                                        │
│  └─ PVC: data-rabbitmq-2 (100Gi)                   │
└────────────────────────────────────────────────────┘

特性：
✅ 镜像队列（高可用）
✅ 自动集群发现（K8s插件）
✅ 持久化消息存储
```

**完整部署清单**：

```yaml
# rabbitmq-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rabbitmq
spec:
  serviceName: rabbitmq
  replicas: 3
  selector:
    matchLabels:
      app: rabbitmq
  template:
    metadata:
      labels:
        app: rabbitmq
    spec:
      serviceAccountName: rabbitmq  # 需要权限查询K8s API
      containers:
      - name: rabbitmq
        image: rabbitmq:3.12-management
        env:
        - name: RABBITMQ_DEFAULT_USER
          value: "admin"
        - name: RABBITMQ_DEFAULT_PASS
          value: "admin123"
        - name: RABBITMQ_ERLANG_COOKIE
          value: "secret-cookie-12345"
        - name: RABBITMQ_USE_LONGNAME
          value: "true"
        - name: RABBITMQ_NODENAME
          value: "rabbit@$(POD_NAME).rabbitmq.$(POD_NAMESPACE).svc.cluster.local"
        - name: K8S_SERVICE_NAME
          value: "rabbitmq"
        - name: K8S_HOSTNAME_SUFFIX
          value: ".rabbitmq.$(POD_NAMESPACE).svc.cluster.local"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        ports:
        - containerPort: 5672
          name: amqp
        - containerPort: 15672
          name: management
        volumeMounts:
        - name: data
          mountPath: /var/lib/rabbitmq
        - name: config
          mountPath: /etc/rabbitmq
        resources:
          requests:
            cpu: 1000m
            memory: 2Gi
          limits:
            cpu: 4000m
            memory: 8Gi
        livenessProbe:
          exec:
            command:
            - rabbitmq-diagnostics
            - ping
          initialDelaySeconds: 60
          periodSeconds: 60
        readinessProbe:
          exec:
            command:
            - rabbitmq-diagnostics
            - check_running
          initialDelaySeconds: 20
          periodSeconds: 10
      volumes:
      - name: config
        configMap:
          name: rabbitmq-config
          items:
          - key: enabled_plugins
            path: enabled_plugins
          - key: rabbitmq.conf
            path: rabbitmq.conf
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
---
# rabbitmq-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: rabbitmq-config
data:
  enabled_plugins: |
    [rabbitmq_management,rabbitmq_peer_discovery_k8s].
  
  rabbitmq.conf: |
    cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s
    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
    cluster_formation.k8s.address_type = hostname
    cluster_formation.k8s.service_name = rabbitmq
    cluster_formation.node_cleanup.interval = 30
    cluster_formation.node_cleanup.only_log_warning = true
    cluster_partition_handling = autoheal
    
    # 镜像队列策略
    queue_master_locator = min-masters
    
    # 持久化配置
    disk_free_limit.absolute = 50GB
```

**验证集群状态**：

```bash
# 查看集群状态
$ kubectl exec rabbitmq-0 -- rabbitmqctl cluster_status
Cluster status of node rabbit@rabbitmq-0.rabbitmq.default.svc.cluster.local ...
[{nodes,[{disc,['rabbit@rabbitmq-0.rabbitmq.default.svc.cluster.local',
                'rabbit@rabbitmq-1.rabbitmq.default.svc.cluster.local',
                'rabbit@rabbitmq-2.rabbitmq.default.svc.cluster.local']}]},
 {running_nodes,['rabbit@rabbitmq-2.rabbitmq.default.svc.cluster.local',
                 'rabbit@rabbitmq-1.rabbitmq.default.svc.cluster.local',
                 'rabbit@rabbitmq-0.rabbitmq.default.svc.cluster.local']}]

# 创建镜像队列策略
$ kubectl exec rabbitmq-0 -- rabbitmqctl set_policy ha-all ".*" '{"ha-mode":"all","ha-sync-mode":"automatic"}'
Setting policy "ha-all" for pattern ".*" to "{"ha-mode":"all","ha-sync-mode":"automatic"}" with priority "0" for vhost "/" ...
```

---

### 8.6.4 备份和恢复策略

#### 使用Volume Snapshot备份

**前提条件**：
- CSI驱动支持VolumeSnapshot特性
- 已安装Snapshot Controller和CRD

**创建VolumeSnapshotClass**：

```yaml
# volumesnapshotclass.yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: csi-snapshot-class
driver: ebs.csi.aws.com  # 对应StorageClass的provisioner
deletionPolicy: Retain
```

**创建MySQL数据快照**：

```yaml
# mysql-snapshot.yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: mysql-snapshot-20240114
spec:
  volumeSnapshotClassName: csi-snapshot-class
  source:
    persistentVolumeClaimName: data-mysql-0  # 备份mysql-0的数据
```

```bash
# 创建快照
$ kubectl apply -f mysql-snapshot.yaml

# 查看快照状态
$ kubectl get volumesnapshot
NAME                      READYTOUSE   SOURCEPVC       SOURCESNAPSHOTCONTENT   RESTORSIZE   SNAPSHOTCLASS          AGE
mysql-snapshot-20240114   true         data-mysql-0                            50Gi         csi-snapshot-class     30s

# 查看快照详情
$ kubectl describe volumesnapshot mysql-snapshot-20240114
Status:
  Bound Volume Snapshot Content Name:  snapcontent-abc123...
  Creation Time:                       2024-01-14T10:30:00Z
  Ready To Use:                        true
  Restore Size:                        50Gi
```

**从快照恢复数据**：

```yaml
# restore-from-snapshot.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-restored-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 50Gi
  dataSource:
    name: mysql-snapshot-20240114
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
```

```bash
# 创建恢复PVC
$ kubectl apply -f restore-from-snapshot.yaml

# PVC自动从快照恢复数据
$ kubectl get pvc mysql-restored-pvc
NAME                  STATUS   VOLUME                                     CAPACITY   STORAGECLASS
mysql-restored-pvc    Bound    pvc-xyz789...                              50Gi       fast-ssd

# 创建Pod使用恢复的PVC验证数据
$ kubectl run mysql-restore-test --image=mysql:8.0 --restart=Never \
  --overrides='{"spec":{"containers":[{"name":"mysql","image":"mysql:8.0","env":[{"name":"MYSQL_ROOT_PASSWORD","value":"password"}],"volumeMounts":[{"name":"data","mountPath":"/var/lib/mysql"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"mysql-restored-pvc"}}]}}'

# 验证数据完整性
$ kubectl exec mysql-restore-test -- mysql -uroot -ppassword -e "SHOW DATABASES;"
+--------------------+
| Database           |
+--------------------+
| testdb             |
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
```

#### 使用CronJob定期备份

**PostgreSQL定期备份方案**：

```yaml
# postgres-backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
spec:
  schedule: "0 2 * * *"  # 每天2点执行
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: backup
            image: postgres:15
            command:
            - /bin/bash
            - -c
            - |
              set -e
              BACKUP_FILE="/backup/postgres-backup-$(date +%Y%m%d-%H%M%S).sql.gz"
              
              echo "Starting backup at $(date)"
              pg_dumpall -h postgres-0.postgres -U postgres | gzip > ${BACKUP_FILE}
              
              echo "Backup completed: ${BACKUP_FILE}"
              ls -lh ${BACKUP_FILE}
              
              # 删除30天前的备份
              find /backup -name "postgres-backup-*.sql.gz" -mtime +30 -delete
              echo "Old backups cleaned up"
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: root-password
            volumeMounts:
            - name: backup
              mountPath: /backup
          volumes:
          - name: backup
            persistentVolumeClaim:
              claimName: postgres-backup-pvc
---
# postgres-backup-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-backup-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: standard-hdd  # 使用低成本存储
  resources:
    requests:
      storage: 1Ti  # 大容量备份空间
```

**手动触发备份**：

```bash
# 手动创建Job
$ kubectl create job postgres-backup-manual --from=cronjob/postgres-backup

# 查看备份进度
$ kubectl logs -f job/postgres-backup-manual
Starting backup at Sun Jan 14 02:00:00 UTC 2024
Backup completed: /backup/postgres-backup-20240114-020000.sql.gz
-rw-r--r-- 1 postgres postgres 5.2G Jan 14 02:05 /backup/postgres-backup-20240114-020000.sql.gz
Old backups cleaned up

# 验证备份文件
$ kubectl exec -it $(kubectl get pod -l app=postgres -o jsonpath='{.items[0].metadata.name}') -- ls -lh /backup/
total 15G
-rw-r--r-- 1 postgres postgres 5.2G Jan 14 02:05 postgres-backup-20240114-020000.sql.gz
-rw-r--r-- 1 postgres postgres 5.1G Jan 13 02:00 postgres-backup-20240113-020000.sql.gz
-rw-r--r-- 1 postgres postgres 5.0G Jan 12 02:00 postgres-backup-20240112-020000.sql.gz
```

**从备份恢复**：

```bash
# 恢复最新备份
$ kubectl exec -it postgres-0 -- bash
postgres@postgres-0:/$ cd /backup
postgres@postgres-0:/backup$ gunzip -c postgres-backup-20240114-020000.sql.gz | psql -U postgres
# 恢复完成
```

---

### 8.6.5 存储容量监控和告警

#### 使用Prometheus监控PVC使用率

**部署metrics-exporter**：

```yaml
# pvc-exporter.yaml（简化示例）
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: pvc-exporter
spec:
  selector:
    matchLabels:
      app: pvc-exporter
  template:
    metadata:
      labels:
        app: pvc-exporter
    spec:
      hostNetwork: true
      containers:
      - name: exporter
        image: node-exporter:latest
        args:
        - --path.rootfs=/host
        volumeMounts:
        - name: root
          mountPath: /host
          readOnly: true
      volumes:
      - name: root
        hostPath:
          path: /
```

**Prometheus查询PVC使用率**：

```promql
# PVC使用率（百分比）
(kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100

# 筛选使用率>80%的PVC
(kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 80

# 按PVC名称聚合
sum by (persistentvolumeclaim) (kubelet_volume_stats_used_bytes)
```

**配置告警规则**：

```yaml
# prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: pvc-alerts
spec:
  groups:
  - name: pvc
    interval: 30s
    rules:
    # PVC使用率>80%
    - alert: PVCHighUsage
      expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PVC {{ $labels.persistentvolumeclaim }} usage is {{ $value }}%"
        description: "PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is using {{ $value }}% of capacity"
    
    # PVC使用率>90%
    - alert: PVCCriticalUsage
      expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 90
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "PVC {{ $labels.persistentvolumeclaim }} usage is {{ $value }}%"
        description: "CRITICAL: PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is using {{ $value }}% of capacity. Immediate action required!"
    
    # PVC即将满（剩余空间<5GB）
    - alert: PVCLowSpace
      expr: kubelet_volume_stats_available_bytes < 5 * 1024 * 1024 * 1024
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PVC {{ $labels.persistentvolumeclaim }} has low available space"
        description: "PVC {{ $labels.persistentvolumeclaim }} has only {{ $value | humanize }}B available"
```

#### Grafana可视化仪表盘

**PVC监控仪表盘查询示例**：

```json
{
  "panels": [
    {
      "title": "PVC Usage by Namespace",
      "targets": [
        {
          "expr": "sum by (namespace) (kubelet_volume_stats_used_bytes)"
        }
      ]
    },
    {
      "title": "Top 10 PVCs by Usage",
      "targets": [
        {
          "expr": "topk(10, kubelet_volume_stats_used_bytes)"
        }
      ]
    },
    {
      "title": "PVC Usage Trend (7 days)",
      "targets": [
        {
          "expr": "kubelet_volume_stats_used_bytes{persistentvolumeclaim=\"data-mysql-0\"}[7d]"
        }
      ]
    }
  ]
}
```

**自动扩容策略（结合Prometheus和Operator）**：

```bash
# 伪代码逻辑（需要自定义Operator实现）
if PVC_USAGE > 80% AND ALLOW_VOLUME_EXPANSION == true:
    NEW_SIZE = CURRENT_SIZE * 1.5
    kubectl patch pvc <pvc-name> -p '{"spec":{"resources":{"requests":{"storage":"<NEW_SIZE>"}}}}'
    send_notification("PVC <pvc-name> auto-expanded to <NEW_SIZE>")
```

---

### 8.6.6 StatefulSet存储最佳实践总结

#### 1. 容量规划

**数据库容量规划公式**：

```
实际容量需求 = 业务数据量 × 增长系数 × 安全系数

示例（MySQL）：
- 当前数据量：100GB
- 年增长率：50%
- 计划使用年限：3年
- 预估数据量 = 100GB × (1 + 0.5)^3 ≈ 338GB
- 安全系数：1.5（预留索引、日志、临时表空间）
- 推荐容量 = 338GB × 1.5 ≈ 507GB
- 实际配置：512GB（向上取整到标准规格）
```

**Kafka容量规划**（参考8.6.3节）：

```
单Broker容量 = (消息速率 × 消息大小 × 86400 × 保留天数 × 副本因子) / Broker数量 × 安全系数

示例：
- 消息速率：10,000 msg/s
- 平均消息大小：1KB
- 保留天数：7天
- 副本因子：3
- Broker数量：3
- 安全系数：1.5
- 单Broker = (10,000 × 1KB × 86400 × 7 × 3) / 3 × 1.5 ≈ 9TB
```

#### 2. StorageClass选择

| 应用类型       | 性能需求      | 推荐StorageClass        | 容量范围        |
|----------------|---------------|-------------------------|-----------------|
| MySQL/PG主节点 | 高IOPS、低延迟| fast-ssd (gp3/io2)      | 100GB - 2TB     |
| MySQL/PG从节点 | 中等IOPS      | standard-ssd (gp3)      | 100GB - 2TB     |
| MongoDB        | 高IOPS、低延迟| fast-ssd                | 200GB - 5TB     |
| Redis          | 极高IOPS      | hot-nvme (Local SSD)    | 10GB - 100GB    |
| Kafka          | 高吞吐量      | throughput-optimized    | 500GB - 10TB    |
| Zookeeper      | 中等IOPS      | fast-ssd                | 10GB - 50GB     |
| RabbitMQ       | 中等IOPS      | fast-ssd                | 50GB - 500GB    |
| 备份存储       | 低成本        | standard-hdd (st1/sc1)  | 1TB - 100TB     |

#### 3. 资源限制配置

**数据库Pod资源推荐**：

```yaml
# MySQL生产环境
resources:
  requests:
    cpu: 2000m           # 2核起步
    memory: 4Gi          # 根据innodb_buffer_pool_size配置
  limits:
    cpu: 8000m           # 允许突发到8核
    memory: 16Gi         # 预留OS和其他进程内存

# PostgreSQL生产环境
resources:
  requests:
    cpu: 2000m
    memory: 4Gi          # 根据shared_buffers配置
  limits:
    cpu: 8000m
    memory: 16Gi

# MongoDB生产环境
resources:
  requests:
    cpu: 4000m           # MongoDB对CPU需求更高
    memory: 8Gi
  limits:
    cpu: 16000m
    memory: 32Gi
```

#### 4. 备份策略

**3-2-1备份原则**：
- **3**份数据副本（1份生产 + 2份备份）
- **2**种不同介质（本地快照 + 远程对象存储）
- **1**份异地备份（不同区域/可用区）

**备份频率建议**：

| 数据重要性 | 全量备份 | 增量备份 | 保留周期 |
|-----------|---------|---------|---------|
| 关键业务   | 每天    | 每小时   | 30天    |
| 重要业务   | 每天    | 每4小时  | 14天    |
| 一般业务   | 每周    | 每天     | 7天     |

#### 5. 故障演练

定期进行以下演练：

```bash
# 演练1：单节点故障恢复
$ kubectl delete pod <statefulset-name>-0 --force --grace-period=0
# 验证：Pod自动重建、数据完整、服务可用

# 演练2：PVC扩容
$ kubectl patch pvc data-<name>-0 -p '{"spec":{"resources":{"requests":{"storage":"200Gi"}}}}'
# 验证：在线扩容成功、业务无中断

# 演练3：从快照恢复
# 1. 创建快照
# 2. 删除原PVC
# 3. 从快照创建新PVC
# 4. 验证数据完整性

# 演练4：跨可用区迁移
# 1. 创建新StatefulSet（不同可用区）
# 2. 数据同步
# 3. 切换流量
# 4. 删除旧StatefulSet
```

---

### 8.6.7 下一节预告

在本节中，我们深入学习了StatefulSet与持久化存储的集成，涵盖了存储语义、数据库和消息队列的持久化部署、备份恢复策略、以及容量监控。我们掌握了：

- ✅ StatefulSet vs Deployment的存储差异
- ✅ volumeClaimTemplates工作机制
- ✅ MySQL主从复制、PostgreSQL高可用、MongoDB副本集部署
- ✅ Kafka和RabbitMQ集群存储配置
- ✅ Volume Snapshot备份和恢复
- ✅ Prometheus监控PVC使用率和告警
- ✅ 容量规划和资源限制最佳实践

然而，Kubernetes存储生态系统的核心是**CSI（Container Storage Interface）插件**。CSI定义了存储供应商如何与Kubernetes集成，实现了存储功能的标准化和可扩展性。

**在下一节（8.7 CSI存储插件详解）中**，我们将学习：
- CSI架构和工作原理
- CSI驱动的核心组件（Controller、Node Plugin）
- 快照、克隆、扩容等CSI高级特性
- 开发自定义CSI插件（NFS CSI示例）
- CSI故障排查和调试技巧

---

**本节完**

*下一节预告：8.7节《CSI存储插件详解》- 深入CSI架构、高级特性和自定义插件开发。*
