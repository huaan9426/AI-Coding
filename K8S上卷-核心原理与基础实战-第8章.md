# 第8章 存储管理

在前面的章节中，我们深入学习了Kubernetes的调度与资源管理机制，掌握了如何高效地将Pod调度到合适的节点上，并通过优先级、亲和性、资源配额等手段实现企业级的资源管理。然而，真实的生产环境中，大部分应用都是**有状态应用**（Stateful Application），它们需要持久化存储来保存数据：

- **数据库**：MySQL、PostgreSQL、MongoDB需要持久化数据库文件
- **消息队列**：Kafka、RabbitMQ需要持久化消息数据
- **文件存储**：MinIO、Ceph需要持久化对象存储
- **日志系统**：Elasticsearch需要持久化日志索引
- **配置中心**：etcd、Consul需要持久化配置数据

容器的本质是**进程**，当Pod被删除或重启时，容器内的数据会随之消失。这对于无状态应用（如Web服务器）不是问题，但对于有状态应用却是致命的。Kubernetes通过强大的**存储管理体系**解决了这一难题，提供了从临时存储到企业级分布式存储的完整解决方案。

本章将系统学习Kubernetes的存储管理机制，从基础概念到高级特性，再到企业级实战，构建完整的存储知识体系。

---

## 本章结构

本章共分为8节，逐步深入Kubernetes存储管理的核心技术：

- **8.1 存储基础概念与架构**：理解Kubernetes存储体系的整体架构，掌握Volume、PV、PVC的核心概念
- **8.2 Volume详解**：深入学习emptyDir、hostPath、configMap、secret等基础Volume类型
- **8.3 持久卷（PersistentVolume）**：掌握PV的生命周期、访问模式、回收策略
- **8.4 持久卷声明（PersistentVolumeClaim）**：理解PVC的绑定机制、存储类选择
- **8.5 存储类（StorageClass）**：学习动态存储供应、参数配置、回收策略
- **8.6 StatefulSet与有状态应用**：掌握StatefulSet的存储管理机制、volumeClaimTemplates
- **8.7 CSI存储插件**：理解Container Storage Interface标准，学习云存储集成
- **8.8 实战项目与本章小结**：通过企业级存储方案实战，总结存储管理最佳实践

---

## 8.1 存储基础概念与架构

在深入具体技术之前，我们需要先建立对Kubernetes存储体系的整体认知。本节将回答以下核心问题：

1. **为什么容器需要持久化存储？** 容器数据的生命周期与存储需求
2. **Kubernetes提供了哪些存储方案？** 临时存储、持久化存储、配置存储的分类
3. **PV、PVC、StorageClass的关系是什么？** 存储抽象的三层架构
4. **存储如何与Pod绑定？** Volume挂载的完整流程

---

### 8.1.1 容器存储的挑战

**问题1：容器数据的临时性**

容器的文件系统是基于镜像的分层文件系统（Union FS），容器内的所有写操作都发生在**可写层**（Writable Layer）：

```
容器文件系统层级结构
┌─────────────────────────┐
│  可写层（Container Layer）  │ ← 容器运行时的所有写操作
├─────────────────────────┤
│  镜像层4（ADD app.jar）    │
├─────────────────────────┤
│  镜像层3（RUN apt update） │
├─────────────────────────┤
│  镜像层2（COPY . /app）    │
├─────────────────────────┤
│  镜像层1（FROM ubuntu）    │ ← 只读基础镜像
└─────────────────────────┘
```

**核心问题：**
- ❌ 容器删除时，可写层数据随之消失
- ❌ 容器重启时，数据重置到镜像初始状态
- ❌ 同一Pod内的多个容器无法共享数据
- ❌ Pod迁移到其他节点时，数据无法跟随

**典型场景的数据丢失：**

```bash
# 场景1：Pod重启导致数据丢失
$ kubectl exec mysql-pod -- mysql -e "CREATE DATABASE test;"
$ kubectl delete pod mysql-pod  # Pod被删除
$ kubectl get pod               # 新Pod被ReplicaSet重建
$ kubectl exec mysql-pod -- mysql -e "SHOW DATABASES;"  # test数据库消失！

# 场景2：容器崩溃导致日志丢失
$ kubectl exec nginx-pod -- sh -c "echo 'important log' >> /var/log/access.log"
$ kubectl exec nginx-pod -- kill 1  # 容器崩溃重启
$ kubectl exec nginx-pod -- cat /var/log/access.log  # 日志文件不存在！
```

---

**问题2：有状态应用的复杂需求**

真实的生产环境中，有状态应用有更复杂的存储需求：

| 应用类型 | 存储需求 | 挑战 |
|---------|---------|------|
| **MySQL主从** | - 主库需要独立持久卷<br>- 从库需要只读访问<br>- 数据目录需要固定路径 | 如何为每个实例分配独立存储？<br>如何保证存储的访问模式正确？ |
| **Elasticsearch集群** | - 每个节点需要独立数据目录<br>- 存储需要高IOPS（SSD）<br>- 需要动态扩容 | 如何根据性能需求选择存储类型？<br>如何支持在线扩容？ |
| **Kafka** | - 日志段需要顺序写入<br>- 消费者offset需要持久化<br>- 存储容量需要动态调整 | 如何保证写入性能？<br>如何处理存储容量不足？ |
| **共享文件服务** | - 多个Pod同时读写<br>- 需要文件锁机制<br>- 跨节点访问 | 如何支持ReadWriteMany模式？<br>如何选择合适的网络存储？ |

---

**问题3：传统存储与容器编排的鸿沟**

在容器化之前，存储管理是运维团队的职责：

```
传统模式（手动管理）
运维团队：创建LUN → 格式化 → 挂载到服务器 → 配置权限
开发团队：在固定路径/data/mysql使用存储
```

容器化后，Pod可能在任意节点启动，传统的手动挂载方式失效：

```
容器化挑战
Pod在node1启动 → 需要自动挂载存储A
Pod被调度到node2 → 需要自动卸载node1并挂载到node2
Pod扩容到3副本 → 需要自动创建3个独立存储
```

**Kubernetes存储体系的设计目标：**
- ✅ **解耦**：应用开发者无需关心底层存储细节（NFS/Ceph/云盘）
- ✅ **自动化**：存储的创建、挂载、卸载、删除全自动
- ✅ **可移植**：同一份YAML可以在不同环境（AWS/阿里云/本地）运行
- ✅ **动态供应**：根据应用需求自动创建存储，无需提前准备

---

### 8.1.2 Kubernetes存储体系架构

Kubernetes通过**三层抽象**解决容器存储难题：

```
┌─────────────────────────────────────────────────────────────┐
│  应用层（Pod）                                                 │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ containers:                                           │  │
│  │   - name: mysql                                       │  │
│  │     volumeMounts:                                     │  │
│  │       - name: data                                    │  │
│  │         mountPath: /var/lib/mysql  ← 应用只关心挂载路径  │  │
│  └──────────────────────────────────────────────────────┘  │
└────────────────────────┬────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│  抽象层（PVC - 存储需求声明）                                    │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ kind: PersistentVolumeClaim                           │  │
│  │ spec:                                                 │  │
│  │   accessModes: [ReadWriteOnce]                        │  │
│  │   resources:                                          │  │
│  │     requests:                                         │  │
│  │       storage: 10Gi           ← 我需要10GB的RWO存储    │  │
│  │   storageClassName: fast-ssd  ← 我需要SSD类型         │  │
│  └──────────────────────────────────────────────────────┘  │
└────────────────────────┬────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│  供应层（StorageClass - 存储供应策略）                           │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ kind: StorageClass                                    │  │
│  │ provisioner: kubernetes.io/aws-ebs                    │  │
│  │ parameters:                                           │  │
│  │   type: gp3                   ← 使用AWS GP3 SSD       │  │
│  │   iopsPerGB: "50"             ← 性能参数              │  │
│  └──────────────────────────────────────────────────────┘  │
└────────────────────────┬────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│  实现层（PV - 真实存储资源）                                     │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ kind: PersistentVolume                                │  │
│  │ spec:                                                 │  │
│  │   capacity:                                           │  │
│  │     storage: 10Gi                                     │  │
│  │   awsElasticBlockStore:                               │  │
│  │     volumeID: vol-0a1b2c3d4e5f  ← 真实的AWS EBS卷ID   │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

**核心概念解析：**

**1. Volume（存储卷）**
- **定义**：Pod中定义的存储抽象，是容器挂载存储的基础单元
- **生命周期**：与Pod绑定，Pod删除时Volume行为取决于类型
- **分类**：
  - **临时卷**：emptyDir（随Pod删除）
  - **节点卷**：hostPath（节点本地路径）
  - **配置卷**：configMap、secret（配置数据）
  - **持久卷**：通过PVC引用PV（独立生命周期）

**2. PersistentVolume（PV，持久卷）**
- **定义**：集群级别的存储资源，由管理员创建或动态供应
- **特点**：
  - 独立于Pod的生命周期（Pod删除后PV依然存在）
  - 包含真实存储的细节（NFS服务器地址、云盘ID等）
  - 有容量、访问模式、回收策略等属性
- **类比**：PV就像数据中心的"存储货架"，是真实的物理资源

**3. PersistentVolumeClaim（PVC，持久卷声明）**
- **定义**：用户对存储的请求声明，描述需要什么样的存储
- **特点**：
  - 命名空间级别（与Pod在同一命名空间）
  - 只描述需求（容量、访问模式、存储类），不关心实现细节
  - 通过绑定机制与PV关联
- **类比**：PVC就像"采购订单"，描述需要多大、什么性能的存储

**4. StorageClass（SC，存储类）**
- **定义**：存储的"配置模板"，定义如何动态创建PV
- **特点**：
  - 包含provisioner（存储供应商插件）
  - 包含parameters（存储参数，如磁盘类型、IOPS）
  - 支持动态供应（PVC创建时自动创建PV）
- **类比**：StorageClass就像"采购合同"，定义了从哪里、如何采购存储

---

### 8.1.3 存储类型分类

Kubernetes支持丰富的存储类型，根据使用场景可以分为以下几类：

**分类1：按生命周期分类**

| 类型 | 生命周期 | 典型场景 | 代表类型 |
|------|---------|---------|---------|
| **临时存储** | 与Pod绑定<br>Pod删除时数据丢失 | - 缓存数据<br>- 临时文件<br>- 容器间共享 | emptyDir |
| **节点存储** | 与节点绑定<br>Pod迁移时数据丢失 | - 节点日志采集<br>- 主机路径访问<br>- DaemonSet数据 | hostPath |
| **持久存储** | 独立生命周期<br>Pod删除后数据保留 | - 数据库数据<br>- 用户上传文件<br>- 持久化队列 | PVC/PV |

**分类2：按访问模式分类**

| 访问模式 | 缩写 | 含义 | 典型场景 | 支持的存储后端 |
|---------|------|------|---------|---------------|
| **ReadWriteOnce** | RWO | 单节点读写 | - 数据库（MySQL/PostgreSQL）<br>- 块存储应用 | AWS EBS、Azure Disk<br>GCE PD、本地磁盘 |
| **ReadOnlyMany** | ROX | 多节点只读 | - 静态资源分发<br>- 共享配置文件 | NFS、CephFS |
| **ReadWriteMany** | RWX | 多节点读写 | - 共享文件服务<br>- 多副本应用共享数据 | NFS、GlusterFS<br>CephFS、Azure File |
| **ReadWriteOncePod** | RWOP | 单Pod读写（K8s 1.22+） | - 独占访问保证<br>- 避免脑裂 | CSI驱动支持 |

**重要提示：**
- ⚠️ **RWO不是指"单个Pod"，而是"单个节点"**：同一节点的多个Pod可以同时挂载RWO卷
- ⚠️ **RWX需要网络存储**：本地磁盘、云盘（EBS/Azure Disk）不支持RWX

**分类3：按存储后端分类**

```
存储后端技术栈
├── 本地存储
│   ├── emptyDir（节点临时目录）
│   ├── hostPath（节点路径）
│   └── local（本地持久卷，1.14+）
├── 网络存储
│   ├── NFS（Network File System）
│   ├── iSCSI（块存储协议）
│   ├── GlusterFS（分布式文件系统）
│   └── CephFS/RBD（Ceph分布式存储）
├── 云存储
│   ├── AWS EBS（Elastic Block Store）
│   ├── Azure Disk（托管磁盘）
│   ├── GCE PD（Persistent Disk）
│   └── 阿里云盘、腾讯云盘
└── 特殊存储
    ├── ConfigMap（配置数据）
    ├── Secret（敏感数据）
    ├── Projected（投射卷，组合多个源）
    └── CSI（Container Storage Interface，统一接口）
```

---

### 8.1.4 PV/PVC绑定机制

**核心流程：**

```
┌──────────────────────────────────────────────────────────────┐
│  步骤1：用户创建PVC（我需要10GB RWO存储）                         │
└────────────────┬─────────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────────┐
│  步骤2：Controller匹配PV（查找满足条件的PV）                      │
│  匹配条件：                                                     │
│  ✓ 容量 >= PVC请求（10GB）                                     │
│  ✓ 访问模式匹配（RWO）                                          │
│  ✓ StorageClass匹配（或都为空）                                │
│  ✓ Selector标签匹配（如果PVC指定了selector）                    │
└────────────────┬─────────────────────────────────────────────┘
                 │
      ┌──────────┴──────────┐
      │                     │
      ▼                     ▼
┌────────────┐        ┌────────────┐
│ 找到匹配PV  │        │ 未找到PV    │
└─────┬──────┘        └─────┬──────┘
      │                     │
      ▼                     ▼
┌────────────┐        ┌────────────┐
│ 绑定PVC到PV │        │ 触发动态供应 │
│ (Bound状态) │        │ (Pending)   │
└─────┬──────┘        └─────┬──────┘
      │                     │
      │                     ▼
      │              ┌────────────┐
      │              │ SC创建PV   │
      │              └─────┬──────┘
      │                     │
      └──────────┬──────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────────┐
│  步骤3：Pod引用PVC（volumeMounts挂载到容器）                      │
└────────────────┬─────────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────────┐
│  步骤4：Kubelet挂载存储（调用Volume Plugin或CSI驱动）             │
│  - Attach阶段：将存储卷附加到节点（如AWS EBS attach）             │
│  - Mount阶段：将存储卷挂载到容器路径                              │
└──────────────────────────────────────────────────────────────┘
```

**绑定规则详解：**

**规则1：容量匹配**
```yaml
# PVC请求10GB
spec:
  resources:
    requests:
      storage: 10Gi

# 可以绑定到15GB的PV（容量 >= 请求即可）
# 但会浪费5GB空间
# ✅ 推荐：PV容量精确等于PVC请求
```

**规则2：访问模式匹配**
```yaml
# PVC请求RWO
accessModes: [ReadWriteOnce]

# ❌ 无法绑定到只支持ROX的PV
# ✅ 可以绑定到同时支持RWO和RWX的PV
# PV的accessModes必须包含PVC请求的所有模式
```

**规则3：StorageClass匹配**
```yaml
# 场景1：静态绑定（都不指定SC）
PVC: storageClassName: ""  # 明确指定为空
PV:  storageClassName: ""  # 不指定SC
结果: ✅ 可以绑定

# 场景2：动态供应（PVC指定SC）
PVC: storageClassName: "fast-ssd"
结果: 触发StorageClass创建PV

# 场景3：类名匹配
PVC: storageClassName: "fast-ssd"
PV:  storageClassName: "fast-ssd"
结果: ✅ 可以绑定

# 场景4：类名不匹配
PVC: storageClassName: "fast-ssd"
PV:  storageClassName: "standard-hdd"
结果: ❌ 无法绑定
```

**规则4：Selector标签匹配**
```yaml
# PVC通过selector精确选择PV
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 10Gi
  selector:
    matchLabels:
      environment: production  # 只绑定带此标签的PV
      tier: database
```

---

### 8.1.5 Volume基础类型详解

在深入PV/PVC之前,我们先了解Pod中直接使用的基础Volume类型。

**类型1：emptyDir（临时目录）**

**定义：** Pod创建时自动创建的空目录，Pod删除时数据随之删除。

**典型场景：**
1. 容器间共享数据（同一Pod内的多个容器）
2. 临时缓存（如编译产物、下载文件）
3. 检查点文件（Checkpoint）

**示例：容器间共享日志文件**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: log-sharing-pod
spec:
  containers:
  # 容器1：生成日志
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx  # Nginx日志写入此目录

  # 容器2：采集日志
  - name: log-collector
    image: busybox:1.35
    command: ['sh', '-c', 'tail -f /logs/access.log']
    volumeMounts:
    - name: shared-logs
      mountPath: /logs  # 读取同一目录的日志

  volumes:
  - name: shared-logs
    emptyDir: {}  # Pod级别的临时存储
```

**高级特性：基于内存的emptyDir**

```yaml
volumes:
- name: cache
  emptyDir:
    medium: Memory  # 使用内存而非磁盘
    sizeLimit: 1Gi  # 限制最大使用1GB内存
```

**使用场景：**
- ✅ 高性能缓存（读写速度快）
- ⚠️ 注意内存限制（会占用Pod的内存资源）

---

**类型2：hostPath（主机路径）**

**定义：** 将宿主机的文件或目录挂载到Pod中。

**典型场景：**
1. 访问宿主机的Docker socket（/var/run/docker.sock）
2. 节点日志采集（/var/log）
3. 时区同步（/etc/localtime）

**⚠️ 安全警告：** hostPath允许Pod访问节点文件系统，具有较大安全风险，生产环境应谨慎使用。

**示例1：Docker-in-Docker（访问Docker守护进程）**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: docker-cli-pod
spec:
  containers:
  - name: docker-cli
    image: docker:20.10
    command: ['sh', '-c', 'docker ps && sleep 3600']
    volumeMounts:
    - name: docker-sock
      mountPath: /var/run/docker.sock  # 容器内访问宿主机Docker

  volumes:
  - name: docker-sock
    hostPath:
      path: /var/run/docker.sock  # 宿主机Docker socket
      type: Socket  # 类型校验：必须是socket文件
```

**示例2：节点日志采集（DaemonSet典型场景）**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-logger
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd:v1.14
        volumeMounts:
        - name: varlog
          mountPath: /var/log  # 读取节点日志
          readOnly: true  # 只读挂载，安全最佳实践

      volumes:
      - name: varlog
        hostPath:
          path: /var/log  # 节点的日志目录
          type: Directory  # 类型校验：必须是已存在的目录
```

**hostPath类型校验（type字段）：**

| type值 | 含义 | 行为 |
|--------|------|------|
| `""` | 默认（不检查） | 无论路径是否存在都挂载 |
| `DirectoryOrCreate` | 目录或创建 | 目录不存在时自动创建 |
| `Directory` | 必须是目录 | 目录不存在时Pod启动失败 |
| `FileOrCreate` | 文件或创建 | 文件不存在时自动创建 |
| `File` | 必须是文件 | 文件不存在时Pod启动失败 |
| `Socket` | 必须是Socket | 不是socket文件时启动失败 |
| `CharDevice` | 字符设备 | 用于设备文件（如/dev/xxx） |
| `BlockDevice` | 块设备 | 用于块设备文件 |

**最佳实践：**
- ✅ 始终指定`type`字段进行类型校验
- ✅ 尽量使用`readOnly: true`只读挂载
- ✅ 避免挂载敏感路径（如/etc/shadow）
- ⚠️ 注意Pod迁移到其他节点时，hostPath路径可能不存在

---

**类型3：configMap（配置数据）**

**定义：** 将ConfigMap的数据以文件形式挂载到Pod中。

**示例：Nginx配置文件注入**

```yaml
# 第一步：创建ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    server {
        listen 80;
        server_name example.com;

        location / {
            root /usr/share/nginx/html;
            index index.html;
        }

        location /api {
            proxy_pass http://backend:8080;
        }
    }
  index.html: |
    <html>
      <body><h1>Hello from ConfigMap!</h1></body>
    </html>

---
# 第二步：Pod挂载ConfigMap
apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-config
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    volumeMounts:
    - name: config-volume
      mountPath: /etc/nginx/conf.d  # 配置文件目录
    - name: html-volume
      mountPath: /usr/share/nginx/html  # 静态文件目录

  volumes:
  - name: config-volume
    configMap:
      name: nginx-config
      items:  # 选择性挂载
      - key: nginx.conf
        path: default.conf  # 挂载为default.conf文件

  - name: html-volume
    configMap:
      name: nginx-config
      items:
      - key: index.html
        path: index.html
```

**挂载后的文件结构：**
```bash
# 容器内查看
$ kubectl exec nginx-with-config -- ls -la /etc/nginx/conf.d
total 4
drwxrwxrwx 3 root root  80 Jan 13 10:00 .
drwxr-xr-x 1 root root  41 Jan 13 10:00 ..
drwxr-xr-x 2 root root  28 Jan 13 10:00 ..2024_01_13_10_00_00.123456789
lrwxrwxrwx 1 root root  32 Jan 13 10:00 ..data -> ..2024_01_13_10_00_00.123456789
lrwxrwxrwx 1 root root  19 Jan 13 10:00 default.conf -> ..data/default.conf

# ConfigMap更新时，Kubernetes会自动更新挂载的文件（有约60秒延迟）
```

---

**类型4：secret（敏感数据）**

**定义：** 与ConfigMap类似，但用于存储敏感信息（密码、证书、Token），数据以Base64编码存储。

**示例：MySQL密码注入**

```yaml
# 第一步：创建Secret
apiVersion: v1
kind: Secret
metadata:
  name: mysql-credentials
type: Opaque
data:
  # Base64编码后的值
  username: cm9vdA==  # root
  password: bXlzcWwxMjM0NTY=  # mysql123456

---
# 第二步：Pod使用Secret
apiVersion: v1
kind: Pod
metadata:
  name: mysql-pod
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    env:
    # 方式1：通过环境变量注入
    - name: MYSQL_ROOT_PASSWORD
      valueFrom:
        secretKeyRef:
          name: mysql-credentials
          key: password

    # 方式2：通过文件挂载（更安全）
    volumeMounts:
    - name: credentials
      mountPath: /etc/mysql/conf.d
      readOnly: true  # 只读挂载，防止容器篡改

  volumes:
  - name: credentials
    secret:
      secretName: mysql-credentials
      defaultMode: 0400  # 文件权限：仅所有者可读
```

**Secret vs ConfigMap：**

| 特性 | Secret | ConfigMap |
|------|--------|-----------|
| **数据编码** | Base64编码 | 明文 |
| **API权限** | 更严格（RBAC） | 宽松 |
| **加密存储** | 支持etcd加密（需开启） | 不支持 |
| **文件权限** | 默认0644，可配置 | 默认0644 |
| **典型用途** | 密码、证书、Token | 配置文件、环境变量 |

⚠️ **重要提示：** Base64编码≠加密，Secret在etcd中默认是明文存储，需要配置[etcd加密](https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)才能真正加密。

---

### 8.1.6 PV/PVC核心概念深入

现在我们深入学习持久化存储的核心：PV和PVC。

**PersistentVolume（PV）核心属性：**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-example
  labels:
    type: local
    environment: production
spec:
  # ========== 容量 ==========
  capacity:
    storage: 10Gi  # PV的总容量

  # ========== 访问模式 ==========
  accessModes:
    - ReadWriteOnce  # 单节点读写
    # - ReadOnlyMany   # 多节点只读
    # - ReadWriteMany  # 多节点读写

  # ========== 回收策略 ==========
  persistentVolumeReclaimPolicy: Retain  # PVC删除后的行为
    # Retain：保留数据，手动回收
    # Delete：自动删除PV和底层存储
    # Recycle（废弃）：擦除数据后重新可用

  # ========== 存储类 ==========
  storageClassName: manual  # 关联的StorageClass名称

  # ========== 挂载选项 ==========
  mountOptions:
    - hard
    - nfsvers=4.1

  # ========== 节点亲和性（Local PV必需） ==========
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1  # 此PV只能在node1上使用

  # ========== 具体存储实现（选择其一） ==========
  nfs:  # NFS存储
    server: 192.168.1.100
    path: /data/pv-example

  # 或者其他类型：
  # hostPath:  # 本地路径
  #   path: /mnt/data
  # awsElasticBlockStore:  # AWS EBS
  #   volumeID: vol-0a1b2c3d4e5f
  #   fsType: ext4
  # cephfs:  # CephFS
  #   monitors: [...]
```

**PersistentVolumeClaim（PVC）核心属性：**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-example
  namespace: default  # PVC是命名空间级别资源
spec:
  # ========== 访问模式（必需） ==========
  accessModes:
    - ReadWriteOnce  # 必须与PV的accessModes匹配

  # ========== 资源请求（必需） ==========
  resources:
    requests:
      storage: 8Gi  # 请求8GB存储（PV容量需 >= 8Gi）

  # ========== 存储类（可选） ==========
  storageClassName: manual  # 指定使用哪个StorageClass
    # "" - 明确禁用动态供应，只绑定无StorageClass的PV
    # <不指定> - 使用默认StorageClass（如果集群有默认SC）
    # <类名> - 使用指定的StorageClass

  # ========== 选择器（可选） ==========
  selector:
    matchLabels:
      type: local  # 只绑定带此标签的PV
      environment: production
    # matchExpressions:  # 更复杂的选择逻辑
    #   - key: tier
    #     operator: In
    #     values: [database, cache]

  # ========== 卷模式（可选，1.13+） ==========
  volumeMode: Filesystem  # 文件系统模式（默认）
    # Filesystem - 挂载为文件系统（需要格式化）
    # Block - 块设备模式（直接访问裸设备）
```

---

### 8.1.7 完整实战案例：MySQL持久化存储

让我们通过一个完整的案例，将上述概念串联起来。

**场景：** 部署一个MySQL数据库，数据持久化到NFS存储，即使Pod删除数据也不会丢失。

**前提条件：** 已有NFS服务器（192.168.1.100），共享目录/data/mysql

**步骤1：创建PV（管理员操作）**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv
  labels:
    app: mysql
    environment: production
spec:
  capacity:
    storage: 20Gi  # 提供20GB存储
  accessModes:
    - ReadWriteOnce  # MySQL需要RWO模式
  persistentVolumeReclaimPolicy: Retain  # 删除PVC后保留数据
  storageClassName: nfs-storage  # 存储类名称
  mountOptions:
    - hard  # NFS硬挂载（推荐）
    - nfsvers=4.1  # NFS版本
  nfs:
    server: 192.168.1.100
    path: /data/mysql  # NFS共享路径
```

```bash
# 创建PV
$ kubectl apply -f mysql-pv.yaml
persistentvolume/mysql-pv created

# 查看PV状态
$ kubectl get pv mysql-pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   AGE
mysql-pv   20Gi       RWO            Retain           Available           nfs-storage    5s
# STATUS=Available 表示PV已就绪，等待PVC绑定
```

**步骤2：创建PVC（开发者操作）**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce  # 与PV的accessModes匹配
  resources:
    requests:
      storage: 15Gi  # 请求15GB（小于PV的20GB）
  storageClassName: nfs-storage  # 与PV的storageClassName匹配
  selector:
    matchLabels:
      app: mysql  # 精确选择带mysql标签的PV
      environment: production
```

```bash
# 创建PVC
$ kubectl apply -f mysql-pvc.yaml
persistentvolumeclaim/mysql-pvc created

# 查看PVC状态
$ kubectl get pvc mysql-pvc
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mysql-pvc   Bound    mysql-pv   20Gi       RWO            nfs-storage    3s
# STATUS=Bound 表示PVC已成功绑定到mysql-pv

# 再次查看PV状态
$ kubectl get pv mysql-pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   AGE
mysql-pv   20Gi       RWO            Retain           Bound    default/mysql-pvc   nfs-storage    2m
# STATUS从Available变为Bound
# CLAIM显示绑定到default/mysql-pvc
```

**步骤3：部署MySQL Pod**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    env:
    - name: MYSQL_ROOT_PASSWORD
      value: "mysql123456"  # 生产环境应使用Secret
    ports:
    - containerPort: 3306
      name: mysql
    volumeMounts:
    - name: mysql-storage
      mountPath: /var/lib/mysql  # MySQL数据目录

  volumes:
  - name: mysql-storage
    persistentVolumeClaim:
      claimName: mysql-pvc  # 引用PVC
```

```bash
# 部署MySQL
$ kubectl apply -f mysql-pod.yaml
pod/mysql created

# 等待Pod运行
$ kubectl get pod mysql
NAME    READY   STATUS    RESTARTS   AGE
mysql   1/1     Running   0          30s

# 进入容器验证挂载
$ kubectl exec -it mysql -- df -h /var/lib/mysql
Filesystem                Size  Used Avail Use% Mounted on
192.168.1.100:/data/mysql  20G  1.2G   18G   6% /var/lib/mysql
# 可以看到NFS存储已成功挂载
```

**步骤4：验证数据持久化**

```bash
# 创建测试数据
$ kubectl exec -it mysql -- mysql -uroot -pmysql123456 -e "
CREATE DATABASE testdb;
USE testdb;
CREATE TABLE users (id INT, name VARCHAR(50));
INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob');
SELECT * FROM users;
"
+------+-------+
| id   | name  |
+------+-------+
|    1 | Alice |
|    2 | Bob   |
+------+-------+

# 删除Pod
$ kubectl delete pod mysql
pod "mysql" deleted

# 重新创建Pod（使用相同的PVC）
$ kubectl apply -f mysql-pod.yaml
pod/mysql created

# 等待Pod运行后验证数据
$ kubectl exec -it mysql -- mysql -uroot -pmysql123456 -e "
USE testdb;
SELECT * FROM users;
"
+------+-------+
| id   | name  |
+------+-------+
|    1 | Alice |
|    2 | Bob   |
+------+-------+
# ✅ 数据完整保留！Pod删除重建后数据依然存在
```

**步骤5：清理资源（注意顺序）**

```bash
# 第一步：删除Pod
$ kubectl delete pod mysql
pod "mysql" deleted

# 第二步：删除PVC
$ kubectl delete pvc mysql-pvc
persistentvolumeclaim "mysql-pvc" deleted

# 查看PV状态
$ kubectl get pv mysql-pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM               STORAGECLASS   AGE
mysql-pv   20Gi       RWO            Retain           Released   default/mysql-pvc   nfs-storage    10m
# STATUS变为Released（已释放，但数据仍保留）

# 第三步：根据需要手动删除PV
$ kubectl delete pv mysql-pv  # 删除PV对象
$ # NFS服务器上的/data/mysql目录数据依然存在（Retain策略）
```

**回收策略的影响：**

| 回收策略 | PVC删除后的行为 | NFS服务器数据 | PV状态 |
|---------|---------------|--------------|--------|
| **Retain** | PV变为Released状态<br>需手动删除PV | 保留 | Released |
| **Delete** | 自动删除PV<br>自动删除底层存储数据 | 删除 | （PV已删除） |
| **Recycle**（废弃） | 执行`rm -rf /volume/*`<br>PV变回Available | 清空 | Available |

---

### 8.1.8 常见问题与排查

**问题1：PVC一直处于Pending状态**

```bash
$ kubectl get pvc
NAME        STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mysql-pvc   Pending                                      nfs-storage    5m
```

**排查步骤：**

```bash
# 1. 查看PVC详细事件
$ kubectl describe pvc mysql-pvc
Events:
  Type     Reason              Message
  ----     ------              -------
  Warning  ProvisioningFailed  Failed to provision volume with StorageClass "nfs-storage": storageclass.storage.k8s.io "nfs-storage" not found

# 可能原因：
# ✓ StorageClass不存在
# ✓ 没有匹配的PV（静态供应场景）
# ✓ PV容量不足
# ✓ 访问模式不匹配
# ✓ 标签选择器不匹配

# 2. 检查是否有可用PV
$ kubectl get pv
# 如果没有PV，需要创建或检查StorageClass是否支持动态供应

# 3. 检查StorageClass是否存在
$ kubectl get storageclass nfs-storage
```

**解决方案：**
- 创建匹配的PV（静态供应）
- 创建/修复StorageClass（动态供应）
- 调整PVC的请求容量或访问模式

---

**问题2：Pod无法挂载PVC**

```bash
$ kubectl get pod mysql
NAME    READY   STATUS              RESTARTS   AGE
mysql   0/1     ContainerCreating   0          2m
```

```bash
$ kubectl describe pod mysql
Events:
  Warning  FailedMount  Unable to attach or mount volumes: failed to attach volume "mysql-pv": rpc error: code = Internal desc = Could not mount "192.168.1.100:/data/mysql"
```

**可能原因：**
1. **NFS服务器不可达**
   ```bash
   # 在节点上测试NFS连接
   $ showmount -e 192.168.1.100
   clnt_create: RPC: Port mapper failure - Unable to receive: errno 113 (No route to host)
   ```
   解决：检查网络连通性、防火墙规则

2. **NFS目录不存在**
   ```bash
   # NFS服务器上检查
   $ ls -la /data/mysql
   ls: cannot access '/data/mysql': No such file or directory
   ```
   解决：创建目录并配置NFS导出

3. **节点缺少NFS客户端工具**
   ```bash
   # 节点上安装NFS客户端
   $ apt-get install -y nfs-common  # Debian/Ubuntu
   $ yum install -y nfs-utils       # CentOS/RHEL
   ```

---

**问题3：多个Pod竞争同一RWO PVC**

```yaml
# Deployment尝试创建2个副本，都使用同一个RWO PVC
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
spec:
  replicas: 2  # ❌ 错误：RWO PVC只能被单节点挂载
  template:
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: mysql-pvc  # RWO PVC
```

**现象：** 第二个Pod无法启动，报错"Multi-Attach error"

```bash
$ kubectl describe pod mysql-xxx
Events:
  Warning  FailedAttachVolume  Multi-Attach error for volume "pvc-xxx": Volume is already exclusively attached to one node and can't be attached to another
```

**解决方案：**
- ✅ 使用StatefulSet代替Deployment（下一节详解）
- ✅ 使用RWX模式的PVC（需要支持的存储后端如NFS）
- ✅ 将replicas设置为1

---

### 8.1.9 最佳实践总结

**1. Volume类型选择决策树**

```
需要持久化数据吗？
├─ 否 → emptyDir（容器间共享）或不使用Volume
└─ 是
   ├─ 数据只在本节点使用？
   │  └─ 是 → hostPath（谨慎使用）或local PV
   └─ 否
      ├─ 需要多节点同时读写（RWX）？
      │  ├─ 是 → NFS、CephFS、GlusterFS
      │  └─ 否 → AWS EBS、Azure Disk、GCE PD（RWO）
      └─ 数据是配置/密钥？
         ├─ 敏感数据 → Secret
         └─ 普通配置 → ConfigMap
```

**2. PV/PVC使用规范**

| 规范 | 说明 | 示例 |
|------|------|------|
| **PV命名** | 描述性名称，包含存储类型和用途 | `mysql-prod-nfs-pv`<br>`redis-cache-local-pv` |
| **PVC命名** | 与应用关联，描述用途 | `mysql-data-pvc`<br>`nginx-logs-pvc` |
| **容量规划** | PV容量略大于PVC请求（10-20%缓冲） | PVC请求10Gi，PV提供12Gi |
| **标签管理** | 使用标签标识环境、应用、团队 | `environment: production`<br>`app: mysql`<br>`team: platform` |
| **回收策略** | 生产环境使用Retain<br>开发环境可用Delete | `persistentVolumeReclaimPolicy: Retain` |
| **访问模式** | 根据应用需求精确选择 | 数据库用RWO<br>共享文件用RWX |

**3. 安全最佳实践**

```yaml
# ✅ 推荐配置
spec:
  containers:
  - name: app
    volumeMounts:
    - name: config
      mountPath: /etc/config
      readOnly: true  # 只读挂载配置

  volumes:
  - name: config
    secret:
      secretName: app-secret
      defaultMode: 0400  # 限制文件权限为只读
```

**4. 性能优化建议**

- ✅ 数据库使用SSD后端（设置StorageClass的type参数）
- ✅ 日志数据使用HDD后端（成本优化）
- ✅ 避免在emptyDir中存储大量数据（占用节点空间）
- ✅ 对于高IOPS需求，使用本地SSD（local PV）

---

### 8.1.10 下一节预告

在本节中，我们建立了Kubernetes存储体系的整体认知：

- ✅ 理解了容器存储的挑战和Kubernetes的解决方案
- ✅ 掌握了PV、PVC、StorageClass的三层架构
- ✅ 学习了emptyDir、hostPath、ConfigMap、Secret等基础Volume类型
- ✅ 深入理解了PV/PVC的绑定机制和生命周期
- ✅ 通过MySQL案例完整实践了静态存储供应流程

然而,我们还有很多细节需要深入：

- **emptyDir的高级特性**：内存模式、大小限制
- **hostPath的安全隐患**：如何在生产环境安全使用
- **projected卷**：如何组合多个ConfigMap和Secret
- **downwardAPI卷**：如何将Pod元数据注入容器

**在下一节（8.2 Volume详解）中**，我们将深入学习各种Volume类型的高级特性、使用场景和最佳实践，为理解PV/PVC和StorageClass打下坚实基础。

---

**本节完**

*下一节预告：8.2节《Volume详解》- 深入emptyDir、hostPath、projected、downwardAPI等Volume类型的高级特性。*
## 8.2 Volume详解

在上一节中，我们建立了Kubernetes存储体系的整体认知，初步接触了emptyDir、hostPath、ConfigMap、Secret等基础Volume类型。本节将深入这些Volume的高级特性，探索更多实用的Volume类型，并通过丰富的实战案例掌握它们在生产环境中的应用。

---

### 8.2.1 emptyDir高级特性

emptyDir是Kubernetes中最简单但也最实用的Volume类型。在8.1节中我们了解了它的基本用法，现在深入其高级特性。

**核心特性回顾：**
- **生命周期**：与Pod绑定，Pod删除时数据随之删除
- **存储位置**：默认存储在节点的`/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~empty-dir/<volume-name>`
- **典型用途**：临时缓存、容器间数据共享、检查点文件

---

#### 特性1：内存模式（Memory-backed emptyDir）

**配置方式：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: memory-cache-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      limits:
        memory: "512Mi"  # 重要：内存emptyDir会占用容器内存配额
    volumeMounts:
    - name: cache
      mountPath: /cache

  volumes:
  - name: cache
    emptyDir:
      medium: Memory  # 关键配置：使用内存而非磁盘
      sizeLimit: 256Mi  # 限制最大使用256MB内存
```

**工作原理：**

```
传统emptyDir（磁盘模式）
Pod容器 → /cache → emptyDir → 节点磁盘 /var/lib/kubelet/...
                              ↓
                          写入速度：~100-500 MB/s（取决于磁盘类型）

内存emptyDir（medium: Memory）
Pod容器 → /cache → emptyDir → 节点内存 tmpfs
                              ↓
                          写入速度：~5-10 GB/s（内存速度）
```

**性能对比测试：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: emptydir-benchmark
spec:
  containers:
  - name: benchmark
    image: ubuntu:20.04
    command:
    - bash
    - -c
    - |
      echo "=== 测试磁盘emptyDir性能 ==="
      dd if=/dev/zero of=/disk-cache/test.dat bs=1M count=100
      
      echo ""
      echo "=== 测试内存emptyDir性能 ==="
      dd if=/dev/zero of=/memory-cache/test.dat bs=1M count=100
      
      echo ""
      echo "测试完成，保持运行..."
      sleep 3600
    volumeMounts:
    - name: disk-cache
      mountPath: /disk-cache
    - name: memory-cache
      mountPath: /memory-cache

  volumes:
  - name: disk-cache
    emptyDir: {}  # 默认磁盘模式

  - name: memory-cache
    emptyDir:
      medium: Memory
      sizeLimit: 200Mi
```

**执行测试：**

```bash
# 创建测试Pod
$ kubectl apply -f emptydir-benchmark.yaml
pod/emptydir-benchmark created

# 等待Pod运行
$ kubectl wait --for=condition=Ready pod/emptydir-benchmark --timeout=60s

# 查看测试结果
$ kubectl logs emptydir-benchmark
=== 测试磁盘emptyDir性能 ===
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.524876 s, 200 MB/s

=== 测试内存emptyDir性能 ===
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.0153821 s, 6.8 GB/s

# 内存模式速度是磁盘模式的 34 倍！
```

**内存模式的注意事项：**

1. **占用Pod内存配额**
   ```yaml
   spec:
     containers:
     - name: app
       resources:
         limits:
           memory: "512Mi"  # emptyDir的256Mi会计入此限制
       volumeMounts:
       - name: cache
         mountPath: /cache
     volumes:
     - name: cache
       emptyDir:
         medium: Memory
         sizeLimit: 256Mi  # 实际Pod可用内存 = 512Mi - 256Mi = 256Mi
   ```

2. **节点内存压力**
   ```bash
   # 如果节点内存不足，内存emptyDir的数据可能被回收
   $ kubectl describe node node1
   Conditions:
     Type             Status  Reason
     ----             ------  ------
     MemoryPressure   True    KubeletHasInsufficientMemory
   
   # 此时内存emptyDir可能被清空，导致数据丢失
   ```

3. **数据丢失风险**
   - ❌ **不要存储关键数据**：内存断电即丢失
   - ❌ **不要超过sizeLimit**：超出限制Pod可能被驱逐
   - ✅ **适用场景**：高频读写的临时缓存、编译产物

**典型应用场景：**

```yaml
# 场景1：Redis缓存加速
apiVersion: v1
kind: Pod
metadata:
  name: redis-with-memory-cache
spec:
  containers:
  - name: redis
    image: redis:7.0
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    # 禁用持久化，纯内存缓存
    volumeMounts:
    - name: redis-data
      mountPath: /data

  volumes:
  - name: redis-data
    emptyDir:
      medium: Memory
      sizeLimit: 1Gi
```

---

#### 特性2：容量限制（sizeLimit）

**作用：** 限制emptyDir的最大使用空间，防止磁盘/内存被耗尽。

**磁盘模式的sizeLimit：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: disk-limit-test
spec:
  containers:
  - name: writer
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      # 尝试写入150MB数据（超过100Mi限制）
      dd if=/dev/zero of=/data/large-file bs=1M count=150 || echo "写入失败"
      df -h /data
      sleep 3600
    volumeMounts:
    - name: limited-volume
      mountPath: /data

  volumes:
  - name: limited-volume
    emptyDir:
      sizeLimit: 100Mi  # 限制最大100MB
```

**测试结果：**

```bash
$ kubectl logs disk-limit-test
dd: error writing '/data/large-file': No space left on device
写入失败
Filesystem      Size  Used Avail Use% Mounted on
tmpfs           100M  100M     0 100% /data
```

**重要机制：**

1. **Kubelet驱逐机制**
   ```bash
   # 当emptyDir使用超过sizeLimit时
   $ kubectl get pod disk-limit-test
   NAME              READY   STATUS    RESTARTS   AGE
   disk-limit-test   0/1     Evicted   0          2m
   
   $ kubectl describe pod disk-limit-test
   Status:  Failed
   Reason:  Evicted
   Message: Pod ephemeral local storage usage exceeds the total limit of containers 100Mi
   ```

2. **监控emptyDir使用量**
   ```bash
   # 进入Pod查看实际使用量
   $ kubectl exec -it <pod> -- df -h /cache
   Filesystem      Size  Used Avail Use% Mounted on
   overlay         100M   45M   55M  45% /cache
   ```

**最佳实践：**

```yaml
# ✅ 推荐配置
volumes:
- name: build-cache
  emptyDir:
    sizeLimit: 5Gi  # 明确设置限制
    # medium: Memory  # 只有需要极致性能时才用内存模式

# ❌ 不推荐
volumes:
- name: build-cache
  emptyDir: {}  # 没有sizeLimit，可能耗尽节点磁盘
```

---

#### 特性3：容器间数据共享

**场景：** 一个Pod中的多个容器需要共享数据（如日志采集、初始化数据）。

**示例1：Sidecar日志采集模式**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: logging-sidecar
spec:
  containers:
  # 主容器：生成日志
  - name: app
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      while true; do
        echo "$(date) - Application log entry" >> /var/log/app.log
        sleep 1
      done
    volumeMounts:
    - name: logs
      mountPath: /var/log

  # Sidecar容器：采集日志
  - name: log-collector
    image: fluent/fluentd:v1.14
    volumeMounts:
    - name: logs
      mountPath: /var/log
      readOnly: true  # 只读挂载，防止误修改

  volumes:
  - name: logs
    emptyDir:
      sizeLimit: 500Mi
```

**示例2：Init容器准备数据**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: init-container-demo
spec:
  # Init容器：下载配置文件
  initContainers:
  - name: config-downloader
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      echo "Downloading config files..."
      echo "server { listen 80; }" > /config/nginx.conf
      echo "Config downloaded successfully"
    volumeMounts:
    - name: config
      mountPath: /config

  # 主容器：使用配置文件
  containers:
  - name: nginx
    image: nginx:1.21
    volumeMounts:
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true

  volumes:
  - name: config
    emptyDir: {}
```

**验证数据共享：**

```bash
# 创建Pod
$ kubectl apply -f init-container-demo.yaml

# 查看Init容器日志
$ kubectl logs init-container-demo -c config-downloader
Downloading config files...
Config downloaded successfully

# 验证主容器是否能访问配置
$ kubectl exec init-container-demo -- cat /etc/nginx/conf.d/nginx.conf
server { listen 80; }
```

---

### 8.2.2 hostPath安全使用指南

hostPath允许Pod访问节点的文件系统，功能强大但风险极大。生产环境必须谨慎使用。

---

#### 安全风险分析

**风险1：容器逃逸**

```yaml
# ❌ 危险示例：挂载Docker socket
apiVersion: v1
kind: Pod
metadata:
  name: dangerous-pod
spec:
  containers:
  - name: hacker
    image: docker:20.10
    command:
    - sh
    - -c
    - |
      # 攻击者可以通过Docker socket控制宿主机
      docker run -v /:/host --privileged alpine chroot /host bash
      # 现在拥有宿主机root权限！
    volumeMounts:
    - name: docker-sock
      mountPath: /var/run/docker.sock

  volumes:
  - name: docker-sock
    hostPath:
      path: /var/run/docker.sock
      type: Socket
```

**风险2：敏感数据泄露**

```yaml
# ❌ 危险示例：挂载系统敏感目录
volumes:
- name: etc-passwd
  hostPath:
    path: /etc  # 暴露整个/etc目录
    type: Directory

# 攻击者可以读取：
# /etc/shadow   - 用户密码哈希
# /etc/ssh/*    - SSH密钥
# /etc/kubernetes/* - K8s证书和配置
```

**风险3：节点资源耗尽**

```yaml
# ❌ 危险示例：写入大量数据到节点磁盘
volumes:
- name: node-disk
  hostPath:
    path: /var/log/pods  # 写入大量日志
    type: DirectoryOrCreate

# 可能导致节点磁盘被填满，影响所有Pod
```

---

#### 安全使用规范

**规范1：最小权限原则**

```yaml
# ✅ 推荐：只读挂载
apiVersion: v1
kind: Pod
metadata:
  name: safe-hostpath-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: timezone
      mountPath: /etc/localtime
      readOnly: true  # 关键：只读挂载

  volumes:
  - name: timezone
    hostPath:
      path: /etc/localtime
      type: File  # 类型校验：必须是文件
```

**规范2：路径白名单**

```yaml
# ✅ 允许的路径（相对安全）
允许：
  - /etc/localtime           # 时区同步
  - /etc/timezone            # 时区配置
  - /var/log                 # 日志采集（只读）
  - /sys/fs/cgroup           # cgroup信息（只读）

# ❌ 禁止的路径（高危）
禁止：
  - /                        # 根目录
  - /etc                     # 系统配置目录
  - /root                    # root用户目录
  - /var/run/docker.sock     # Docker socket
  - /etc/kubernetes          # K8s配置目录
  - /var/lib/kubelet         # Kubelet数据目录
```

**规范3：使用PodSecurityPolicy限制（K8s 1.21前）**

```yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted-hostpath
spec:
  # 允许的hostPath路径
  allowedHostPaths:
  - pathPrefix: /etc/localtime
    readOnly: true
  - pathPrefix: /var/log
    readOnly: true

  # 禁止挂载Docker socket
  volumes:
  - configMap
  - emptyDir
  - secret
  - hostPath  # 虽然允许hostPath，但通过allowedHostPaths限制路径

  # 禁止特权模式
  privileged: false
  
  # 禁止访问主机网络
  hostNetwork: false
  hostPID: false
  hostIPC: false
```

**规范4：使用Pod Security Standards（K8s 1.23+）**

```yaml
# Namespace级别限制
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

# restricted策略会自动拒绝hostPath（除非特别配置）
```

---

#### 典型安全场景

**场景1：时区同步（安全）**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-with-timezone
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:1.0
        volumeMounts:
        - name: localtime
          mountPath: /etc/localtime
          readOnly: true  # ✅ 只读

      volumes:
      - name: localtime
        hostPath:
          path: /etc/localtime
          type: File  # ✅ 类型校验
```

**场景2：节点日志采集（DaemonSet专用）**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: log-collector
  namespace: kube-system  # ✅ 系统命名空间，受严格RBAC控制
spec:
  selector:
    matchLabels:
      app: log-collector
  template:
    metadata:
      labels:
        app: log-collector
    spec:
      serviceAccountName: log-collector  # ✅ 专用ServiceAccount
      
      containers:
      - name: fluentd
        image: fluent/fluentd:v1.14
        volumeMounts:
        - name: varlog
          mountPath: /var/log
          readOnly: true  # ✅ 只读
        - name: containers
          mountPath: /var/lib/docker/containers
          readOnly: true  # ✅ 只读

      volumes:
      - name: varlog
        hostPath:
          path: /var/log
          type: Directory
      - name: containers
        hostPath:
          path: /var/lib/docker/containers
          type: DirectoryOrCreate

      # ✅ 节点选择器（可选）
      nodeSelector:
        logging: enabled
```

**场景3：监控节点指标（安全）**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: node-exporter
spec:
  hostNetwork: true  # 需要访问主机网络
  hostPID: true      # 需要访问主机进程
  
  containers:
  - name: node-exporter
    image: prom/node-exporter:v1.3.1
    args:
    - --path.procfs=/host/proc
    - --path.sysfs=/host/sys
    - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
    
    volumeMounts:
    - name: proc
      mountPath: /host/proc
      readOnly: true  # ✅ 只读
    - name: sys
      mountPath: /host/sys
      readOnly: true  # ✅ 只读

  volumes:
  - name: proc
    hostPath:
      path: /proc
      type: Directory
  - name: sys
    hostPath:
      path: /sys
      type: Directory
```

---

### 8.2.3 projected卷：组合多个数据源

**定义：** projected卷可以将多个Volume源（ConfigMap、Secret、DownwardAPI、ServiceAccountToken）组合到同一个目录。

**优势：**
- ✅ 统一挂载点，简化配置
- ✅ 避免目录冲突
- ✅ 支持权限统一设置

---

#### 基础语法

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: projected-volume-demo
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: all-in-one
      mountPath: /projected-volume
      readOnly: true

  volumes:
  - name: all-in-one
    projected:
      defaultMode: 0644  # 统一设置文件权限
      sources:
      # 数据源1：ConfigMap
      - configMap:
          name: app-config
          items:
          - key: app.properties
            path: config/app.properties

      # 数据源2：Secret
      - secret:
          name: db-credentials
          items:
          - key: username
            path: secrets/db-user
          - key: password
            path: secrets/db-pass

      # 数据源3：DownwardAPI
      - downwardAPI:
          items:
          - path: labels
            fieldRef:
              fieldPath: metadata.labels
          - path: namespace
            fieldRef:
              fieldPath: metadata.namespace

      # 数据源4：ServiceAccountToken
      - serviceAccountToken:
          path: token
          expirationSeconds: 3600
          audience: my-service
```

**挂载后的目录结构：**

```bash
$ kubectl exec projected-volume-demo -- tree /projected-volume
/projected-volume
├── config
│   └── app.properties       # 来自ConfigMap
├── secrets
│   ├── db-user              # 来自Secret
│   └── db-pass              # 来自Secret
├── labels                   # 来自DownwardAPI
├── namespace                # 来自DownwardAPI
└── token                    # ServiceAccountToken

2 directories, 5 files
```

---

#### 实战案例：应用配置集中管理

**场景：** 一个Web应用需要：
1. 应用配置文件（ConfigMap）
2. 数据库密码（Secret）
3. Pod元数据（DownwardAPI）
4. 服务间认证Token（ServiceAccountToken）

**步骤1：创建ConfigMap和Secret**

```yaml
# 应用配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config
data:
  application.yaml: |
    server:
      port: 8080
    logging:
      level: INFO
    feature-flags:
      new-ui: true

---
# 数据库密码
apiVersion: v1
kind: Secret
metadata:
  name: webapp-secret
type: Opaque
data:
  db-password: bXlzcWwxMjM0NTY=  # mysql123456
  api-key: YWJjZDEyMzQ1Njc4OTA=   # abcd1234567890
```

**步骤2：创建使用projected卷的Pod**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
    version: v1.0
    environment: production
spec:
  serviceAccountName: webapp-sa  # 专用ServiceAccount
  
  containers:
  - name: app
    image: mywebapp:1.0
    env:
    # 从projected卷读取密码（通过环境变量）
    - name: DB_PASSWORD_FILE
      value: /app-config/secrets/db-password
    - name: API_KEY_FILE
      value: /app-config/secrets/api-key
    
    volumeMounts:
    - name: app-config
      mountPath: /app-config
      readOnly: true

  volumes:
  - name: app-config
    projected:
      defaultMode: 0400  # 只读权限（重要：保护敏感数据）
      sources:
      # 1. 应用配置
      - configMap:
          name: webapp-config
          items:
          - key: application.yaml
            path: config/application.yaml

      # 2. 敏感信息
      - secret:
          name: webapp-secret
          items:
          - key: db-password
            path: secrets/db-password
          - key: api-key
            path: secrets/api-key

      # 3. Pod元数据
      - downwardAPI:
          items:
          - path: metadata/pod-name
            fieldRef:
              fieldPath: metadata.name
          - path: metadata/pod-namespace
            fieldRef:
              fieldPath: metadata.namespace
          - path: metadata/pod-ip
            fieldRef:
              fieldPath: status.podIP
          - path: metadata/labels
            fieldRef:
              fieldPath: metadata.labels
          - path: metadata/annotations
            fieldRef:
              fieldPath: metadata.annotations

      # 4. 服务账户Token（用于调用其他K8s服务）
      - serviceAccountToken:
          path: tokens/api-token
          expirationSeconds: 7200  # 2小时过期
          audience: kubernetes.default.svc
```

**步骤3：应用内读取配置**

```python
# Python应用示例
import os
import yaml

# 读取配置文件
with open('/app-config/config/application.yaml') as f:
    config = yaml.safe_load(f)
    print(f"Server port: {config['server']['port']}")

# 读取Secret
with open('/app-config/secrets/db-password') as f:
    db_password = f.read().strip()
    print(f"DB Password loaded: {'*' * len(db_password)}")

# 读取Pod元数据
with open('/app-config/metadata/pod-name') as f:
    pod_name = f.read().strip()
    print(f"Running in Pod: {pod_name}")

with open('/app-config/metadata/pod-ip') as f:
    pod_ip = f.read().strip()
    print(f"Pod IP: {pod_ip}")

# 读取ServiceAccount Token
with open('/app-config/tokens/api-token') as f:
    token = f.read().strip()
    print(f"Token loaded: {token[:20]}...")
```

**验证：**

```bash
# 创建资源
$ kubectl apply -f webapp-config.yaml
$ kubectl apply -f webapp.yaml

# 查看挂载的文件
$ kubectl exec webapp -- ls -la /app-config
total 0
drwxr-xr-x 5 root root  80 Jan 13 15:30 .
drwxr-xr-x 1 root root  20 Jan 13 15:30 ..
drwxr-xr-x 2 root root  40 Jan 13 15:30 config
drwxr-xr-x 2 root root  40 Jan 13 15:30 metadata
drwxr-xr-x 2 root root  40 Jan 13 15:30 secrets
drwxr-xr-x 2 root root  20 Jan 13 15:30 tokens

# 查看配置文件
$ kubectl exec webapp -- cat /app-config/config/application.yaml
server:
  port: 8080
logging:
  level: INFO
feature-flags:
  new-ui: true

# 查看Pod元数据
$ kubectl exec webapp -- cat /app-config/metadata/pod-name
webapp

$ kubectl exec webapp -- cat /app-config/metadata/labels
app="webapp"
environment="production"
version="v1.0"
```

---

### 8.2.4 downwardAPI卷：Pod元数据注入

**定义：** downwardAPI卷允许将Pod的元数据（labels、annotations、资源限制等）以文件形式暴露给容器。

**支持的字段：**

| 字段类别 | fieldRef可用字段 | resourceFieldRef可用字段 |
|---------|----------------|------------------------|
| **基本信息** | `metadata.name`<br>`metadata.namespace`<br>`metadata.uid` | - |
| **标签和注解** | `metadata.labels`<br>`metadata.annotations` | - |
| **网络信息** | `status.podIP`<br>`status.hostIP` | - |
| **服务账户** | `spec.serviceAccountName` | - |
| **节点信息** | `spec.nodeName` | - |
| **资源信息** | - | `limits.cpu`<br>`limits.memory`<br>`requests.cpu`<br>`requests.memory` |

---

#### 基础示例

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: downwardapi-demo
  labels:
    app: myapp
    tier: frontend
  annotations:
    version: "1.0.0"
    build-id: "20240113-abc123"
spec:
  containers:
  - name: app
    image: busybox:1.35
    command: ['sh', '-c', 'sleep 3600']
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "200m"
        memory: "256Mi"
    volumeMounts:
    - name: podinfo
      mountPath: /etc/podinfo

  volumes:
  - name: podinfo
    downwardAPI:
      items:
      # Pod基本信息
      - path: pod-name
        fieldRef:
          fieldPath: metadata.name
      - path: pod-namespace
        fieldRef:
          fieldPath: metadata.namespace
      - path: pod-ip
        fieldRef:
          fieldPath: status.podIP

      # 标签和注解（以键值对形式）
      - path: labels
        fieldRef:
          fieldPath: metadata.labels
      - path: annotations
        fieldRef:
          fieldPath: metadata.annotations

      # 资源限制
      - path: cpu-limit
        resourceFieldRef:
          containerName: app
          resource: limits.cpu
      - path: memory-limit
        resourceFieldRef:
          containerName: app
          resource: limits.memory
```

**查看注入的数据：**

```bash
# 查看Pod名称
$ kubectl exec downwardapi-demo -- cat /etc/podinfo/pod-name
downwardapi-demo

# 查看Pod IP
$ kubectl exec downwardapi-demo -- cat /etc/podinfo/pod-ip
10.244.1.15

# 查看标签
$ kubectl exec downwardapi-demo -- cat /etc/podinfo/labels
app="myapp"
tier="frontend"

# 查看注解
$ kubectl exec downwardapi-demo -- cat /etc/podinfo/annotations
version="1.0.0"
build-id="20240113-abc123"

# 查看资源限制
$ kubectl exec downwardapi-demo -- cat /etc/podinfo/cpu-limit
1  # 200m转换为CPU核心数 = 0.2（显示为整数1是因为单位转换）

$ kubectl exec downwardapi-demo -- cat /etc/podinfo/memory-limit
268435456  # 256Mi = 256 * 1024 * 1024 字节
```

---

#### 实战案例：应用自适应配置

**场景：** 应用需要根据自身的资源配额自动调整参数（如JVM堆大小、线程池大小）。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: java-app-autotune
  labels:
    app: java-app
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
spec:
  containers:
  - name: app
    image: openjdk:11-jre
    command:
    - bash
    - -c
    - |
      # 读取内存限制
      MEMORY_LIMIT=$(cat /etc/podinfo/memory-limit)
      MEMORY_LIMIT_MB=$((MEMORY_LIMIT / 1024 / 1024))
      
      # 自动计算JVM堆大小（80%的Pod内存限制）
      HEAP_SIZE=$((MEMORY_LIMIT_MB * 80 / 100))
      
      # 读取CPU限制
      CPU_LIMIT=$(cat /etc/podinfo/cpu-limit)
      
      echo "Pod Memory Limit: ${MEMORY_LIMIT_MB}MB"
      echo "JVM Heap Size: ${HEAP_SIZE}MB"
      echo "CPU Limit: ${CPU_LIMIT} cores"
      
      # 启动Java应用
      java -Xmx${HEAP_SIZE}m \
           -XX:+UseG1GC \
           -XX:ParallelGCThreads=${CPU_LIMIT} \
           -jar /app/application.jar

    resources:
      requests:
        cpu: "500m"
        memory: "512Mi"
      limits:
        cpu: "1000m"
        memory: "1Gi"

    volumeMounts:
    - name: podinfo
      mountPath: /etc/podinfo
      readOnly: true

  volumes:
  - name: podinfo
    downwardAPI:
      items:
      - path: memory-limit
        resourceFieldRef:
          containerName: app
          resource: limits.memory
          divisor: "1"  # 返回字节数
      - path: cpu-limit
        resourceFieldRef:
          containerName: app
          resource: limits.cpu
          divisor: "1m"  # 返回毫核数
```

**运行结果：**

```bash
$ kubectl logs java-app-autotune
Pod Memory Limit: 1024MB
JVM Heap Size: 819MB
CPU Limit: 1000 cores
```

---

### 8.2.5 subPath和subPathExpr

**问题：** 默认情况下，Volume会覆盖挂载目标目录的所有内容。

```yaml
# ❌ 问题示例
containers:
- name: app
  image: nginx:1.21
  volumeMounts:
  - name: config
    mountPath: /etc/nginx  # 整个/etc/nginx目录被覆盖，原有文件丢失
volumes:
- name: config
  configMap:
    name: nginx-config
```

**解决方案：** 使用`subPath`只挂载Volume中的单个文件。

---

#### subPath基础用法

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  custom.conf: |
    server {
        listen 8080;
        server_name example.com;
    }

---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-subpath
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    volumeMounts:
    - name: config
      mountPath: /etc/nginx/conf.d/custom.conf  # 挂载到具体文件
      subPath: custom.conf  # 只挂载ConfigMap中的custom.conf

  volumes:
  - name: config
    configMap:
      name: nginx-config
```

**验证：**

```bash
# 查看/etc/nginx目录
$ kubectl exec nginx-subpath -- ls -la /etc/nginx
total 40
drwxr-xr-x 1 root root 4096 Jan 13 16:00 .
drwxr-xr-x 1 root root 4096 Jan 13 16:00 ..
drwxr-xr-x 2 root root 4096 Jan 13 16:00 conf.d  # 原有目录保留
-rw-r--r-- 1 root root 1007 Jan  1 12:00 fastcgi_params  # 原有文件保留
-rw-r--r-- 1 root root  648 Jan  1 12:00 mime.types
-rw-r--r-- 1 root root  636 Jan  1 12:00 nginx.conf

# 查看挂载的文件
$ kubectl exec nginx-subpath -- cat /etc/nginx/conf.d/custom.conf
server {
    listen 8080;
    server_name example.com;
}
```

---

#### subPathExpr动态路径

**场景：** 需要根据Pod信息动态生成挂载路径（如按Pod名称隔离日志目录）。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: logger-pod-001
spec:
  containers:
  - name: app
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      echo "Logging from $(hostname)" >> /logs/app.log
      tail -f /logs/app.log
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    volumeMounts:
    - name: logs
      mountPath: /logs
      subPathExpr: $(POD_NAME)  # 动态路径：/var/log/pods/logger-pod-001

  volumes:
  - name: logs
    hostPath:
      path: /var/log/pods
      type: DirectoryOrCreate
```

**验证：**

```bash
# 创建多个Pod
$ kubectl apply -f logger-pod.yaml
$ sed 's/logger-pod-001/logger-pod-002/g' logger-pod.yaml | kubectl apply -f -

# 查看节点目录结构
$ kubectl get pod -o wide  # 找到Pod所在节点
$ ssh node1
$ tree /var/log/pods
/var/log/pods
├── logger-pod-001
│   └── app.log
└── logger-pod-002
    └── app.log

# 每个Pod的日志隔离在独立目录
```

---

### 8.2.6 临时卷（Ephemeral Volumes）

Kubernetes 1.23+引入了更多临时卷类型。

---

#### Generic Ephemeral Volume（通用临时卷）

**定义：** 支持任何StorageClass的临时PVC，Pod删除时自动清理。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-volume-demo
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: scratch
      mountPath: /scratch

  volumes:
  - name: scratch
    ephemeral:
      volumeClaimTemplate:
        metadata:
          labels:
            type: scratch-space
        spec:
          accessModes: [ "ReadWriteOnce" ]
          storageClassName: "fast-ssd"  # 使用SSD StorageClass
          resources:
            requests:
              storage: 1Gi
```

**特点：**
- ✅ 自动创建PVC（命名格式：`<pod-name>-<volume-name>`）
- ✅ Pod删除时自动删除PVC和底层存储
- ✅ 支持所有StorageClass特性（快照、克隆、扩容等）

---

### 8.2.7 完整实战案例：多层Volume组合

**场景：** 部署一个完整的微服务应用，综合使用多种Volume类型。

**应用架构：**
- **应用容器**：Node.js Web服务
- **Sidecar容器**：Nginx反向代理
- **Init容器**：下载静态资源

**存储需求：**
1. 应用配置（ConfigMap）
2. TLS证书（Secret）
3. 临时缓存（emptyDir内存模式）
4. 静态文件（emptyDir共享）
5. 日志存储（hostPath，DaemonSet采集）
6. Pod元数据（downwardAPI）

**完整配置：**

```yaml
# 第一步：创建ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  app.json: |
    {
      "port": 3000,
      "logLevel": "info",
      "cacheEnabled": true
    }
  nginx.conf: |
    upstream app {
        server 127.0.0.1:3000;
    }
    server {
        listen 80;
        location / {
            proxy_pass http://app;
        }
    }

---
# 第二步：创建Secret
apiVersion: v1
kind: Secret
metadata:
  name: tls-secret
type: kubernetes.io/tls
data:
  tls.crt: LS0tLS1CRUdJTi...  # Base64编码的证书
  tls.key: LS0tLS1CRUdJTi...  # Base64编码的私钥

---
# 第三步：部署Pod
apiVersion: v1
kind: Pod
metadata:
  name: microservice-app
  labels:
    app: microservice
    tier: backend
  annotations:
    version: "2.0.0"
    maintainer: "devops@example.com"
spec:
  # Init容器：下载静态资源
  initContainers:
  - name: assets-downloader
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      echo "Downloading assets..."
      mkdir -p /static/css /static/js
      echo "body { color: blue; }" > /static/css/style.css
      echo "console.log('App loaded');" > /static/js/app.js
      echo "Assets downloaded successfully"
    volumeMounts:
    - name: static-files
      mountPath: /static

  # 主容器1：Node.js应用
  containers:
  - name: app
    image: node:16-alpine
    command:
    - node
    - -e
    - |
      const http = require('http');
      const fs = require('fs');
      
      // 读取配置
      const config = JSON.parse(fs.readFileSync('/config/app.json'));
      
      // 读取Pod元数据
      const podName = fs.readFileSync('/podinfo/pod-name', 'utf8');
      const podIP = fs.readFileSync('/podinfo/pod-ip', 'utf8');
      
      const server = http.createServer((req, res) => {
        const log = `${new Date().toISOString()} - ${req.method} ${req.url}\n`;
        fs.appendFileSync('/logs/access.log', log);
        
        res.writeHead(200);
        res.end(`Hello from ${podName} (${podIP})\n`);
      });
      
      server.listen(config.port, () => {
        console.log(`Server running on port ${config.port}`);
      });
    
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "200m"
        memory: "256Mi"
    
    volumeMounts:
    # 配置文件
    - name: app-config
      mountPath: /config
      readOnly: true
    # 临时缓存（内存）
    - name: cache
      mountPath: /cache
    # 静态文件（与Init容器共享）
    - name: static-files
      mountPath: /static
      readOnly: true
    # 日志
    - name: logs
      mountPath: /logs
    # Pod元数据
    - name: podinfo
      mountPath: /podinfo
      readOnly: true

  # 主容器2：Nginx Sidecar
  - name: nginx
    image: nginx:1.21
    ports:
    - containerPort: 80
    volumeMounts:
    # Nginx配置
    - name: nginx-config
      mountPath: /etc/nginx/conf.d/default.conf
      subPath: nginx.conf
    # TLS证书
    - name: tls
      mountPath: /etc/nginx/ssl
      readOnly: true
    # 静态文件（与App容器共享）
    - name: static-files
      mountPath: /usr/share/nginx/html/static
      readOnly: true
    # 日志（与App容器共享）
    - name: logs
      mountPath: /var/log/nginx

  # Volume定义
  volumes:
  # 1. ConfigMap：应用配置
  - name: app-config
    configMap:
      name: app-config
      items:
      - key: app.json
        path: app.json

  # 2. ConfigMap：Nginx配置
  - name: nginx-config
    configMap:
      name: app-config

  # 3. Secret：TLS证书
  - name: tls
    secret:
      secretName: tls-secret
      defaultMode: 0400

  # 4. emptyDir：内存缓存
  - name: cache
    emptyDir:
      medium: Memory
      sizeLimit: 100Mi

  # 5. emptyDir：静态文件共享
  - name: static-files
    emptyDir:
      sizeLimit: 500Mi

  # 6. hostPath：日志持久化
  - name: logs
    hostPath:
      path: /var/log/pods/microservice-app
      type: DirectoryOrCreate

  # 7. downwardAPI：Pod元数据
  - name: podinfo
    downwardAPI:
      items:
      - path: pod-name
        fieldRef:
          fieldPath: metadata.name
      - path: pod-ip
        fieldRef:
          fieldPath: status.podIP
      - path: labels
        fieldRef:
          fieldPath: metadata.labels
```

**部署和验证：**

```bash
# 创建资源
$ kubectl apply -f app-config.yaml
$ kubectl apply -f tls-secret.yaml
$ kubectl apply -f microservice-app.yaml

# 等待Pod运行
$ kubectl wait --for=condition=Ready pod/microservice-app --timeout=60s

# 查看Init容器日志
$ kubectl logs microservice-app -c assets-downloader
Downloading assets...
Assets downloaded successfully

# 查看应用日志
$ kubectl logs microservice-app -c app
Server running on port 3000

# 测试应用
$ kubectl exec microservice-app -c nginx -- curl localhost
Hello from microservice-app (10.244.1.25)

# 验证Volume挂载
$ kubectl exec microservice-app -c app -- ls -la /config
-rw-r--r-- 1 root root  65 Jan 13 17:00 app.json

$ kubectl exec microservice-app -c app -- ls -la /static
drwxr-xr-x 2 root root   40 Jan 13 17:00 css
drwxr-xr-x 2 root root   40 Jan 13 17:00 js

$ kubectl exec microservice-app -c app -- cat /podinfo/labels
app="microservice"
tier="backend"

# 查看日志文件
$ kubectl exec microservice-app -c app -- cat /logs/access.log
2024-01-13T17:05:23.123Z - GET /
2024-01-13T17:05:24.456Z - GET /health
```

---

### 8.2.8 Volume使用最佳实践总结

**1. Volume类型选择决策**

```
需求场景                    → 推荐Volume类型
─────────────────────────────────────────
容器间临时数据共享           → emptyDir
高性能临时缓存               → emptyDir (medium: Memory)
配置文件注入                 → ConfigMap
敏感信息（密码/证书）        → Secret
Pod元数据访问               → downwardAPI
多数据源组合                 → projected
节点文件访问（谨慎）         → hostPath (readOnly: true)
持久化数据                   → PVC (下一节详解)
```

**2. 性能优化建议**

| 场景 | 配置 | 性能提升 |
|------|------|---------|
| **编译缓存** | `emptyDir` + `medium: Memory` | 10-50倍 |
| **静态资源** | `emptyDir` + CDN分发 | 减少网络I/O |
| **日志缓冲** | `emptyDir` + 异步刷盘 | 降低磁盘压力 |
| **大文件读写** | PVC + SSD StorageClass | 2-5倍 |

**3. 安全加固清单**

```yaml
# ✅ 安全配置模板
volumes:
- name: sensitive-config
  secret:
    secretName: app-secret
    defaultMode: 0400  # 只有所有者可读
    
volumeMounts:
- name: sensitive-config
  mountPath: /etc/secrets
  readOnly: true  # 只读挂载

# ✅ hostPath严格限制
- name: logs
  hostPath:
    path: /var/log/app  # 白名单路径
    type: Directory     # 类型校验
volumeMounts:
- name: logs
  mountPath: /logs
  readOnly: true  # 只读（日志采集场景）
```

**4. 资源配额建议**

```yaml
# emptyDir容量规划
volumes:
- name: build-cache
  emptyDir:
    sizeLimit: 5Gi  # ✅ 始终设置限制

containers:
- name: app
  resources:
    limits:
      memory: "1Gi"  # ✅ 内存emptyDir计入此限制
  volumeMounts:
  - name: cache
    mountPath: /cache
    
volumes:
- name: cache
  emptyDir:
    medium: Memory
    sizeLimit: 256Mi  # 实际可用内存 = 1Gi - 256Mi
```

---

### 8.2.9 下一节预告

在本节中，我们深入学习了Volume的高级特性：

- ✅ emptyDir的内存模式和容量限制
- ✅ hostPath的安全风险和使用规范
- ✅ projected卷的多数据源组合
- ✅ downwardAPI卷的Pod元数据注入
- ✅ subPath和subPathExpr的文件级挂载
- ✅ 完整微服务应用的多层Volume组合实战

这些Volume类型适用于临时数据、配置管理和元数据访问。然而，对于需要持久化的数据（如数据库、用户上传文件），我们需要使用**PersistentVolume（PV）和PersistentVolumeClaim（PVC）**。

**在下一节（8.3 持久卷PersistentVolume）中**，我们将深入学习：
- PV的生命周期管理（Available → Bound → Released → Failed）
- 访问模式的深入理解（RWO/RWX/ROX的选择和限制）
- 回收策略的实际影响（Retain/Delete/Recycle）
- 静态供应的完整实战（NFS/iSCSI/Local PV）
- PV容量回收和状态恢复

---

**本节完**

*下一节预告：8.3节《持久卷PersistentVolume》- 深入PV的生命周期、访问模式、回收策略和静态供应实战。*
## 8.3 持久卷（PersistentVolume）

在前两节中，我们学习了Kubernetes存储体系的整体架构和各种Volume类型的使用。emptyDir、hostPath等Volume适用于临时数据和配置管理，但它们的生命周期与Pod紧密绑定，无法满足数据持久化的需求。本节将深入学习**PersistentVolume（PV）**，它是Kubernetes中真正实现数据持久化的核心机制。

---

### 8.3.1 PV生命周期管理

PersistentVolume是集群级别的存储资源，独立于Pod的生命周期存在。理解PV的生命周期是掌握持久化存储的关键。

---

#### PV的五种状态

```
PV生命周期状态机
┌──────────────┐
│  Available   │ ← 初始状态：PV已创建，等待PVC绑定
└──────┬───────┘
       │ PVC创建并匹配
       ▼
┌──────────────┐
│    Bound     │ ← PV与PVC成功绑定，正在被Pod使用
└──────┬───────┘
       │ PVC被删除
       ▼
┌──────────────┐
│   Released   │ ← PV已释放，但数据未清理（Retain策略）
└──────┬───────┘
       │ 手动清理 / 自动回收
       ▼
┌──────────────┐
│   Available  │ ← 重新可用（Recycle策略，已废弃）
│      或       │
│    Deleted   │ ← 自动删除（Delete策略）
└──────┬───────┘
       │ 出现错误
       ▼
┌──────────────┐
│    Failed    │ ← PV回收失败，需要手动干预
└──────────────┘
```

**状态详解：**

| 状态 | 含义 | 可用性 | 典型场景 |
|------|------|--------|---------|
| **Available** | PV空闲可用 | ✅ 可以被PVC绑定 | 刚创建的PV<br>Recycle后的PV |
| **Bound** | 已绑定到PVC | ❌ 不可被其他PVC绑定 | 正在被Pod使用 |
| **Released** | PVC已删除，数据未清理 | ❌ 不可被新PVC绑定 | PVC删除后<br>（Retain策略） |
| **Failed** | 回收失败 | ❌ 需要手动修复 | 自动回收出错<br>存储后端故障 |

---

#### 完整生命周期演示

**场景：** 创建PV → 绑定PVC → 使用 → 删除PVC → 状态变化观察

**步骤1：创建Available状态的PV**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-lifecycle-demo
  labels:
    type: nfs
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain  # 关键：使用Retain观察Released状态
  storageClassName: manual
  nfs:
    server: 192.168.1.100
    path: /data/pv-lifecycle
```

```bash
# 创建PV
$ kubectl apply -f pv-lifecycle-demo.yaml
persistentvolume/pv-lifecycle-demo created

# 查看状态：Available
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Available           manual         5s
```

**步骤2：创建PVC触发绑定 → Bound状态**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-lifecycle-demo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi  # 小于PV的5Gi
  storageClassName: manual
```

```bash
# 创建PVC
$ kubectl apply -f pvc-lifecycle-demo.yaml
persistentvolumeclaim/pvc-lifecycle-demo created

# 查看PVC状态：Bound
$ kubectl get pvc pvc-lifecycle-demo
NAME                 STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle-demo   Bound    pv-lifecycle-demo   5Gi        RWO            manual         3s

# 再次查看PV状态：从Available变为Bound
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                        STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Bound    default/pvc-lifecycle-demo   manual         2m
#                                                               ^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^^
#                                                               状态变化  显示绑定的PVC
```

**步骤3：Pod使用PVC**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-pvc
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: data
      mountPath: /usr/share/nginx/html
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-lifecycle-demo
```

```bash
# 创建Pod
$ kubectl apply -f pod-using-pvc.yaml
pod/pod-using-pvc created

# 写入测试数据
$ kubectl exec pod-using-pvc -- sh -c 'echo "Hello from PV" > /usr/share/nginx/html/index.html'

# 验证数据
$ kubectl exec pod-using-pvc -- cat /usr/share/nginx/html/index.html
Hello from PV
```

**步骤4：删除Pod（PV仍为Bound）**

```bash
# 删除Pod
$ kubectl delete pod pod-using-pvc
pod "pod-using-pvc" deleted

# PV状态依然是Bound（因为PVC还存在）
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                        STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Bound    default/pvc-lifecycle-demo   manual         5m
```

**步骤5：删除PVC → Released状态**

```bash
# 删除PVC
$ kubectl delete pvc pvc-lifecycle-demo
persistentvolumeclaim "pvc-lifecycle-demo" deleted

# PV状态变为Released
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                        STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Released   default/pvc-lifecycle-demo   manual         6m
#                                                               ^^^^^^^^
#                                                               注意：从Bound变为Released

# 查看详细信息
$ kubectl describe pv pv-lifecycle-demo
Name:            pv-lifecycle-demo
Status:          Released
Claim:           default/pvc-lifecycle-demo  # 仍然记录之前的PVC
Reclaim Policy:  Retain
Access Modes:    RWO
Message:         # 可能有消息说明为什么不能重新绑定
```

**步骤6：验证数据是否保留**

```bash
# 在NFS服务器上检查数据
$ ssh 192.168.1.100
$ cat /data/pv-lifecycle/index.html
Hello from PV
# ✅ 数据完整保留！
```

**步骤7：尝试重新绑定（失败）**

```bash
# 尝试创建新的PVC
$ kubectl apply -f pvc-lifecycle-demo.yaml
persistentvolumeclaim/pvc-lifecycle-demo created

# 新PVC无法绑定到Released状态的PV
$ kubectl get pvc pvc-lifecycle-demo
NAME                 STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle-demo   Pending                                      manual         10s
#                    ^^^^^^^
#                    一直处于Pending状态

# PV状态依然是Released
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM   STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Released           manual         10m
```

**步骤8：手动回收PV → 重新Available**

```bash
# 方法1：删除PV并重新创建（简单粗暴）
$ kubectl delete pv pv-lifecycle-demo
$ kubectl apply -f pv-lifecycle-demo.yaml

# 方法2：编辑PV，移除claimRef字段（推荐）
$ kubectl edit pv pv-lifecycle-demo
# 删除以下部分：
#   claimRef:
#     apiVersion: v1
#     kind: PersistentVolumeClaim
#     name: pvc-lifecycle-demo
#     namespace: default
#     resourceVersion: "123456"
#     uid: abcd-1234-5678-efgh

# 保存后，PV状态变为Available
$ kubectl get pv pv-lifecycle-demo
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   AGE
pv-lifecycle-demo   5Gi        RWO            Retain           Available           manual         12m
#                                                               ^^^^^^^^^

# 现在新PVC可以成功绑定
$ kubectl get pvc pvc-lifecycle-demo
NAME                 STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle-demo   Bound    pv-lifecycle-demo   5Gi        RWO            manual         2m
```

---

### 8.3.2 访问模式深入理解

访问模式（Access Modes）定义了PV如何被挂载到节点上。理解访问模式的限制是选择存储方案的关键。

---

#### 三种访问模式详解

**1. ReadWriteOnce（RWO）- 单节点读写**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-rwo
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce  # 单节点读写
  awsElasticBlockStore:
    volumeID: vol-0a1b2c3d4e5f
    fsType: ext4
```

**特点：**
- ✅ **同一节点的多个Pod可以同时挂载**
- ❌ **不同节点的Pod无法同时挂载**
- ✅ **适用于块存储**：AWS EBS、Azure Disk、GCE PD、本地磁盘

**验证实验：**

```yaml
# 实验1：同一节点的2个Pod同时挂载RWO PVC
apiVersion: v1
kind: Pod
metadata:
  name: pod1-rwo
spec:
  nodeSelector:
    kubernetes.io/hostname: node1  # 强制调度到node1
  containers:
  - name: app
    image: busybox:1.35
    command: ['sh', '-c', 'echo "Pod1" >> /data/test.txt && sleep 3600']
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-rwo

---
apiVersion: v1
kind: Pod
metadata:
  name: pod2-rwo
spec:
  nodeSelector:
    kubernetes.io/hostname: node1  # 同样调度到node1
  containers:
  - name: app
    image: busybox:1.35
    command: ['sh', '-c', 'echo "Pod2" >> /data/test.txt && sleep 3600']
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-rwo
```

```bash
# 创建Pod
$ kubectl apply -f pod1-rwo.yaml
$ kubectl apply -f pod2-rwo.yaml

# 两个Pod都成功运行（因为在同一节点）
$ kubectl get pod -o wide
NAME        READY   STATUS    NODE    
pod1-rwo    1/1     Running   node1   
pod2-rwo    1/1     Running   node1   

# 验证数据共享
$ kubectl exec pod1-rwo -- cat /data/test.txt
Pod1
Pod2
# ✅ 同一节点的多个Pod可以同时读写
```

```yaml
# 实验2：不同节点的Pod尝试挂载RWO PVC
apiVersion: v1
kind: Pod
metadata:
  name: pod3-rwo
spec:
  nodeSelector:
    kubernetes.io/hostname: node2  # 调度到不同节点node2
  containers:
  - name: app
    image: busybox:1.35
    command: ['sh', '-c', 'sleep 3600']
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-rwo  # 同一个PVC
```

```bash
# 创建Pod
$ kubectl apply -f pod3-rwo.yaml

# Pod无法启动
$ kubectl get pod pod3-rwo
NAME       READY   STATUS              RESTARTS   AGE
pod3-rwo   0/1     ContainerCreating   0          2m

# 查看详细事件
$ kubectl describe pod pod3-rwo
Events:
  Warning  FailedAttachVolume  Multi-Attach error for volume "pvc-xxx": Volume is already exclusively attached to one node and can't be attached to another
# ❌ RWO模式不允许跨节点挂载
```

---

**2. ReadWriteMany（RWX）- 多节点读写**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-rwx
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteMany  # 多节点读写
  nfs:
    server: 192.168.1.100
    path: /data/shared
```

**特点：**
- ✅ **多个节点的Pod可以同时挂载**
- ✅ **所有Pod可以并发读写**
- ✅ **适用于网络存储**：NFS、GlusterFS、CephFS、Azure Files
- ❌ **不支持块存储**：AWS EBS、Azure Disk不支持RWX

**验证实验：**

```yaml
# 3个Pod分别调度到不同节点，都挂载RWX PVC
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-rwx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      affinity:
        podAntiAffinity:  # 强制分散到不同节点
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: nginx
            topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: nginx:1.21
        volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
      volumes:
      - name: html
        persistentVolumeClaim:
          claimName: pvc-rwx  # RWX模式的PVC
```

```bash
# 创建Deployment
$ kubectl apply -f nginx-rwx.yaml

# 所有Pod都成功运行在不同节点
$ kubectl get pod -o wide
NAME                         READY   STATUS    NODE
nginx-rwx-5d7f8b9c4-abc12    1/1     Running   node1
nginx-rwx-5d7f8b9c4-def34    1/1     Running   node2
nginx-rwx-5d7f8b9c4-ghi56    1/1     Running   node3

# 在一个Pod中写入数据
$ kubectl exec nginx-rwx-5d7f8b9c4-abc12 -- sh -c 'echo "Shared data" > /usr/share/nginx/html/index.html'

# 在其他Pod中读取数据
$ kubectl exec nginx-rwx-5d7f8b9c4-def34 -- cat /usr/share/nginx/html/index.html
Shared data

$ kubectl exec nginx-rwx-5d7f8b9c4-ghi56 -- cat /usr/share/nginx/html/index.html
Shared data
# ✅ 所有节点的Pod都能读写共享数据
```

---

**3. ReadOnlyMany（ROX）- 多节点只读**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-rox
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadOnlyMany  # 多节点只读
  nfs:
    server: 192.168.1.100
    path: /data/static-content
```

**特点：**
- ✅ **多个节点的Pod可以同时挂载**
- ✅ **所有Pod只能读取，不能写入**
- ✅ **典型场景**：静态资源分发、配置文件共享

**验证实验：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-rox
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: static
      mountPath: /usr/share/nginx/html
      readOnly: true  # 强制只读
  volumes:
  - name: static
    persistentVolumeClaim:
      claimName: pvc-rox  # ROX模式的PVC
```

```bash
# 尝试写入数据
$ kubectl exec pod-rox -- sh -c 'echo "test" > /usr/share/nginx/html/test.txt'
sh: can't create /usr/share/nginx/html/test.txt: Read-only file system
# ❌ 只读文件系统，无法写入

# 可以正常读取
$ kubectl exec pod-rox -- cat /usr/share/nginx/html/index.html
<html>...</html>
# ✅ 读取成功
```

---

#### 访问模式与存储后端兼容性

| 存储后端 | RWO | RWX | ROX | 说明 |
|---------|-----|-----|-----|------|
| **AWS EBS** | ✅ | ❌ | ❌ | 块存储，仅支持单节点 |
| **Azure Disk** | ✅ | ❌ | ❌ | 块存储，仅支持单节点 |
| **GCE PD** | ✅ | ❌ | ✅ | 块存储+只读多节点 |
| **NFS** | ✅ | ✅ | ✅ | 网络文件系统，全支持 |
| **CephFS** | ✅ | ✅ | ✅ | 分布式文件系统，全支持 |
| **GlusterFS** | ✅ | ✅ | ✅ | 分布式文件系统，全支持 |
| **iSCSI** | ✅ | ❌ | ❌ | 块存储，仅支持单节点 |
| **Local PV** | ✅ | ❌ | ❌ | 本地磁盘，仅支持单节点 |
| **Azure Files** | ✅ | ✅ | ✅ | SMB协议，全支持 |

**重要提示：**
- ⚠️ **PV的accessModes必须包含PVC请求的所有模式**
  ```yaml
  # ✅ 可以绑定
  PV: accessModes: [ReadWriteOnce, ReadWriteMany]
  PVC: accessModes: [ReadWriteOnce]
  
  # ❌ 无法绑定
  PV: accessModes: [ReadWriteOnce]
  PVC: accessModes: [ReadWriteMany]
  ```

---

### 8.3.3 回收策略详解

回收策略（Reclaim Policy）决定了PVC删除后，PV和底层存储数据的处理方式。

---

#### 三种回收策略对比

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-example
spec:
  persistentVolumeReclaimPolicy: Retain  # 回收策略
  # 可选值：
  # - Retain：保留（默认静态PV）
  # - Delete：删除（默认动态PV）
  # - Recycle：回收（已废弃）
```

| 策略 | PVC删除后的行为 | PV状态 | 底层存储 | 适用场景 |
|------|---------------|--------|---------|---------|
| **Retain** | PV变为Released<br>需手动清理 | Released | 数据保留 | 生产环境数据库<br>重要数据备份 |
| **Delete** | 自动删除PV<br>自动删除存储数据 | （PV已删除） | 数据删除 | 临时数据<br>可重建数据 |
| **Recycle**<br>（已废弃） | 执行`rm -rf /volume/*`<br>PV重新可用 | Available | 数据清空 | 不推荐使用 |

---

#### Retain策略实战

**场景：** 数据库数据需要在PVC删除后保留，用于备份或迁移。

```yaml
# PV配置
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv-retain
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain  # 保留策略
  storageClassName: manual
  hostPath:
    path: /data/mysql-retain
    type: DirectoryOrCreate

---
# PVC配置
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: manual

---
# MySQL Pod
apiVersion: v1
kind: Pod
metadata:
  name: mysql
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    env:
    - name: MYSQL_ROOT_PASSWORD
      value: "mysql123"
    volumeMounts:
    - name: data
      mountPath: /var/lib/mysql
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: mysql-pvc
```

**操作流程：**

```bash
# 1. 创建资源
$ kubectl apply -f mysql-pv-retain.yaml
$ kubectl apply -f mysql-pvc.yaml
$ kubectl apply -f mysql-pod.yaml

# 2. 写入测试数据
$ kubectl exec mysql -- mysql -uroot -pmysql123 -e "
CREATE DATABASE testdb;
USE testdb;
CREATE TABLE users (id INT, name VARCHAR(50));
INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob');
"

# 3. 删除Pod和PVC
$ kubectl delete pod mysql
$ kubectl delete pvc mysql-pvc

# 4. 查看PV状态
$ kubectl get pv mysql-pv-retain
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM               STORAGECLASS   AGE
mysql-pv-retain    20Gi       RWO            Retain           Released   default/mysql-pvc   manual         5m
#                                            ^^^^^^           ^^^^^^^^
#                                            Retain策略        Released状态

# 5. 验证数据是否保留
$ ls -la /data/mysql-retain/
total 180M
drwxr-x--- 6 999  999   4.0K Jan 14 10:00 testdb/      # ✅ 数据库文件完整保留
drwxr-x--- 2 999  999   4.0K Jan 14 10:00 mysql/
-rw-r----- 1 999  999    56M Jan 14 10:00 ib_logfile0
-rw-r----- 1 999  999    12M Jan 14 10:00 ibdata1

# 6. 手动恢复数据（创建新PVC前，先清理PV的claimRef）
$ kubectl patch pv mysql-pv-retain -p '{"spec":{"claimRef": null}}'

# 7. 创建新PVC绑定到同一PV
$ kubectl apply -f mysql-pvc.yaml

# 8. 创建新Pod验证数据
$ kubectl apply -f mysql-pod.yaml
$ kubectl exec mysql -- mysql -uroot -pmysql123 -e "USE testdb; SELECT * FROM users;"
+------+-------+
| id   | name  |
+------+-------+
|    1 | Alice |
|    2 | Bob   |
+------+-------+
# ✅ 数据完整恢复！
```

---

#### Delete策略实战

**场景：** 动态供应的临时数据，PVC删除后自动清理存储。

```yaml
# StorageClass配置（动态供应）
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-delete
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
reclaimPolicy: Delete  # StorageClass级别的回收策略
volumeBindingMode: Immediate

---
# PVC配置（动态供应）
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: temp-data-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: fast-delete  # 使用Delete策略的StorageClass
```

**操作流程：**

```bash
# 1. 创建PVC（自动创建PV）
$ kubectl apply -f temp-data-pvc.yaml

# 2. 查看自动创建的PV
$ kubectl get pv
NAME                                       CAPACITY   RECLAIM POLICY   STATUS   CLAIM
pvc-abc123-def456-ghi789                   10Gi       Delete           Bound    default/temp-data-pvc
#                                                     ^^^^^^
#                                                     自动设置为Delete

# 3. 使用PVC
$ kubectl run test-pod --image=nginx --overrides='
{
  "spec": {
    "volumes": [{
      "name": "data",
      "persistentVolumeClaim": {"claimName": "temp-data-pvc"}
    }],
    "containers": [{
      "name": "nginx",
      "image": "nginx:1.21",
      "volumeMounts": [{
        "name": "data",
        "mountPath": "/data"
      }]
    }]
  }
}'

# 4. 写入数据
$ kubectl exec test-pod -- sh -c 'echo "Temporary data" > /data/test.txt'

# 5. 删除PVC
$ kubectl delete pvc temp-data-pvc
persistentvolumeclaim "temp-data-pvc" deleted

# 6. PV自动删除
$ kubectl get pv
No resources found
# ✅ PV已被自动删除

# 7. 底层AWS EBS卷也被删除
$ aws ec2 describe-volumes --volume-ids vol-abc123
An error occurred (InvalidVolume.NotFound)
# ✅ 底层存储也被自动删除
```

---

#### Recycle策略（已废弃）

**定义：** 执行`rm -rf /volume/*`清空数据，PV重新变为Available。

**为什么废弃：**
- ❌ **不安全**：简单的`rm -rf`无法彻底清除敏感数据
- ❌ **不可靠**：某些文件系统可能清理失败
- ❌ **不灵活**：无法自定义清理逻辑

**替代方案：** 使用Dynamic Provisioning（动态供应）+ Delete策略。

---

### 8.3.4 静态供应完整实战

静态供应（Static Provisioning）是指管理员手动创建PV，开发者创建PVC进行绑定。适用于已有存储资源或需要精细控制的场景。

---

#### 实战1：NFS静态供应

**场景：** 企业内部有NFS服务器，提供共享存储给Kubernetes集群。

**前提条件：**
```bash
# NFS服务器配置（192.168.1.100）
$ sudo apt-get install -y nfs-kernel-server

# 创建共享目录
$ sudo mkdir -p /data/k8s-nfs/{pv1,pv2,pv3}
$ sudo chmod 777 /data/k8s-nfs/*

# 配置NFS导出
$ sudo tee /etc/exports <<EOF
/data/k8s-nfs/pv1  *(rw,sync,no_subtree_check,no_root_squash)
/data/k8s-nfs/pv2  *(rw,sync,no_subtree_check,no_root_squash)
/data/k8s-nfs/pv3  *(rw,sync,no_subtree_check,no_root_squash)
EOF

# 应用配置
$ sudo exportfs -ra
$ sudo systemctl restart nfs-kernel-server

# 验证导出
$ showmount -e localhost
Export list for localhost:
/data/k8s-nfs/pv1 *
/data/k8s-nfs/pv2 *
/data/k8s-nfs/pv3 *
```

**步骤1：创建多个NFS PV**

```yaml
# nfs-pvs.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-1
  labels:
    tier: gold
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    server: 192.168.1.100
    path: /data/k8s-nfs/pv1

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-2
  labels:
    tier: silver
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  nfs:
    server: 192.168.1.100
    path: /data/k8s-nfs/pv2

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-3
  labels:
    tier: bronze
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce  # 注意：NFS支持RWX，但这里设置为RWO演示访问模式匹配
  persistentVolumeReclaimPolicy: Delete
  storageClassName: nfs
  nfs:
    server: 192.168.1.100
    path: /data/k8s-nfs/pv3
```

```bash
# 创建PV
$ kubectl apply -f nfs-pvs.yaml
persistentvolume/nfs-pv-1 created
persistentvolume/nfs-pv-2 created
persistentvolume/nfs-pv-3 created

# 查看PV
$ kubectl get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      STORAGECLASS   AGE
nfs-pv-1   10Gi       RWX            Retain           Available   nfs            10s
nfs-pv-2   5Gi        RWX            Retain           Available   nfs            10s
nfs-pv-3   2Gi        RWO            Delete           Available   nfs            10s
```

**步骤2：创建PVC并验证绑定规则**

```yaml
# pvc-tier-gold.yaml（使用标签选择器）
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-gold
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 8Gi  # 小于10Gi
  storageClassName: nfs
  selector:
    matchLabels:
      tier: gold  # 精确选择gold标签的PV
```

```bash
# 创建PVC
$ kubectl apply -f pvc-tier-gold.yaml

# 验证绑定到nfs-pv-1
$ kubectl get pvc pvc-gold
NAME       STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-gold   Bound    nfs-pv-1   10Gi       RWX            nfs            5s

$ kubectl get pv nfs-pv-1
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   AGE
nfs-pv-1   10Gi       RWX            Retain           Bound    default/pvc-gold   nfs            2m
```

**步骤3：部署应用使用NFS存储**

```yaml
# wordpress-nfs.yaml
apiVersion: v1
kind: Service
metadata:
  name: wordpress
spec:
  selector:
    app: wordpress
  ports:
  - port: 80
  type: LoadBalancer

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
spec:
  replicas: 3  # 多副本共享NFS存储
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:6.0-apache
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql:3306
        - name: WORDPRESS_DB_PASSWORD
          value: "wordpress123"
        ports:
        - containerPort: 80
        volumeMounts:
        - name: wordpress-data
          mountPath: /var/www/html
      volumes:
      - name: wordpress-data
        persistentVolumeClaim:
          claimName: pvc-gold  # 使用NFS RWX PVC
```

```bash
# 部署WordPress
$ kubectl apply -f wordpress-nfs.yaml

# 验证3个Pod都运行在不同节点
$ kubectl get pod -o wide
NAME                         READY   STATUS    NODE
wordpress-7d8f9b6c5-abc12    1/1     Running   node1
wordpress-7d8f9b6c5-def34    1/1     Running   node2
wordpress-7d8f9b6c5-ghi56    1/1     Running   node3

# 访问WordPress初始化
$ kubectl get svc wordpress
NAME        TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)        AGE
wordpress   LoadBalancer   10.96.100.200   203.0.113.10    80:30080/TCP   2m

# 浏览器访问 http://203.0.113.10 完成WordPress安装

# 验证NFS服务器上的数据
$ ssh 192.168.1.100
$ ls -la /data/k8s-nfs/pv1/
total 15M
drwxr-xr-x 5 www-data www-data 4.0K Jan 14 11:00 wp-admin/
drwxr-xr-x 9 www-data www-data 4.0K Jan 14 11:00 wp-content/
drwxr-xr-x 2 www-data www-data 4.0K Jan 14 11:00 wp-includes/
-rw-r--r-- 1 www-data www-data 405  Jan 14 11:00 index.php
# ✅ WordPress文件成功写入NFS共享存储
```

---

#### 实战2：Local PV静态供应

**场景：** 需要高性能本地SSD，用于数据库或缓存应用。

**重要特性：**
- ✅ **性能最佳**：本地SSD，无网络延迟
- ⚠️ **节点亲和性**：Pod必须调度到PV所在节点
- ❌ **无法跨节点迁移**：节点故障时数据不可用

**步骤1：准备节点本地磁盘**

```bash
# 在每个节点创建本地目录
$ ssh node1
$ sudo mkdir -p /mnt/disks/ssd1
$ sudo chmod 777 /mnt/disks/ssd1

$ ssh node2
$ sudo mkdir -p /mnt/disks/ssd2
$ sudo chmod 777 /mnt/disks/ssd2
```

**步骤2：创建Local PV**

```yaml
# local-pv-node1.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-node1
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1  # 本地路径
  nodeAffinity:  # 关键：必须设置节点亲和性
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1  # 此PV只能在node1使用

---
# local-pv-node2.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-node2
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd2
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node2
```

```bash
# 创建Local PV
$ kubectl apply -f local-pv-node1.yaml
$ kubectl apply -f local-pv-node2.yaml

# 查看PV（注意NODE AFFINITY列）
$ kubectl get pv
NAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      STORAGECLASS    VOLUMEATTRIBUTESCLASS   AGE
local-pv-node1    100Gi      RWO            Retain           Available   local-storage   <unset>                 10s
local-pv-node2    100Gi      RWO            Retain           Available   local-storage   <unset>                 10s

$ kubectl describe pv local-pv-node1 | grep -A 5 "Node Affinity"
Node Affinity:
  Required Terms:
    Term 0:  kubernetes.io/hostname in [node1]
```

**步骤3：部署StatefulSet使用Local PV**

```yaml
# redis-statefulset-local.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis
spec:
  clusterIP: None  # Headless Service
  selector:
    app: redis
  ports:
  - port: 6379

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
spec:
  serviceName: redis
  replicas: 2
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7.0
        ports:
        - containerPort: 6379
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: local-storage
      resources:
        requests:
          storage: 50Gi
```

```bash
# 部署StatefulSet
$ kubectl apply -f redis-statefulset-local.yaml

# 查看Pod调度情况
$ kubectl get pod -o wide
NAME      READY   STATUS    NODE    PV
redis-0   1/1     Running   node1   local-pv-node1
redis-1   1/1     Running   node2   local-pv-node2
# ✅ Pod根据PV的nodeAffinity自动调度到正确节点

# 查看PVC绑定
$ kubectl get pvc
NAME           STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS    AGE
data-redis-0   Bound    local-pv-node1   100Gi      RWO            local-storage   2m
data-redis-1   Bound    local-pv-node2   100Gi      RWO            local-storage   2m

# 测试性能
$ kubectl exec redis-0 -- redis-benchmark -t set,get -n 100000 -q
SET: 125000.00 requests per second
GET: 142857.14 requests per second
# ✅ 本地SSD性能远超网络存储
```

---

### 8.3.5 PV容量管理和扩容

**容量匹配规则：**
- PV容量 >= PVC请求容量
- PVC实际获得的容量 = PV容量（不是PVC请求容量）

**示例：**

```yaml
# PV: 100Gi
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-large
spec:
  capacity:
    storage: 100Gi
  # ...

---
# PVC: 请求10Gi
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-small
spec:
  resources:
    requests:
      storage: 10Gi
  # ...
```

```bash
# PVC绑定后，容量显示为100Gi（而不是10Gi）
$ kubectl get pvc pvc-small
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-small   Bound    pv-large   100Gi      RWO            manual         10s
#                               ^^^^^
#                               PV的容量

# 浪费了90Gi空间！
```

**最佳实践：** PV容量应精确匹配PVC请求，或使用动态供应自动匹配。

---

### 8.3.6 PV状态恢复和故障排查

#### 场景1：Released状态的PV无法重新绑定

**问题：**

```bash
$ kubectl get pv
NAME     CAPACITY   STATUS     CLAIM             STORAGECLASS   AGE
my-pv    10Gi       Released   default/old-pvc   manual         10m

$ kubectl get pvc new-pvc
NAME      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
new-pvc   Pending                                      manual         2m
# PVC无法绑定到Released状态的PV
```

**原因：** Released状态的PV仍然保留着之前PVC的`claimRef`引用。

**解决方案1：编辑PV，删除claimRef**

```bash
$ kubectl edit pv my-pv
# 删除以下部分：
#   claimRef:
#     apiVersion: v1
#     kind: PersistentVolumeClaim
#     name: old-pvc
#     namespace: default
#     resourceVersion: "123456"
#     uid: abcd-1234-5678

# 或使用patch命令
$ kubectl patch pv my-pv -p '{"spec":{"claimRef": null}}'

# PV状态变为Available
$ kubectl get pv my-pv
NAME    CAPACITY   STATUS      CLAIM   STORAGECLASS   AGE
my-pv   10Gi       Available           manual         11m

# 新PVC成功绑定
$ kubectl get pvc new-pvc
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
new-pvc   Bound    my-pv    10Gi       RWO            manual         3m
```

**解决方案2：删除PV并重新创建**

```bash
# 备份数据（如果需要）
$ cp -r /data/pv-path /backup/

# 删除PV
$ kubectl delete pv my-pv

# 重新创建PV
$ kubectl apply -f my-pv.yaml

# PVC自动绑定
```

---

#### 场景2：PV一直处于Pending状态

**可能原因：**

1. **没有匹配的PV**
   ```bash
   $ kubectl get pv
   No resources found  # 没有PV
   
   $ kubectl get pvc
   NAME      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
   my-pvc    Pending                                      manual         5m
   ```
   
   **解决：** 创建匹配的PV

2. **容量不足**
   ```bash
   $ kubectl get pv
   NAME    CAPACITY   STATUS      CLAIM   STORAGECLASS   AGE
   small   5Gi        Available           manual         1m
   
   $ kubectl get pvc
   NAME      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
   large     Pending                                      manual         1m
   
   $ kubectl describe pvc large
   Spec:
     Resources:
       Requests:
         storage:  10Gi  # 请求10Gi，但只有5Gi的PV
   ```
   
   **解决：** 创建更大容量的PV

3. **访问模式不匹配**
   ```bash
   $ kubectl describe pvc my-pvc
   Spec:
     Access Modes:
       ReadWriteMany  # 请求RWX
   
   $ kubectl describe pv my-pv
   Spec:
     Access Modes:
       ReadWriteOnce  # 只支持RWO
   ```
   
   **解决：** 使用支持RWX的存储后端（如NFS）

4. **StorageClass不匹配**
   ```bash
   $ kubectl describe pvc my-pvc
   Spec:
     Storage Class Name:  fast-ssd
   
   $ kubectl get pv
   NAME    CAPACITY   STORAGECLASS   STATUS
   my-pv   10Gi       slow-hdd       Available
   ```
   
   **解决：** 创建匹配StorageClass的PV

---

### 8.3.7 PV最佳实践总结

**1. 生命周期管理**
```yaml
# ✅ 生产环境推荐配置
apiVersion: v1
kind: PersistentVolume
metadata:
  name: prod-db-pv
  labels:
    environment: production
    app: database
    backup: required
spec:
  persistentVolumeReclaimPolicy: Retain  # 生产数据使用Retain
  # ...
```

**2. 访问模式选择**
```
应用类型              → 推荐访问模式
───────────────────────────────────
单实例数据库          → RWO (块存储)
多副本只读服务        → ROX (NFS)
多副本读写服务        → RWX (NFS/CephFS)
StatefulSet          → RWO (每个Pod独立PV)
```

**3. 容量规划**
```yaml
# ✅ 精确匹配容量
PV:  capacity: storage: 100Gi
PVC: requests: storage: 100Gi

# ❌ 避免浪费
PV:  capacity: storage: 100Gi
PVC: requests: storage: 10Gi  # 浪费90Gi
```

**4. 标签管理**
```yaml
metadata:
  labels:
    environment: production  # 环境标识
    tier: database           # 应用层级
    performance: high        # 性能等级
    backup-policy: daily     # 备份策略
```

---

### 8.3.8 下一节预告

在本节中，我们深入学习了PersistentVolume的核心知识：

- ✅ PV生命周期的五种状态（Available/Bound/Released/Failed）
- ✅ 访问模式的深入理解（RWO/RWX/ROX及存储后端兼容性）
- ✅ 回收策略的实际影响（Retain/Delete/Recycle）
- ✅ NFS和Local PV的完整静态供应实战
- ✅ PV容量管理和状态恢复

然而，静态供应需要管理员手动创建PV，当集群规模扩大、PVC数量增多时，手动管理变得不可行。Kubernetes通过**PersistentVolumeClaim（PVC）**和**动态供应**解决了这一难题。

**在下一节（8.4 持久卷声明PersistentVolumeClaim）中**，我们将深入学习：
- PVC的绑定机制和选择器
- PVC的状态管理和生命周期
- PVC的扩容操作（Volume Expansion）
- PVC的克隆和快照（CSI Snapshot）
- PVC与Pod的绑定关系

---

**本节完**

*下一节预告：8.4节《持久卷声明PersistentVolumeClaim》- 深入PVC绑定机制、扩容、克隆和快照操作。*
## 8.4 持久卷声明（PersistentVolumeClaim）

在上一节中，我们深入学习了PersistentVolume的生命周期、访问模式和静态供应。PV是集群管理员创建的存储资源，而**PersistentVolumeClaim（PVC）**则是用户对存储的请求声明。PVC将用户与底层存储实现解耦，使开发者无需关心具体使用的是NFS、Ceph还是云盘，只需声明"我需要10GB RWO存储"即可。

本节将深入学习PVC的核心机制，掌握如何高效使用和管理持久化存储。

---

### 8.4.1 PVC绑定机制详解

PVC的核心功能是与PV建立绑定关系。理解绑定机制是掌握PVC的关键。

---

#### 绑定过程详解

```
PVC绑定流程
┌─────────────────────────────────────────────────────────────┐
│  第1步：用户创建PVC                                            │
│  apiVersion: v1                                              │
│  kind: PersistentVolumeClaim                                 │
│  spec:                                                       │
│    accessModes: [ReadWriteOnce]                              │
│    resources:                                                │
│      requests:                                               │
│        storage: 10Gi                                         │
│    storageClassName: fast-ssd                                │
└────────────────────┬────────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────────┐
│  第2步：PV Controller扫描可用PV                                │
│  筛选条件：                                                    │
│  ✓ storageClassName匹配（或都为空）                            │
│  ✓ 访问模式兼容（PV包含PVC请求的所有模式）                       │
│  ✓ 容量满足（PV >= PVC请求）                                   │
│  ✓ 选择器匹配（如果PVC指定了selector）                          │
│  ✓ PV状态为Available                                          │
└────────────────────┬────────────────────────────────────────┘
                     │
      ┌──────────────┴──────────────┐
      │                             │
      ▼                             ▼
┌────────────┐              ┌────────────────┐
│ 找到匹配PV  │              │ 未找到匹配PV     │
└─────┬──────┘              └─────┬──────────┘
      │                           │
      ▼                           ▼
┌────────────┐              ┌────────────────┐
│ 第3步：     │              │ StorageClass   │
│ 建立绑定    │              │ 存在？         │
│            │              └─────┬──────────┘
│ PVC.Status │                    │
│ = Bound    │        ┌───────────┴──────────┐
│            │        │                      │
│ PV.Status  │        ▼                      ▼
│ = Bound    │   ┌─────────┐          ┌──────────┐
└─────┬──────┘   │ 触发动态 │          │ PVC保持  │
      │          │ 供应     │          │ Pending  │
      │          └─────┬───┘          └──────────┘
      │                │
      │                ▼
      │          ┌─────────────┐
      │          │ Provisioner │
      │          │ 创建PV      │
      │          └─────┬───────┘
      │                │
      └────────────────┴────────────────┐
                                        │
                                        ▼
                              ┌──────────────────┐
                              │ 第4步：绑定完成   │
                              │ PVC可被Pod使用   │
                              └──────────────────┘
```

---

#### 绑定规则优先级

**规则1：精确匹配优先**

```yaml
# 场景：有3个PV
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-10gi
spec:
  capacity:
    storage: 10Gi
  accessModes: [ReadWriteOnce]
  storageClassName: standard

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-20gi
spec:
  capacity:
    storage: 20Gi
  accessModes: [ReadWriteOnce]
  storageClassName: standard

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-50gi
spec:
  capacity:
    storage: 50Gi
  accessModes: [ReadWriteOnce]
  storageClassName: standard
```

```yaml
# PVC请求10Gi
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-test
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard
```

```bash
# PVC会绑定到pv-10gi（最小满足容量的PV）
$ kubectl get pvc pvc-test
NAME       STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-test   Bound    pv-10gi   10Gi       RWO            standard       5s
#                   ^^^^^^^
#                   精确匹配，避免浪费
```

---

**规则2：选择器优先**

```yaml
# PV带有标签
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-ssd-prod
  labels:
    type: ssd
    environment: production
spec:
  capacity:
    storage: 100Gi
  accessModes: [ReadWriteOnce]
  storageClassName: premium

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hdd-dev
  labels:
    type: hdd
    environment: development
spec:
  capacity:
    storage: 100Gi
  accessModes: [ReadWriteOnce]
  storageClassName: premium
```

```yaml
# PVC使用选择器精确匹配
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: db-pvc
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 50Gi
  storageClassName: premium
  selector:
    matchLabels:
      type: ssd              # 必须匹配
      environment: production
```

```bash
# PVC只会绑定到pv-ssd-prod
$ kubectl get pvc db-pvc
NAME     STATUS   VOLUME         CAPACITY   ACCESS MODES   STORAGECLASS   AGE
db-pvc   Bound    pv-ssd-prod    100Gi      RWO            premium        3s

# pv-hdd-dev虽然容量足够，但标签不匹配，不会绑定
$ kubectl get pv pv-hdd-dev
NAME         CAPACITY   STATUS      CLAIM   STORAGECLASS   AGE
pv-hdd-dev   100Gi      Available           premium        1m
```

---

**规则3：访问模式匹配**

```yaml
# 场景：PV支持RWX
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 50Gi
  accessModes:
    - ReadWriteMany
    - ReadWriteOnce  # 同时支持RWX和RWO
  nfs:
    server: 192.168.1.100
    path: /data/nfs
```

```yaml
# PVC请求RWO
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-rwo
spec:
  accessModes:
    - ReadWriteOnce  # 只请求RWO
  resources:
    requests:
      storage: 10Gi
```

```bash
# ✅ 可以绑定（PV支持RWO）
$ kubectl get pvc pvc-rwo
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   AGE
pvc-rwo   Bound    nfs-pv   50Gi       RWO,RWX        5s
#                                      ^^^^^^^^
#                                      PV实际支持的模式
```

```yaml
# PVC请求RWX
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-rwx
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
```

```bash
# ❌ 如果只有RWO的PV，无法绑定
# 如果有支持RWX的PV，可以绑定
```

---

#### 选择器高级用法

**matchExpressions表达式**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: advanced-pvc
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 20Gi
  selector:
    matchExpressions:
    # 表达式1：type必须是ssd或nvme
    - key: type
      operator: In
      values:
      - ssd
      - nvme
    
    # 表达式2：environment不能是testing
    - key: environment
      operator: NotIn
      values:
      - testing
    
    # 表达式3：必须存在backup标签
    - key: backup
      operator: Exists
    
    # 表达式4：不能存在deprecated标签
    - key: deprecated
      operator: DoesNotExist
```

**选择器操作符详解：**

| 操作符 | 含义 | 示例 |
|--------|------|------|
| `In` | 值在列表中 | `type In [ssd, nvme]` |
| `NotIn` | 值不在列表中 | `env NotIn [testing]` |
| `Exists` | 键存在（值任意） | `backup Exists` |
| `DoesNotExist` | 键不存在 | `deprecated DoesNotExist` |

---

### 8.4.2 PVC状态管理和生命周期

PVC有三种主要状态，理解状态转换是排查问题的关键。

---

#### PVC的三种状态

```
PVC状态机
┌──────────────┐
│   Pending    │ ← 初始状态：等待绑定到PV
└──────┬───────┘
       │ 找到匹配PV或动态供应完成
       ▼
┌──────────────┐
│    Bound     │ ← 绑定成功，可被Pod使用
└──────┬───────┘
       │ PVC被删除
       ▼
┌──────────────┐
│   Deleted    │ ← PVC已删除，PV根据回收策略处理
└──────────────┘
```

| 状态 | 含义 | 原因 | 解决方案 |
|------|------|------|---------|
| **Pending** | 等待绑定 | - 没有匹配的PV<br>- 动态供应中<br>- 容量/模式不匹配 | - 创建匹配的PV<br>- 检查StorageClass<br>- 调整PVC请求 |
| **Bound** | 已绑定 | 成功绑定到PV | 正常状态 |
| **Lost** | 绑定丢失 | PV被意外删除 | 重新创建PV或PVC |

---

#### PVC生命周期演示

**步骤1：创建PVC（Pending状态）**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-lifecycle
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: manual  # 静态供应
```

```bash
# 创建PVC（此时没有匹配的PV）
$ kubectl apply -f pvc-lifecycle.yaml
persistentvolumeclaim/pvc-lifecycle created

# 查看状态：Pending
$ kubectl get pvc pvc-lifecycle
NAME            STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle   Pending                                      manual         10s

# 查看详细事件
$ kubectl describe pvc pvc-lifecycle
Events:
  Type     Reason              Message
  ----     ------              -------
  Warning  ProvisioningFailed  no persistent volumes available for this claim and no storage class is set
```

**步骤2：创建匹配的PV（Bound状态）**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-for-lifecycle
spec:
  capacity:
    storage: 10Gi  # 大于PVC请求的5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: /data/pv-lifecycle
    type: DirectoryOrCreate
```

```bash
# 创建PV
$ kubectl apply -f pv-for-lifecycle.yaml
persistentvolume/pv-for-lifecycle created

# PVC自动绑定到PV
$ kubectl get pvc pvc-lifecycle
NAME            STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle   Bound    pv-for-lifecycle   10Gi       RWO            manual         2m
#               ^^^^^
#               从Pending变为Bound

# PV也显示绑定信息
$ kubectl get pv pv-for-lifecycle
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS   AGE
pv-for-lifecycle   10Gi       RWO            Retain           Bound    default/pvc-lifecycle   manual         30s
#                                                             ^^^^^    ^^^^^^^^^^^^^^^^^^^
#                                                             Bound    绑定到这个PVC
```

**步骤3：Pod使用PVC**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-pvc
spec:
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: data
      mountPath: /usr/share/nginx/html
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-lifecycle  # 引用PVC
```

```bash
# 创建Pod
$ kubectl apply -f pod-using-pvc.yaml
pod/pod-using-pvc created

# 查看Pod状态
$ kubectl get pod pod-using-pvc
NAME            READY   STATUS    RESTARTS   AGE
pod-using-pvc   1/1     Running   0          15s

# 写入数据
$ kubectl exec pod-using-pvc -- sh -c 'echo "Hello from PVC" > /usr/share/nginx/html/index.html'

# 验证数据
$ kubectl exec pod-using-pvc -- cat /usr/share/nginx/html/index.html
Hello from PVC
```

**步骤4：删除Pod（PVC仍为Bound）**

```bash
# 删除Pod
$ kubectl delete pod pod-using-pvc
pod "pod-using-pvc" deleted

# PVC状态不变
$ kubectl get pvc pvc-lifecycle
NAME            STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-lifecycle   Bound    pv-for-lifecycle   10Gi       RWO            manual         5m
#               ^^^^^
#               仍然是Bound
```

**步骤5：删除PVC（根据回收策略处理）**

```bash
# 删除PVC
$ kubectl delete pvc pvc-lifecycle
persistentvolumeclaim "pvc-lifecycle" deleted

# PV状态变为Released（因为回收策略是Retain）
$ kubectl get pv pv-for-lifecycle
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                   STORAGECLASS   AGE
pv-for-lifecycle   10Gi       RWO            Retain           Released   default/pvc-lifecycle   manual         6m
#                                                             ^^^^^^^^
#                                                             从Bound变为Released
```

---

### 8.4.3 PVC扩容操作（Volume Expansion）

Kubernetes 1.11+支持在线扩容PVC，无需重建Pod。

---

#### 扩容前提条件

1. **StorageClass支持扩容**
   ```yaml
   apiVersion: storage.k8s.io/v1
   kind: StorageClass
   metadata:
     name: expandable-sc
   provisioner: kubernetes.io/aws-ebs
   parameters:
     type: gp3
   allowVolumeExpansion: true  # ← 必须开启
   ```

2. **存储后端支持扩容**
   
   | 存储类型 | 支持扩容 | 说明 |
   |---------|---------|------|
   | AWS EBS | ✅ | 支持在线扩容 |
   | GCE PD | ✅ | 支持在线扩容 |
   | Azure Disk | ✅ | 支持在线扩容 |
   | Ceph RBD | ✅ | 支持在线扩容 |
   | NFS | ❌ | 通常不支持自动扩容 |
   | HostPath | ❌ | 不支持扩容 |

3. **PVC必须是Bound状态**

---

#### 在线扩容完整演示

**步骤1：创建支持扩容的StorageClass**

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-expandable
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iopsPerGB: "50"
allowVolumeExpansion: true  # 关键配置
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

**步骤2：创建PVC（初始10Gi）**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc-expand
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi  # 初始容量10Gi
  storageClassName: fast-expandable
```

```bash
# 创建PVC
$ kubectl apply -f mysql-pvc-expand.yaml
persistentvolumeclaim/mysql-pvc-expand created

# 查看状态
$ kubectl get pvc mysql-pvc-expand
NAME               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
mysql-pvc-expand   Bound    pvc-abc123-def456-ghi789                   10Gi       RWO            fast-expandable    30s
```

**步骤3：部署MySQL Pod**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysql-expand-demo
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    env:
    - name: MYSQL_ROOT_PASSWORD
      value: "mysql123"
    volumeMounts:
    - name: data
      mountPath: /var/lib/mysql
    resources:
      requests:
        memory: "512Mi"
        cpu: "500m"
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: mysql-pvc-expand
```

```bash
# 部署MySQL
$ kubectl apply -f mysql-expand-demo.yaml

# 写入一些数据
$ kubectl exec mysql-expand-demo -- mysql -uroot -pmysql123 -e "
CREATE DATABASE testdb;
USE testdb;
CREATE TABLE users (id INT, name VARCHAR(50));
INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob');
"

# 查看当前容量
$ kubectl exec mysql-expand-demo -- df -h /var/lib/mysql
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvdf       9.8G  200M  9.1G   3% /var/lib/mysql
#               ^^^^
#               10Gi (9.8G实际可用)
```

**步骤4：在线扩容到20Gi**

```bash
# 方法1：kubectl edit
$ kubectl edit pvc mysql-pvc-expand
# 修改：
#   resources:
#     requests:
#       storage: 20Gi  # 从10Gi改为20Gi

# 方法2：kubectl patch（推荐）
$ kubectl patch pvc mysql-pvc-expand -p '{"spec":{"resources":{"requests":{"storage":"20Gi"}}}}'
persistentvolumeclaim/mysql-pvc-expand patched

# 查看扩容进度
$ kubectl get pvc mysql-pvc-expand
NAME               STATUS   VOLUME                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
mysql-pvc-expand   Bound    pvc-abc123-def456-ghi789   10Gi       RWO            fast-expandable    5m
#                                                       ^^^^
#                                                       还是10Gi，扩容中...

# 查看详细事件
$ kubectl describe pvc mysql-pvc-expand
Events:
  Type     Reason                      Message
  ----     ------                      -------
  Normal   Resizing                    External resizer is resizing volume pvc-abc123-def456-ghi789
  Normal   FileSystemResizeRequired    Require file system resize of volume on node
```

**步骤5：等待扩容完成**

```bash
# 等待一段时间后（通常30秒-2分钟）
$ kubectl get pvc mysql-pvc-expand
NAME               STATUS   VOLUME                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
mysql-pvc-expand   Bound    pvc-abc123-def456-ghi789   20Gi       RWO            fast-expandable    7m
#                                                       ^^^^
#                                                       扩容完成！

# 在Pod内验证
$ kubectl exec mysql-expand-demo -- df -h /var/lib/mysql
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvdf        20G  200M   19G   2% /var/lib/mysql
#                ^^^
#                扩容成功！

# 验证数据完整性
$ kubectl exec mysql-expand-demo -- mysql -uroot -pmysql123 -e "USE testdb; SELECT * FROM users;"
+------+-------+
| id   | name  |
+------+-------+
|    1 | Alice |
|    2 | Bob   |
+------+-------+
# ✅ 数据完整，无需重启Pod！
```

---

#### 文件系统扩容说明

**自动扩容（大部分情况）：**
- Kubernetes 1.15+支持自动文件系统扩容
- 无需手动执行`resize2fs`或`xfs_growfs`

**手动扩容（旧版本）：**

```bash
# 如果自动扩容失败，需要手动操作
$ kubectl exec mysql-expand-demo -- sh -c '
# 对于ext4文件系统
resize2fs /dev/xvdf

# 对于xfs文件系统
xfs_growfs /var/lib/mysql
'
```

---

#### 扩容限制和注意事项

**限制1：只能扩容，不能缩容**

```bash
# ❌ 尝试缩容（从20Gi改为10Gi）
$ kubectl patch pvc mysql-pvc-expand -p '{"spec":{"resources":{"requests":{"storage":"10Gi"}}}}'
Error: spec.resources.requests.storage: Forbidden: field can not be less than previous value
# 不允许缩小容量
```

**限制2：某些存储需要离线扩容**

```yaml
# 查看StorageClass的扩容能力
$ kubectl get sc fast-expandable -o yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
allowVolumeExpansion: true
# 如果没有此字段或为false，则不支持扩容
```

**限制3：扩容失败处理**

```bash
# 查看PVC状态
$ kubectl describe pvc mysql-pvc-expand
Events:
  Type     Reason                Message
  ----     ------                -------
  Warning  VolumeResizeFailed    resize volume failed: rpc error: code = Internal desc = Could not resize volume

# 可能原因：
# 1. 底层存储配额不足（云盘配额用完）
# 2. PV已达到最大容量限制
# 3. 存储后端不支持在线扩容

# 解决方案：
# - 检查云服务商配额
# - 确认存储后端支持扩容
# - 必要时重建PVC和Pod
```

---

### 8.4.4 PVC克隆和快照（CSI Snapshot）

Kubernetes 1.17+通过CSI（Container Storage Interface）支持卷快照和克隆。

---

#### 卷快照（Volume Snapshot）

**前提条件：**
1. 使用支持快照的CSI驱动（如AWS EBS CSI、GCE PD CSI）
2. 创建VolumeSnapshotClass

**步骤1：创建VolumeSnapshotClass**

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: aws-ebs-snapshot-class
driver: ebs.csi.aws.com  # CSI驱动名称
deletionPolicy: Delete   # 快照删除策略
parameters:
  tagSpecification_1: "Name=MySnapshot"
  tagSpecification_2: "Environment=Production"
```

**步骤2：创建数据源PVC**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-data-original
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: aws-ebs-gp3
```

```bash
# 部署MySQL并写入数据
$ kubectl apply -f mysql-pod.yaml

$ kubectl exec mysql-pod -- mysql -uroot -pmysql123 -e "
CREATE DATABASE proddb;
USE proddb;
CREATE TABLE orders (id INT, total DECIMAL(10,2));
INSERT INTO orders VALUES (1, 99.99), (2, 199.99);
"
```

**步骤3：创建快照**

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: mysql-snapshot-20240114
spec:
  volumeSnapshotClassName: aws-ebs-snapshot-class
  source:
    persistentVolumeClaimName: mysql-data-original  # 源PVC
```

```bash
# 创建快照
$ kubectl apply -f mysql-snapshot.yaml
volumesnapshot.snapshot.storage.k8s.io/mysql-snapshot-20240114 created

# 查看快照状态
$ kubectl get volumesnapshot
NAME                      READYTOUSE   SOURCEPVC             SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS             AGE
mysql-snapshot-20240114   true         mysql-data-original                           50Gi          aws-ebs-snapshot-class    30s
#                         ^^^^
#                         快照已就绪

# 查看详细信息
$ kubectl describe volumesnapshot mysql-snapshot-20240114
Status:
  Bound Volume Snapshot Content Name:  snapcontent-abc123-def456
  Creation Time:                       2024-01-14T10:30:00Z
  Ready To Use:                        true
  Restore Size:                        50Gi
```

**步骤4：从快照恢复数据**

```yaml
# 创建新PVC，从快照恢复
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-data-restored
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: aws-ebs-gp3
  dataSource:
    name: mysql-snapshot-20240114  # 从快照恢复
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
```

```bash
# 创建PVC
$ kubectl apply -f mysql-pvc-restored.yaml
persistentvolumeclaim/mysql-data-restored created

# 查看PVC
$ kubectl get pvc mysql-data-restored
NAME                  STATUS   VOLUME                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mysql-data-restored   Bound    pvc-restored-xyz789        50Gi       RWO            aws-ebs-gp3    20s

# 部署新Pod使用恢复的PVC
$ kubectl apply -f mysql-pod-restored.yaml

# 验证数据
$ kubectl exec mysql-pod-restored -- mysql -uroot -pmysql123 -e "USE proddb; SELECT * FROM orders;"
+------+--------+
| id   | total  |
+------+--------+
|    1 |  99.99 |
|    2 | 199.99 |
+------+--------+
# ✅ 数据成功恢复！
```

---

#### PVC克隆（Volume Cloning）

**定义：** 直接从现有PVC创建新PVC，无需先创建快照。

```yaml
# 源PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: source-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: fast-ssd

---
# 克隆PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cloned-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi  # 必须 >= 源PVC容量
  storageClassName: fast-ssd
  dataSource:
    name: source-pvc  # 直接引用源PVC
    kind: PersistentVolumeClaim
```

```bash
# 创建克隆PVC
$ kubectl apply -f cloned-pvc.yaml
persistentvolumeclaim/cloned-pvc created

# 查看克隆状态
$ kubectl get pvc cloned-pvc
NAME         STATUS   VOLUME                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
cloned-pvc   Bound    pvc-cloned-abc123          20Gi       RWO            fast-ssd       15s

# 验证数据完整性
$ kubectl run test-pod --image=busybox --rm -it --restart=Never -- \
  --overrides='{"spec":{"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"cloned-pvc"}}],"containers":[{"name":"busybox","image":"busybox","command":["ls","-la","/data"],"volumeMounts":[{"name":"data","mountPath":"/data"}]}]}}'
# 输出源PVC的所有文件
```

---

#### 快照 vs 克隆对比

| 特性 | 快照（Snapshot） | 克隆（Clone） |
|------|----------------|--------------|
| **创建方式** | PVC → Snapshot → 新PVC | PVC → 新PVC（直接） |
| **中间产物** | 生成VolumeSnapshot对象 | 无 |
| **用途** | 备份、多次恢复 | 一次性复制 |
| **性能** | 两步操作，较慢 | 一步操作，较快 |
| **存储占用** | 快照独立存储 | 新PVC独立存储 |
| **保留时间** | 可长期保留 | 通常临时使用 |
| **典型场景** | 生产备份、灾难恢复 | 开发测试、数据迁移 |

---

### 8.4.5 PVC与Pod的绑定关系

理解PVC与Pod的绑定机制，对于排查存储问题至关重要。

---

#### 绑定关系详解

```
Pod → Volume → PVC → PV → 底层存储

示例：
┌─────────────────────┐
│  Pod: mysql         │
│  containers:        │
│    volumeMounts:    │
│      - name: data   │ ←─┐
│        mountPath:   │   │
│        /var/lib/...│   │
└─────────────────────┘   │
                          │ 引用
┌─────────────────────┐   │
│  volumes:           │   │
│    - name: data     │ ──┘
│      persistentVol..│ ←─┐
│        claimName:   │   │
│        mysql-pvc    │   │
└─────────────────────┘   │
                          │ 引用
┌─────────────────────┐   │
│  PVC: mysql-pvc     │ ──┘
│  status:            │
│    phase: Bound     │
│    volume: pv-001   │ ←─┐
└─────────────────────┘   │
                          │ 绑定
┌─────────────────────┐   │
│  PV: pv-001         │ ──┘
│  spec:              │
│    awsElasticBlock..│ ←─┐
│      volumeID:      │   │
│      vol-abc123     │   │
└─────────────────────┘   │
                          │ 引用
┌─────────────────────┐   │
│  AWS EBS:           │ ──┘
│  vol-abc123         │
│  /dev/xvdf          │
└─────────────────────┘
```

---

#### 多Pod共享PVC

**场景1：RWO模式，同一节点多Pod**

```yaml
# Pod1和Pod2在同一节点，共享RWO PVC
apiVersion: v1
kind: Pod
metadata:
  name: pod1-rwo
spec:
  nodeSelector:
    kubernetes.io/hostname: node1  # 强制同节点
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: shared
      mountPath: /data
  volumes:
  - name: shared
    persistentVolumeClaim:
      claimName: rwo-pvc

---
apiVersion: v1
kind: Pod
metadata:
  name: pod2-rwo
spec:
  nodeSelector:
    kubernetes.io/hostname: node1  # 同一节点
  containers:
  - name: app
    image: nginx:1.21
    volumeMounts:
    - name: shared
      mountPath: /data
  volumes:
  - name: shared
    persistentVolumeClaim:
      claimName: rwo-pvc  # 同一个PVC
```

```bash
# 两个Pod都成功运行
$ kubectl get pod -o wide
NAME       READY   STATUS    NODE
pod1-rwo   1/1     Running   node1
pod2-rwo   1/1     Running   node1

# Pod1写入数据
$ kubectl exec pod1-rwo -- sh -c 'echo "from pod1" > /data/test.txt'

# Pod2读取数据
$ kubectl exec pod2-rwo -- cat /data/test.txt
from pod1
# ✅ RWO模式下，同节点多Pod可共享
```

---

**场景2：RWX模式，跨节点多Pod**

```yaml
# Deployment 3副本，共享RWX PVC
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shared-storage-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: shared-app
  template:
    metadata:
      labels:
        app: shared-app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        volumeMounts:
        - name: shared-html
          mountPath: /usr/share/nginx/html
      volumes:
      - name: shared-html
        persistentVolumeClaim:
          claimName: nfs-rwx-pvc  # RWX模式的NFS PVC
```

```bash
# 3个Pod分布在不同节点
$ kubectl get pod -o wide
NAME                                 READY   STATUS    NODE
shared-storage-app-5d7f8b9c4-abc12   1/1     Running   node1
shared-storage-app-5d7f8b9c4-def34   1/1     Running   node2
shared-storage-app-5d7f8b9c4-ghi56   1/1     Running   node3

# 任意Pod写入数据
$ kubectl exec shared-storage-app-5d7f8b9c4-abc12 -- sh -c 'echo "Shared content" > /usr/share/nginx/html/index.html'

# 其他Pod都能读取
$ kubectl exec shared-storage-app-5d7f8b9c4-def34 -- cat /usr/share/nginx/html/index.html
Shared content

$ kubectl exec shared-storage-app-5d7f8b9c4-ghi56 -- cat /usr/share/nginx/html/index.html
Shared content
# ✅ RWX模式下，跨节点多Pod可共享
```

---

#### PVC保护机制

**1. 使用中保护（Storage Object in Use Protection）**

```bash
# 创建PVC和Pod
$ kubectl apply -f pvc.yaml
$ kubectl apply -f pod.yaml

# 尝试删除正在使用的PVC
$ kubectl delete pvc my-pvc
persistentvolumeclaim "my-pvc" deleted
# 命令立即返回，但PVC实际未删除

# 查看PVC状态
$ kubectl get pvc my-pvc
NAME     STATUS        VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
my-pvc   Terminating   pv-001   10Gi       RWO            standard       5m
#        ^^^^^^^^^^^
#        处于Terminating状态，等待Pod删除

$ kubectl describe pvc my-pvc
Finalizers:  [kubernetes.io/pvc-protection]
# PVC被finalizer保护

# 删除Pod后，PVC才真正删除
$ kubectl delete pod my-pod
pod "my-pod" deleted

$ kubectl get pvc my-pvc
Error from server (NotFound): persistentvolumeclaims "my-pvc" not found
# ✅ PVC已删除
```

---

### 8.4.6 完整实战案例：WordPress高可用存储方案

**场景：** 部署高可用WordPress，使用NFS共享存储，支持扩容和快照备份。

**架构：**
```
┌─────────────────────────────────────────────────┐
│  3副本WordPress Pod（跨节点）                      │
│  ├── Pod1 (node1) ──┐                           │
│  ├── Pod2 (node2) ──┼── 共享NFS RWX PVC        │
│  └── Pod3 (node3) ──┘                           │
└─────────────────────────────────────────────────┘
              │
              ▼
┌─────────────────────────────────────────────────┐
│  PVC: wordpress-pvc (RWX, 20Gi → 扩容到50Gi)    │
└─────────────────────────────────────────────────┘
              │
              ▼
┌─────────────────────────────────────────────────┐
│  PV: NFS (192.168.1.100:/data/wordpress)        │
└─────────────────────────────────────────────────┘
```

**步骤1：创建NFS PV**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: wordpress-nfs-pv
spec:
  capacity:
    storage: 50Gi  # 提前准备足够容量
  accessModes:
    - ReadWriteMany  # 支持多节点读写
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs-storage
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    server: 192.168.1.100
    path: /data/wordpress
```

**步骤2：创建PVC（初始20Gi）**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wordpress-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 20Gi  # 初始容量
  storageClassName: nfs-storage
```

**步骤3：部署MySQL（使用独立RWO存储）**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: wordpress-mysql
spec:
  selector:
    app: wordpress-mysql
  ports:
  - port: 3306
  clusterIP: None  # Headless Service

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
    - ReadWriteOnce  # MySQL使用RWO
  resources:
    requests:
      storage: 20Gi
  storageClassName: local-storage  # 高性能本地存储

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wordpress-mysql
  template:
    metadata:
      labels:
        app: wordpress-mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "wordpress123"
        - name: MYSQL_DATABASE
          value: "wordpress"
        - name: MYSQL_USER
          value: "wpuser"
        - name: MYSQL_PASSWORD
          value: "wppass123"
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-data
        persistentVolumeClaim:
          claimName: mysql-pvc
```

**步骤4：部署WordPress（3副本共享NFS）**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: wordpress
spec:
  selector:
    app: wordpress
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
spec:
  replicas: 3  # 3副本高可用
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      affinity:
        podAntiAffinity:  # 分散到不同节点
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: wordpress
            topologyKey: kubernetes.io/hostname
      
      containers:
      - name: wordpress
        image: wordpress:6.0-apache
        env:
        - name: WORDPRESS_DB_HOST
          value: "wordpress-mysql:3306"
        - name: WORDPRESS_DB_USER
          value: "wpuser"
        - name: WORDPRESS_DB_PASSWORD
          value: "wppass123"
        - name: WORDPRESS_DB_NAME
          value: "wordpress"
        ports:
        - containerPort: 80
        volumeMounts:
        - name: wordpress-data
          mountPath: /var/www/html
      
      volumes:
      - name: wordpress-data
        persistentVolumeClaim:
          claimName: wordpress-pvc  # 共享NFS存储
```

**步骤5：部署和验证**

```bash
# 创建所有资源
$ kubectl apply -f wordpress-nfs-pv.yaml
$ kubectl apply -f wordpress-pvc.yaml
$ kubectl apply -f wordpress-mysql.yaml
$ kubectl apply -f wordpress.yaml

# 查看Pod分布
$ kubectl get pod -o wide
NAME                               READY   STATUS    NODE
wordpress-mysql-7d8f9b6c5-abc12    1/1     Running   node1
wordpress-5d7f8b9c4-def34          1/1     Running   node1
wordpress-5d7f8b9c4-ghi56          1/1     Running   node2
wordpress-5d7f8b9c4-jkl78          1/1     Running   node3
# ✅ WordPress 3副本分散在不同节点

# 查看PVC
$ kubectl get pvc
NAME            STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS    AGE
wordpress-pvc   Bound    wordpress-nfs-pv    50Gi       RWX            nfs-storage     2m
mysql-pvc       Bound    local-pv-node1      20Gi       RWO            local-storage   2m

# 访问WordPress
$ kubectl get svc wordpress
NAME        TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)        AGE
wordpress   LoadBalancer   10.96.100.200   203.0.113.10    80:30080/TCP   3m

# 浏览器访问 http://203.0.113.10 完成WordPress安装
```

**步骤6：模拟扩容（20Gi → 50Gi）**

```bash
# 编辑PVC扩容
$ kubectl patch pvc wordpress-pvc -p '{"spec":{"resources":{"requests":{"storage":"50Gi"}}}}'

# 查看扩容结果
$ kubectl get pvc wordpress-pvc
NAME            STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
wordpress-pvc   Bound    wordpress-nfs-pv    50Gi       RWX            nfs-storage    10m

# 验证WordPress仍正常运行
$ kubectl get pod -l app=wordpress
NAME                        READY   STATUS    RESTARTS   AGE
wordpress-5d7f8b9c4-def34   1/1     Running   0          11m
wordpress-5d7f8b9c4-ghi56   1/1     Running   0          11m
wordpress-5d7f8b9c4-jkl78   1/1     Running   0          11m
# ✅ 扩容完成，无需重启Pod
```

---

### 8.4.7 PVC最佳实践总结

**1. 容量规划**
```yaml
# ✅ 预留充足空间
resources:
  requests:
    storage: 20Gi  # 数据库初始数据5Gi，预留4倍空间

# ✅ 使用支持扩容的StorageClass
storageClassName: expandable-sc
```

**2. 访问模式选择**
```
应用场景              → PVC访问模式    → 存储后端
─────────────────────────────────────────────────
单实例数据库          → RWO           → Local PV/EBS
多副本只读Web        → ROX           → NFS
多副本读写文件服务    → RWX           → NFS/CephFS
StatefulSet         → RWO（每Pod独立） → EBS/Azure Disk
```

**3. 命名规范**
```yaml
# ✅ 描述性命名
metadata:
  name: mysql-prod-data-pvc
  # 格式：<应用>-<环境>-<用途>-pvc
```

**4. 标签管理**
```yaml
metadata:
  labels:
    app: wordpress
    component: storage
    environment: production
    backup-required: "true"
```

---

### 8.4.8 下一节预告

在本节中，我们深入学习了PersistentVolumeClaim的核心知识：

- ✅ PVC绑定机制和选择器（matchLabels/matchExpressions）
- ✅ PVC状态管理和生命周期（Pending → Bound → Deleted）
- ✅ PVC在线扩容操作（10Gi → 20Gi无需重启）
- ✅ PVC快照和克隆（CSI Snapshot/Volume Cloning）
- ✅ PVC与Pod的绑定关系（RWO/RWX共享机制）
- ✅ WordPress高可用完整实战

到目前为止，我们学习的都是静态供应或手动创建的PVC。在生产环境中，当集群规模扩大时，手动创建PV变得不可行。Kubernetes通过**StorageClass**和**动态供应**彻底解决了这一难题。

**在下一节（8.5 存储类StorageClass）中**，我们将深入学习：
- StorageClass的工作原理和Provisioner
- 动态供应的完整流程
- StorageClass参数配置（不同存储后端）
- VolumeBindingMode（延迟绑定 vs 立即绑定）
- 默认StorageClass的设置和使用

---

**本节完**

*下一节预告：8.5节《存储类StorageClass》- 深入动态供应、Provisioner配置和VolumeBindingMode机制。*
