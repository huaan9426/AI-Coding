"""
RAG æŠ€æœ¯æ ˆ - å®æˆ˜ç»ƒä¹ å‚è€ƒç­”æ¡ˆ

åŒ…å«ä¸‰ä¸ªç»ƒä¹ çš„å®Œæ•´å®ç°ï¼š
1. æ‰‹å†™ Embedding æŸ¥æ‰¾è¡¨
2. æ‰‹å†™ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢
3. æ¨¡æ‹Ÿ RAG å®Œæ•´æµç¨‹
"""
========================
import numpy as np
from typing import List, Dict, Tuple
import heapq
import time

# ============================================================================
# ç»ƒä¹  1: æ‰‹å†™ Embedding æŸ¥æ‰¾è¡¨
# ============================================================================

class SimpleEmbedding:
    """ç®€åŒ–çš„ Embedding æŸ¥æ‰¾è¡¨å®ç°ï¼ˆå®Œæ•´ç­”æ¡ˆï¼‰"""

    def __init__(self, vocab_size: int = 5000, embedding_dim: int = 128):
        """åˆå§‹åŒ– Embedding å±‚"""
        # ç­”æ¡ˆï¼šåˆå§‹åŒ–éšæœºæƒé‡çŸ©é˜µ
        np.random.seed(42)
        self.embedding_matrix = np.random.randn(vocab_size, embedding_dim) * 0.01

        # æ¨¡æ‹Ÿ BPE è¯è¡¨
        self.token_to_id = {
            "<PAD>": 0,
            "<UNK>": 1,
            "hello": 2,
            "world": 3,
            "##ing": 4,
            "embed": 5,
            "##ding": 6,
            "test": 7,
            "rag": 8,
        }
        self.id_to_token = {v: k for k, v in self.token_to_id.items()}

    def tokenize(self, text: str) -> List[str]:
        """ç®€åŒ–çš„ BPE åˆ†è¯ï¼ˆç­”æ¡ˆï¼‰"""
        text = text.lower().strip()
        words = text.split()
        tokens = []

        for word in words:
            # æ£€æŸ¥æ˜¯å¦åœ¨è¯è¡¨ä¸­
            if word in self.token_to_id:
                tokens.append(word)
            else:
                # ç®€åŒ–ç‰ˆå­è¯åˆ†è§£ï¼ˆå®é™… BPE æ›´å¤æ‚ï¼‰
                # å°è¯•åˆ†è§£ä¸ºå·²çŸ¥å­è¯
                found = False
                for prefix_len in range(len(word), 0, -1):
                    prefix = word[:prefix_len]
                    if prefix in self.token_to_id:
                        tokens.append(prefix)
                        # å¤„ç†å‰©ä½™éƒ¨åˆ†
                        suffix = word[prefix_len:]
                        if suffix:
                            suffix_token = f"##{suffix}"
                            if suffix_token in self.token_to_id:
                                tokens.append(suffix_token)
                            else:
                                tokens.append("<UNK>")
                        found = True
                        break

                if not found:
                    tokens.append("<UNK>")

        return tokens

    def encode(self, tokens: List[str]) -> List[int]:
        """å°† token è½¬æ¢ä¸º IDï¼ˆç­”æ¡ˆï¼‰"""
        return [self.token_to_id.get(token, 1) for token in tokens]

    def lookup(self, token_ids: List[int]) -> np.ndarray:
        """æŸ¥æ‰¾å‘é‡ï¼ˆç­”æ¡ˆï¼‰"""
        return self.embedding_matrix[token_ids]

    def forward(self, text: str) -> np.ndarray:
        """å®Œæ•´çš„å‰å‘ä¼ æ’­ï¼ˆç­”æ¡ˆï¼‰"""
        tokens = self.tokenize(text)
        ids = self.encode(tokens)
        vectors = self.lookup(ids)
        return vectors


# ============================================================================
# ç»ƒä¹  2: æ‰‹å†™ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢
# ============================================================================

class VectorSearch:
    """ç®€åŒ–çš„å‘é‡æœç´¢å¼•æ“ï¼ˆå®Œæ•´ç­”æ¡ˆï¼‰"""

    def __init__(self, vectors: np.ndarray, metadata: List[str] = None):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        self.vectors = vectors
        self.normalized_vectors = None
        self.metadata = metadata or [f"Doc_{i}" for i in range(len(vectors))]
        self.normalize_vectors()

    def normalize_vectors(self):
        """å½’ä¸€åŒ–å‘é‡ï¼ˆç­”æ¡ˆï¼‰"""
        # è®¡ç®— L2 èŒƒæ•°
        norms = np.linalg.norm(self.vectors, axis=1, keepdims=True)

        # é¿å…é™¤ä»¥é›¶
        norms = np.where(norms == 0, 1, norms)

        # å½’ä¸€åŒ–
        self.normalized_vectors = self.vectors / norms

    def cosine_similarity(self, query: np.ndarray, doc: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆç­”æ¡ˆï¼‰"""
        # ç‚¹ç§¯
        dot_product = np.dot(query, doc)

        # èŒƒæ•°
        query_norm = np.linalg.norm(query)
        doc_norm = np.linalg.norm(doc)

        # é¿å…é™¤ä»¥é›¶
        if query_norm == 0 or doc_norm == 0:
            return 0.0

        # ä½™å¼¦ç›¸ä¼¼åº¦
        return dot_product / (query_norm * doc_norm)

    def cosine_similarity_optimized(self, query: np.ndarray) -> np.ndarray:
        """ä¼˜åŒ–çš„æ‰¹é‡ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆç­”æ¡ˆï¼‰"""
        # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
        query_norm = query / np.linalg.norm(query)

        # çŸ©é˜µä¹˜æ³•ï¼ˆå½’ä¸€åŒ–åï¼Œä½™å¼¦ç›¸ä¼¼åº¦ = ç‚¹ç§¯ï¼‰
        scores = np.dot(self.normalized_vectors, query_norm)

        return scores

    def l2_distance(self, query: np.ndarray, doc: np.ndarray) -> float:
        """è®¡ç®— L2 è·ç¦»ï¼ˆç­”æ¡ˆï¼‰"""
        return np.linalg.norm(query - doc)

    def search(
        self,
        query: np.ndarray,
        k: int = 3,
        metric: str = "cosine"
    ) -> List[Tuple[int, float, str]]:
        """Top-K æ£€ç´¢ï¼ˆç­”æ¡ˆï¼‰"""
        if metric == "cosine":
            # ä½¿ç”¨ä¼˜åŒ–çš„ä½™å¼¦ç›¸ä¼¼åº¦
            all_scores = self.cosine_similarity_optimized(query)

            # Top-Kï¼ˆé™åºï¼‰
            top_k_indices = np.argsort(all_scores)[::-1][:k]

            # æ„å»ºç»“æœ
            results = [
                (idx, all_scores[idx], self.metadata[idx])
                for idx in top_k_indices
            ]

        elif metric == "l2":
            # è®¡ç®—æ‰€æœ‰ L2 è·ç¦»
            all_distances = [
                self.l2_distance(query, doc)
                for doc in self.vectors
            ]

            # Top-Kï¼ˆå‡åºï¼Œè·ç¦»è¶Šå°è¶Šå¥½ï¼‰
            top_k_indices = heapq.nsmallest(
                k,
                range(len(all_distances)),
                key=lambda i: all_distances[i]
            )

            # æ„å»ºç»“æœ
            results = [
                (idx, all_distances[idx], self.metadata[idx])
                for idx in top_k_indices
            ]

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åº¦é‡: {metric}")

        return results


# ============================================================================
# ç»ƒä¹  3: æ¨¡æ‹Ÿ RAG å®Œæ•´æµç¨‹
# ============================================================================

class SimpleRAG:
    """ç®€åŒ–çš„ RAG ç³»ç»Ÿå®ç°ï¼ˆå®Œæ•´ç­”æ¡ˆï¼‰"""

    def __init__(self, embedding_dim: int = 128):
        """åˆå§‹åŒ– RAG ç³»ç»Ÿ"""
        self.embedding_dim = embedding_dim
        self.documents = []
        self.doc_embeddings = None
        self.chunk_metadata = []

    def chunk_documents(
        self,
        documents: List[str],
        chunk_size: int = 200,
        chunk_overlap: int = 50
    ) -> List[Dict]:
        """æ–‡æ¡£åˆ†å—ï¼ˆç­”æ¡ˆï¼‰"""
        chunks = []

        for doc_id, doc in enumerate(documents):
            start = 0
            chunk_id = 0

            while start < len(doc):
                end = min(start + chunk_size, len(doc))
                chunk_text = doc[start:end].strip()

                # è¿‡æ»¤ç©ºå—
                if chunk_text:
                    chunks.append({
                        "text": chunk_text,
                        "doc_id": doc_id,
                        "chunk_id": chunk_id,
                        "start": start,
                        "end": end
                    })
                    chunk_id += 1

                # æ»‘åŠ¨çª—å£
                start += (chunk_size - chunk_overlap)

        return chunks

    def embed_text(self, text: str) -> np.ndarray:
        """æ¨¡æ‹Ÿæ–‡æœ¬å‘é‡åŒ–ï¼ˆç­”æ¡ˆï¼‰"""
        # ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œä½œä¸ºç§å­ï¼Œç¡®ä¿ç›¸åŒæ–‡æœ¬å¾—åˆ°ç›¸åŒå‘é‡
        seed = hash(text) % (2**32)
        np.random.seed(seed)

        # ç”Ÿæˆéšæœºå‘é‡
        vec = np.random.randn(self.embedding_dim)

        # L2 å½’ä¸€åŒ–
        norm = np.linalg.norm(vec)
        if norm == 0:
            return vec
        return vec / norm

    def index_documents(self, documents: List[str]):
        """ç´¢å¼•æ–‡æ¡£ï¼ˆç­”æ¡ˆï¼‰"""
        print(f"ğŸ“„ æ­£åœ¨ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£...")

        # 1. æ–‡æ¡£åˆ†å—
        self.documents = documents
        self.chunk_metadata = self.chunk_documents(documents)

        # 2. å‘é‡åŒ–æ¯ä¸ªå—
        embeddings = []
        for chunk in self.chunk_metadata:
            vec = self.embed_text(chunk["text"])
            embeddings.append(vec)

        self.doc_embeddings = np.array(embeddings)

        print(f"âœ… ç´¢å¼•å®Œæˆ: {len(self.chunk_metadata)} ä¸ªæ–‡æ¡£å—")

    def retrieve(
        self,
        query: str,
        top_k: int = 3
    ) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆç­”æ¡ˆï¼‰"""
        # 1. å‘é‡åŒ–æŸ¥è¯¢
        query_vec = self.embed_text(query)

        # 2. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå½’ä¸€åŒ–å‘é‡çš„ç‚¹ç§¯ï¼‰
        scores = np.dot(self.doc_embeddings, query_vec)

        # 3. Top-K æ’åº
        top_k_indices = np.argsort(scores)[::-1][:top_k]

        # 4. æ„å»ºç»“æœ
        results = [
            {
                "text": self.chunk_metadata[idx]["text"],
                "score": float(scores[idx]),
                "metadata": self.chunk_metadata[idx]
            }
            for idx in top_k_indices
        ]

        return results

    def build_prompt(
        self,
        query: str,
        contexts: List[Dict]
    ) -> str:
        """æ„å»ºå¢å¼º Promptï¼ˆç­”æ¡ˆï¼‰"""
        # æ‹¼æ¥ä¸Šä¸‹æ–‡
        context_text = "\n\n".join([
            f"ã€å‚è€ƒæ–‡æ¡£ {i+1}ã€‘(ç›¸ä¼¼åº¦: {ctx['score']:.4f})\n{ctx['text']}"
            for i, ctx in enumerate(contexts)
        ])

        # Prompt æ¨¡æ¿
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å‚è€ƒæ–‡æ¡£:
{context_text}

ç”¨æˆ·é—®é¢˜: {query}

è¦æ±‚:
1. ä»…åŸºäºå‚è€ƒæ–‡æ¡£å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
3. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ç¼–å·ï¼ˆå¦‚"æ ¹æ®å‚è€ƒæ–‡æ¡£1..."ï¼‰
4. ä¿æŒå›ç­”ç®€æ´ä¸“ä¸š

ä½ çš„å›ç­”:"""

        return prompt

    def generate(self, prompt: str, contexts: List[Dict]) -> str:
        """æ¨¡æ‹Ÿ LLM ç”Ÿæˆï¼ˆç­”æ¡ˆï¼‰"""
        # ç®€åŒ–å®ç°ï¼šæå–æœ€ç›¸å…³æ–‡æ¡£çš„å…³é”®ä¿¡æ¯
        if not contexts:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"

        # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£
        best_context = contexts[0]

        # æ¨¡æ‹Ÿç”Ÿæˆï¼ˆå®é™…åº”è°ƒç”¨ OpenAI APIï¼‰
        answer = f"""æ ¹æ®å‚è€ƒæ–‡æ¡£ï¼Œä»¥ä¸‹æ˜¯ç›¸å…³ä¿¡æ¯ï¼š

{best_context['text'][:200]}...

ï¼ˆè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå›ç­”ï¼Œå®é™…åº”ä½¿ç”¨ LLM API ç”Ÿæˆæ›´è‡ªç„¶çš„ç­”æ¡ˆï¼‰
"""
        return answer.strip()

    def ask(self, query: str, top_k: int = 3, verbose: bool = True) -> Dict:
        """å®Œæ•´ RAG æµç¨‹ï¼ˆç­”æ¡ˆï¼‰"""
        if verbose:
            print(f"\nâ“ é—®é¢˜: {query}")

        # 1. æ£€ç´¢
        if verbose:
            print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
        contexts = self.retrieve(query, top_k)

        # 2. æ„å»º Prompt
        prompt = self.build_prompt(query, contexts)

        # 3. ç”Ÿæˆç­”æ¡ˆ
        if verbose:
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
        answer = self.generate(prompt, contexts)

        # æ˜¾ç¤ºç»“æœ
        if verbose:
            print(f"\nğŸ’¡ ç­”æ¡ˆ:\n{answer}")
            print(f"\nğŸ“š å‚è€ƒæ¥æº:")
            for i, ctx in enumerate(contexts, 1):
                print(f"  {i}. [ç›¸ä¼¼åº¦: {ctx['score']:.4f}] {ctx['text'][:80]}...")

        return {
            "answer": answer,
            "contexts": contexts,
            "prompt": prompt
        }


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

def test_exercise_1():
    """æµ‹è¯•ç»ƒä¹  1: Embedding æŸ¥æ‰¾è¡¨"""
    print("\n" + "="*70)
    print("ç»ƒä¹  1: Embedding æŸ¥æ‰¾è¡¨æµ‹è¯•")
    print("="*70)

    embed = SimpleEmbedding(vocab_size=5000, embedding_dim=128)

    # æµ‹è¯•ç”¨ä¾‹
    text = "hello world"
    print(f"\nè¾“å…¥æ–‡æœ¬: {text}")

    # åˆ†è¯
    tokens = embed.tokenize(text)
    print(f"åˆ†è¯ç»“æœ: {tokens}")

    # ç¼–ç 
    ids = embed.encode(tokens)
    print(f"ID åºåˆ—: {ids}")

    # æŸ¥æ‰¾å‘é‡
    vectors = embed.lookup(ids)
    print(f"å‘é‡å½¢çŠ¶: {vectors.shape}")

    # å®Œæ•´æµç¨‹
    output = embed.forward(text)
    print(f"æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {output.shape}")

    print("\nâœ… ç»ƒä¹  1 æµ‹è¯•é€šè¿‡ï¼")


def test_exercise_2():
    """æµ‹è¯•ç»ƒä¹  2: ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢"""
    print("\n" + "="*70)
    print("ç»ƒä¹  2: ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢æµ‹è¯•")
    print("="*70)

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®åº“
    np.random.seed(42)
    num_docs = 5
    dim = 128

    vectors = np.random.randn(num_docs, dim)
    metadata = [
        "æ–‡æ¡£1: RAG æ¶æ„ä»‹ç»",
        "æ–‡æ¡£2: Embedding åŸç†",
        "æ–‡æ¡£3: å‘é‡æ•°æ®åº“å¯¹æ¯”",
        "æ–‡æ¡£4: LLM è®­ç»ƒæŠ€å·§",
        "æ–‡æ¡£5: æ•°æ®å¢å¼ºæ–¹æ³•"
    ]

    # åˆå§‹åŒ–æœç´¢å¼•æ“
    search_engine = VectorSearch(vectors, metadata)

    # åˆ›å»ºæŸ¥è¯¢å‘é‡ï¼ˆä¸æ–‡æ¡£2 æœ€ç›¸ä¼¼ï¼‰
    query = vectors[1] + np.random.randn(dim) * 0.1

    # æµ‹è¯•ä½™å¼¦ç›¸ä¼¼åº¦
    print("\n=== ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢ ===")
    results_cosine = search_engine.search(query, k=3, metric="cosine")
    for rank, (doc_id, score, meta) in enumerate(results_cosine, 1):
        print(f"{rank}. {meta} (ç›¸ä¼¼åº¦: {score:.4f})")

    # æµ‹è¯• L2 è·ç¦»
    print("\n=== L2 è·ç¦»æœç´¢ ===")
    results_l2 = search_engine.search(query, k=3, metric="l2")
    for rank, (doc_id, dist, meta) in enumerate(results_l2, 1):
        print(f"{rank}. {meta} (è·ç¦»: {dist:.4f})")

    # æ€§èƒ½å¯¹æ¯”
    print("\n=== æ€§èƒ½å¯¹æ¯” ===")
    # æš´åŠ›æ³•
    start = time.time()
    for i in range(100):
        _ = [search_engine.cosine_similarity(query, doc) for doc in vectors]
    t_brute = time.time() - start

    # ä¼˜åŒ–æ³•
    start = time.time()
    for i in range(100):
        _ = search_engine.cosine_similarity_optimized(query)
    t_opt = time.time() - start

    print(f"æš´åŠ›æ³• (100æ¬¡): {t_brute:.4f}s")
    print(f"ä¼˜åŒ–æ³• (100æ¬¡): {t_opt:.4f}s")
    print(f"åŠ é€Ÿæ¯”: {t_brute / t_opt:.2f}x")

    print("\nâœ… ç»ƒä¹  2 æµ‹è¯•é€šè¿‡ï¼")


def test_exercise_3():
    """æµ‹è¯•ç»ƒä¹  3: RAG å®Œæ•´æµç¨‹"""
    print("\n" + "="*70)
    print("ç»ƒä¹  3: RAG å®Œæ•´æµç¨‹æµ‹è¯•")
    print("="*70)

    # å‡†å¤‡æµ‹è¯•æ–‡æ¡£
    documents = [
        """
        RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚
        å®ƒé¦–å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œç„¶åå°†è¿™äº›æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥ç»™å¤§è¯­è¨€æ¨¡å‹ã€‚
        RAG çš„ä¼˜åŠ¿åœ¨äºå¯ä»¥ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†ï¼Œå‡å°‘æ¨¡å‹å¹»è§‰ï¼Œæé«˜ç­”æ¡ˆå‡†ç¡®æ€§ã€‚
        å…¸å‹çš„ RAG æµç¨‹åŒ…æ‹¬ï¼šæ–‡æ¡£åˆ†å—ã€å‘é‡åŒ–ã€ç´¢å¼•ã€æ£€ç´¢ã€ç”Ÿæˆäº”ä¸ªæ­¥éª¤ã€‚
        """,
        """
        Embedding æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡çš„è¿‡ç¨‹ã€‚å¸¸è§çš„æ¨¡å‹åŒ…æ‹¬ Word2Vecã€GloVe å’Œ BERTã€‚
        OpenAI çš„ text-embedding-ada-002 æ˜¯ç›®å‰æœ€æµè¡Œçš„ Embedding æ¨¡å‹ä¹‹ä¸€ã€‚
        å‘é‡ç»´åº¦é€šå¸¸åœ¨ 128 åˆ° 1536 ä¹‹é—´ï¼Œç»´åº¦è¶Šé«˜è¡¨è¾¾èƒ½åŠ›è¶Šå¼ºä½†è®¡ç®—æˆæœ¬ä¹Ÿè¶Šé«˜ã€‚
        Embedding çš„è´¨é‡ç›´æ¥å½±å“æ£€ç´¢æ•ˆæœï¼Œéœ€è¦åœ¨æˆæœ¬å’Œæ€§èƒ½é—´æƒè¡¡ã€‚
        """,
        """
        å‘é‡æ•°æ®åº“ç”¨äºé«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢å‘é‡ã€‚ä¸»æµäº§å“åŒ…æ‹¬ Chromaã€Pineconeã€Weaviate ç­‰ã€‚
        ç´¢å¼•ç®—æ³•ä¸»è¦æœ‰ HNSWï¼ˆå±‚æ¬¡åŒ–å¯å¯¼èˆªå°ä¸–ç•Œå›¾ï¼‰å’Œ IVFï¼ˆå€’æ’æ–‡ä»¶ï¼‰ã€‚
        HNSW åœ¨æŸ¥è¯¢é€Ÿåº¦å’Œå¬å›ç‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œé€‚åˆå¤§è§„æ¨¡æ£€ç´¢åœºæ™¯ã€‚
        å‘é‡æ•°æ®åº“è¿˜éœ€è¦è€ƒè™‘æŒä¹…åŒ–ã€å¹¶å‘ã€åˆ†å¸ƒå¼ç­‰å·¥ç¨‹é—®é¢˜ã€‚
        """,
        """
        LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰çš„æ ¸å¿ƒæ˜¯ Transformer æ¶æ„ã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µã€‚
        é¢„è®­ç»ƒä½¿ç”¨æµ·é‡æ— æ ‡æ³¨æ–‡æœ¬ï¼Œå¾®è°ƒåˆ™é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚
        GPT-4ã€Claude ç­‰æ¨¡å‹éƒ½åŸºäºè¿™ä¸€æ¶æ„ï¼Œå‚æ•°é‡ä»å‡ åäº¿åˆ°ä¸Šä¸‡äº¿ä¸ç­‰ã€‚
        Temperature å‚æ•°æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼ŒTop-p é‡‡æ ·å¯ä»¥å¹³è¡¡å¤šæ ·æ€§å’Œè´¨é‡ã€‚
        """
    ]

    # åˆå§‹åŒ– RAG ç³»ç»Ÿ
    rag = SimpleRAG(embedding_dim=128)

    # ç´¢å¼•æ–‡æ¡£
    rag.index_documents(documents)

    # æµ‹è¯•é—®ç­”
    test_queries = [
        "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ",
        "Embedding æ¨¡å‹æœ‰å“ªäº›ï¼Ÿ",
        "HNSW ç´¢å¼•çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
    ]

    for query in test_queries:
        result = rag.ask(query, top_k=2)
        print("\n" + "-"*70)

    print("\nâœ… ç»ƒä¹  3 æµ‹è¯•é€šè¿‡ï¼")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*70)
    print("RAG æŠ€æœ¯æ ˆ - å®æˆ˜ç»ƒä¹ å‚è€ƒç­”æ¡ˆ")
    print("="*70)

    test_exercise_1()
    test_exercise_2()
    test_exercise_3()

    print("\n" + "="*70)
    print("ğŸ‰ æ‰€æœ‰ç»ƒä¹ æµ‹è¯•å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()
