# 第4章: Service与Ingress - 服务发现与负载均衡

在前几章中，我们深入学习了Kubernetes的核心组件、Pod概念以及各种工作负载控制器。这些知识让我们能够成功部署和管理容器化应用。但在生产环境中，我们还面临一个关键问题：**如何让这些Pod之间以及外部用户能够稳定、高效地访问我们的服务？**

考虑以下实际场景：

1. **Pod IP不稳定**: Pod随时可能因故障、升级、扩缩容而重建，每次重建IP地址都会变化
2. **负载均衡需求**: 一个Deployment通常运行多个副本，需要在它们之间分配流量
3. **服务发现**: 微服务架构下，服务A如何找到服务B的最新地址？
4. **外部访问**: 集群外的用户如何访问集群内的服务？
5. **域名管理**: 如何通过统一的域名入口管理多个服务？

这就是本章要解决的核心问题。Kubernetes提供了两个关键抽象来应对这些挑战：

- **Service**: 为一组Pod提供稳定的网络端点和负载均衡
- **Ingress**: 为HTTP(S)服务提供统一的外部访问入口和智能路由

## 本章学习目标

通过本章学习，你将掌握：

✅ **Service核心概念**: 理解Service如何通过标签选择器关联Pod，提供稳定的ClusterIP
✅ **Service类型详解**: 掌握ClusterIP、NodePort、LoadBalancer、ExternalName四种类型的使用场景
✅ **Endpoint机制**: 深入理解Endpoint/EndpointSlice如何实现Pod发现和流量转发
✅ **DNS服务发现**: 学习CoreDNS如何为Service提供DNS解析，实现服务间通信
✅ **Ingress控制器**: 完整部署Nginx Ingress，实现基于域名的HTTP路由
✅ **TLS/SSL配置**: 掌握证书管理、HTTPS配置、强制跳转
✅ **高级流量管理**: 实现路径重写、速率限制、认证、灰度发布
✅ **生产实战**: 部署完整的多层微服务架构（前端+API+数据库+缓存）

## 本章内容概览

```
第4章: Service与Ingress
├── 4.1 Service基础概念
│   ├── Service定义与作用
│   ├── 标签选择器与Pod关联
│   ├── ClusterIP虚拟IP原理
│   └── kube-proxy负载均衡机制 (iptables/ipvs模式)
│
├── 4.2 Service类型详解
│   ├── ClusterIP (集群内访问)
│   ├── NodePort (节点端口暴露)
│   ├── LoadBalancer (云负载均衡器)
│   ├── ExternalName (外部服务代理)
│   └── Headless Service (无ClusterIP)
│
├── 4.3 Endpoint与EndpointSlice
│   ├── Endpoint资源对象
│   ├── EndpointSlice性能优化
│   ├── 自定义Endpoint (无选择器Service)
│   └── 就绪/未就绪Pod处理
│
├── 4.4 DNS服务发现机制
│   ├── CoreDNS工作原理
│   ├── Service DNS记录格式
│   ├── Headless Service DNS
│   ├── Pod DNS策略
│   └── DNS调试技巧
│
├── 4.5 Ingress控制器
│   ├── Ingress概念与架构
│   ├── Nginx Ingress完整部署
│   ├── 基于域名的路由规则
│   ├── 路径匹配规则 (前缀/精确/正则)
│   └── 默认后端配置
│
├── 4.6 Ingress高级特性
│   ├── TLS/SSL证书配置
│   ├── HTTPS强制跳转
│   ├── 路径重写与URL重定向
│   ├── 速率限制 (Rate Limiting)
│   ├── Basic认证与白名单
│   ├── 自定义响应头
│   └── 灰度发布 (Canary Deployment)
│
├── 4.7 实战项目: 生产级微服务架构
│   ├── 架构设计 (前端+API Gateway+微服务+数据库)
│   ├── 内部Service通信
│   ├── Ingress统一入口
│   ├── TLS证书配置
│   ├── 完整部署脚本
│   └── 健康检查与监控
│
└── 4.8 本章小结
    ├── Service vs Ingress对比
    ├── 最佳实践总结
    ├── 常用命令速查
    ├── 故障排查清单
    └── 第5章预告
```

## 章节引言

在微服务架构中，服务发现和负载均衡是最基础也是最重要的能力。传统架构中，我们通常使用硬编码的IP地址或配置文件来管理服务间通信，这在动态的容器环境中显然不可行。

Kubernetes通过Service和Ingress两个抽象层，优雅地解决了这一问题：

**Service解决的是"找到服务"的问题**：
- 为一组Pod提供稳定的虚拟IP（ClusterIP）
- 自动发现Pod变化，更新Endpoint列表
- 内置负载均衡，在多个Pod副本间分配流量
- 支持多种暴露方式（集群内/节点端口/云负载均衡器）

**Ingress解决的是"统一入口"的问题**：
- 基于域名的HTTP/HTTPS路由
- SSL/TLS证书管理
- 路径重写、认证、限流等高级功能
- 减少LoadBalancer成本（多个Service共享一个入口）

让我们通过一个简单示例直观理解它们的作用：

```
外部用户
   │
   ├─> https://www.example.com/api     ──┐
   ├─> https://www.example.com/blog    ──┤
   └─> https://api.example.com         ──┤
                                          │
                                    [Ingress]  # 统一HTTPS入口
                                          │
                          ┌───────────────┼───────────────┐
                          ▼               ▼               ▼
                    [API Service]  [Blog Service]  [Admin Service]
                          │               │               │
                ┌─────────┼────┐   ┌──────┼──────┐ ┌─────┼─────┐
                ▼         ▼    ▼   ▼      ▼      ▼ ▼     ▼     ▼
              [Pod] [Pod] [Pod]  [Pod] [Pod] [Pod] [Pod] [Pod] [Pod]
```

在这个架构中：
- **Ingress**: 作为统一的HTTPS入口，根据域名和路径将流量路由到不同的Service
- **Service**: 作为稳定的服务端点，在多个Pod副本之间负载均衡
- **Pod**: 实际运行的应用实例

接下来，让我们深入学习这些概念，并通过实战项目掌握它们的使用方法！

---


## 4.1 Service基础概念

### 4.1.1 Service定义与作用

**什么是Service？**

Service是Kubernetes中的一个核心抽象，它定义了一组Pod的逻辑集合以及访问这些Pod的策略。简单来说，Service为一组功能相同的Pod提供了一个统一的、稳定的访问入口。

**为什么需要Service？**

在Kubernetes中，Pod是短暂的、易变的：
- Pod可能因节点故障而重启（IP变化）
- Deployment更新时Pod会重建（IP变化）
- HPA自动扩缩容会创建/删除Pod（IP地址动态变化）

如果直接使用Pod IP进行通信，会面临以下问题：

```
问题场景：
Frontend Pod (10.244.1.5) ──X──> Backend Pod (10.244.2.8)
                                       ▲
                                       │ Pod重启后IP变为10.244.2.15
                                       │ Frontend无法感知，连接失败！
```

**Service的解决方案：**

```
稳定访问：
Frontend Pod ────> Backend Service (ClusterIP: 10.96.100.10)
                         │
                         ├──> Backend Pod-1 (10.244.2.8)   ◄── 自动发现
                         ├──> Backend Pod-2 (10.244.2.15)  ◄── 自动发现
                         └──> Backend Pod-3 (10.244.3.20)  ◄── 自动发现
```

Service提供了以下核心能力：

✅ **稳定的网络端点**: Service有固定的ClusterIP，不会因Pod变化而改变
✅ **服务发现**: 通过标签选择器自动发现符合条件的Pod
✅ **负载均衡**: 在多个Pod副本之间自动分配流量
✅ **DNS支持**: 通过DNS名称访问服务（如`backend-service.default.svc.cluster.local`）

### 4.1.2 Service YAML配置详解

让我们通过一个完整的示例来理解Service的配置：

```yaml
# backend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: backend-service           # Service名称
  namespace: default
  labels:                         # Service自身的标签（用于管理）
    app: backend
    tier: backend
  annotations:
    description: "Backend API服务"
spec:
  # ===== 核心配置 =====
  type: ClusterIP                 # Service类型（ClusterIP/NodePort/LoadBalancer/ExternalName）
  selector:                       # Pod选择器（关键！）
    app: backend                  # 选择标签app=backend的所有Pod
    version: v1

  # ===== 端口映射 =====
  ports:
  - name: http                    # 端口名称（可选，但推荐设置）
    protocol: TCP                 # 协议（TCP/UDP/SCTP）
    port: 80                      # Service暴露的端口（其他Pod访问此端口）
    targetPort: 8080              # Pod容器的端口（流量转发到容器的这个端口）
    # targetPort也可以使用容器的端口名称：
    # targetPort: http-port

  - name: https                   # 支持多端口
    protocol: TCP
    port: 443
    targetPort: 8443

  # ===== 会话保持 =====
  sessionAffinity: ClientIP       # 会话亲和性（None/ClientIP）
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800       # 会话超时时间（3小时）

  # ===== ClusterIP配置 =====
  clusterIP: 10.96.100.10         # 手动指定ClusterIP（可选，通常自动分配）
  # clusterIP: None               # 设置为None创建Headless Service

  # ===== 其他配置 =====
  publishNotReadyAddresses: false # 是否将未就绪的Pod加入Endpoint（默认false）
  ipFamilyPolicy: SingleStack     # IP族策略（SingleStack/PreferDualStack/RequireDualStack）
  ipFamilies:                     # IP族（IPv4/IPv6）
  - IPv4
```

**关键字段说明：**

| 字段 | 说明 | 示例 |
|------|------|------|
| `selector` | 通过标签选择Pod | `app: backend` |
| `ports[].port` | Service端口（访问入口） | `80` |
| `ports[].targetPort` | Pod端口（流量目标） | `8080` |
| `type` | Service类型 | `ClusterIP` |
| `sessionAffinity` | 会话保持策略 | `ClientIP` |
| `clusterIP` | 虚拟IP地址 | `10.96.100.10` |

### 4.1.3 标签选择器与Pod关联

Service通过**标签选择器（Selector）**来关联Pod，这是Kubernetes中最重要的设计模式之一。

**工作原理：**

```
1. Service定义selector: { app: backend, version: v1 }
                │
                ▼
2. Kubernetes Controller扫描所有Pod
                │
                ▼
3. 匹配到3个符合条件的Pod:
   ├─> backend-deployment-abc123 (10.244.1.5)  ✓ app=backend, version=v1
   ├─> backend-deployment-def456 (10.244.2.8)  ✓ app=backend, version=v1
   └─> backend-deployment-ghi789 (10.244.3.10) ✓ app=backend, version=v1
                │
                ▼
4. 创建Endpoint对象，记录这3个Pod的IP和端口
                │
                ▼
5. kube-proxy监听Endpoint变化，更新iptables/ipvs规则
                │
                ▼
6. 流量到达Service IP时，根据规则转发到后端Pod
```

**完整示例：**

```yaml
# 1. 创建Deployment（定义Pod标签）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
      version: v1
  template:
    metadata:
      labels:                 # ← Pod的标签
        app: backend
        version: v1
        tier: backend
    spec:
      containers:
      - name: backend
        image: nginx:1.25
        ports:
        - name: http-port     # 容器端口名称
          containerPort: 8080

---
# 2. 创建Service（通过selector关联Pod）
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  selector:                   # ← 选择app=backend且version=v1的Pod
    app: backend
    version: v1
  ports:
  - port: 80
    targetPort: http-port     # 使用容器端口名称（推荐）
    # targetPort: 8080        # 或直接使用端口号
```

**验证Pod关联：**

```bash
# 1. 查看Service详情
kubectl describe service backend-service

# 输出示例：
# Endpoints: 10.244.1.5:8080,10.244.2.8:8080,10.244.3.10:8080
#            ↑ 自动发现的3个Pod IP

# 2. 查看Endpoint对象
kubectl get endpoints backend-service -o yaml

# 输出示例：
# subsets:
# - addresses:
#   - ip: 10.244.1.5
#     targetRef:
#       kind: Pod
#       name: backend-deployment-abc123
#   - ip: 10.244.2.8
#     targetRef:
#       kind: Pod
#       name: backend-deployment-def456
#   ports:
#   - port: 8080
#     protocol: TCP

# 3. 查看Pod标签
kubectl get pods -l app=backend,version=v1 --show-labels
```

**动态更新示例：**

```bash
# 场景：扩容Deployment
kubectl scale deployment backend-deployment --replicas=5

# Service自动感知Pod变化：
# ┌─────────────────────────────────────────┐
# │ Deployment扩容 → 新增2个Pod           │
# │         ↓                               │
# │ Endpoint Controller监听Pod变化        │
# │         ↓                               │
# │ 更新Endpoint对象（新增2个IP）          │
# │         ↓                               │
# │ kube-proxy更新iptables规则            │
# │         ↓                               │
# │ 流量自动分配到5个Pod                   │
# └─────────────────────────────────────────┘

# 验证Endpoint更新
kubectl get endpoints backend-service
# Endpoints: 10.244.1.5:8080,10.244.2.8:8080,10.244.3.10:8080,10.244.4.12:8080,10.244.5.15:8080
#            ↑ 5个Pod IP（自动更新）
```

### 4.1.4 ClusterIP虚拟IP原理

**什么是ClusterIP？**

ClusterIP是Service的默认类型，它为Service分配一个集群内部的虚拟IP地址。这个IP地址：
- 仅在集群内可访问（Pod之间可以互相访问）
- 稳定不变（Service删除前不会改变）
- 由Kubernetes自动分配（从`service-cluster-ip-range`中分配）

**ClusterIP工作原理：**

```
客户端Pod
   │ 1. 发起请求：curl http://10.96.100.10:80
   ▼
┌────────────────────────────────────────────┐
│ Node网络命名空间                           │
│                                            │
│  2. 数据包经过iptables/ipvs规则链          │
│                                            │
│  iptables规则示例：                        │
│  -A KUBE-SERVICES                          │
│    -d 10.96.100.10/32 -p tcp --dport 80   │
│    -j KUBE-SVC-BACKEND                    │
│                                            │
│  3. 负载均衡（随机选择一个Pod）            │
│  -A KUBE-SVC-BACKEND                       │
│    -m statistic --mode random              │
│      --probability 0.33333                 │
│    -j KUBE-SEP-POD1  (跳转到Pod 1)        │
│    -m statistic --mode random              │
│      --probability 0.50000                 │
│    -j KUBE-SEP-POD2  (跳转到Pod 2)        │
│    -j KUBE-SEP-POD3  (跳转到Pod 3)        │
│                                            │
│  4. DNAT转换（目标IP改写）                 │
│  -A KUBE-SEP-POD1                          │
│    -j DNAT --to-destination 10.244.1.5:8080│
└────────────────────────────────────────────┘
   │
   ▼ 5. 数据包转发到Pod
[backend-pod-1] 10.244.1.5:8080
```

**ClusterIP地址段配置：**

ClusterIP从API Server配置的地址段中分配：

```bash
# 查看API Server配置
kubectl cluster-info dump | grep service-cluster-ip-range

# 输出示例：
# --service-cluster-ip-range=10.96.0.0/12
#                             ↑ Service IP地址池
#                             可分配范围：10.96.0.1 - 10.111.255.254

# 查看已分配的Service IP
kubectl get services -A -o wide
# NAMESPACE     NAME         TYPE        CLUSTER-IP      EXTERNAL-IP
# default       kubernetes   ClusterIP   10.96.0.1       <none>
# default       backend-svc  ClusterIP   10.96.100.10    <none>
# kube-system   kube-dns     ClusterIP   10.96.0.10      <none>
```

**手动指定ClusterIP：**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  type: ClusterIP
  clusterIP: 10.96.100.50      # 手动指定（必须在service-cluster-ip-range范围内）
  selector:
    app: backend
  ports:
  - port: 80
    targetPort: 8080
```

**注意事项：**

⚠️ **ClusterIP冲突**：手动指定IP可能导致冲突，建议使用自动分配
⚠️ **IP不可达**：ClusterIP是虚拟IP，无法ping通（没有网卡绑定）
⚠️ **无法从集群外访问**：需要使用NodePort或LoadBalancer类型

### 4.1.5 kube-proxy负载均衡机制

**kube-proxy是什么？**

kube-proxy是运行在每个节点上的网络代理，负责实现Service的虚拟IP和负载均衡功能。它监听API Server的Service和Endpoint变化，并在本地节点上配置网络规则。

**三种工作模式对比：**

| 特性 | **iptables模式** | **ipvs模式** | **userspace模式**（已弃用） |
|------|------------------|--------------|---------------------------|
| **性能** | 中等（规则多时性能下降） | 高（内核级负载均衡） | 低（用户态代理） |
| **负载均衡算法** | 随机（random） | 多种（rr/lc/dh/sh等） | 轮询（round-robin） |
| **规则复杂度** | O(n)线性增长 | O(1)哈希查找 | N/A |
| **会话保持** | 支持（ClientIP） | 支持（多种模式） | 支持 |
| **生产推荐** | ✓（中小规模集群） | ✓✓（大规模集群） | ✗（已弃用） |

#### iptables模式（默认）

**工作原理：**

```
1. kube-proxy监听Service/Endpoint变化
         ↓
2. 生成iptables规则链
         ↓
3. 数据包经过规则链进行DNAT转换
         ↓
4. 转发到后端Pod
```

**iptables规则示例：**

```bash
# 查看Service相关的iptables规则
sudo iptables-save | grep backend-service

# 输出示例：
# ===== 1. KUBE-SERVICES链（入口） =====
-A KUBE-SERVICES \
  -d 10.96.100.10/32 -p tcp -m tcp --dport 80 \
  -m comment --comment "default/backend-service cluster IP" \
  -j KUBE-SVC-BACKEND7XHJK2P3

# ===== 2. KUBE-SVC-XXX链（负载均衡） =====
-A KUBE-SVC-BACKEND7XHJK2P3 \
  -m comment --comment "default/backend-service -> 10.244.1.5:8080" \
  -m statistic --mode random --probability 0.33333 \
  -j KUBE-SEP-POD1ABC
#  ↑ 1/3概率跳转到Pod 1

-A KUBE-SVC-BACKEND7XHJK2P3 \
  -m comment --comment "default/backend-service -> 10.244.2.8:8080" \
  -m statistic --mode random --probability 0.50000 \
  -j KUBE-SEP-POD2DEF
#  ↑ 1/2概率跳转到Pod 2（因为已经有1/3被Pod 1拦截）

-A KUBE-SVC-BACKEND7XHJK2P3 \
  -m comment --comment "default/backend-service -> 10.244.3.10:8080" \
  -j KUBE-SEP-POD3GHI
#  ↑ 剩余流量全部到Pod 3

# ===== 3. KUBE-SEP-XXX链（DNAT转换） =====
-A KUBE-SEP-POD1ABC \
  -p tcp -m tcp \
  -j DNAT --to-destination 10.244.1.5:8080
#  ↑ 目标地址改为Pod 1的IP

-A KUBE-SEP-POD2DEF \
  -p tcp -m tcp \
  -j DNAT --to-destination 10.244.2.8:8080

-A KUBE-SEP-POD3GHI \
  -p tcp -m tcp \
  -j DNAT --to-destination 10.244.3.10:8080
```

**配置iptables模式：**

```yaml
# kube-proxy ConfigMap配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy
  namespace: kube-system
data:
  config.conf: |
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    kind: KubeProxyConfiguration
    mode: "iptables"              # 使用iptables模式
    iptables:
      masqueradeAll: false        # 是否对所有流量进行SNAT
      masqueradeBit: 14           # SNAT标记位
      minSyncPeriod: 0s           # 最小同步间隔
      syncPeriod: 30s             # 同步周期（30秒）
    clusterCIDR: "10.244.0.0/16"  # Pod网段
```

#### ipvs模式（生产推荐）

**工作原理：**

```
1. kube-proxy监听Service/Endpoint变化
         ↓
2. 调用ipvsadm创建虚拟服务器
         ↓
3. Linux内核IPVS模块处理流量
         ↓
4. 高性能负载均衡到后端Pod
```

**ipvs优势：**

✅ **性能更高**: 基于内核哈希表，查找复杂度O(1)
✅ **支持多种负载均衡算法**: rr（轮询）/lc（最少连接）/dh（目标哈希）/sh（源哈希）等
✅ **规模更大**: 适合大规模集群（1000+ Service）
✅ **连接重用**: 支持连接池和持久连接

**启用ipvs模式：**

```bash
# 1. 加载必需的内核模块
cat > /etc/modules-load.d/ipvs.conf <<EOF
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
EOF

# 加载模块
modprobe ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack

# 验证模块加载
lsmod | grep -e ip_vs -e nf_conntrack

# 2. 安装ipvsadm工具
apt-get install -y ipvsadm   # Ubuntu/Debian
yum install -y ipvsadm       # CentOS/RHEL

# 3. 配置kube-proxy使用ipvs
kubectl edit configmap kube-proxy -n kube-system

# 修改配置：
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"                    # 改为ipvs模式
ipvs:
  scheduler: "rr"               # 负载均衡算法：rr（轮询）
  # scheduler: "lc"             # 最少连接
  # scheduler: "sh"             # 源地址哈希（会话保持）
  minSyncPeriod: 0s
  syncPeriod: 30s
  strictARP: false              # 严格ARP模式

# 4. 重启kube-proxy
kubectl rollout restart daemonset kube-proxy -n kube-system

# 5. 验证ipvs模式
kubectl logs -n kube-system kube-proxy-xxxxx | grep "Using ipvs Proxier"
# 输出: Using ipvs Proxier
```

**查看ipvs规则：**

```bash
# 查看虚拟服务器列表
sudo ipvsadm -Ln

# 输出示例：
# IP Virtual Server version 1.2.1 (size=4096)
# Prot LocalAddress:Port Scheduler Flags
#   -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
#
# TCP  10.96.100.10:80 rr
#   -> 10.244.1.5:8080              Masq    1      0          0
#   -> 10.244.2.8:8080              Masq    1      0          0
#   -> 10.244.3.10:8080             Masq    1      0          0
#
# ↑ Service ClusterIP     ↑ 负载均衡算法(rr=轮询)
#                          ↑ 3个后端Pod

# 查看连接统计
sudo ipvsadm -Ln --stats

# 查看速率统计
sudo ipvsadm -Ln --rate

# 清空计数器
sudo ipvsadm -Z
```

**ipvs负载均衡算法选择：**

```yaml
# 不同场景推荐的算法
ipvs:
  # 1. 轮询（默认推荐）- 流量均匀分配
  scheduler: "rr"

  # 2. 最少连接 - 适合长连接场景（WebSocket/gRPC）
  # scheduler: "lc"

  # 3. 源地址哈希 - 实现会话保持
  # scheduler: "sh"

  # 4. 目标地址哈希 - 实现缓存亲和性
  # scheduler: "dh"

  # 5. 加权轮询 - 不同权重的后端（需手动配置权重）
  # scheduler: "wrr"
```

### 4.1.6 会话保持（Session Affinity）

**什么是会话保持？**

会话保持（也称会话亲和性）确保来自同一客户端的请求始终路由到同一个后端Pod。这在以下场景中非常重要：
- 有状态应用（会话数据存储在内存中）
- WebSocket连接（需要保持长连接）
- 文件上传（分块上传到同一Pod）

**配置示例：**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: stateful-app-service
spec:
  selector:
    app: stateful-app
  ports:
  - port: 80
    targetPort: 8080

  # ===== 会话保持配置 =====
  sessionAffinity: ClientIP       # 基于客户端IP的会话保持
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800       # 超时时间：3小时（默认10800秒）
```

**工作原理：**

```
客户端 (IP: 192.168.1.100)
   │
   ├─> 请求1 ─────> Service ──> Pod-1 (10.244.1.5) ✓ 首次请求
   │                   │
   ├─> 请求2 ─────> Service ──> Pod-1 (10.244.1.5) ✓ 同一客户端IP,路由到同一Pod
   │                   │
   └─> 请求3 ─────> Service ──> Pod-1 (10.244.1.5) ✓ 会话保持

客户端 (IP: 192.168.1.200)
   │
   └─> 请求1 ─────> Service ──> Pod-2 (10.244.2.8) ✓ 不同客户端IP,路由到不同Pod
```

**iptables实现（查看规则）：**

```bash
# 查看带会话保持的iptables规则
sudo iptables-save | grep -A 10 KUBE-SVC

# 输出示例：
-A KUBE-SVC-XXX \
  -m recent --name KUBE-SEP-POD1 --rcheck --seconds 10800 --reap \
  -j KUBE-SEP-POD1
#  ↑ 如果客户端IP在10800秒内访问过Pod1,则继续转发到Pod1

-A KUBE-SVC-XXX \
  -m recent --name KUBE-SEP-POD2 --rcheck --seconds 10800 --reap \
  -j KUBE-SEP-POD2

# ... 负载均衡规则（新连接使用）
```

**ipvs实现（源地址哈希）：**

```yaml
# 使用ipvs的sh算法实现会话保持
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"
ipvs:
  scheduler: "sh"      # 源地址哈希（Source Hashing）
  # 同一源IP始终哈希到同一后端
```

**验证会话保持：**

```bash
# 1. 创建测试Service
kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: session-test
spec:
  selector:
    app: nginx
  ports:
  - port: 80
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 60
EOF

# 2. 创建3副本Deployment
kubectl create deployment nginx --image=nginx:1.25 --replicas=3
kubectl label pods -l app=nginx app=nginx

# 3. 测试会话保持
POD_NAME=$(kubectl run test-pod --image=busybox:1.36 --restart=Never --rm -it -- /bin/sh)

# 在Pod内执行多次请求
for i in {1..10}; do
  wget -qO- http://session-test | grep "Server name:"
done

# 输出示例：
# Server name: nginx-abc123  ← 10次请求都路由到同一个Pod
# Server name: nginx-abc123
# Server name: nginx-abc123
# ...

# 4. 查看Endpoint分布
kubectl get endpoints session-test
```

### 4.1.7 完整实战示例

让我们通过一个完整的示例，将上述知识点串联起来：

```yaml
# ===== 1. 创建Deployment =====
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: default
  labels:
    app: web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
      version: v1
  template:
    metadata:
      labels:
        app: web-app
        version: v1
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx:1.25-alpine
        ports:
        - name: http
          containerPort: 80
        - name: https
          containerPort: 443

        # 健康检查
        livenessProbe:
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 10
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 5
          periodSeconds: 5

        # 资源限制
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi

---
# ===== 2. 创建ClusterIP Service =====
apiVersion: v1
kind: Service
metadata:
  name: web-app-service
  namespace: default
  labels:
    app: web-app
spec:
  type: ClusterIP
  selector:
    app: web-app
    version: v1
  ports:
  - name: http
    port: 80
    targetPort: http
    protocol: TCP

  # 会话保持（可选）
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600
```

**部署与验证：**

```bash
# 1. 部署应用
kubectl apply -f web-app.yaml

# 2. 查看资源状态
kubectl get deployment web-app
kubectl get pods -l app=web-app -o wide
kubectl get service web-app-service

# 输出示例：
# NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
# web-app-service   ClusterIP   10.96.150.20    <none>        80/TCP    10s

# 3. 查看Endpoint详情
kubectl describe endpoints web-app-service

# 输出示例：
# Name:         web-app-service
# Namespace:    default
# Subsets:
#   Addresses:          10.244.1.10,10.244.2.15,10.244.3.20
#   NotReadyAddresses:  <none>
#   Ports:
#     Name  Port  Protocol
#     ----  ----  --------
#     http  80    TCP

# 4. 从集群内测试访问
kubectl run test-client --image=busybox:1.36 --rm -it --restart=Never -- /bin/sh

# 在Pod内执行：
wget -qO- http://web-app-service
#        ↑ 使用Service名称访问（DNS自动解析）

wget -qO- http://web-app-service.default.svc.cluster.local
#        ↑ 完整的DNS名称

wget -qO- http://10.96.150.20
#        ↑ 直接使用ClusterIP

# 5. 测试负载均衡
for i in {1..20}; do
  wget -qO- http://web-app-service 2>/dev/null | grep "Server name:" || echo "Request $i"
done

# 6. 查看iptables规则
sudo iptables-save | grep web-app-service

# 7. 查看ipvs规则（如果使用ipvs模式）
sudo ipvsadm -Ln | grep -A 5 "10.96.150.20"
```

**故障排查：**

```bash
# 1. Service无法访问 - 检查Endpoint
kubectl get endpoints web-app-service
# 如果Addresses为空，说明没有Pod匹配selector

# 2. 检查Pod标签
kubectl get pods -l app=web-app,version=v1 --show-labels

# 3. 检查Pod就绪状态
kubectl get pods -l app=web-app -o wide
# 确保STATUS为Running且READY为1/1

# 4. 检查Service配置
kubectl describe service web-app-service

# 5. 测试Pod直接访问（绕过Service）
POD_IP=$(kubectl get pod -l app=web-app -o jsonpath='{.items[0].status.podIP}')
kubectl run test --image=busybox:1.36 --rm -it --restart=Never -- wget -qO- http://$POD_IP

# 6. 检查kube-proxy日志
kubectl logs -n kube-system -l k8s-app=kube-proxy --tail=50

# 7. 检查DNS解析
kubectl run test --image=busybox:1.36 --rm -it --restart=Never -- nslookup web-app-service
```

至此，我们已经深入理解了Service的基础概念。接下来的小节将详细讲解不同类型的Service以及它们的使用场景。

---


## 4.2 Service类型详解

Kubernetes提供了四种Service类型，每种类型都针对不同的使用场景进行了优化：

| 类型 | 使用场景 | ClusterIP分配 | 外部访问 | 典型应用 |
|------|----------|--------------|----------|----------|
| **ClusterIP** | 集群内部通信 | ✓ | ✗ | 微服务间调用 |
| **NodePort** | 通过节点端口暴露 | ✓ | ✓（节点IP:端口） | 测试/开发环境 |
| **LoadBalancer** | 云负载均衡器 | ✓ | ✓（LB公网IP） | 生产环境（云平台） |
| **ExternalName** | 外部服务代理 | ✗ | N/A | 外部数据库/API |

### 4.2.1 ClusterIP类型

**定义**：ClusterIP是默认的Service类型，为Service分配一个仅在集群内部可访问的虚拟IP地址。

**适用场景**：
- ✅ 微服务间的内部通信（如Frontend → Backend → Database）
- ✅ 集群内的数据库服务（MySQL/PostgreSQL/Redis）
- ✅ 内部API网关

**配置示例：**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend-api
  namespace: prod
spec:
  type: ClusterIP               # 默认类型（可省略）
  selector:
    app: backend
    tier: api
  ports:
  - name: http
    port: 8080                  # Service端口
    targetPort: 8080            # Pod端口
    protocol: TCP
  - name: grpc
    port: 9090
    targetPort: 9090
    protocol: TCP
```

**访问方式：**

```bash
# 1. 通过Service名称（同一Namespace）
curl http://backend-api:8080/api/v1/users

# 2. 通过完整DNS名称（跨Namespace）
curl http://backend-api.prod.svc.cluster.local:8080/api/v1/users

# 3. 通过ClusterIP（不推荐，IP可能变化）
curl http://10.96.100.20:8080/api/v1/users

# 4. 环境变量方式（Pod自动注入）
echo $BACKEND_API_SERVICE_HOST      # 10.96.100.20
echo $BACKEND_API_SERVICE_PORT      # 8080
curl http://$BACKEND_API_SERVICE_HOST:$BACKEND_API_SERVICE_PORT/api/v1/users
```

**特点**：
- ✅ 性能最优（无额外网络跳转）
- ✅ 安全性高（不暴露到集群外）
- ✅ 支持多端口
- ⚠️ 无法从集群外部访问

### 4.2.2 NodePort类型

**定义**：NodePort在所有节点上开放一个相同的端口（30000-32767），外部流量可以通过`<NodeIP>:<NodePort>`访问Service。

**工作原理：**

```
外部客户端
   │
   ├─> http://192.168.1.10:30080  ─┐
   ├─> http://192.168.1.11:30080  ─┼─> NodePort (30080)
   └─> http://192.168.1.12:30080  ─┘       │
                                            ▼
                                    ClusterIP (10.96.100.30:80)
                                            │
                                  ┌─────────┼─────────┐
                                  ▼         ▼         ▼
                               [Pod-1]  [Pod-2]  [Pod-3]
```

**配置示例：**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-frontend
  namespace: default
spec:
  type: NodePort                # NodePort类型
  selector:
    app: frontend
    version: v1
  ports:
  - name: http
    port: 80                    # Service端口（ClusterIP）
    targetPort: 8080            # Pod端口
    nodePort: 30080             # 节点端口（可选，不指定则自动分配30000-32767）
    protocol: TCP
```

**端口映射关系：**

```
NodePort: 30080  ──┐
                   ├──> ClusterIP Port: 80 ──> Target Port: 8080
外部访问端口       │    Service端口              Pod容器端口
```

**访问方式：**

```bash
# 1. 通过任意节点IP + NodePort访问
curl http://192.168.1.10:30080
curl http://192.168.1.11:30080
curl http://192.168.1.12:30080
# 流量会被路由到任意健康的Pod（即使该节点上没有Pod）

# 2. 集群内部仍可通过ClusterIP访问
curl http://web-frontend:80
curl http://10.96.100.30:80

# 3. 查看NodePort分配
kubectl get service web-frontend

# 输出示例：
# NAME           TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
# web-frontend   NodePort   10.96.100.30    <none>        80:30080/TCP   5m
#                                                          ↑  ↑
#                                                    Service Port
#                                                             NodePort
```

**端口范围配置：**

```bash
# 查看API Server的NodePort范围配置
kubectl cluster-info dump | grep service-node-port-range

# 输出示例：
# --service-node-port-range=30000-32767

# 修改范围（需要重启API Server）
vi /etc/kubernetes/manifests/kube-apiserver.yaml

# 添加参数：
--service-node-port-range=30000-40000  # 扩大端口范围
```

**自动分配 vs 手动指定：**

```yaml
# 方式1：自动分配NodePort（推荐）
apiVersion: v1
kind: Service
metadata:
  name: auto-nodeport
spec:
  type: NodePort
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080
    # nodePort未指定，Kubernetes自动分配

---
# 方式2：手动指定NodePort
apiVersion: v1
kind: Service
metadata:
  name: manual-nodeport
spec:
  type: NodePort
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30080           # 手动指定（必须在范围内且未被占用）
```

**生产环境使用建议：**

```yaml
# 生产环境NodePort配置模板
apiVersion: v1
kind: Service
metadata:
  name: prod-web-service
  namespace: production
  labels:
    app: web
    env: production
  annotations:
    description: "生产环境Web服务NodePort"
spec:
  type: NodePort
  selector:
    app: web
    env: production
  ports:
  - name: https
    port: 443                 # 建议使用443端口（HTTPS）
    targetPort: 8443
    nodePort: 30443
    protocol: TCP

  # 会话保持（重要！）
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800   # 3小时

  # 外部流量策略
  externalTrafficPolicy: Local  # 保留客户端源IP（重要！）
  # externalTrafficPolicy: Cluster  # 默认值，会进行二次SNAT
```

**externalTrafficPolicy详解：**

| 配置 | **Cluster（默认）** | **Local** |
|------|---------------------|-----------|
| **流量分配** | 所有Pod（跨节点） | 仅本节点Pod |
| **客户端IP** | 被SNAT修改 | 保留真实IP |
| **负载均衡** | 均匀分配 | 可能不均匀 |
| **健康检查** | 全局 | 本地 |
| **适用场景** | 均匀负载 | 需要源IP日志/限流 |

```yaml
# 场景1：需要获取客户端真实IP（日志/限流/IP白名单）
apiVersion: v1
kind: Service
metadata:
  name: web-with-ip-logging
spec:
  type: NodePort
  externalTrafficPolicy: Local  # ← 保留客户端IP
  selector:
    app: web
  ports:
  - port: 80
    nodePort: 30080

---
# 场景2：追求负载均衡（默认行为）
apiVersion: v1
kind: Service
metadata:
  name: web-load-balanced
spec:
  type: NodePort
  externalTrafficPolicy: Cluster  # ← 均匀分配流量
  selector:
    app: web
  ports:
  - port: 80
    nodePort: 30081
```

**验证客户端IP保留：**

```bash
# 1. 部署测试应用（显示客户端IP）
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ip-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ip-test
  template:
    metadata:
      labels:
        app: ip-test
    spec:
      containers:
      - name: nginx
        image: nginx:1.25-alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/conf.d/default.conf
          subPath: default.conf
      volumes:
      - name: config
        configMap:
          name: nginx-ip-log-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-ip-log-config
data:
  default.conf: |
    server {
      listen 80;
      location / {
        return 200 "Client IP: \$remote_addr\n";
        add_header Content-Type text/plain;
      }
    }
---
apiVersion: v1
kind: Service
metadata:
  name: ip-test-local
spec:
  type: NodePort
  externalTrafficPolicy: Local    # 保留客户端IP
  selector:
    app: ip-test
  ports:
  - port: 80
    nodePort: 30082
---
apiVersion: v1
kind: Service
metadata:
  name: ip-test-cluster
spec:
  type: NodePort
  externalTrafficPolicy: Cluster  # 不保留客户端IP
  selector:
    app: ip-test
  ports:
  - port: 80
    nodePort: 30083
EOF

# 2. 测试对比
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')

# Local策略 - 显示真实客户端IP
curl http://$NODE_IP:30082
# Client IP: 192.168.1.100  ← 真实客户端IP

# Cluster策略 - 显示被SNAT后的IP
curl http://$NODE_IP:30083
# Client IP: 10.244.2.1  ← 节点IP（被修改）
```

**NodePort故障排查：**

```bash
# 1. 检查NodePort是否被占用
netstat -tuln | grep 30080

# 2. 检查防火墙规则
# Ubuntu/Debian
sudo ufw status
sudo ufw allow 30080/tcp

# CentOS/RHEL
sudo firewall-cmd --list-ports
sudo firewall-cmd --add-port=30080/tcp --permanent
sudo firewall-cmd --reload

# 3. 检查iptables规则
sudo iptables-save | grep 30080

# 4. 测试节点本地访问
curl http://127.0.0.1:30080

# 5. 测试节点IP访问
curl http://$NODE_IP:30080

# 6. 检查Service和Endpoint
kubectl get service web-frontend
kubectl get endpoints web-frontend

# 7. 查看kube-proxy日志
kubectl logs -n kube-system -l k8s-app=kube-proxy | grep -i error
```

**NodePort的优缺点：**

✅ **优点**：
- 简单易用，无需额外组件
- 任意节点都可访问
- 适合测试和开发环境

⚠️ **缺点**：
- 端口范围有限（30000-32767）
- 需要暴露节点IP
- 缺少高级功能（SSL终止/路径路由/限流）
- 单点故障（需要外部负载均衡器）

**NodePort + 外部负载均衡器（推荐生产架构）：**

```
                  ┌────────────────────────┐
                  │   外部负载均衡器         │
                  │   (Nginx/HAProxy)       │
                  │   HTTPS: 443           │
                  └───────────┬────────────┘
                              │
          ┌───────────────────┼───────────────────┐
          │                   │                   │
          ▼                   ▼                   ▼
    Node1:30443         Node2:30443         Node3:30443
          │                   │                   │
          └───────────────────┼───────────────────┘
                              │
                        ClusterIP Service
                              │
                   ┌──────────┼──────────┐
                   ▼          ▼          ▼
                [Pod-1]   [Pod-2]   [Pod-3]
```

配置示例（Nginx负载均衡器）：

```nginx
# /etc/nginx/conf.d/k8s-nodeport.conf
upstream k8s_web_backend {
    # 健康检查
    least_conn;
    
    # 所有Kubernetes节点
    server 192.168.1.10:30443 max_fails=3 fail_timeout=30s;
    server 192.168.1.11:30443 max_fails=3 fail_timeout=30s;
    server 192.168.1.12:30443 max_fails=3 fail_timeout=30s;
}

server {
    listen 443 ssl http2;
    server_name www.example.com;

    # SSL证书
    ssl_certificate /etc/nginx/ssl/example.com.crt;
    ssl_certificate_key /etc/nginx/ssl/example.com.key;

    location / {
        proxy_pass https://k8s_web_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```


### 4.2.3 LoadBalancer类型

**定义**：LoadBalancer类型在云平台（AWS/GCP/Azure/阿里云）上自动创建一个外部负载均衡器，并将流量转发到Kubernetes Service。

**工作原理（云平台集成）：**

```
                     ┌──────────────────────────┐
                     │  云负载均衡器 (ELB/ALB)  │
                     │  公网IP: 1.2.3.4          │
                     └────────────┬─────────────┘
                                  │
                   ┌──────────────┼──────────────┐
                   ▼              ▼              ▼
             Node1:30080    Node2:30080    Node3:30080
                   │              │              │
                   └──────────────┼──────────────┘
                                  │
                          ClusterIP Service
                                  │
                       ┌──────────┼──────────┐
                       ▼          ▼          ▼
                    [Pod-1]   [Pod-2]   [Pod-3]
```

**配置示例：**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-loadbalancer
  namespace: production
  annotations:
    # AWS ELB注解
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"  # 使用NLB（网络负载均衡器）
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "tcp"
    
    # GCP注解
    # cloud.google.com/load-balancer-type: "Internal"  # 内部LB
    
    # Azure注解
    # service.beta.kubernetes.io/azure-load-balancer-internal: "true"
spec:
  type: LoadBalancer
  selector:
    app: web
    tier: frontend
  ports:
  - name: http
    port: 80
    targetPort: 8080
    protocol: TCP
  - name: https
    port: 443
    targetPort: 8443
    protocol: TCP

  # 外部流量策略
  externalTrafficPolicy: Local  # 保留客户端源IP

  # 会话保持
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800

  # 负载均衡器源IP范围（白名单）
  loadBalancerSourceRanges:
  - 10.0.0.0/8      # 仅允许内网访问
  - 192.168.1.0/24  # 办公网络
```

**云平台特定配置：**

#### AWS ELB/ALB/NLB配置

```yaml
apiVersion: v1
kind: Service
metadata:
  name: aws-lb-service
  annotations:
    # ===== 负载均衡器类型 =====
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"  # nlb/alb/clb
    
    # ===== 网络配置 =====
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"  # internet-facing/internal
    service.beta.kubernetes.io/aws-load-balancer-subnets: "subnet-12345,subnet-67890"
    
    # ===== 跨区域负载均衡 =====
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    
    # ===== SSL/TLS配置 =====
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "arn:aws:acm:us-east-1:123456789012:certificate/xxxxx"
    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "443"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"  # http/https/tcp/ssl
    
    # ===== 健康检查 =====
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: "HTTP"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: "/health"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "30"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: "5"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: "2"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: "2"
    
    # ===== 访问日志 =====
    service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: "my-lb-logs"
    
    # ===== 连接配置 =====
    service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: "60"
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "60"
spec:
  type: LoadBalancer
  selector:
    app: web
  ports:
  - port: 443
    targetPort: 8080
  externalTrafficPolicy: Local
```

#### GCP负载均衡器配置

```yaml
apiVersion: v1
kind: Service
metadata:
  name: gcp-lb-service
  annotations:
    # ===== 负载均衡器类型 =====
    cloud.google.com/load-balancer-type: "External"  # External/Internal
    
    # ===== 网络配置 =====
    cloud.google.com/network-tier: "Premium"  # Premium/Standard
    
    # ===== 后端配置 =====
    cloud.google.com/backend-config: '{"ports": {"80":"backend-config"}}'
    
    # ===== NEG（网络端点组）=====
    cloud.google.com/neg: '{"ingress": true}'
spec:
  type: LoadBalancer
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080
```

#### Azure负载均衡器配置

```yaml
apiVersion: v1
kind: Service
metadata:
  name: azure-lb-service
  annotations:
    # ===== 负载均衡器类型 =====
    service.beta.kubernetes.io/azure-load-balancer-internal: "false"  # 外部LB
    
    # ===== 资源组 =====
    service.beta.kubernetes.io/azure-load-balancer-resource-group: "my-resource-group"
    
    # ===== 健康检查 =====
    service.beta.kubernetes.io/azure-load-balancer-health-probe-protocol: "http"
    service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path: "/health"
    service.beta.kubernetes.io/azure-load-balancer-health-probe-interval: "5"
    service.beta.kubernetes.io/azure-load-balancer-health-probe-num-of-probe: "2"
spec:
  type: LoadBalancer
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080
```

**查看LoadBalancer状态：**

```bash
# 1. 查看Service详情
kubectl get service web-loadbalancer

# 输出示例：
# NAME               TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)         AGE
# web-loadbalancer   LoadBalancer   10.96.150.50    1.2.3.4           80:30123/TCP    5m
#                                                    ↑                 ↑  ↑
#                                            云LB公网IP          Service Port
#                                                                    NodePort

# 2. 查看LoadBalancer详细信息
kubectl describe service web-loadbalancer

# 输出示例：
# Type:                     LoadBalancer
# IP:                       10.96.150.50
# LoadBalancer Ingress:     1.2.3.4  ← 云LB的公网IP
# Port:                     http  80/TCP
# TargetPort:               8080/TCP
# NodePort:                 http  30123/TCP
# Endpoints:                10.244.1.5:8080,10.244.2.8:8080,10.244.3.10:8080

# 3. 等待LoadBalancer就绪（可能需要几分钟）
kubectl get service web-loadbalancer -w
# web-loadbalancer   LoadBalancer   10.96.150.50   <pending>       80:30123/TCP    10s
# web-loadbalancer   LoadBalancer   10.96.150.50   1.2.3.4         80:30123/TCP    2m
#                                                   ↑ 云LB创建完成

# 4. 测试访问
curl http://1.2.3.4
curl http://1.2.3.4:443  # HTTPS
```

**本地开发环境模拟LoadBalancer（MetalLB）：**

对于裸机（Bare Metal）集群或本地开发环境，可以使用MetalLB来模拟LoadBalancer：

```bash
# 1. 安装MetalLB
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml

# 2. 配置IP地址池
kubectl apply -f - <<EOF
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: first-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.1.240-192.168.1.250  # 可用的IP地址范围（需在局域网内）
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: example
  namespace: metallb-system
spec:
  ipAddressPools:
  - first-pool
EOF

# 3. 创建LoadBalancer Service
kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: nginx-lb
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
  - port: 80
    targetPort: 80
EOF

# 4. 查看分配的IP
kubectl get service nginx-lb
# NAME       TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)        AGE
# nginx-lb   LoadBalancer   10.96.150.60    192.168.1.240    80:31234/TCP   10s
#                                           ↑ MetalLB分配的IP

# 5. 测试访问
curl http://192.168.1.240
```

**LoadBalancer成本优化：**

```yaml
# ⚠️ 问题：每个LoadBalancer Service都会创建一个云LB（费用高）

# ✅ 解决方案1：共享LoadBalancer（使用Ingress）
# 多个Service共享一个LoadBalancer，通过Ingress进行路由
# 详见4.5节Ingress控制器

# ✅ 解决方案2：使用NodePort + 自建负载均衡器
# 详见4.2.2节NodePort部分

# ✅ 解决方案3：使用ClusterIP + Ingress Controller
# Ingress Controller本身使用LoadBalancer，其他Service使用ClusterIP
```

### 4.2.4 ExternalName类型

**定义**：ExternalName类型用于将Service映射到外部DNS名称，提供了一种在集群内访问外部服务的标准方式。

**使用场景**：
- ✅ 访问外部数据库（RDS/CloudSQL/Azure SQL）
- ✅ 访问外部API服务
- ✅ 服务迁移（从外部逐步迁移到集群内）
- ✅ 多集群通信

**工作原理：**

```
Pod内应用
   │
   ├─> 请求: mysql.default.svc.cluster.local
   │
   ▼
ExternalName Service (mysql)
   │
   ├─> DNS CNAME: prod-db.c9akciq32.us-east-1.rds.amazonaws.com
   │
   ▼
AWS RDS数据库
```

**配置示例：**

```yaml
# ===== 场景1：外部MySQL数据库 =====
apiVersion: v1
kind: Service
metadata:
  name: mysql
  namespace: default
spec:
  type: ExternalName
  externalName: prod-db.c9akciq32.us-east-1.rds.amazonaws.com  # 外部数据库域名
  # 注意：ExternalName类型不使用selector和ports
```

**应用使用示例：**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: app
        image: myapp:1.0
        env:
        - name: DB_HOST
          value: "mysql.default.svc.cluster.local"  # ← 使用Service名称
          # Kubernetes DNS会解析为：prod-db.c9akciq32.us-east-1.rds.amazonaws.com
        - name: DB_PORT
          value: "3306"
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: password
```

**DNS解析验证：**

```bash
# 1. 创建ExternalName Service
kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: external-api
  namespace: default
spec:
  type: ExternalName
  externalName: api.example.com
EOF

# 2. 测试DNS解析
kubectl run test --image=busybox:1.36 --rm -it --restart=Never -- nslookup external-api

# 输出示例：
# Server:         10.96.0.10  (CoreDNS)
# Address:        10.96.0.10#53
#
# external-api.default.svc.cluster.local canonical name = api.example.com
# Name:   api.example.com
# Address: 93.184.216.34

# 3. 测试访问
kubectl run test --image=curlimages/curl:8.5.0 --rm -it --restart=Never -- \
  curl http://external-api/api/v1/data
```

**完整实战：外部服务迁移**

```yaml
# ===== 阶段1：初始状态（外部Redis） =====
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: prod
spec:
  type: ExternalName
  externalName: redis.abc123.0001.use1.cache.amazonaws.com  # AWS ElastiCache
  # 应用通过 redis.prod.svc.cluster.local:6379 访问

---
# ===== 阶段2：迁移到集群内Redis（双写测试） =====
# 部署集群内Redis
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
  namespace: prod
spec:
  serviceName: redis-internal
  replicas: 3
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7.2-alpine
        ports:
        - containerPort: 6379

---
# 创建内部Service（不同名称）
apiVersion: v1
kind: Service
metadata:
  name: redis-internal
  namespace: prod
spec:
  type: ClusterIP
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379

# 此时：
# - 老应用继续访问 redis (ExternalName -> 外部Redis)
# - 新应用访问 redis-internal (ClusterIP -> 集群内Redis)

---
# ===== 阶段3：切换流量 =====
# 删除ExternalName Service
kubectl delete service redis -n prod

# 重命名内部Service
kubectl patch service redis-internal -n prod -p '{"metadata":{"name":"redis"}}'

# 现在 redis.prod.svc.cluster.local 指向集群内Redis
```

**ExternalName + Endpoints（高级用法）：**

有时候外部服务没有DNS名称，只有IP地址。这时可以结合Endpoints使用：

```yaml
# ===== 场景：外部服务只有IP地址 =====
# 方式1：ExternalName + IP（不推荐，ExternalName需要DNS名称）

# 方式2：无选择器Service + 手动Endpoints（推荐）
apiVersion: v1
kind: Service
metadata:
  name: external-db
  namespace: default
spec:
  type: ClusterIP         # 使用ClusterIP类型
  # 注意：不设置selector  ← 关键！
  ports:
  - port: 3306
    targetPort: 3306
    protocol: TCP

---
# 手动创建Endpoints
apiVersion: v1
kind: Endpoints
metadata:
  name: external-db       # 必须与Service同名
  namespace: default
subsets:
- addresses:
  - ip: 10.10.1.50        # 外部数据库IP
  - ip: 10.10.1.51        # 备用IP
  ports:
  - port: 3306
    protocol: TCP
```

**验证：**

```bash
# 1. 查看Service
kubectl get service external-db
# NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
# external-db   ClusterIP   10.96.150.80    <none>        3306/TCP   10s

# 2. 查看Endpoints
kubectl get endpoints external-db
# NAME          ENDPOINTS                       AGE
# external-db   10.10.1.50:3306,10.10.1.51:3306 10s

# 3. 测试连接
kubectl run mysql-client --image=mysql:8.0 --rm -it --restart=Never -- \
  mysql -h external-db.default.svc.cluster.local -u root -p
```

### 4.2.5 Headless Service（无头服务）

**定义**：Headless Service是一种特殊的ClusterIP Service，通过将`clusterIP`设置为`None`来创建。它不分配ClusterIP，DNS查询返回所有Pod IP列表。

**使用场景**：
- ✅ StatefulSet（需要稳定的网络标识）
- ✅ 服务发现（客户端自己实现负载均衡）
- ✅ 数据库集群（直接连接特定Pod）

**配置示例：**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql-headless
  namespace: default
spec:
  clusterIP: None         # ← 关键：设置为None
  selector:
    app: mysql
  ports:
  - port: 3306
    targetPort: 3306
```

**DNS解析对比：**

```bash
# ===== 普通Service（ClusterIP） =====
kubectl run test --image=busybox:1.36 --rm -it --restart=Never -- \
  nslookup mysql-service

# 输出：
# Name:   mysql-service.default.svc.cluster.local
# Address: 10.96.150.90  ← 返回ClusterIP（虚拟IP）

# ===== Headless Service =====
kubectl run test --image=busybox:1.36 --rm -it --restart=Never -- \
  nslookup mysql-headless

# 输出：
# Name:   mysql-headless.default.svc.cluster.local
# Address: 10.244.1.10  ← Pod 1的IP
# Address: 10.244.2.15  ← Pod 2的IP
# Address: 10.244.3.20  ← Pod 3的IP
# ↑ 返回所有Pod IP（无ClusterIP）
```

**StatefulSet + Headless Service（完整示例）：**

```yaml
# ===== 1. Headless Service =====
apiVersion: v1
kind: Service
metadata:
  name: mysql
  namespace: default
spec:
  clusterIP: None         # Headless Service
  selector:
    app: mysql
  ports:
  - port: 3306
    name: mysql

---
# ===== 2. StatefulSet =====
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: default
spec:
  serviceName: mysql      # ← 关联Headless Service
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        ports:
        - containerPort: 3306
          name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "password"
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
```

**Pod DNS名称：**

```bash
# StatefulSet + Headless Service 提供稳定的DNS名称：
# <pod-name>.<service-name>.<namespace>.svc.cluster.local

# 示例：
# mysql-0.mysql.default.svc.cluster.local  ← Pod 0
# mysql-1.mysql.default.svc.cluster.local  ← Pod 1
# mysql-2.mysql.default.svc.cluster.local  ← Pod 2

# 测试解析
kubectl run test --image=busybox:1.36 --rm -it --restart=Never -- \
  nslookup mysql-0.mysql.default.svc.cluster.local

# 输出：
# Name:   mysql-0.mysql.default.svc.cluster.local
# Address: 10.244.1.10  ← Pod 0的固定IP
```

**应用场景：MySQL主从复制**

```yaml
# 应用配置示例
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  database.conf: |
    # 主库（写）
    master_host=mysql-0.mysql.default.svc.cluster.local
    master_port=3306
    
    # 从库（读）
    slave_hosts=mysql-1.mysql.default.svc.cluster.local:3306,mysql-2.mysql.default.svc.cluster.local:3306
```

**Headless Service + 普通Service（同时提供两种访问方式）：**

```yaml
# ===== 1. Headless Service（用于直接访问特定Pod） =====
apiVersion: v1
kind: Service
metadata:
  name: mysql-headless
spec:
  clusterIP: None
  selector:
    app: mysql
  ports:
  - port: 3306

---
# ===== 2. 普通Service（用于负载均衡） =====
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  type: ClusterIP
  selector:
    app: mysql
  ports:
  - port: 3306

# 使用方式：
# - 读写分离：写操作 -> mysql-headless (直接连接主库)
#             读操作 -> mysql (负载均衡到所有从库)
# - 管理操作：mysql-0.mysql-headless (直接操作特定Pod)
```

### 4.2.6 Service类型对比总结

| 特性 | ClusterIP | NodePort | LoadBalancer | ExternalName | Headless |
|------|-----------|----------|--------------|--------------|----------|
| **ClusterIP分配** | ✓ | ✓ | ✓ | ✗ | ✗（None） |
| **外部访问** | ✗ | ✓（节点端口） | ✓（云LB） | N/A | ✗ |
| **DNS返回** | ClusterIP | ClusterIP | ClusterIP | CNAME | Pod IP列表 |
| **负载均衡** | ✓ | ✓ | ✓ | N/A | ✗（客户端自己实现） |
| **适用场景** | 集群内通信 | 测试/开发 | 生产（云） | 外部服务 | StatefulSet/服务发现 |
| **成本** | 免费 | 免费 | 付费（云LB） | 免费 | 免费 |
| **复杂度** | 低 | 中 | 中 | 低 | 中 |

**选择建议：**

```
┌─────────────────────────────────────────────────────────┐
│ 需要从集群外访问？                                      │
│                                                         │
│ YES ──> 是否在云平台？                                  │
│         │                                               │
│         YES ──> LoadBalancer（推荐）                   │
│         │       或 NodePort + Ingress（成本优化）      │
│         │                                               │
│         NO  ──> NodePort + 外部LB                      │
│                 或 Ingress Controller                   │
│                                                         │
│ NO  ──> 集群内通信？                                    │
│         │                                               │
│         YES ──> 是否需要直接访问特定Pod？               │
│                 │                                       │
│                 YES ──> Headless Service               │
│                 │                                       │
│                 NO  ──> ClusterIP（默认推荐）          │
│                                                         │
│         NO  ──> 访问外部服务？                         │
│                 │                                       │
│                 YES ──> ExternalName                   │
│                         或 无选择器Service + Endpoints│
└─────────────────────────────────────────────────────────┘
```

至此，我们已经全面学习了Kubernetes的四种Service类型。接下来将深入探讨Endpoint和EndpointSlice的工作原理。

---


## 4.3 Endpoint与EndpointSlice

### 4.3.1 Endpoint资源对象

**什么是Endpoint？**

Endpoint是Kubernetes中的一个核心资源对象，它记录了Service背后所有Pod的IP地址和端口。当创建Service时，Kubernetes会自动创建同名的Endpoint对象。

**工作流程：**

```
1. 创建Service（定义selector）
         ↓
2. Endpoint Controller扫描匹配的Pod
         ↓
3. 创建/更新Endpoint对象（记录Pod IP列表）
         ↓
4. kube-proxy监听Endpoint变化
         ↓
5. 更新节点上的iptables/ipvs规则
         ↓
6. 流量转发到后端Pod
```

**Endpoint对象结构：**

```bash
# 查看Endpoint对象
kubectl get endpoints backend-service -o yaml

# 输出示例：
```

```yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: backend-service
  namespace: default
subsets:                       # 子集列表
- addresses:                   # 就绪的Pod地址
  - ip: 10.244.1.5
    nodeName: node-1
    targetRef:                 # 引用的Pod对象
      kind: Pod
      name: backend-deployment-abc123
      namespace: default
      uid: 12345678-1234-1234-1234-123456789012
  - ip: 10.244.2.8
    nodeName: node-2
    targetRef:
      kind: Pod
      name: backend-deployment-def456
      namespace: default
      uid: 87654321-4321-4321-4321-210987654321
  
  notReadyAddresses:           # 未就绪的Pod地址（Readiness Probe失败）
  - ip: 10.244.3.10
    nodeName: node-3
    targetRef:
      kind: Pod
      name: backend-deployment-ghi789
      namespace: default
      uid: abcdefgh-abcd-abcd-abcd-abcdefghijkl
  
  ports:                       # 端口信息
  - name: http
    port: 8080
    protocol: TCP
```

**查看Endpoint详情：**

```bash
# 1. 查看Endpoint列表
kubectl get endpoints

# 输出示例：
# NAME              ENDPOINTS                                               AGE
# backend-service   10.244.1.5:8080,10.244.2.8:8080                         5m
# kubernetes        192.168.1.10:6443                                       10d

# 2. 查看详细信息
kubectl describe endpoints backend-service

# 输出示例：
# Name:         backend-service
# Namespace:    default
# Labels:       <none>
# Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2025-01-18T10:30:00Z
# Subsets:
#   Addresses:          10.244.1.5,10.244.2.8
#   NotReadyAddresses:  10.244.3.10
#   Ports:
#     Name  Port  Protocol
#     ----  ----  --------
#     http  8080  TCP
# Events:  <none>

# 3. 监听Endpoint变化
kubectl get endpoints backend-service -w
```

**手动创建Endpoint（无选择器Service）：**

```yaml
# ===== 场景：Service指向集群外的服务 =====

# 1. 创建无选择器的Service
apiVersion: v1
kind: Service
metadata:
  name: external-database
  namespace: default
spec:
  type: ClusterIP
  # 注意：不设置selector  ← 关键！
  ports:
  - port: 3306
    targetPort: 3306
    protocol: TCP

---
# 2. 手动创建Endpoint（名称必须与Service一致）
apiVersion: v1
kind: Endpoints
metadata:
  name: external-database     # 必须与Service同名
  namespace: default
subsets:
- addresses:
  - ip: 10.10.1.50            # 外部数据库IP 1
  - ip: 10.10.1.51            # 外部数据库IP 2（高可用）
  ports:
  - port: 3306
    protocol: TCP
```

**验证：**

```bash
# 1. 查看Service
kubectl get service external-database

# 2. 查看Endpoint
kubectl get endpoints external-database
# ENDPOINTS
# 10.10.1.50:3306,10.10.1.51:3306

# 3. 测试连接
kubectl run mysql-client --image=mysql:8.0 --rm -it --restart=Never -- \
  mysql -h external-database.default.svc.cluster.local -u root -p
```

**Endpoint数量限制：**

在大规模集群中，单个Endpoint对象可能包含数千个Pod IP，这会导致性能问题：
- ⚠️ Endpoint对象过大（>1MB）
- ⚠️ 频繁更新导致API Server压力
- ⚠️ 网络传输开销增大

**解决方案：** EndpointSlice（Kubernetes 1.21+）

### 4.3.2 EndpointSlice性能优化

**什么是EndpointSlice？**

EndpointSlice是Endpoint的改进版本（Kubernetes 1.21+默认启用），通过将大型Endpoint拆分为多个小片段（Slice），解决了规模化问题。

**Endpoint vs EndpointSlice对比：**

| 特性 | **Endpoint** | **EndpointSlice** |
|------|--------------|-------------------|
| **最大Pod数** | 无限制（单对象） | 每个Slice最多100个（默认） |
| **对象数量** | 1个Endpoint | 多个Slice（动态分片） |
| **更新粒度** | 全量更新 | 增量更新（仅更新变化的Slice） |
| **API负载** | 高（大对象频繁更新） | 低（小对象增量更新） |
| **网络开销** | 大 | 小 |
| **生产推荐** | ✗（已弃用） | ✓（Kubernetes 1.21+） |

**EndpointSlice对象结构：**

```bash
# 查看EndpointSlice
kubectl get endpointslices

# 输出示例（一个Service可能有多个Slice）：
# NAME                          ADDRESSTYPE   PORTS   ENDPOINTS   AGE
# backend-service-abc123        IPv4          8080    50          5m
# backend-service-def456        IPv4          8080    50          5m
# backend-service-ghi789        IPv4          8080    45          5m
#                                              ↑       ↑
#                                           端口    此Slice的Pod数

# 查看详细信息
kubectl get endpointslices backend-service-abc123 -o yaml
```

```yaml
apiVersion: discovery.k8s.io/v1
kind: EndpointSlice
metadata:
  name: backend-service-abc123
  namespace: default
  labels:
    kubernetes.io/service-name: backend-service  # 关联的Service
  ownerReferences:                               # 所有者引用
  - apiVersion: v1
    kind: Service
    name: backend-service
    uid: 12345678-1234-1234-1234-123456789012
addressType: IPv4                                # 地址类型（IPv4/IPv6/FQDN）
endpoints:                                       # 端点列表（最多100个）
- addresses:
  - 10.244.1.5
  conditions:
    ready: true                                  # 就绪状态
    serving: true                                # 服务状态
    terminating: false                           # 终止状态
  nodeName: node-1
  targetRef:
    kind: Pod
    name: backend-deployment-abc123
    namespace: default
  zone: us-east-1a                               # 可用区（拓扑感知）
  
- addresses:
  - 10.244.2.8
  conditions:
    ready: false                                 # 未就绪
    serving: false
    terminating: false
  nodeName: node-2
  targetRef:
    kind: Pod
    name: backend-deployment-def456
    namespace: default
  zone: us-east-1b

ports:                                           # 端口信息
- name: http
  port: 8080
  protocol: TCP
```

**分片策略：**

```bash
# EndpointSlice默认配置（kube-controller-manager）
--max-endpoints-per-slice=100   # 每个Slice最多100个端点

# 分片示例：
# 假设Service有250个Pod副本：
# 
# Endpoint方式（旧）：
#   1个Endpoint对象，包含250个IP（对象大小：~50KB）
#   每次Pod变化都需要更新整个50KB对象
#
# EndpointSlice方式（新）：
#   3个EndpointSlice对象：
#   - backend-service-1: 100个端点（~20KB）
#   - backend-service-2: 100个端点（~20KB）
#   - backend-service-3: 50个端点（~10KB）
#   
#   Pod变化时仅更新对应的Slice（~20KB），而非全部（~50KB）
```

**启用EndpointSlice：**

```bash
# 1. 检查是否启用（Kubernetes 1.21+默认启用）
kubectl get endpointslices

# 如果命令成功，说明已启用

# 2. 如果未启用，需要在kube-apiserver添加Feature Gate
vi /etc/kubernetes/manifests/kube-apiserver.yaml

# 添加参数：
--feature-gates=EndpointSliceProxying=true

# 3. 在kube-proxy中启用
kubectl edit configmap kube-proxy -n kube-system

# 添加配置：
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  EndpointSliceProxying: true  # 使用EndpointSlice而非Endpoint

# 4. 重启kube-proxy
kubectl rollout restart daemonset kube-proxy -n kube-system
```

**性能对比测试：**

```bash
# 创建大规模Service（1000个Pod副本）
kubectl create deployment large-app --image=nginx:1.25 --replicas=1000

kubectl expose deployment large-app --port=80 --target-port=80

# 观察Endpoint vs EndpointSlice
# Endpoint（旧）：
kubectl get endpoints large-app -o yaml | wc -l
# 输出：~5000行（一个巨大对象）

# EndpointSlice（新）：
kubectl get endpointslices -l kubernetes.io/service-name=large-app
# 输出：10个Slice（每个100端点）

# 更新测试：删除一个Pod
kubectl delete pod large-app-xxxxx

# Endpoint方式：整个5000行对象需要更新
# EndpointSlice方式：仅1个Slice（~500行）需要更新

# API Server负载对比：
# - Endpoint: 每次更新 ~200KB
# - EndpointSlice: 每次更新 ~20KB（减少90%！）
```

### 4.3.3 拓扑感知路由（Topology-Aware Routing）

**什么是拓扑感知？**

拓扑感知路由（Topology-Aware Routing）优先将流量路由到同一可用区/节点的Pod，减少跨区域流量成本和延迟。

**启用拓扑感知：**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-app
  namespace: default
  annotations:
    service.kubernetes.io/topology-mode: "Auto"  # 启用拓扑感知
spec:
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080
```

**拓扑感知策略：**

```yaml
# 策略1：Auto（推荐）- 自动选择最佳拓扑
annotations:
  service.kubernetes.io/topology-mode: "Auto"

# 策略2：PreferClose - 优先同区域，无则跨区域
# annotations:
#   service.kubernetes.io/topology-mode: "PreferClose"

# 策略3：Disabled - 禁用拓扑感知
# annotations:
#   service.kubernetes.io/topology-mode: "Disabled"
```

**工作原理：**

```
客户端Pod (node-1, zone: us-east-1a)
   │
   ├─> Service (拓扑感知启用)
   │
   ├─ 优先级1：同节点Pod ────────> Pod-A (node-1, zone: us-east-1a) ✓ 优先
   │
   ├─ 优先级2：同可用区Pod ───────> Pod-B (node-2, zone: us-east-1a)
   │
   └─ 优先级3：跨可用区Pod ───────> Pod-C (node-3, zone: us-east-1b) (仅在1、2不可用时)
```

**配置节点拓扑标签：**

```bash
# 1. 查看节点拓扑标签
kubectl get nodes --show-labels | grep topology

# 输出示例：
# node-1   topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1a
# node-2   topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1a
# node-3   topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1b

# 2. 手动添加标签（如果缺失）
kubectl label nodes node-1 topology.kubernetes.io/zone=us-east-1a
kubectl label nodes node-1 topology.kubernetes.io/region=us-east-1
```

**验证拓扑感知：**

```bash
# 1. 部署多副本应用到不同可用区
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      # Pod反亲和性：分散到不同节点
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: web
              topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: nginx:1.25-alpine
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: web-app
  annotations:
    service.kubernetes.io/topology-mode: "Auto"
spec:
  selector:
    app: web
  ports:
  - port: 80
EOF

# 2. 查看Pod分布
kubectl get pods -o wide -l app=web

# 3. 测试拓扑感知（从特定节点的Pod访问Service）
NODE_NAME=node-1
kubectl run test-from-node1 --image=curlimages/curl:8.5.0 --rm -it --restart=Never \
  --overrides='{"spec":{"nodeName":"'$NODE_NAME'"}}' -- \
  sh -c "for i in {1..100}; do curl -s http://web-app | grep 'Server address:'; done | sort | uniq -c"

# 输出示例（同节点Pod被访问次数更多）：
#   70 Server address: 10.244.1.5:80  ← 同节点Pod（node-1）
#   20 Server address: 10.244.1.8:80  ← 同可用区不同节点Pod
#   10 Server address: 10.244.3.10:80 ← 跨可用区Pod
```

至此，我们已经深入理解了Endpoint和EndpointSlice的工作原理。接下来将学习DNS服务发现机制。

---

## 4.4 DNS服务发现机制

### 4.4.1 CoreDNS工作原理

**什么是CoreDNS？**

CoreDNS是Kubernetes默认的DNS服务器（从1.13版本开始替代kube-dns），负责为Service和Pod提供DNS解析服务。

**工作流程：**

```
Pod内应用
   │ 1. DNS查询: nslookup backend-service
   ▼
/etc/resolv.conf
   │ nameserver 10.96.0.10  ← CoreDNS ClusterIP
   ▼
CoreDNS Service (kube-dns)
   │
   ├─> CoreDNS Pod-1 (node-1)
   ├─> CoreDNS Pod-2 (node-2)  ← 高可用（多副本）
   │
   ▼
Kubernetes API
   │ 2. 查询Service/Endpoint信息
   ▼
返回DNS记录
   │ backend-service.default.svc.cluster.local → 10.96.100.10
   ▼
Pod收到解析结果
```

**CoreDNS部署验证：**

```bash
# 1. 查看CoreDNS Deployment
kubectl get deployment -n kube-system coredns

# 输出示例：
# NAME      READY   UP-TO-DATE   AVAILABLE   AGE
# coredns   2/2     2            2           10d

# 2. 查看CoreDNS Service
kubectl get service -n kube-system kube-dns

# 输出示例：
# NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
# kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   10d
#            ↑           ↑ 所有Pod的DNS服务器地址

# 3. 查看CoreDNS ConfigMap配置
kubectl get configmap -n kube-system coredns -o yaml
```

**CoreDNS配置详解：**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    # === 集群域（cluster.local） ===
    .:53 {
        errors                        # 错误日志
        health {                      # 健康检查端点 :8080/health
           lameduck 5s
        }
        ready                         # 就绪检查端点 :8181/ready
        
        # Kubernetes插件（核心）
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure              # 启用Pod DNS记录
           fallthrough in-addr.arpa ip6.arpa
           ttl 30                     # DNS记录TTL（30秒）
        }
        
        # Prometheus监控
        prometheus :9153
        
        # 转发到上游DNS（集群外域名查询）
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        
        # 缓存（30秒TTL）
        cache 30
        
        # 自动重载配置
        reload
        
        # 负载均衡
        loadbalance
    }
```

**DNS记录格式：**

| 资源类型 | DNS记录格式 | 解析结果 | 示例 |
|----------|-------------|----------|------|
| **Service (ClusterIP)** | `<service>.<namespace>.svc.cluster.local` | ClusterIP | `backend.default.svc.cluster.local` → `10.96.100.10` |
| **Service (Headless)** | `<service>.<namespace>.svc.cluster.local` | 所有Pod IP列表 | `mysql.default.svc.cluster.local` → `10.244.1.5, 10.244.2.8` |
| **Pod (StatefulSet)** | `<pod-name>.<service>.<namespace>.svc.cluster.local` | Pod IP | `mysql-0.mysql.default.svc.cluster.local` → `10.244.1.5` |
| **Pod (通用)** | `<pod-ip-dash>.<namespace>.pod.cluster.local` | Pod IP | `10-244-1-5.default.pod.cluster.local` → `10.244.1.5` |

### 4.4.2 Service DNS记录

**完整DNS名称：**

```bash
# 格式：<service-name>.<namespace>.svc.<cluster-domain>

# 示例：
backend-service.default.svc.cluster.local
│               │       │   │
Service名称     Namespace  固定  集群域（可配置）

# 简写规则（同Namespace）：
backend-service                          # 仅名称（同Namespace）
backend-service.default                  # 名称 + Namespace
backend-service.default.svc              # 名称 + Namespace + svc
backend-service.default.svc.cluster.local # 完整FQDN（推荐）
```

**测试DNS解析：**

```bash
# 1. 创建测试Service
kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: test-service
  namespace: default
spec:
  selector:
    app: test
  ports:
  - port: 80
    targetPort: 80
EOF

# 2. 创建测试Pod
kubectl create deployment test --image=nginx:1.25 --replicas=2
kubectl label pods -l app=test app=test

# 3. 进入Pod测试DNS
kubectl run dns-test --image=busybox:1.36 --rm -it --restart=Never -- /bin/sh

# 在Pod内执行：

# 方式1：短名称（同Namespace）
nslookup test-service
# Server:    10.96.0.10
# Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local
# Name:      test-service
# Address 1: 10.96.150.100 test-service.default.svc.cluster.local

# 方式2：完整FQDN
nslookup test-service.default.svc.cluster.local
# Server:    10.96.0.10
# Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local
# Name:      test-service.default.svc.cluster.local
# Address 1: 10.96.150.100

# 方式3：跨Namespace访问
nslookup kube-dns.kube-system.svc.cluster.local
# Name:      kube-dns.kube-system.svc.cluster.local
# Address 1: 10.96.0.10

# 方式4：使用dig工具（更详细）
kubectl run dns-test --image=tutum/dnsutils --rm -it --restart=Never -- /bin/sh
dig test-service.default.svc.cluster.local

# 输出示例：
# ;; ANSWER SECTION:
# test-service.default.svc.cluster.local. 30 IN A 10.96.150.100
```

**SRV记录（服务发现）：**

```bash
# SRV记录格式：_<port-name>._<protocol>.<service>.<namespace>.svc.cluster.local

# 示例Service：
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  selector:
    app: web
  ports:
  - name: http       # ← 端口名称
    port: 80
    protocol: TCP
  - name: https
    port: 443
    protocol: TCP

# 查询HTTP端口的SRV记录
dig SRV _http._tcp.web-service.default.svc.cluster.local

# 输出示例：
# ;; ANSWER SECTION:
# _http._tcp.web-service.default.svc.cluster.local. 30 IN SRV 0 100 80 web-service.default.svc.cluster.local.
#                                                             ↑  ↑  ↑  ↑
#                                                        优先级 权重 端口 目标主机

# 应用场景：客户端可以通过SRV记录自动发现服务端口
```

### 4.4.3 Headless Service DNS

**Headless Service DNS行为：**

```bash
# 创建Headless Service
kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: mysql-headless
spec:
  clusterIP: None       # ← Headless Service
  selector:
    app: mysql
  ports:
  - port: 3306
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql-headless
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "password"
        ports:
        - containerPort: 3306
EOF

# DNS查询Headless Service
kubectl run test --image=busybox:1.36 --rm -it --restart=Never -- \
  nslookup mysql-headless.default.svc.cluster.local

# 输出示例（返回所有Pod IP）：
# Name:      mysql-headless.default.svc.cluster.local
# Address 1: 10.244.1.10 mysql-0.mysql-headless.default.svc.cluster.local
# Address 2: 10.244.2.15 mysql-1.mysql-headless.default.svc.cluster.local
# Address 3: 10.244.3.20 mysql-2.mysql-headless.default.svc.cluster.local
#            ↑ 3个Pod的IP地址（无ClusterIP）

# StatefulSet Pod的DNS记录
nslookup mysql-0.mysql-headless.default.svc.cluster.local

# 输出示例（单个Pod IP）：
# Name:      mysql-0.mysql-headless.default.svc.cluster.local
# Address 1: 10.244.1.10
```

**应用场景：主从数据库**

```python
# Python应用示例（使用Headless Service DNS）
import pymysql
from dns import resolver

# 解析Headless Service获取所有Pod IP
def get_mysql_pods():
    answers = resolver.resolve('mysql-headless.default.svc.cluster.local', 'A')
    return [str(rdata) for rdata in answers]

# 连接主库（mysql-0）
master_conn = pymysql.connect(
    host='mysql-0.mysql-headless.default.svc.cluster.local',
    user='root',
    password='password',
    database='myapp'
)

# 连接从库（负载均衡）
slave_ips = get_mysql_pods()[1:]  # 跳过主库
slave_conn = pymysql.connect(
    host=random.choice(slave_ips),
    user='root',
    password='password',
    database='myapp'
)

# 写操作 -> 主库
cursor = master_conn.cursor()
cursor.execute("INSERT INTO users (name) VALUES ('Alice')")
master_conn.commit()

# 读操作 -> 从库
cursor = slave_conn.cursor()
cursor.execute("SELECT * FROM users")
```

### 4.4.4 Pod DNS策略

**DNS策略类型：**

| 策略 | 说明 | /etc/resolv.conf | 适用场景 |
|------|------|------------------|----------|
| **Default** | 继承节点DNS配置 | 节点的`/etc/resolv.conf` | 访问外部服务 |
| **ClusterFirst**（默认） | 优先使用集群DNS | `nameserver: 10.96.0.10`<br>`search: <namespace>.svc.cluster.local` | 集群内通信（推荐） |
| **ClusterFirstWithHostNet** | 使用hostNetwork时使用集群DNS | 同ClusterFirst | hostNetwork Pod |
| **None** | 完全自定义DNS | 由`dnsConfig`指定 | 特殊需求 |

**配置示例：**

```yaml
# ===== 策略1：ClusterFirst（默认推荐） =====
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  dnsPolicy: ClusterFirst    # 默认值（可省略）
  containers:
  - name: app
    image: nginx:1.25

# Pod内/etc/resolv.conf：
# nameserver 10.96.0.10
# search default.svc.cluster.local svc.cluster.local cluster.local
# options ndots:5

---
# ===== 策略2：Default（使用节点DNS） =====
apiVersion: v1
kind: Pod
metadata:
  name: external-app
spec:
  dnsPolicy: Default         # 继承节点DNS
  containers:
  - name: app
    image: nginx:1.25

# Pod内/etc/resolv.conf：
# nameserver 8.8.8.8  ← 节点配置的DNS服务器
# search localdomain

---
# ===== 策略3：None（自定义DNS） =====
apiVersion: v1
kind: Pod
metadata:
  name: custom-dns-pod
spec:
  dnsPolicy: None            # 完全自定义
  dnsConfig:
    nameservers:             # DNS服务器列表
    - 1.1.1.1
    - 8.8.8.8
    searches:                # 搜索域
    - my-company.com
    - example.com
    options:                 # 选项
    - name: ndots
      value: "2"
    - name: timeout
      value: "5"
  containers:
  - name: app
    image: nginx:1.25

# Pod内/etc/resolv.conf：
# nameserver 1.1.1.1
# nameserver 8.8.8.8
# search my-company.com example.com
# options ndots:2 timeout:5

---
# ===== 策略4：ClusterFirst + 自定义DNS（组合） =====
apiVersion: v1
kind: Pod
metadata:
  name: hybrid-dns-pod
spec:
  dnsPolicy: ClusterFirst    # 保留集群DNS
  dnsConfig:
    nameservers:             # 追加额外DNS服务器
    - 1.1.1.1
    searches:                # 追加搜索域
    - my-company.com
  containers:
  - name: app
    image: nginx:1.25

# Pod内/etc/resolv.conf：
# nameserver 10.96.0.10      ← 集群DNS（优先）
# nameserver 1.1.1.1         ← 自定义DNS（备用）
# search default.svc.cluster.local svc.cluster.local cluster.local my-company.com
# options ndots:5
```

**ndots参数详解：**

```bash
# ndots: 如果域名包含的点数少于此值，会依次尝试所有search域

# 示例：ndots=5（默认值）
# 查询域名：api
# 
# 尝试顺序：
# 1. api.default.svc.cluster.local
# 2. api.svc.cluster.local
# 3. api.cluster.local
# 4. api（原始域名）

# 查询域名：www.google.com（包含2个点，少于5）
# 
# 尝试顺序：
# 1. www.google.com.default.svc.cluster.local
# 2. www.google.com.svc.cluster.local
# 3. www.google.com.cluster.local
# 4. www.google.com（原始域名） ✓ 成功

# ⚠️ 问题：外部域名查询会尝试多次（延迟高）
# 
# 解决方案：使用完整域名（以.结尾）
curl https://www.google.com.  ← 末尾的点表示完整域名，跳过search

# 或者调整ndots值
apiVersion: v1
kind: Pod
spec:
  dnsPolicy: ClusterFirst
  dnsConfig:
    options:
    - name: ndots
      value: "2"  # 减小ndots，减少不必要的查询
```

### 4.4.5 DNS调试技巧

**常用调试工具：**

```bash
# 1. 创建DNS调试Pod（包含所有工具）
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: dns-debug
spec:
  containers:
  - name: dnsutils
    image: tutum/dnsutils
    command:
    - sleep
    - "3600"
EOF

kubectl exec -it dns-debug -- /bin/bash

# 在Pod内使用以下工具：

# ===== nslookup（简单查询） =====
nslookup backend-service.default.svc.cluster.local

# ===== dig（详细查询） =====
dig backend-service.default.svc.cluster.local

# 输出示例：
# ;; ANSWER SECTION:
# backend-service.default.svc.cluster.local. 30 IN A 10.96.100.10

# ===== host（简洁查询） =====
host backend-service.default.svc.cluster.local

# ===== 查看Pod的DNS配置 =====
cat /etc/resolv.conf

# ===== 测试DNS服务器连接 =====
nc -zv 10.96.0.10 53  # 测试CoreDNS端口

# ===== 查询所有DNS记录（A/AAAA/SRV/CNAME） =====
dig ANY backend-service.default.svc.cluster.local

# ===== 查询PTR记录（反向解析） =====
dig -x 10.96.100.10

# ===== 追踪DNS查询路径 =====
dig +trace backend-service.default.svc.cluster.local
```

**CoreDNS日志查看：**

```bash
# 1. 查看CoreDNS Pod日志
kubectl logs -n kube-system -l k8s-app=kube-dns --tail=100

# 2. 启用详细日志（临时）
kubectl edit configmap coredns -n kube-system

# 修改Corefile，添加log插件：
.:53 {
    errors
    log  # ← 添加此行（记录所有DNS查询）
    health
    ...
}

# 重启CoreDNS
kubectl rollout restart deployment coredns -n kube-system

# 3. 查看查询日志
kubectl logs -n kube-system -l k8s-app=kube-dns -f

# 输出示例：
# [INFO] 10.244.1.5:54321 - 12345 "A IN backend-service.default.svc.cluster.local. udp 67 false 512" NOERROR qr,aa,rd 106 0.000123s
```

**常见DNS问题排查：**

```bash
# ===== 问题1：Service无法解析 =====
# 症状：nslookup返回NXDOMAIN

# 排查步骤：
# 1. 检查Service是否存在
kubectl get service backend-service

# 2. 检查CoreDNS是否运行
kubectl get pods -n kube-system -l k8s-app=kube-dns

# 3. 测试CoreDNS连通性
kubectl run test --image=busybox:1.36 --rm -it --restart=Never -- \
  nc -zv 10.96.0.10 53

# 4. 检查Pod的DNS配置
kubectl run test --image=busybox:1.36 --rm -it --restart=Never -- \
  cat /etc/resolv.conf

# ===== 问题2：DNS查询超时 =====
# 症状：DNS查询很慢或超时

# 排查步骤：
# 1. 检查CoreDNS负载
kubectl top pods -n kube-system -l k8s-app=kube-dns

# 2. 查看CoreDNS错误日志
kubectl logs -n kube-system -l k8s-app=kube-dns | grep -i error

# 3. 增加CoreDNS副本
kubectl scale deployment coredns -n kube-system --replicas=3

# 4. 启用CoreDNS缓存
kubectl edit configmap coredns -n kube-system
# 修改cache值：cache 300（增加到5分钟）

# ===== 问题3：外部域名无法解析 =====
# 症状：集群内域名正常，外部域名失败

# 排查步骤：
# 1. 测试外部DNS
kubectl run test --image=busybox:1.36 --rm -it --restart=Never -- \
  nslookup www.google.com

# 2. 检查CoreDNS转发配置
kubectl get configmap coredns -n kube-system -o yaml | grep forward

# 输出应包含：
# forward . /etc/resolv.conf

# 3. 检查节点DNS配置
cat /etc/resolv.conf  # 在节点上执行

# 4. 手动指定上游DNS
kubectl edit configmap coredns -n kube-system

# 修改forward配置：
forward . 8.8.8.8 1.1.1.1  # 使用公共DNS

# ===== 问题4：跨Namespace访问失败 =====
# 症状：同Namespace可以访问，跨Namespace失败

# 正确的跨Namespace访问方式：
# <service>.<namespace>.svc.cluster.local

# 示例：
kubectl run test --image=busybox:1.36 --rm -it --restart=Never -- \
  wget -qO- http://kube-dns.kube-system.svc.cluster.local:9153/metrics
```

至此，我们已经全面掌握了Kubernetes的DNS服务发现机制。接下来将学习Ingress控制器，实现基于域名的HTTP路由和外部访问。

---


## 4.5 Ingress控制器

在前面的章节中，我们学习了Service如何为Pod提供稳定的访问端点和负载均衡。但Service主要解决的是**集群内部服务发现**的问题。那么，如何让外部用户访问集群内的HTTP/HTTPS服务呢？

### 4.5.1 为什么需要Ingress？

**问题背景：Service暴露服务的局限性**

使用Service暴露服务有以下几种方式，但都存在不同的问题：

```
方式1: NodePort Service
问题：
- 每个Service占用一个节点端口（30000-32767）
- 需要记住端口号，用户体验差
- 无法实现基于域名的路由
- 无法实现HTTPS和证书管理

┌─────────────────────────────────────┐
│ 外部用户需要访问多个服务：           │
│ http://node-ip:30001 → 前端服务     │
│ http://node-ip:30002 → API服务      │
│ http://node-ip:30003 → 管理后台     │
└─────────────────────────────────────┘

方式2: LoadBalancer Service
问题：
- 每个Service需要一个云负载均衡器（成本高）
- 公有云每个LB都要收费
- 无法共享IP地址
- 无法实现基于路径的路由

┌─────────────────────────────────────┐
│ 云负载均衡器成本：                   │
│ LB1 (公网IP1) → 前端Service ($$$)   │
│ LB2 (公网IP2) → API Service  ($$$)  │
│ LB3 (公网IP3) → 管理Service  ($$$)  │
│ 总成本 = N个服务 × LB单价            │
└─────────────────────────────────────┘
```

**Ingress的解决方案：**

Ingress作为HTTP(S)层的统一入口，可以：

✅ **共享单个IP/负载均衡器**: 多个Service共享一个入口，降低成本
✅ **基于域名路由**: `www.example.com` → 前端Service，`api.example.com` → API Service
✅ **基于路径路由**: `/api` → API Service，`/blog` → 博客Service
✅ **SSL/TLS终止**: 统一管理证书，实现HTTPS访问
✅ **高级功能**: 路径重写、认证、限流、灰度发布等

```
理想架构：使用Ingress统一管理

外部用户
   │
   ├─> https://www.example.com        ─┐
   ├─> https://api.example.com        ─┤
   └─> https://admin.example.com      ─┤
                                        │
                   [单个LoadBalancer/公网IP]  ← 成本降低！
                             │
                       [Ingress Controller]   ← 七层路由
                             │
              ┌──────────────┼──────────────┐
              ▼              ▼              ▼
        [Frontend Svc] [API Svc]  [Admin Svc]
              │              │              │
           [Pods]        [Pods]        [Pods]
```

### 4.5.2 Ingress架构与工作原理

**核心组件：**

Ingress系统由两个核心部分组成：

1. **Ingress资源对象** (声明式配置)
   - 定义路由规则（域名、路径、后端Service）
   - 定义TLS证书配置
   - 定义高级功能（重写、认证等）

2. **Ingress Controller** (控制器实现)
   - 监听Ingress资源的变化
   - 将规则转换为实际的反向代理配置
   - 常见实现：Nginx Ingress、Traefik、HAProxy、Istio Gateway

**工作流程：**

```
┌─────────────────────────────────────────────────────────┐
│ 1. 用户创建Ingress资源                                   │
│    kubectl apply -f ingress.yaml                        │
└──────────────────┬──────────────────────────────────────┘
                   │
                   ▼
┌─────────────────────────────────────────────────────────┐
│ 2. Ingress Controller监听到变化                         │
│    - 读取Ingress规则                                    │
│    - 读取关联的Service和Endpoint                        │
└──────────────────┬──────────────────────────────────────┘
                   │
                   ▼
┌─────────────────────────────────────────────────────────┐
│ 3. 生成Nginx配置文件                                     │
│    server {                                             │
│      server_name api.example.com;                       │
│      location / {                                       │
│        proxy_pass http://api-service:8080;              │
│      }                                                  │
│    }                                                    │
└──────────────────┬──────────────────────────────────────┘
                   │
                   ▼
┌─────────────────────────────────────────────────────────┐
│ 4. 热重载Nginx                                           │
│    nginx -s reload                                      │
└──────────────────┬──────────────────────────────────────┘
                   │
                   ▼
┌─────────────────────────────────────────────────────────┐
│ 5. 外部流量通过Ingress路由到后端Service                  │
│    用户请求 → LoadBalancer → Ingress Pod → Service → Pod│
└─────────────────────────────────────────────────────────┘
```

**常见Ingress Controller对比：**

| 对比维度 | Nginx Ingress | Traefik | HAProxy Ingress | Istio Gateway |
|---------|--------------|---------|-----------------|---------------|
| **性能** | ⭐⭐⭐⭐⭐ 极高 | ⭐⭐⭐⭐ 高 | ⭐⭐⭐⭐⭐ 极高 | ⭐⭐⭐ 中等 |
| **功能丰富度** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **配置复杂度** | 中等 | 简单 | 中等 | 复杂 |
| **社区活跃度** | 非常高 | 高 | 中等 | 高 |
| **企业支持** | ✅ | ✅ | ✅ | ✅ |
| **推荐场景** | 通用场景 | 快速上手 | 高性能需求 | 服务网格 |

### 4.5.3 部署Nginx Ingress Controller

**方式1: 使用官方YAML部署（推荐生产环境）**

```bash
# 1. 下载官方部署清单（版本1.9.5）
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.5/deploy/static/provider/cloud/deploy.yaml

# 2. 查看部署资源
kubectl apply -f deploy.yaml --dry-run=client

# 部署的资源包括：
# - Namespace: ingress-nginx
# - ServiceAccount: ingress-nginx
# - ConfigMap: ingress-nginx-controller (配置项)
# - ClusterRole/ClusterRoleBinding: RBAC权限
# - Service: ingress-nginx-controller (LoadBalancer类型)
# - Deployment: ingress-nginx-controller

# 3. 正式部署
kubectl apply -f deploy.yaml

# 4. 等待Pod就绪
kubectl wait --namespace ingress-nginx \
  --for=condition=ready pod \
  --selector=app.kubernetes.io/component=controller \
  --timeout=120s

# 5. 检查部署状态
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

# 输出示例：
# NAME                                        READY   STATUS
# ingress-nginx-controller-5b4d8c9f8d-abc12   1/1     Running
#
# NAME                                 TYPE           EXTERNAL-IP
# ingress-nginx-controller             LoadBalancer   192.168.1.100
```

**方式2: 使用Helm部署（推荐开发环境）**

```bash
# 1. 添加Helm仓库
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

# 2. 查看Chart信息
helm show values ingress-nginx/ingress-nginx > values.yaml

# 3. 自定义配置（可选）
cat > custom-values.yaml <<EOF
controller:
  replicaCount: 2  # 高可用部署
  resources:
    requests:
      cpu: 100m
      memory: 90Mi
    limits:
      cpu: 200m
      memory: 256Mi

  # NodePort模式（本地环境）
  service:
    type: NodePort
    nodePorts:
      http: 30080
      https: 30443

  # 启用Prometheus指标
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

  # 自定义配置
  config:
    use-forwarded-headers: "true"
    compute-full-forwarded-for: "true"
    proxy-body-size: "50m"
EOF

# 4. 安装
helm install ingress-nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --create-namespace \
  --values custom-values.yaml

# 5. 验证安装
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

# 6. 升级配置（如需修改）
helm upgrade ingress-nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --values custom-values.yaml
```

**方式3: Minikube/Kind本地环境启用**

```bash
# Minikube环境
minikube addons enable ingress
kubectl get pods -n ingress-nginx

# Kind环境（需要特殊配置）
# 1. 创建Kind集群时配置端口映射
cat > kind-config.yaml <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP
EOF

kind create cluster --config kind-config.yaml

# 2. 部署Ingress Controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml

# 3. 等待就绪
kubectl wait --namespace ingress-nginx \
  --for=condition=ready pod \
  --selector=app.kubernetes.io/component=controller \
  --timeout=90s
```

**验证Ingress Controller是否正常工作：**

```bash
# 1. 检查Controller版本
kubectl exec -it -n ingress-nginx \
  $(kubectl get pods -n ingress-nginx -l app.kubernetes.io/component=controller -o jsonpath='{.items[0].metadata.name}') \
  -- /nginx-ingress-controller --version

# 输出示例：
# NGINX Ingress controller
#   Release:       v1.9.5
#   Build:         1e9f0b409d15ee29c0406783738ef04f7ba74c4c
#   nginx version: nginx/1.21.6

# 2. 查看Nginx配置（调试用）
kubectl exec -it -n ingress-nginx \
  $(kubectl get pods -n ingress-nginx -l app.kubernetes.io/component=controller -o jsonpath='{.items[0].metadata.name}') \
  -- cat /etc/nginx/nginx.conf

# 3. 查看Controller日志
kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller -f
```

### 4.5.4 创建第一个Ingress资源

现在我们已经部署了Ingress Controller，接下来创建Ingress规则来暴露服务。

**场景：暴露一个简单的Web应用**

```yaml
# 1. 首先部署后端应用
# app-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  labels:
    app: web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: nginx
        image: nginx:1.25-alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
      volumes:
      - name: html
        configMap:
          name: web-content

---
# 2. 创建Service
apiVersion: v1
kind: Service
metadata:
  name: web-app-service
spec:
  selector:
    app: web-app
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP  # ⚠️ 注意：Ingress后端只需ClusterIP

---
# 3. 创建页面内容
apiVersion: v1
kind: ConfigMap
metadata:
  name: web-content
data:
  index.html: |
    <!DOCTYPE html>
    <html>
    <head><title>Kubernetes Ingress Demo</title></head>
    <body>
      <h1>Welcome to Kubernetes Ingress!</h1>
      <p>This page is served via Ingress Controller</p>
      <p>Hostname: <span id="hostname"></span></p>
      <script>
        fetch('/hostname')
          .then(r => r.text())
          .then(h => document.getElementById('hostname').innerText = h);
      </script>
    </body>
    </html>
```

**创建Ingress规则：**

```yaml
# ingress-basic.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-app-ingress
  annotations:
    # Ingress Controller特定的注解
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  # 指定Ingress Controller类（如果集群中有多个Controller）
  ingressClassName: nginx

  # 路由规则
  rules:
  - host: www.example.com  # ⚠️ 域名（需要配置DNS或本地hosts）
    http:
      paths:
      - path: /            # 路径匹配规则
        pathType: Prefix   # 匹配类型：Prefix（前缀）/Exact（精确）
        backend:
          service:
            name: web-app-service  # 后端Service名称
            port:
              number: 80           # Service端口
```

**部署并测试：**

```bash
# 1. 部署所有资源
kubectl apply -f app-deployment.yaml
kubectl apply -f ingress-basic.yaml

# 2. 查看Ingress状态
kubectl get ingress web-app-ingress

# 输出示例：
# NAME              CLASS   HOSTS             ADDRESS         PORTS   AGE
# web-app-ingress   nginx   www.example.com   192.168.1.100   80      1m

# 3. 查看详细信息
kubectl describe ingress web-app-ingress

# 输出关键信息：
# Rules:
#   Host             Path  Backends
#   ----             ----  --------
#   www.example.com
#                    /     web-app-service:80 (10.244.1.5:80,10.244.2.8:80,10.244.3.9:80)
# Events:
#   Type    Reason  Age   From                      Message
#   ----    ------  ----  ----                      -------
#   Normal  Sync    1m    nginx-ingress-controller  Scheduled for sync

# 4. 配置本地hosts（测试用）
# Linux/Mac:
echo "192.168.1.100 www.example.com" | sudo tee -a /etc/hosts

# Windows (管理员权限):
# echo 192.168.1.100 www.example.com >> C:\Windows\System32\drivers\etc\hosts

# 5. 测试访问
curl http://www.example.com

# 或使用浏览器访问：http://www.example.com

# 6. 不配置hosts的测试方法
curl -H "Host: www.example.com" http://192.168.1.100
```

### 4.5.5 路径匹配规则详解

Ingress支持三种路径匹配类型，理解它们的区别对于正确配置路由至关重要。

**PathType类型：**

| PathType | 匹配规则 | 示例 | 匹配 | 不匹配 |
|----------|---------|------|------|--------|
| **Exact** | 精确匹配 | `/api/users` | `/api/users` | `/api/users/`<br>`/api/users/123` |
| **Prefix** | 前缀匹配 | `/api` | `/api`<br>`/api/`<br>`/api/users`<br>`/api/v1/posts` | `/ap`<br>`/apis` |
| **ImplementationSpecific** | 由Ingress Controller决定 | - | 依赖具体实现 | - |

**完整示例：多路径路由**

```yaml
# ingress-multi-path.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-path-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2  # ⚠️ 路径重写（下节详解）
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      # 1. API v1路由
      - path: /v1(/|$)(.*)  # 捕获组用于路径重写
        pathType: ImplementationSpecific
        backend:
          service:
            name: api-v1-service
            port:
              number: 8080

      # 2. API v2路由
      - path: /v2(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: api-v2-service
            port:
              number: 8080

      # 3. 健康检查路由（精确匹配）
      - path: /healthz
        pathType: Exact
        backend:
          service:
            name: health-service
            port:
              number: 8080

      # 4. 默认路由（最低优先级）
      - path: /
        pathType: Prefix
        backend:
          service:
            name: default-service
            port:
              number: 80
```

**匹配优先级：**

Nginx Ingress Controller的匹配顺序（从高到低）：

```
1. Exact (精确匹配)
   /api/users (Exact) 优先于 /api (Prefix)

2. Prefix (最长前缀匹配)
   /api/v1/users (Prefix) 优先于 /api (Prefix)

3. 正则表达式（ImplementationSpecific）
   按照在Ingress中定义的顺序

4. Default Backend (默认后端)
   所有规则都不匹配时使用
```

**实际测试：**

```bash
# 假设有以下路由规则：
# /api/v1    → api-v1-service
# /api/v2    → api-v2-service
# /api       → api-default-service
# /          → frontend-service

# 请求路径匹配示例：
curl http://api.example.com/api/v1/users
# → 转发到 api-v1-service

curl http://api.example.com/api/v2/posts
# → 转发到 api-v2-service

curl http://api.example.com/api/legacy
# → 转发到 api-default-service

curl http://api.example.com/
# → 转发到 frontend-service

curl http://api.example.com/unknown
# → 转发到 frontend-service (因为'/'匹配所有前缀)
```

### 4.5.6 多域名路由

在实际生产环境中，我们通常需要为不同的域名配置不同的路由规则。

**场景：企业多域名配置**

```yaml
# ingress-multi-domain.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-domain-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  ingressClassName: nginx
  rules:

  # 1. 主站（www.example.com）
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80

  # 2. API服务（api.example.com）
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: backend-api-service
            port:
              number: 8080

  # 3. 管理后台（admin.example.com）
  - host: admin.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: admin-service
            port:
              number: 3000

  # 4. 静态资源CDN（static.example.com）
  - host: static.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: static-assets-service
            port:
              number: 80
```

**分离Ingress资源管理（推荐）：**

在大型项目中，建议将不同域名的Ingress规则拆分到不同的文件中管理：

```yaml
# ingress-frontend.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend-ingress
  namespace: production
  labels:
    app: frontend
spec:
  ingressClassName: nginx
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80

---
# ingress-api.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-ingress
  namespace: production
  labels:
    app: backend-api
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: backend-api-service
            port:
              number: 8080
```

**查看和管理多个Ingress：**

```bash
# 1. 查看所有Ingress
kubectl get ingress --all-namespaces

# 输出示例：
# NAMESPACE    NAME                HOSTS                ADDRESS         PORTS
# production   frontend-ingress    www.example.com      192.168.1.100   80
# production   api-ingress         api.example.com      192.168.1.100   80
# production   admin-ingress       admin.example.com    192.168.1.100   80

# 2. 查看特定域名的路由
kubectl get ingress -o json | jq '.items[] | select(.spec.rules[].host=="api.example.com")'

# 3. 测试多域名访问
curl -H "Host: www.example.com" http://192.168.1.100
curl -H "Host: api.example.com" http://192.168.1.100
curl -H "Host: admin.example.com" http://192.168.1.100
```

### 4.5.7 默认后端配置

当请求的域名或路径不匹配任何Ingress规则时，Ingress Controller会将流量转发到默认后端。

**配置自定义默认后端：**

```yaml
# default-backend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: default-backend
  namespace: ingress-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: default-backend
  template:
    metadata:
      labels:
        app: default-backend
    spec:
      containers:
      - name: default-backend
        image: registry.k8s.io/defaultbackend-amd64:1.5
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 10m
            memory: 20Mi
          limits:
            cpu: 50m
            memory: 50Mi

---
apiVersion: v1
kind: Service
metadata:
  name: default-backend
  namespace: ingress-nginx
spec:
  selector:
    app: default-backend
  ports:
  - port: 80
    targetPort: 8080
```

**在Ingress Controller中启用：**

```bash
# 方式1: Helm部署时配置
helm upgrade ingress-nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --set defaultBackend.enabled=true \
  --set defaultBackend.image.repository=registry.k8s.io/defaultbackend-amd64 \
  --set defaultBackend.image.tag=1.5

# 方式2: 修改Deployment
kubectl edit deployment ingress-nginx-controller -n ingress-nginx

# 添加参数：
# - --default-backend-service=ingress-nginx/default-backend
```

**自定义404页面：**

```yaml
# custom-404-backend.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-404-page
  namespace: ingress-nginx
data:
  index.html: |
    <!DOCTYPE html>
    <html>
    <head>
      <title>404 - Page Not Found</title>
      <style>
        body { font-family: Arial; text-align: center; padding: 50px; }
        h1 { color: #e74c3c; }
      </style>
    </head>
    <body>
      <h1>404 - Page Not Found</h1>
      <p>The page you are looking for does not exist.</p>
      <p>Please check the URL and try again.</p>
    </body>
    </html>

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: custom-404-backend
  namespace: ingress-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: custom-404
  template:
    metadata:
      labels:
        app: custom-404
    spec:
      containers:
      - name: nginx
        image: nginx:1.25-alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
      volumes:
      - name: html
        configMap:
          name: custom-404-page

---
apiVersion: v1
kind: Service
metadata:
  name: custom-404-backend
  namespace: ingress-nginx
spec:
  selector:
    app: custom-404
  ports:
  - port: 80
    targetPort: 80
```

通过本节学习，我们已经掌握了Ingress Controller的部署、基本路由配置、多域名管理和默认后端设置。下一节将深入学习Ingress的高级特性，包括TLS/HTTPS配置、路径重写、认证、限流和灰度发布等生产环境必备功能。

---


## 4.6 Ingress高级特性

在掌握了Ingress的基本用法后，本节将深入探讨生产环境中必备的高级特性。这些功能能够帮助我们构建安全、高性能、灵活的HTTP(S)路由系统。

### 4.6.1 TLS/SSL证书配置

在生产环境中，HTTPS是标配。Ingress支持统一管理TLS证书，实现SSL终止（SSL Termination）。

**核心概念：**

```
HTTPS流程：
客户端 ─────HTTPS加密流量─────> Ingress Controller
                                      │ SSL终止（解密）
                                      ▼
                              HTTP明文流量 ──> 后端Service
                                               │
                                            [Pod]
```

**步骤1: 生成自签名证书（测试用）**

```bash
# 生成私钥和证书
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout tls.key \
  -out tls.crt \
  -subj "/CN=www.example.com/O=example-org"

# 查看证书信息
openssl x509 -in tls.crt -text -noout

# 验证私钥和证书匹配
openssl rsa -noout -modulus -in tls.key | openssl md5
openssl x509 -noout -modulus -in tls.crt | openssl md5
# 两个输出的MD5值应该相同
```

**步骤2: 创建Secret存储证书**

```bash
# 方式1: 使用kubectl创建
kubectl create secret tls example-tls \
  --cert=tls.crt \
  --key=tls.key \
  --namespace=default

# 方式2: 使用YAML定义
cat > tls-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: example-tls
  namespace: default
type: kubernetes.io/tls
data:
  tls.crt: $(cat tls.crt | base64 | tr -d '\n')
  tls.key: $(cat tls.key | base64 | tr -d '\n')
EOF

kubectl apply -f tls-secret.yaml

# 验证Secret
kubectl get secret example-tls -o yaml
kubectl describe secret example-tls
```

**步骤3: 在Ingress中引用证书**

```yaml
# ingress-tls.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tls-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"  # 强制HTTPS
spec:
  ingressClassName: nginx

  # TLS配置
  tls:
  - hosts:
    - www.example.com
    - api.example.com
    secretName: example-tls  # 引用Secret

  # 路由规则
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80

  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080
```

**测试HTTPS访问：**

```bash
# 部署配置
kubectl apply -f ingress-tls.yaml

# 查看Ingress状态
kubectl get ingress tls-ingress
# NAME          CLASS   HOSTS                              ADDRESS   PORTS     AGE
# tls-ingress   nginx   www.example.com,api.example.com    ...       80, 443   1m

# 测试HTTPS访问（忽略自签名证书警告）
curl -k https://www.example.com

# 使用curl查看证书信息
curl -vk https://www.example.com 2>&1 | grep -A 10 'Server certificate'

# 使用openssl查看证书
echo | openssl s_client -servername www.example.com \
  -connect 192.168.1.100:443 2>/dev/null | \
  openssl x509 -text -noout

# 测试HTTP自动跳转到HTTPS
curl -I http://www.example.com
# 应该返回301/302重定向到https://
```

**多域名使用不同证书：**

```yaml
# ingress-multi-tls.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-tls-ingress
spec:
  ingressClassName: nginx
  tls:

  # 域名1使用证书A
  - hosts:
    - www.example.com
    secretName: example-com-tls

  # 域名2使用证书B
  - hosts:
    - api.example.org
    secretName: example-org-tls

  # 通配符域名使用证书C
  - hosts:
    - "*.apps.example.com"
    secretName: wildcard-tls

  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: www-service
            port:
              number: 80

  - host: api.example.org
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080

  - host: app1.apps.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app1-service
            port:
              number: 80
```

**使用Let's Encrypt自动化证书管理（cert-manager）：**

```bash
# 1. 安装cert-manager
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.yaml

# 2. 等待cert-manager就绪
kubectl wait --for=condition=ready pod \
  -l app.kubernetes.io/instance=cert-manager \
  -n cert-manager \
  --timeout=120s

# 3. 创建Let's Encrypt ClusterIssuer
cat > letsencrypt-issuer.yaml <<EOF
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    # Let's Encrypt生产环境
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@example.com  # ⚠️ 修改为真实邮箱
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    # HTTP-01验证
    - http01:
        ingress:
          class: nginx
EOF

kubectl apply -f letsencrypt-issuer.yaml

# 4. 配置Ingress自动申请证书
cat > ingress-auto-tls.yaml <<EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: auto-tls-ingress
  annotations:
    # 指定证书签发者
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    # 强制HTTPS
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - www.example.com
    secretName: example-com-tls-auto  # cert-manager自动创建
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
EOF

kubectl apply -f ingress-auto-tls.yaml

# 5. 查看证书申请状态
kubectl get certificate
kubectl describe certificate example-com-tls-auto

# 6. 查看cert-manager日志
kubectl logs -n cert-manager -l app=cert-manager -f
```

### 4.6.2 HTTPS强制跳转与安全配置

**强制HTTPS跳转：**

```yaml
# ingress-force-https.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: force-https-ingress
  annotations:
    # 方式1: 强制SSL跳转（推荐）
    nginx.ingress.kubernetes.io/ssl-redirect: "true"

    # 方式2: 永久重定向（301）
    nginx.ingress.kubernetes.io/permanent-redirect: "https://www.example.com$request_uri"

    # 方式3: 临时重定向（302）
    nginx.ingress.kubernetes.io/temporal-redirect: "https://www.example.com$request_uri"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - www.example.com
    secretName: example-tls
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
```

**HSTS (HTTP Strict Transport Security) 配置：**

```yaml
# ingress-hsts.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hsts-ingress
  annotations:
    # 启用HSTS
    nginx.ingress.kubernetes.io/hsts: "true"
    # HSTS最大有效期（秒）
    nginx.ingress.kubernetes.io/hsts-max-age: "31536000"  # 1年
    # 包含子域名
    nginx.ingress.kubernetes.io/hsts-include-subdomains: "true"
    # 允许浏览器预加载HSTS列表
    nginx.ingress.kubernetes.io/hsts-preload: "true"

    # 强制HTTPS
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - www.example.com
    secretName: example-tls
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
```

**测试HSTS：**

```bash
# 查看响应头
curl -Ik https://www.example.com

# 应该包含：
# Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
```

**TLS版本和加密套件配置：**

```yaml
# ingress-ssl-config.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ssl-config-ingress
  annotations:
    # 指定TLS协议版本（只允许TLS 1.2和1.3）
    nginx.ingress.kubernetes.io/ssl-protocols: "TLSv1.2 TLSv1.3"

    # 加密套件（推荐配置）
    nginx.ingress.kubernetes.io/ssl-ciphers: |
      ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:
      ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:
      ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:
      DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384

    # 服务器优先选择加密套件
    nginx.ingress.kubernetes.io/ssl-prefer-server-ciphers: "true"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - www.example.com
    secretName: example-tls
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
```

### 4.6.3 路径重写与URL重定向

**路径重写（Rewrite）：**

路径重写允许我们在转发请求到后端时修改URL路径，这在微服务架构中非常有用。

```yaml
# ingress-rewrite.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rewrite-ingress
  annotations:
    # 基本重写：将/api/v1/* 重写为 /*
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      # 访问 /api/v1/users → 转发到后端 /users
      - path: /api/v1(/|$)(.*)  # $2捕获(.*)部分
        pathType: ImplementationSpecific
        backend:
          service:
            name: api-v1-service
            port:
              number: 8080

      # 访问 /api/v2/posts → 转发到后端 /posts
      - path: /api/v2(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: api-v2-service
            port:
              number: 8080
```

**测试路径重写：**

```bash
# 访问: http://api.example.com/api/v1/users
# 实际转发: http://api-v1-service:8080/users

# 后端服务会收到的请求路径：/users（而不是/api/v1/users）
```

**复杂的路径重写示例：**

```yaml
# ingress-advanced-rewrite.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: advanced-rewrite-ingress
  annotations:
    # 使用capture groups实现复杂重写
    nginx.ingress.kubernetes.io/rewrite-target: /api/$2
    # 保留原始URI在请求头中
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_set_header X-Original-URI $request_uri;
      proxy_set_header X-Rewrite-URL $uri;
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      # /app/users/123 → /api/users/123
      - path: /app(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: backend-service
            port:
              number: 8080
```

**URL重定向（Redirect）：**

```yaml
# ingress-redirect.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: redirect-ingress
  annotations:
    # 永久重定向（301）
    nginx.ingress.kubernetes.io/permanent-redirect: "https://www.newdomain.com"

    # 或者使用临时重定向（302）
    # nginx.ingress.kubernetes.io/temporal-redirect: "https://www.newdomain.com"
spec:
  ingressClassName: nginx
  rules:
  - host: old.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: dummy-service  # 不会被使用
            port:
              number: 80
```

**条件重定向（基于路径）：**

```yaml
# ingress-conditional-redirect.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: conditional-redirect-ingress
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      # 将/old-path重定向到/new-path
      if ($request_uri ~ ^/old-path) {
        return 301 https://www.example.com/new-path;
      }

      # 将/legacy/*重定向到新域名
      if ($request_uri ~ ^/legacy/(.*)$) {
        return 301 https://new.example.com/$1;
      }
spec:
  ingressClassName: nginx
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
```

**URL追加（App Root）：**

```yaml
# ingress-app-root.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-root-ingress
  annotations:
    # 访问根路径时自动跳转到/app
    nginx.ingress.kubernetes.io/app-root: /app
spec:
  ingressClassName: nginx
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
```

### 4.6.4 速率限制（Rate Limiting）

速率限制用于防止API滥用、DDoS攻击，保护后端服务。

**基于IP的速率限制：**

```yaml
# ingress-rate-limit.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rate-limit-ingress
  annotations:
    # 每秒允许的请求数（rps）
    nginx.ingress.kubernetes.io/limit-rps: "10"

    # 或者使用每分钟请求数（rpm）
    # nginx.ingress.kubernetes.io/limit-rpm: "600"

    # 突发请求数（允许短时间内超过限制）
    nginx.ingress.kubernetes.io/limit-burst-multiplier: "5"

    # 限制的键（默认基于IP）
    # nginx.ingress.kubernetes.io/limit-rate-key: "$binary_remote_addr"
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080
```

**工作原理：**

```
速率限制计算：
- limit-rps: 10          → 每秒10个请求
- limit-burst-multiplier: 5  → 突发允许50个请求（10 × 5）

流量模式：
正常情况：
  0s: ✅✅✅✅✅✅✅✅✅✅ (10个请求通过)
  1s: ✅✅✅✅✅✅✅✅✅✅ (10个请求通过)

突发流量：
  0s: ✅✅✅✅✅...✅ (50个请求通过，消耗突发配额)
  1s: ❌❌❌❌❌ (所有请求被限制，503错误)
  2s: ✅✅✅✅✅✅✅✅✅✅ (恢复正常)
```

**不同路径使用不同限制：**

```yaml
# ingress-multi-rate-limit.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-rate-limit-ingress
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      # 公开API：严格限制
      - path: /public
        pathType: Prefix
        backend:
          service:
            name: public-api-service
            port:
              number: 8080
        # ⚠️ 注意：路径级别的限制需要单独的Ingress

---
# 公开API - 严格限制
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: public-api-ingress
  annotations:
    nginx.ingress.kubernetes.io/limit-rps: "5"  # 每秒5个请求
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /public
        pathType: Prefix
        backend:
          service:
            name: public-api-service
            port:
              number: 8080

---
# 认证API - 宽松限制
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: auth-api-ingress
  annotations:
    nginx.ingress.kubernetes.io/limit-rps: "50"  # 每秒50个请求
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: auth-api-service
            port:
              number: 8080
```

**基于请求头的速率限制：**

```yaml
# ingress-custom-key-rate-limit.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: custom-key-rate-limit-ingress
  annotations:
    # 基于API Key限制（而不是IP）
    nginx.ingress.kubernetes.io/limit-rate-key: "$http_x_api_key"
    nginx.ingress.kubernetes.io/limit-rps: "100"
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080
```

**测试速率限制：**

```bash
# 1. 快速发送多个请求
for i in {1..20}; do
  curl -w "Request $i: %{http_code}\n" \
    http://api.example.com/test \
    -o /dev/null -s
  sleep 0.05  # 50ms间隔
done

# 输出示例：
# Request 1: 200
# Request 2: 200
# ...
# Request 11: 503  ← 超过限制
# Request 12: 503
# ...

# 2. 使用ab (Apache Bench) 压测
ab -n 100 -c 10 http://api.example.com/test

# 3. 查看被限制的请求统计
kubectl logs -n ingress-nginx \
  -l app.kubernetes.io/component=controller | \
  grep "limiting requests"
```

### 4.6.5 Basic认证与IP白名单

**Basic认证（HTTP Basic Authentication）：**

```bash
# 1. 创建密码文件
htpasswd -c auth admin
# 输入密码：admin123

# 添加更多用户
htpasswd auth user1
htpasswd auth user2

# 2. 创建Secret
kubectl create secret generic basic-auth \
  --from-file=auth \
  --namespace=default

# 3. 验证Secret
kubectl get secret basic-auth -o yaml
```

```yaml
# ingress-basic-auth.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: basic-auth-ingress
  annotations:
    # 启用Basic认证
    nginx.ingress.kubernetes.io/auth-type: basic
    # 引用Secret
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    # 认证提示信息
    nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"
spec:
  ingressClassName: nginx
  rules:
  - host: admin.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: admin-service
            port:
              number: 3000
```

**测试Basic认证：**

```bash
# 1. 无认证访问（应该返回401）
curl -I http://admin.example.com

# 输出：
# HTTP/1.1 401 Unauthorized
# WWW-Authenticate: Basic realm="Authentication Required"

# 2. 使用正确的用户名密码
curl -u admin:admin123 http://admin.example.com

# 3. 使用错误的密码
curl -u admin:wrongpass http://admin.example.com
# 返回：401 Unauthorized
```

**IP白名单（Whitelist）：**

```yaml
# ingress-whitelist.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: whitelist-ingress
  annotations:
    # 允许访问的IP地址（CIDR格式）
    nginx.ingress.kubernetes.io/whitelist-source-range: |
      192.168.1.0/24,
      10.0.0.0/8,
      172.16.0.100
spec:
  ingressClassName: nginx
  rules:
  - host: internal.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: internal-service
            port:
              number: 8080
```

**测试IP白名单：**

```bash
# 从允许的IP访问（应该成功）
curl http://internal.example.com

# 从不允许的IP访问（应该返回403）
curl http://internal.example.com
# 返回：403 Forbidden

# 查看Nginx日志
kubectl logs -n ingress-nginx \
  -l app.kubernetes.io/component=controller | \
  grep "access forbidden"
```

**组合认证：Basic认证 + IP白名单**

```yaml
# ingress-combined-auth.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: combined-auth-ingress
  annotations:
    # IP白名单
    nginx.ingress.kubernetes.io/whitelist-source-range: "192.168.1.0/24,10.0.0.0/8"

    # Basic认证
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    nginx.ingress.kubernetes.io/auth-realm: "Restricted Area"

    # 认证策略：满足任一条件即可（or）或必须同时满足（and）
    nginx.ingress.kubernetes.io/satisfy: "any"  # or / and
spec:
  ingressClassName: nginx
  rules:
  - host: secure.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: secure-service
            port:
              number: 8080
```

**satisfy参数说明：**

```
satisfy: "any" （或关系）
- IP白名单通过 OR Basic认证通过 → 允许访问
- 适用场景：内网IP免认证，外网需要密码

satisfy: "and" （且关系）
- IP白名单通过 AND Basic认证通过 → 允许访问
- 适用场景：高安全要求，双重验证
```

### 4.6.6 自定义响应头

**添加安全响应头：**

```yaml
# ingress-security-headers.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: security-headers-ingress
  annotations:
    # 自定义响应头
    nginx.ingress.kubernetes.io/configuration-snippet: |
      # 防止XSS攻击
      add_header X-XSS-Protection "1; mode=block" always;

      # 防止点击劫持
      add_header X-Frame-Options "SAMEORIGIN" always;

      # 禁止MIME类型嗅探
      add_header X-Content-Type-Options "nosniff" always;

      # 推荐的Referrer策略
      add_header Referrer-Policy "strict-origin-when-cross-origin" always;

      # 内容安全策略（CSP）
      add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline';" always;

      # 权限策略
      add_header Permissions-Policy "geolocation=(), microphone=(), camera=()" always;
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - www.example.com
    secretName: example-tls
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
```

**CORS（跨域资源共享）配置：**

```yaml
# ingress-cors.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: cors-ingress
  annotations:
    # 启用CORS
    nginx.ingress.kubernetes.io/enable-cors: "true"

    # 允许的源（Origin）
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://app.example.com, https://admin.example.com"

    # 允许的HTTP方法
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, PUT, DELETE, OPTIONS"

    # 允许的请求头
    nginx.ingress.kubernetes.io/cors-allow-headers: "Content-Type, Authorization, X-Requested-With"

    # 暴露的响应头
    nginx.ingress.kubernetes.io/cors-expose-headers: "Content-Length, Content-Range"

    # 预检请求缓存时间（秒）
    nginx.ingress.kubernetes.io/cors-max-age: "86400"

    # 是否允许携带凭证（Cookie）
    nginx.ingress.kubernetes.io/cors-allow-credentials: "true"
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080
```

**测试CORS配置：**

```bash
# 1. 发送OPTIONS预检请求
curl -X OPTIONS \
  -H "Origin: https://app.example.com" \
  -H "Access-Control-Request-Method: POST" \
  -H "Access-Control-Request-Headers: Content-Type" \
  -I http://api.example.com/users

# 应该返回：
# HTTP/1.1 204 No Content
# Access-Control-Allow-Origin: https://app.example.com
# Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
# Access-Control-Allow-Headers: Content-Type, Authorization, X-Requested-With
# Access-Control-Max-Age: 86400

# 2. 实际跨域请求
curl -X POST \
  -H "Origin: https://app.example.com" \
  -H "Content-Type: application/json" \
  -d '{"name":"test"}' \
  http://api.example.com/users
```

**自定义业务响应头：**

```yaml
# ingress-custom-headers.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: custom-headers-ingress
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      # 添加API版本号
      add_header X-API-Version "v1.0.0" always;

      # 添加服务器标识
      add_header X-Served-By "ingress-nginx" always;

      # 添加请求ID（用于追踪）
      add_header X-Request-ID $request_id always;

      # 隐藏服务器版本信息
      more_clear_headers 'Server';
      more_set_headers 'Server: CustomServer';
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080
```

### 4.6.7 灰度发布（Canary Deployment）

灰度发布允许我们将一部分流量路由到新版本服务，逐步验证新版本的稳定性。

**基于权重的金丝雀发布：**

```yaml
# 1. 生产版本Ingress（稳定版）
# ingress-production.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: production-ingress
  labels:
    version: stable
spec:
  ingressClassName: nginx
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-v1-service  # 稳定版本
            port:
              number: 80

---
# 2. 金丝雀版本Ingress（新版本）
# ingress-canary.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: canary-ingress
  labels:
    version: canary
  annotations:
    # 启用金丝雀发布
    nginx.ingress.kubernetes.io/canary: "true"

    # 方式1: 基于权重（20%流量到新版本）
    nginx.ingress.kubernetes.io/canary-weight: "20"
spec:
  ingressClassName: nginx
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-v2-service  # 新版本
            port:
              number: 80
```

**流量分配示意：**

```
100个请求到达：
├─ 80个请求 → app-v1-service (稳定版本)
└─ 20个请求 → app-v2-service (金丝雀版本)
```

**基于请求头的金丝雀发布：**

```yaml
# ingress-canary-header.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: canary-header-ingress
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"

    # 方式2: 基于请求头
    # 如果请求头 "canary: always"，则路由到金丝雀版本
    nginx.ingress.kubernetes.io/canary-by-header: "canary"

    # 或者指定请求头的值
    # nginx.ingress.kubernetes.io/canary-by-header-value: "true"
spec:
  ingressClassName: nginx
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-v2-service
            port:
              number: 80
```

**测试基于请求头的金丝雀：**

```bash
# 普通请求 → 稳定版本
curl http://www.example.com

# 带金丝雀标记的请求 → 新版本
curl -H "canary: always" http://www.example.com

# 或使用自定义值
curl -H "canary: true" http://www.example.com
```

**基于Cookie的金丝雀发布：**

```yaml
# ingress-canary-cookie.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: canary-cookie-ingress
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"

    # 方式3: 基于Cookie
    # 如果Cookie中有 "canary=always"，则路由到金丝雀版本
    nginx.ingress.kubernetes.io/canary-by-cookie: "canary"
spec:
  ingressClassName: nginx
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-v2-service
            port:
              number: 80
```

**完整的灰度发布流程示例：**

```yaml
# 场景：将20%流量引导到v2版本，且允许内部测试人员强制访问v2

# 1. 生产版本（v1）
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: production-v1
spec:
  ingressClassName: nginx
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-v1
            port:
              number: 80

---
# 2. 金丝雀版本（v2）
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: canary-v2
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"

    # 优先级1: 带测试Cookie的用户100%访问v2
    nginx.ingress.kubernetes.io/canary-by-cookie: "beta-tester"

    # 优先级2: 其他用户20%访问v2
    nginx.ingress.kubernetes.io/canary-weight: "20"
spec:
  ingressClassName: nginx
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-v2
            port:
              number: 80
```

**金丝雀发布优先级：**

```
优先级从高到低：
1. canary-by-header
2. canary-by-cookie
3. canary-weight

示例：
Request 1: Header "canary: always"       → v2 (100%)
Request 2: Cookie "beta-tester=always"   → v2 (100%)
Request 3: 普通请求                      → v1 (80%) / v2 (20%)
```

**完整的灰度发布实战流程：**

```bash
# 阶段1: 部署v1版本（100%流量）
kubectl apply -f production-v1-ingress.yaml

# 阶段2: 部署v2版本，5%流量进行灰度
cat > canary-5percent.yaml <<EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: canary-v2
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "5"
spec:
  ingressClassName: nginx
  rules:
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-v2
            port:
              number: 80
EOF

kubectl apply -f canary-5percent.yaml

# 观察v2版本的错误率、响应时间等指标
kubectl logs -l app=app-v2 -f
kubectl top pods -l app=app-v2

# 阶段3: 逐步增加流量到20%
kubectl patch ingress canary-v2 -p '{"metadata":{"annotations":{"nginx.ingress.kubernetes.io/canary-weight":"20"}}}'

# 继续观察...

# 阶段4: 增加到50%
kubectl patch ingress canary-v2 -p '{"metadata":{"annotations":{"nginx.ingress.kubernetes.io/canary-weight":"50"}}}'

# 阶段5: 全量切换（删除金丝雀Ingress，更新生产Ingress）
kubectl delete ingress canary-v2
kubectl patch ingress production-v1 -p '{"spec":{"rules":[{"host":"www.example.com","http":{"paths":[{"path":"/","pathType":"Prefix","backend":{"service":{"name":"app-v2","port":{"number":80}}}}]}}]}}'

# 或者回滚（如果v2有问题）
kubectl delete ingress canary-v2
# 保持v1继续运行
```

通过本节的学习，我们掌握了Ingress的7大高级特性：TLS/HTTPS配置、HTTPS强制跳转、路径重写、速率限制、认证与授权、自定义响应头、灰度发布。这些功能组合使用，可以构建安全、高性能、灵活的HTTP(S)路由系统。

下一节我们将通过一个完整的实战项目，整合本章所学的所有知识点，部署一个生产级的微服务架构。

---

