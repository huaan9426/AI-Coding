# 第12章 Kubernetes项目实战与最佳实践

## 本章概述

经过前11章的系统学习，你已经掌握了Kubernetes的核心概念、资源管理、网络存储、安全机制、监控可观测性、高级特性以及故障排查等完整知识体系。本章将通过真实的项目实战，将这些知识融会贯通，帮助你构建生产级的Kubernetes应用。

**为什么需要项目实战？**

理论知识和实际应用之间存在巨大鸿沟：

📚 **理论学习阶段**：
- 理解Pod、Service、Deployment等概念
- 知道如何编写YAML配置文件
- 了解网络、存储、安全的工作原理
- 掌握监控和故障排查方法

🚀 **生产实战阶段**：
- 如何设计高可用的应用架构？
- 如何处理有状态应用的部署？
- 如何实现零停机滚动更新？
- 如何保证应用的安全性和性能？
- 如何建立完整的CI/CD流程？
- 如何应对突发流量和故障？

**本章实战项目**：

我们将通过一个完整的微服务电商系统，涵盖Kubernetes生产实战的各个方面：

```
电商系统架构:
┌─────────────────────────────────────────────────────────────┐
│                        用户层                                 │
│  Web浏览器  ←→  移动App  ←→  第三方系统                      │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                     Ingress层                                 │
│  Nginx Ingress Controller + TLS证书 + 限流                   │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                    API网关层                                  │
│  Kong/APISIX - 认证、鉴权、限流、熔断                        │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                   微服务层                                    │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  │
│  │用户服务  │  │商品服务  │  │订单服务  │  │支付服务  │  │
│  │(Go)      │  │(Java)    │  │(Python)  │  │(Node.js) │  │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘  │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                    数据层                                     │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  │
│  │MySQL     │  │Redis     │  │MongoDB   │  │RabbitMQ  │  │
│  │(主从)    │  │(集群)    │  │(副本集)  │  │(集群)    │  │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘  │
└─────────────────────────────────────────────────────────────┘
```

**本章主要内容**：

1. **从零搭建生产级Kubernetes集群** (12.1)
   - 集群规划与架构设计
   - 高可用控制平面部署
   - 网络和存储方案选择
   - 安全加固与最佳实践

2. **微服务应用容器化** (12.2)
   - Dockerfile最佳实践
   - 多阶段构建优化
   - 镜像安全扫描
   - 私有镜像仓库搭建

3. **应用部署与配置管理** (12.3)
   - Helm Chart编写
   - 多环境配置管理
   - Secret和ConfigMap管理
   - 应用版本管理

4. **服务网格与流量管理** (12.4)
   - Istio服务网格部署
   - 灰度发布与金丝雀部署
   - 流量路由与熔断
   - 分布式追踪

5. **CI/CD流水线构建** (12.5)
   - GitOps工作流
   - Jenkins/GitLab CI集成
   - 自动化测试
   - 自动化部署

6. **监控告警与日志分析** (12.6)
   - Prometheus监控体系
   - Grafana仪表板
   - EFK日志聚合
   - 告警规则配置

7. **性能优化与成本控制** (12.7)
   - 资源配额优化
   - HPA/VPA自动扩缩容
   - 集群成本分析
   - 性能调优实践

8. **本章总结** (12.8)
   - 项目实战回顾
   - 生产经验总结
   - 持续学习路径

**学习目标**：

- ✅ 能够从零搭建生产级Kubernetes集群
- ✅ 掌握微服务应用的容器化和部署
- ✅ 理解服务网格和流量管理
- ✅ 建立完整的CI/CD流水线
- ✅ 实施全面的监控告警体系
- ✅ 进行性能优化和成本控制
- ✅ 具备独立运维生产集群的能力

**前置知识**：

- 熟练掌握第1-11章的所有内容
- 具备Linux系统管理经验
- 了解Docker容器技术
- 熟悉至少一门编程语言
- 理解微服务架构理念

**技能等级要求**：

```yaml
初级工程师:
  - 能够按照文档搭建测试集群
  - 会部署简单的无状态应用
  - 了解基本的故障排查方法

中级工程师:
  - 能够搭建生产级高可用集群
  - 掌握有状态应用的部署
  - 会配置CI/CD流水线
  - 能够进行性能调优

高级工程师/架构师:
  - 能够设计大规模集群架构
  - 掌握服务网格和高级特性
  - 会制定技术选型和最佳实践
  - 能够解决复杂的生产问题
```

---

## 本章内容概览

```
第12章: Kubernetes项目实战与最佳实践
├── 12.1 从零搭建生产级Kubernetes集群
│   ├── 集群规划与架构设计
│   ├── 高可用控制平面部署
│   ├── 网络方案选择与配置
│   ├── 存储方案选择与配置
│   └── 安全加固与最佳实践
│
├── 12.2 微服务应用容器化
│   ├── Dockerfile最佳实践
│   ├── 多阶段构建优化
│   ├── 镜像安全扫描
│   └── 私有镜像仓库搭建
│
├── 12.3 应用部署与配置管理
│   ├── Helm Chart编写
│   ├── 多环境配置管理
│   ├── Secret和ConfigMap管理
│   └── 应用版本管理
│
├── 12.4 服务网格与流量管理
│   ├── Istio服务网格部署
│   ├── 灰度发布与金丝雀部署
│   ├── 流量路由与熔断
│   └── 分布式追踪
│
├── 12.5 CI/CD流水线构建
│   ├── GitOps工作流
│   ├── Jenkins/GitLab CI集成
│   ├── 自动化测试
│   └── 自动化部署
│
├── 12.6 监控告警与日志分析
│   ├── Prometheus监控体系
│   ├── Grafana仪表板
│   ├── EFK日志聚合
│   └── 告警规则配置
│
├── 12.7 性能优化与成本控制
│   ├── 资源配额优化
│   ├── HPA/VPA自动扩缩容
│   ├── 集群成本分析
│   └── 性能调优实践
│
└── 12.8 本章总结
    ├── 项目实战回顾
    ├── 生产经验总结
    └── 持续学习路径
```

---

## 12.1 从零搭建生产级Kubernetes集群

搭建一个生产级的Kubernetes集群是一项系统工程，需要综合考虑高可用性、安全性、性能和可维护性。本节将带你从零开始，构建一个符合生产标准的Kubernetes集群。

### 12.1.1 集群规划与架构设计

在开始部署之前，必须进行充分的规划和设计。

#### 集群规模评估

**根据业务需求确定集群规模**：

```yaml
小型集群 (开发/测试环境):
  节点数量: 3-5个节点
  Master节点: 1个
  Worker节点: 2-4个
  适用场景:
    - 开发测试环境
    - 小型应用 (<50个Pod)
    - 学习和实验

中型集群 (生产环境):
  节点数量: 10-50个节点
  Master节点: 3个 (高可用)
  Worker节点: 7-47个
  适用场景:
    - 中小型企业生产环境
    - 应用数量: 50-500个Pod
    - 日均请求: 百万级

大型集群 (大规模生产):
  节点数量: 50-5000个节点
  Master节点: 3-5个 (高可用 + 负载均衡)
  Worker节点: 47-4995个
  适用场景:
    - 大型互联网公司
    - 应用数量: 500+个Pod
    - 日均请求: 千万级以上
```

**资源规划示例**：

```yaml
# 中型生产集群资源规划
集群总览:
  节点总数: 15个
  Master节点: 3个
  Worker节点: 12个

Master节点配置:
  CPU: 4核
  内存: 16GB
  磁盘:
    - 系统盘: 100GB SSD
    - etcd数据盘: 200GB SSD (IOPS >3000)
  网络: 万兆网卡

Worker节点配置:
  CPU: 16核
  内存: 64GB
  磁盘:
    - 系统盘: 100GB SSD
    - 容器存储: 500GB SSD
    - 数据存储: 1TB SSD (根据需求)
  网络: 万兆网卡

网络规划:
  Pod CIDR: 10.244.0.0/16 (支持65536个Pod)
  Service CIDR: 10.96.0.0/12 (支持1048576个Service)
  节点网络: 192.168.1.0/24
```

#### 高可用架构设计

**生产级高可用架构**：

```
                    ┌─────────────────────────────────────┐
                    │      外部负载均衡器 (HAProxy/LVS)    │
                    │      VIP: 192.168.1.100             │
                    └──────────────┬──────────────────────┘
                                   │
                    ┌──────────────┴──────────────────────┐
                    │                                      │
         ┌──────────┴──────────┐            ┌────────────┴─────────┐
         │                     │            │                      │
    ┌────┴────┐          ┌────┴────┐  ┌────┴────┐
    │ Master1 │          │ Master2 │  │ Master3 │
    │         │          │         │  │         │
    │ API     │          │ API     │  │ API     │
    │ Server  │          │ Server  │  │ Server  │
    │         │          │         │  │         │
    │ Sched   │          │ Sched   │  │ Sched   │
    │ uler    │          │ uler    │  │ uler    │
    │         │          │         │  │         │
    │ Ctrl    │          │ Ctrl    │  │ Ctrl    │
    │ Manager │          │ Manager │  │ Manager │
    └────┬────┘          └────┬────┘  └────┬────┘
         │                    │            │
         └────────────────────┴────────────┘
                              │
                    ┌─────────┴─────────┐
                    │   etcd集群 (3节点) │
                    │   - etcd1          │
                    │   - etcd2          │
                    │   - etcd3          │
                    └─────────┬─────────┘
                              │
         ┌────────────────────┴────────────────────┐
         │                                          │
    ┌────┴────┐  ┌──────────┐  ┌──────────┐  ┌────┴────┐
    │ Worker1 │  │ Worker2  │  │ Worker3  │  │ Worker12│
    │         │  │          │  │          │  │         │
    │ kubelet │  │ kubelet  │  │ kubelet  │  │ kubelet │
    │ kube-   │  │ kube-    │  │ kube-    │  │ kube-   │
    │ proxy   │  │ proxy    │  │ proxy    │  │ proxy   │
    │         │  │          │  │          │  │         │
    │ 容器    │  │ 容器     │  │ 容器     │  │ 容器    │
    │ 运行时  │  │ 运行时   │  │ 运行时   │  │ 运行时  │
    └─────────┘  └──────────┘  └──────────┘  └─────────┘
```

**高可用关键点**：

1. **控制平面高可用**：
   - 3个Master节点，奇数个避免脑裂
   - 通过负载均衡器提供统一入口
   - API Server无状态，可水平扩展

2. **etcd集群高可用**：
   - 3个etcd节点，支持1个节点故障
   - 独立部署或与Master节点混部
   - 使用SSD磁盘，确保IOPS >3000

3. **负载均衡器高可用**：
   - HAProxy + Keepalived实现VIP漂移
   - 或使用云厂商的负载均衡服务

#### 节点角色规划

**节点分类与职责**：

```yaml
Master节点 (控制平面):
  组件:
    - kube-apiserver: API服务器
    - kube-scheduler: 调度器
    - kube-controller-manager: 控制器管理器
    - etcd: 分布式键值存储
  特点:
    - 不运行业务Pod (通过污点实现)
    - 需要高可用和高性能
    - 资源需求相对稳定

Worker节点 (工作负载):
  组件:
    - kubelet: 节点代理
    - kube-proxy: 网络代理
    - 容器运行时: containerd/CRI-O
  特点:
    - 运行业务Pod
    - 可根据负载动态扩缩容
    - 资源需求波动较大

边缘节点 (可选):
  用途:
    - 运行Ingress Controller
    - 运行监控组件
    - 运行日志收集组件
  特点:
    - 固定IP或域名
    - 较高的网络带宽
    - 独立的资源配额
```

#### 网络方案选择

**常见CNI插件对比**：

| CNI插件 | 性能 | 功能 | 复杂度 | 适用场景 |
|---------|------|------|--------|----------|
| **Flannel** | ⭐⭐⭐ | ⭐⭐ | ⭐ 简单 | 小型集群、学习环境 |
| **Calico** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ 中等 | 生产环境、需要NetworkPolicy |
| **Cilium** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ 复杂 | 大规模集群、需要eBPF |
| **Weave** | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ 简单 | 中小型集群 |

**推荐方案：Calico**

```yaml
选择理由:
  1. 性能优秀: 基于BGP路由，性能接近原生网络
  2. 功能完善: 支持NetworkPolicy、IPAM、BGP等
  3. 社区活跃: 文档完善，问题容易解决
  4. 生产验证: 大量企业生产环境使用

网络模式选择:
  - IPIP模式: 跨子网通信，兼容性好
  - BGP模式: 同子网通信，性能最佳
  - VXLAN模式: 云环境推荐
```

#### 存储方案选择

**存储类型对比**：

```yaml
本地存储 (Local PV):
  优点:
    - 性能最好 (直接访问本地磁盘)
    - 延迟最低
    - 成本最低
  缺点:
    - 不支持Pod迁移
    - 数据可靠性依赖节点
  适用场景:
    - 缓存数据
    - 临时数据
    - 高性能数据库 (配合副本)

网络存储 (NFS/Ceph/GlusterFS):
  优点:
    - 支持Pod迁移
    - 数据高可用
    - 支持多节点共享
  缺点:
    - 性能较差
    - 网络依赖
    - 运维复杂
  适用场景:
    - 共享文件存储
    - 日志收集
    - 配置文件

云存储 (EBS/云盘):
  优点:
    - 高可用
    - 易于管理
    - 按需扩容
  缺点:
    - 成本较高
    - 性能受限于云厂商
    - 厂商锁定
  适用场景:
    - 云上部署
    - 快速上线
    - 中小规模应用
```

**推荐方案：混合存储**

```yaml
存储分层策略:
  高性能层 (Local PV + Rook-Ceph):
    - 数据库: MySQL/PostgreSQL/MongoDB
    - 缓存: Redis/Memcached
    - 消息队列: Kafka/RabbitMQ
  
  标准层 (Ceph RBD):
    - 应用数据
    - 用户上传文件
    - 日志数据
  
  归档层 (对象存储):
    - 备份数据
    - 历史日志
    - 冷数据
```

### 12.1.2 高可用控制平面部署

使用kubeadm部署高可用Kubernetes集群。

#### 环境准备

**节点信息**：

```bash
# 节点规划
192.168.1.11  k8s-master1  # Master节点1
192.168.1.12  k8s-master2  # Master节点2
192.168.1.13  k8s-master3  # Master节点3
192.168.1.21  k8s-worker1  # Worker节点1
192.168.1.22  k8s-worker2  # Worker节点2
192.168.1.23  k8s-worker3  # Worker节点3
192.168.1.100 k8s-api-vip  # API Server VIP
```

**系统要求**：

```bash
# 操作系统
Ubuntu 22.04 LTS / CentOS 8 Stream / Rocky Linux 8

# 内核版本
>= 4.19

# 系统配置
- 关闭swap
- 关闭SELinux/AppArmor (或配置为Permissive)
- 配置防火墙规则
- 同步时间
```

#### 步骤1：所有节点基础配置

```bash
# 1. 配置主机名和hosts
cat >> /etc/hosts << EOF
192.168.1.11 k8s-master1
192.168.1.12 k8s-master2
192.168.1.13 k8s-master3
192.168.1.21 k8s-worker1
192.168.1.22 k8s-worker2
192.168.1.23 k8s-worker3
192.168.1.100 k8s-api-vip
EOF

# 2. 关闭swap
swapoff -a
sed -i '/swap/d' /etc/fstab

# 3. 加载内核模块
cat > /etc/modules-load.d/k8s.conf << EOF
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

# 4. 配置内核参数
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sysctl --system

# 5. 安装容器运行时 (containerd)
# Ubuntu
apt-get update
apt-get install -y containerd

# CentOS/Rocky
dnf install -y containerd

# 配置containerd
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml

# 修改配置使用systemd cgroup driver
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

# 重启containerd
systemctl restart containerd
systemctl enable containerd

# 6. 安装kubeadm、kubelet、kubectl
# Ubuntu
apt-get update
apt-get install -y apt-transport-https ca-certificates curl
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list
apt-get update
apt-get install -y kubelet=1.28.0-1.1 kubeadm=1.28.0-1.1 kubectl=1.28.0-1.1
apt-mark hold kubelet kubeadm kubectl

# CentOS/Rocky
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.28/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.28/rpm/repodata/repomd.xml.key
EOF

dnf install -y kubelet-1.28.0 kubeadm-1.28.0 kubectl-1.28.0
systemctl enable kubelet
```

#### 步骤2：部署负载均衡器

在所有Master节点上部署HAProxy + Keepalived：

```bash
# 安装HAProxy和Keepalived
apt-get install -y haproxy keepalived

# 配置HAProxy
cat > /etc/haproxy/haproxy.cfg << 'HAPROXY_EOF'
global
    log /dev/log local0
    log /dev/log local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    log     global
    mode    tcp
    option  tcplog
    option  dontlognull
    timeout connect 5000
    timeout client  50000
    timeout server  50000

frontend k8s-api
    bind *:6443
    mode tcp
    option tcplog
    default_backend k8s-api-backend

backend k8s-api-backend
    mode tcp
    option tcp-check
    balance roundrobin
    server k8s-master1 192.168.1.11:6443 check fall 3 rise 2
    server k8s-master2 192.168.1.12:6443 check fall 3 rise 2
    server k8s-master3 192.168.1.13:6443 check fall 3 rise 2
HAPROXY_EOF

# 配置Keepalived (Master1)
cat > /etc/keepalived/keepalived.conf << 'KEEPALIVED_EOF'
vrrp_script check_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
    weight 2
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass k8s_ha_pass
    }
    virtual_ipaddress {
        192.168.1.100
    }
    track_script {
        check_haproxy
    }
}
KEEPALIVED_EOF

# Master2和Master3的priority分别设置为99和98

# 启动服务
systemctl restart haproxy
systemctl enable haproxy
systemctl restart keepalived
systemctl enable keepalived
```

#### 步骤3：初始化第一个Master节点

```bash
# 在k8s-master1上执行

# 创建kubeadm配置文件
cat > kubeadm-config.yaml << 'KUBEADM_EOF'
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: "192.168.1.100:6443"
networking:
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"
apiServer:
  certSANs:
  - "192.168.1.100"
  - "k8s-api-vip"
  - "192.168.1.11"
  - "192.168.1.12"
  - "192.168.1.13"
  extraArgs:
    authorization-mode: "Node,RBAC"
etcd:
  local:
    dataDir: /var/lib/etcd
    extraArgs:
      listen-metrics-urls: "http://0.0.0.0:2381"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: systemd
KUBEADM_EOF

# 初始化集群
kubeadm init --config=kubeadm-config.yaml --upload-certs

# 配置kubectl
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# 记录输出的join命令，后续需要使用
```

#### 步骤4：加入其他Master节点

```bash
# 在k8s-master2和k8s-master3上执行
# 使用kubeadm init输出的join命令

kubeadm join 192.168.1.100:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash> \
    --control-plane --certificate-key <certificate-key>

# 配置kubectl
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
```

#### 步骤5：加入Worker节点

```bash
# 在所有Worker节点上执行

kubeadm join 192.168.1.100:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash>
```

#### 步骤6：安装网络插件

```bash
# 在任意Master节点上执行

# 安装Calico
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml

# 等待所有Pod运行
kubectl get pods -n kube-system -w

# 验证节点状态
kubectl get nodes
```

#### 步骤7：验证集群

```bash
# 1. 检查节点状态
kubectl get nodes
# 所有节点应该是Ready状态

# 2. 检查组件状态
kubectl get pods -n kube-system

# 3. 检查etcd集群
kubectl -n kube-system exec -it etcd-k8s-master1 -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list

# 4. 测试高可用
# 停止master1的kubelet
systemctl stop kubelet

# 在其他节点验证API Server仍然可用
kubectl get nodes

# 恢复master1
systemctl start kubelet
```


### 12.1.3 网络和存储配置

#### Calico网络配置优化

```bash
# 下载Calico配置文件
curl -O https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml

# 修改配置
# 1. 修改Pod CIDR (如果与kubeadm配置不同)
# 2. 启用IP-in-IP或VXLAN模式
# 3. 配置MTU大小

# 应用配置
kubectl apply -f calico.yaml

# 验证Calico状态
kubectl get pods -n kube-system -l k8s-app=calico-node
kubectl get pods -n kube-system -l k8s-app=calico-kube-controllers

# 查看Calico节点状态
kubectl exec -n kube-system calico-node-xxxxx -- calicoctl node status
```

**Calico NetworkPolicy示例**：

```yaml
# 限制Pod只能被特定命名空间访问
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-frontend
  namespace: backend
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend
    ports:
    - protocol: TCP
      port: 8080
```

#### 部署Rook-Ceph存储

```bash
# 1. 克隆Rook仓库
git clone --single-branch --branch v1.12.0 https://github.com/rook/rook.git
cd rook/deploy/examples

# 2. 部署Rook Operator
kubectl apply -f crds.yaml
kubectl apply -f common.yaml
kubectl apply -f operator.yaml

# 3. 验证Operator运行
kubectl -n rook-ceph get pod

# 4. 创建Ceph集群
kubectl apply -f cluster.yaml

# 5. 等待集群就绪
kubectl -n rook-ceph get cephcluster

# 6. 创建StorageClass
kubectl apply -f csi/rbd/storageclass.yaml

# 7. 验证StorageClass
kubectl get storageclass
```

**Ceph存储配置示例**：

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v17.2.6
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    count: 2
    allowMultiplePerNode: false
  dashboard:
    enabled: true
    ssl: true
  storage:
    useAllNodes: true
    useAllDevices: false
    deviceFilter: "^sd[b-z]"
    config:
      osdsPerDevice: "1"
```

### 12.1.4 安全加固与最佳实践

#### RBAC权限配置

```yaml
# 创建只读用户
apiVersion: v1
kind: ServiceAccount
metadata:
  name: readonly-user
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: readonly-role
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: readonly-binding
subjects:
- kind: ServiceAccount
  name: readonly-user
  namespace: default
roleRef:
  kind: ClusterRole
  name: readonly-role
  apiGroup: rbac.authorization.k8s.io
```

#### Pod Security Standards

```yaml
# 在命名空间级别启用Pod Security
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

#### 审计日志配置

```yaml
# API Server审计策略
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: Metadata
  resources:
  - group: ""
    resources: ["secrets", "configmaps"]
- level: RequestResponse
  resources:
  - group: ""
    resources: ["pods"]
  verbs: ["create", "delete", "update", "patch"]
- level: Metadata
  omitStages:
  - RequestReceived
```

#### 证书管理

```bash
# 检查证书有效期
kubeadm certs check-expiration

# 续期所有证书
kubeadm certs renew all

# 重启控制平面组件
systemctl restart kubelet
```

#### 安全加固检查清单

```yaml
基础安全:
  - [ ] 关闭不必要的端口
  - [ ] 配置防火墙规则
  - [ ] 禁用root SSH登录
  - [ ] 使用密钥认证
  - [ ] 定期更新系统补丁

Kubernetes安全:
  - [ ] 启用RBAC
  - [ ] 配置Pod Security Standards
  - [ ] 启用审计日志
  - [ ] 配置NetworkPolicy
  - [ ] 使用Secret管理敏感信息
  - [ ] 定期轮换证书
  - [ ] 限制API Server访问
  - [ ] 配置资源配额

容器安全:
  - [ ] 使用非root用户运行容器
  - [ ] 设置只读根文件系统
  - [ ] 禁用特权容器
  - [ ] 扫描镜像漏洞
  - [ ] 使用私有镜像仓库
  - [ ] 配置镜像拉取策略

网络安全:
  - [ ] 启用TLS加密
  - [ ] 配置Ingress TLS
  - [ ] 使用NetworkPolicy隔离
  - [ ] 限制出站流量
  - [ ] 配置DNS策略

数据安全:
  - [ ] 加密etcd数据
  - [ ] 加密Secret
  - [ ] 配置存储加密
  - [ ] 定期备份
  - [ ] 测试恢复流程
```

---

**本节小结**

在12.1节中，我们完成了生产级Kubernetes集群的搭建：

1. **集群规划**：根据业务需求确定集群规模，设计高可用架构，选择合适的网络和存储方案
2. **高可用部署**：使用kubeadm部署3节点Master集群，配置HAProxy+Keepalived实现负载均衡和VIP漂移
3. **网络配置**：部署Calico网络插件，配置NetworkPolicy实现网络隔离
4. **存储配置**：部署Rook-Ceph提供分布式存储，支持动态卷供应
5. **安全加固**：配置RBAC、Pod Security Standards、审计日志，建立完整的安全体系

通过本节的实践，你已经掌握了从零搭建生产级Kubernetes集群的完整流程。接下来，我们将在这个集群上部署微服务应用！

---


## 12.2 微服务应用容器化

在搭建好Kubernetes集群后，下一步是将微服务应用容器化。本节将以电商系统的四个核心服务为例，讲解如何编写高质量的Dockerfile，优化镜像大小和构建速度，并确保镜像安全。

### 12.2.1 Dockerfile最佳实践

#### 基础镜像选择

**镜像选择原则**：

```yaml
生产环境镜像选择:
  优先级1 - 官方镜像:
    - 来源可信，定期更新
    - 安全漏洞及时修复
    - 社区支持完善
  
  优先级2 - Alpine镜像:
    - 体积小 (5MB vs 100MB+)
    - 安全性高 (攻击面小)
    - 适合Go/Node.js等静态编译语言
  
  优先级3 - Distroless镜像:
    - 只包含应用和运行时依赖
    - 无shell，安全性最高
    - 适合Java/Python等
  
  避免使用:
    - latest标签 (不可追溯)
    - 过大的基础镜像 (ubuntu:latest 77MB)
    - 未维护的镜像
```

**常用基础镜像对比**：

| 镜像 | 大小 | 安全性 | 适用场景 |
|------|------|--------|----------|
| alpine:3.18 | 7MB | ⭐⭐⭐⭐⭐ | Go、Node.js、Python |
| debian:12-slim | 74MB | ⭐⭐⭐⭐ | 需要完整工具链 |
| ubuntu:22.04 | 77MB | ⭐⭐⭐ | 开发测试环境 |
| distroless/base | 20MB | ⭐⭐⭐⭐⭐ | 生产环境 |
| scratch | 0MB | ⭐⭐⭐⭐⭐ | 静态编译的Go程序 |

#### Go服务Dockerfile示例

**用户服务 (Go语言)**：

```dockerfile
# 不推荐的写法 ❌
FROM golang:1.21
WORKDIR /app
COPY . .
RUN go build -o user-service
CMD ["./user-service"]

# 问题:
# 1. 镜像过大 (golang:1.21 约1GB)
# 2. 包含编译工具和源代码
# 3. 安全风险高
```

**推荐的写法 ✅**：

```dockerfile
# 多阶段构建 - 用户服务
FROM golang:1.21-alpine AS builder

# 设置工作目录
WORKDIR /build

# 复制go mod文件并下载依赖 (利用缓存)
COPY go.mod go.sum ./
RUN go mod download

# 复制源代码
COPY . .

# 编译 - 静态链接，禁用CGO
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build     -ldflags="-w -s"     -o user-service     ./cmd/user-service

# 最终镜像
FROM alpine:3.18

# 安装CA证书 (HTTPS请求需要)
RUN apk --no-cache add ca-certificates tzdata

# 创建非root用户
RUN addgroup -g 1000 appuser &&     adduser -D -u 1000 -G appuser appuser

# 设置工作目录
WORKDIR /app

# 从构建阶段复制二进制文件
COPY --from=builder /build/user-service .

# 复制配置文件
COPY configs/config.yaml ./configs/

# 切换到非root用户
USER appuser

# 暴露端口
EXPOSE 8080

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3     CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1

# 启动命令
CMD ["./user-service"]
```

**Dockerfile最佳实践要点**：

```yaml
1. 使用多阶段构建:
   - 构建阶段: 包含编译工具
   - 运行阶段: 只包含运行时依赖
   - 镜像大小: 从1GB降到20MB

2. 优化层缓存:
   - 先复制依赖文件 (go.mod)
   - 再复制源代码
   - 依赖不变时可复用缓存

3. 最小化镜像:
   - 使用alpine或distroless
   - 删除不必要的文件
   - 使用.dockerignore

4. 安全加固:
   - 使用非root用户运行
   - 不包含shell (可选)
   - 定期更新基础镜像

5. 添加元数据:
   - LABEL维护者信息
   - LABEL版本信息
   - HEALTHCHECK健康检查
```

#### Java服务Dockerfile示例

**商品服务 (Java Spring Boot)**：

```dockerfile
# 多阶段构建 - 商品服务
FROM maven:3.9-eclipse-temurin-17 AS builder

WORKDIR /build

# 复制pom.xml并下载依赖
COPY pom.xml .
RUN mvn dependency:go-offline

# 复制源代码并构建
COPY src ./src
RUN mvn clean package -DskipTests

# 最终镜像 - 使用JRE而非JDK
FROM eclipse-temurin:17-jre-alpine

# 安装必要工具
RUN apk --no-cache add curl

# 创建非root用户
RUN addgroup -g 1000 appuser &&     adduser -D -u 1000 -G appuser appuser

WORKDIR /app

# 从构建阶段复制jar包
COPY --from=builder /build/target/product-service-*.jar app.jar

# 切换用户
USER appuser

# 暴露端口
EXPOSE 8080

# JVM参数优化
ENV JAVA_OPTS="-Xms512m -Xmx1024m -XX:+UseG1GC -XX:MaxGCPauseMillis=200"

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=40s --retries=3     CMD curl -f http://localhost:8080/actuator/health || exit 1

# 启动命令
ENTRYPOINT ["sh", "-c", "java $JAVA_OPTS -jar app.jar"]
```

#### Python服务Dockerfile示例

**订单服务 (Python Flask)**：

```dockerfile
# 多阶段构建 - 订单服务
FROM python:3.11-slim AS builder

WORKDIR /build

# 安装构建依赖
RUN apt-get update &&     apt-get install -y --no-install-recommends gcc &&     rm -rf /var/lib/apt/lists/*

# 复制requirements并安装依赖
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# 最终镜像
FROM python:3.11-slim

# 安装运行时依赖
RUN apt-get update &&     apt-get install -y --no-install-recommends curl &&     rm -rf /var/lib/apt/lists/*

# 创建非root用户
RUN useradd -m -u 1000 appuser

WORKDIR /app

# 从构建阶段复制Python包
COPY --from=builder /root/.local /home/appuser/.local

# 复制应用代码
COPY --chown=appuser:appuser . .

# 切换用户
USER appuser

# 设置Python路径
ENV PATH=/home/appuser/.local/bin:$PATH
ENV PYTHONUNBUFFERED=1

# 暴露端口
EXPOSE 5000

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3     CMD curl -f http://localhost:5000/health || exit 1

# 启动命令 - 使用gunicorn
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "4", "app:app"]
```

#### Node.js服务Dockerfile示例

**支付服务 (Node.js Express)**：

```dockerfile
# 多阶段构建 - 支付服务
FROM node:20-alpine AS builder

WORKDIR /build

# 复制package文件并安装依赖
COPY package*.json ./
RUN npm ci --only=production

# 最终镜像
FROM node:20-alpine

# 安装dumb-init (正确处理信号)
RUN apk --no-cache add dumb-init

# 创建非root用户
RUN addgroup -g 1000 appuser &&     adduser -D -u 1000 -G appuser appuser

WORKDIR /app

# 从构建阶段复制node_modules
COPY --from=builder /build/node_modules ./node_modules

# 复制应用代码
COPY --chown=appuser:appuser . .

# 切换用户
USER appuser

# 暴露端口
EXPOSE 3000

# 环境变量
ENV NODE_ENV=production

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3     CMD node healthcheck.js || exit 1

# 使用dumb-init启动
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "server.js"]
```

#### .dockerignore文件

```bash
# .dockerignore - 减少构建上下文大小

# Git相关
.git
.gitignore
.gitattributes

# 文档
README.md
CHANGELOG.md
docs/
*.md

# 测试文件
*_test.go
test/
tests/
__tests__/
*.test.js

# 开发工具配置
.vscode/
.idea/
*.swp
*.swo

# 依赖目录 (会在镜像中重新安装)
node_modules/
vendor/
target/

# 构建产物
dist/
build/
*.exe
*.dll
*.so

# 日志和临时文件
*.log
tmp/
temp/
.DS_Store

# 环境变量文件
.env
.env.local
*.pem
*.key
```


### 12.2.2 多阶段构建优化

多阶段构建是Docker的强大特性，可以显著减小镜像大小并提高安全性。

#### 构建缓存优化

**利用BuildKit加速构建**：

```bash
# 启用BuildKit
export DOCKER_BUILDKIT=1

# 使用缓存挂载加速依赖下载
docker build --build-arg BUILDKIT_INLINE_CACHE=1 -t myapp:latest .

# 使用外部缓存
docker build --cache-from myapp:latest -t myapp:v2 .
```

**Go服务优化示例**：

```dockerfile
# 优化的多阶段构建
FROM golang:1.21-alpine AS base
WORKDIR /build
RUN apk add --no-cache git ca-certificates tzdata

# 依赖阶段 - 单独缓存
FROM base AS dependencies
COPY go.mod go.sum ./
RUN --mount=type=cache,target=/go/pkg/mod     go mod download

# 构建阶段
FROM dependencies AS builder
COPY . .
RUN --mount=type=cache,target=/go/pkg/mod     --mount=type=cache,target=/root/.cache/go-build     CGO_ENABLED=0 go build -ldflags="-w -s" -o app .

# 测试阶段 (可选)
FROM builder AS tester
RUN go test -v ./...

# 最终镜像
FROM scratch
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo
COPY --from=builder /build/app /app
ENTRYPOINT ["/app"]
```

**构建命令**：

```bash
# 构建并跳过测试阶段
docker build --target builder -t myapp:latest .

# 构建并运行测试
docker build --target tester -t myapp:test .

# 构建最终镜像
docker build -t myapp:latest .
```

#### 镜像大小对比

**优化前后对比**：

```yaml
用户服务 (Go):
  优化前: 1.2GB (golang:1.21)
  优化后: 15MB (scratch + 静态编译)
  减少: 98.75%

商品服务 (Java):
  优化前: 450MB (openjdk:17)
  优化后: 180MB (eclipse-temurin:17-jre-alpine)
  减少: 60%

订单服务 (Python):
  优化前: 950MB (python:3.11)
  优化后: 120MB (python:3.11-slim + 多阶段)
  减少: 87.4%

支付服务 (Node.js):
  优化前: 1.1GB (node:20)
  优化后: 85MB (node:20-alpine)
  减少: 92.3%
```

#### 并行构建优化

**使用BuildKit并行构建**：

```dockerfile
# syntax=docker/dockerfile:1.4

FROM golang:1.21-alpine AS builder-user
WORKDIR /build
COPY services/user/ .
RUN go build -o user-service .

FROM golang:1.21-alpine AS builder-product
WORKDIR /build
COPY services/product/ .
RUN go build -o product-service .

# 并行构建多个服务
FROM alpine:3.18 AS user-service
COPY --from=builder-user /build/user-service /app/
ENTRYPOINT ["/app/user-service"]

FROM alpine:3.18 AS product-service
COPY --from=builder-product /build/product-service /app/
ENTRYPOINT ["/app/product-service"]
```

#### 构建参数和环境变量

```dockerfile
# 使用ARG传递构建参数
FROM golang:1.21-alpine AS builder

# 构建参数
ARG VERSION=dev
ARG BUILD_TIME
ARG GIT_COMMIT

WORKDIR /build
COPY . .

# 编译时注入版本信息
RUN CGO_ENABLED=0 go build     -ldflags="-X main.Version=${VERSION}               -X main.BuildTime=${BUILD_TIME}               -X main.GitCommit=${GIT_COMMIT}               -w -s"     -o app .

FROM alpine:3.18
COPY --from=builder /build/app /app

# 运行时环境变量
ENV APP_ENV=production     LOG_LEVEL=info

ENTRYPOINT ["/app"]
```

**构建命令**：

```bash
# 传递构建参数
docker build   --build-arg VERSION=v1.2.3   --build-arg BUILD_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")   --build-arg GIT_COMMIT=$(git rev-parse --short HEAD)   -t myapp:v1.2.3 .
```

### 12.2.3 镜像安全扫描

确保容器镜像的安全性是生产环境的关键要求。

#### 使用Trivy扫描镜像

**安装Trivy**：

```bash
# Ubuntu/Debian
wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list
sudo apt-get update
sudo apt-get install trivy

# macOS
brew install trivy

# 使用Docker运行
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock   aquasec/trivy image myapp:latest
```

**扫描镜像**：

```bash
# 扫描本地镜像
trivy image myapp:latest

# 只显示高危和严重漏洞
trivy image --severity HIGH,CRITICAL myapp:latest

# 输出JSON格式
trivy image --format json --output result.json myapp:latest

# 扫描并在发现漏洞时失败
trivy image --exit-code 1 --severity CRITICAL myapp:latest
```

**扫描结果示例**：

```
myapp:latest (alpine 3.18.0)
============================
Total: 2 (HIGH: 1, CRITICAL: 1)

┌───────────────┬────────────────┬──────────┬───────────────────┬───────────────┬────────────────────────────────────┐
│   Library     │ Vulnerability  │ Severity │ Installed Version │ Fixed Version │              Title                 │
├───────────────┼────────────────┼──────────┼───────────────────┼───────────────┼────────────────────────────────────┤
│ openssl       │ CVE-2023-12345 │ CRITICAL │ 3.1.0-r0          │ 3.1.1-r0      │ OpenSSL: Buffer overflow           │
│ libcrypto3    │ CVE-2023-12346 │ HIGH     │ 3.1.0-r0          │ 3.1.1-r0      │ OpenSSL: Memory leak               │
└───────────────┴────────────────┴──────────┴───────────────────┴───────────────┴────────────────────────────────────┘
```

#### 修复漏洞

```dockerfile
# 修复前
FROM alpine:3.18.0
RUN apk add --no-cache openssl

# 修复后 - 更新到最新版本
FROM alpine:3.18
RUN apk add --no-cache --upgrade openssl libcrypto3

# 或使用更新的基础镜像
FROM alpine:3.19
```

#### 集成到CI/CD

**GitLab CI示例**：

```yaml
# .gitlab-ci.yml
stages:
  - build
  - scan
  - deploy

build:
  stage: build
  script:
    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .
    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA

security-scan:
  stage: scan
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 0 --severity LOW,MEDIUM $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
    - trivy image --exit-code 1 --severity HIGH,CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  allow_failure: false

deploy:
  stage: deploy
  script:
    - kubectl set image deployment/myapp myapp=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  only:
    - main
```

#### 使用Cosign签名镜像

**安装Cosign**：

```bash
# 安装cosign
wget https://github.com/sigstore/cosign/releases/download/v2.2.0/cosign-linux-amd64
chmod +x cosign-linux-amd64
sudo mv cosign-linux-amd64 /usr/local/bin/cosign

# 生成密钥对
cosign generate-key-pair

# 签名镜像
cosign sign --key cosign.key myregistry.com/myapp:v1.0.0

# 验证签名
cosign verify --key cosign.pub myregistry.com/myapp:v1.0.0
```

#### 镜像安全最佳实践

```yaml
基础镜像安全:
  - [ ] 使用官方或可信镜像
  - [ ] 使用特定版本标签，避免latest
  - [ ] 定期更新基础镜像
  - [ ] 使用最小化镜像 (alpine/distroless)

构建安全:
  - [ ] 不在镜像中包含敏感信息
  - [ ] 使用.dockerignore排除敏感文件
  - [ ] 使用多阶段构建减少攻击面
  - [ ] 不在镜像中包含构建工具

运行时安全:
  - [ ] 使用非root用户运行
  - [ ] 设置只读根文件系统
  - [ ] 限制容器权限 (capabilities)
  - [ ] 使用安全上下文 (SecurityContext)

漏洞管理:
  - [ ] 定期扫描镜像漏洞
  - [ ] 及时修复高危漏洞
  - [ ] 建立漏洞响应流程
  - [ ] 使用镜像签名验证完整性
```


### 12.2.4 私有镜像仓库搭建

生产环境通常需要私有镜像仓库来管理内部镜像。

#### 使用Harbor搭建企业级镜像仓库

**Harbor特性**：

```yaml
核心功能:
  - 镜像存储和分发
  - 镜像复制 (多地域同步)
  - 漏洞扫描 (集成Trivy)
  - 镜像签名 (Notary)
  - RBAC权限管理
  - 审计日志
  - Webhook通知

企业特性:
  - 高可用部署
  - LDAP/AD集成
  - 配额管理
  - 垃圾回收
  - 镜像代理缓存
```

**使用Helm部署Harbor**：

```bash
# 添加Harbor Helm仓库
helm repo add harbor https://helm.goharbor.io
helm repo update

# 创建命名空间
kubectl create namespace harbor

# 创建values.yaml配置文件
cat > harbor-values.yaml << 'EOF'
expose:
  type: ingress
  tls:
    enabled: true
    certSource: secret
    secret:
      secretName: harbor-tls
  ingress:
    hosts:
      core: harbor.example.com
    className: nginx

externalURL: https://harbor.example.com

persistence:
  enabled: true
  resourcePolicy: "keep"
  persistentVolumeClaim:
    registry:
      storageClass: "rook-ceph-block"
      size: 500Gi
    database:
      storageClass: "rook-ceph-block"
      size: 10Gi
    redis:
      storageClass: "rook-ceph-block"
      size: 5Gi

harborAdminPassword: "ChangeMe123!"

database:
  type: internal

redis:
  type: internal

trivy:
  enabled: true

notary:
  enabled: true

metrics:
  enabled: true
  serviceMonitor:
    enabled: true
EOF

# 部署Harbor
helm install harbor harbor/harbor   -f harbor-values.yaml   -n harbor

# 等待所有Pod运行
kubectl get pods -n harbor -w

# 获取Harbor访问地址
echo "Harbor URL: https://harbor.example.com"
echo "Username: admin"
echo "Password: ChangeMe123!"
```

#### 配置Docker使用私有仓库

```bash
# 1. 登录Harbor
docker login harbor.example.com
# 输入用户名和密码

# 2. 标记镜像
docker tag myapp:latest harbor.example.com/library/myapp:latest

# 3. 推送镜像
docker push harbor.example.com/library/myapp:latest

# 4. 拉取镜像
docker pull harbor.example.com/library/myapp:latest
```

#### 配置Kubernetes使用私有仓库

**创建Docker Registry Secret**：

```bash
# 方法1: 使用kubectl create secret
kubectl create secret docker-registry harbor-secret   --docker-server=harbor.example.com   --docker-username=admin   --docker-password=ChangeMe123!   --docker-email=admin@example.com   -n default

# 方法2: 使用现有的docker config
kubectl create secret generic harbor-secret   --from-file=.dockerconfigjson=$HOME/.docker/config.json   --type=kubernetes.io/dockerconfigjson   -n default
```

**在Pod中使用Secret**：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: myapp
    image: harbor.example.com/library/myapp:latest
  imagePullSecrets:
  - name: harbor-secret
```

**在ServiceAccount中配置**：

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: default
  namespace: default
imagePullSecrets:
- name: harbor-secret
```

#### Harbor项目和用户管理

**创建项目**：

```bash
# 使用Harbor API创建项目
curl -X POST "https://harbor.example.com/api/v2.0/projects"   -H "Content-Type: application/json"   -u "admin:ChangeMe123!"   -d '{
    "project_name": "ecommerce",
    "public": false,
    "metadata": {
      "auto_scan": "true",
      "severity": "high"
    }
  }'
```

**配置镜像复制**：

```yaml
# 配置多地域镜像复制
复制规则:
  源仓库: harbor-beijing.example.com
  目标仓库: harbor-shanghai.example.com
  触发方式: 
    - 手动触发
    - 定时触发 (每天凌晨2点)
    - 事件触发 (镜像推送时)
  过滤规则:
    - 项目: ecommerce/*
    - 标签: v*
```

#### 镜像清理策略

```yaml
# Harbor垃圾回收配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: harbor-gc-config
  namespace: harbor
data:
  config.yaml: |
    # 保留策略
    retention:
      # 保留最近30天的镜像
      - selector:
          tagSelectors:
            - kind: doublestar
              decoration: matches
              pattern: "**"
        action: retain
        params:
          latestPushedK: 30
      
      # 删除未打标签的镜像
      - selector:
          untagged: true
        action: delete
    
    # 定时清理
    schedule:
      cron: "0 2 * * *"  # 每天凌晨2点
```

#### 镜像构建和推送流程

**完整的CI/CD流程**：

```bash
#!/bin/bash
# build-and-push.sh

set -e

# 配置
HARBOR_URL="harbor.example.com"
PROJECT="ecommerce"
IMAGE_NAME="user-service"
VERSION="${CI_COMMIT_TAG:-latest}"
FULL_IMAGE="${HARBOR_URL}/${PROJECT}/${IMAGE_NAME}:${VERSION}"

echo "Building image: ${FULL_IMAGE}"

# 1. 构建镜像
docker build   --build-arg VERSION=${VERSION}   --build-arg BUILD_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")   --build-arg GIT_COMMIT=$(git rev-parse --short HEAD)   -t ${FULL_IMAGE}   .

# 2. 扫描镜像
echo "Scanning image for vulnerabilities..."
trivy image --exit-code 1 --severity CRITICAL ${FULL_IMAGE}

# 3. 推送镜像
echo "Pushing image to Harbor..."
docker push ${FULL_IMAGE}

# 4. 签名镜像 (可选)
if [ -f "cosign.key" ]; then
  echo "Signing image..."
  cosign sign --key cosign.key ${FULL_IMAGE}
fi

# 5. 触发部署
echo "Image pushed successfully: ${FULL_IMAGE}"
```

---

**本节小结**

在12.2节中，我们完成了微服务应用的容器化：

1. **Dockerfile最佳实践**：
   - 选择合适的基础镜像（Alpine、Distroless）
   - 编写了Go、Java、Python、Node.js四种语言的优化Dockerfile
   - 使用非root用户、健康检查等安全措施
   - 配置.dockerignore减少构建上下文

2. **多阶段构建优化**：
   - 利用BuildKit加速构建和缓存
   - 镜像大小平均减少85%以上
   - 使用构建参数注入版本信息
   - 并行构建多个服务

3. **镜像安全扫描**：
   - 使用Trivy扫描镜像漏洞
   - 集成到CI/CD流程
   - 使用Cosign签名镜像
   - 建立漏洞修复流程

4. **私有镜像仓库**：
   - 使用Harbor搭建企业级镜像仓库
   - 配置镜像复制和清理策略
   - 集成漏洞扫描和镜像签名
   - 建立完整的镜像管理流程

通过本节的实践，你已经掌握了如何将微服务应用容器化，并建立了完整的镜像构建、扫描、存储和分发体系。接下来，我们将学习如何使用Helm部署这些容器化应用！

---

