# 第12章 Kubernetes项目实战与最佳实践

## 本章概述

经过前11章的系统学习，你已经掌握了Kubernetes的核心概念、资源管理、网络存储、安全机制、监控可观测性、高级特性以及故障排查等完整知识体系。本章将通过真实的项目实战，将这些知识融会贯通，帮助你构建生产级的Kubernetes应用。

**为什么需要项目实战？**

理论知识和实际应用之间存在巨大鸿沟：

📚 **理论学习阶段**：
- 理解Pod、Service、Deployment等概念
- 知道如何编写YAML配置文件
- 了解网络、存储、安全的工作原理
- 掌握监控和故障排查方法

🚀 **生产实战阶段**：
- 如何设计高可用的应用架构？
- 如何处理有状态应用的部署？
- 如何实现零停机滚动更新？
- 如何保证应用的安全性和性能？
- 如何建立完整的CI/CD流程？
- 如何应对突发流量和故障？

**本章实战项目**：

我们将通过一个完整的微服务电商系统，涵盖Kubernetes生产实战的各个方面：

```
电商系统架构:
┌─────────────────────────────────────────────────────────────┐
│                        用户层                                 │
│  Web浏览器  ←→  移动App  ←→  第三方系统                      │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                     Ingress层                                 │
│  Nginx Ingress Controller + TLS证书 + 限流                   │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                    API网关层                                  │
│  Kong/APISIX - 认证、鉴权、限流、熔断                        │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                   微服务层                                    │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  │
│  │用户服务  │  │商品服务  │  │订单服务  │  │支付服务  │  │
│  │(Go)      │  │(Java)    │  │(Python)  │  │(Node.js) │  │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘  │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                    数据层                                     │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  │
│  │MySQL     │  │Redis     │  │MongoDB   │  │RabbitMQ  │  │
│  │(主从)    │  │(集群)    │  │(副本集)  │  │(集群)    │  │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘  │
└─────────────────────────────────────────────────────────────┘
```

**本章主要内容**：

1. **从零搭建生产级Kubernetes集群** (12.1)
   - 集群规划与架构设计
   - 高可用控制平面部署
   - 网络和存储方案选择
   - 安全加固与最佳实践

2. **微服务应用容器化** (12.2)
   - Dockerfile最佳实践
   - 多阶段构建优化
   - 镜像安全扫描
   - 私有镜像仓库搭建

3. **应用部署与配置管理** (12.3)
   - Helm Chart编写
   - 多环境配置管理
   - Secret和ConfigMap管理
   - 应用版本管理

4. **服务网格与流量管理** (12.4)
   - Istio服务网格部署
   - 灰度发布与金丝雀部署
   - 流量路由与熔断
   - 分布式追踪

5. **CI/CD流水线构建** (12.5)
   - GitOps工作流
   - Jenkins/GitLab CI集成
   - 自动化测试
   - 自动化部署

6. **监控告警与日志分析** (12.6)
   - Prometheus监控体系
   - Grafana仪表板
   - EFK日志聚合
   - 告警规则配置

7. **性能优化与成本控制** (12.7)
   - 资源配额优化
   - HPA/VPA自动扩缩容
   - 集群成本分析
   - 性能调优实践

8. **本章总结** (12.8)
   - 项目实战回顾
   - 生产经验总结
   - 持续学习路径

**学习目标**：

- ✅ 能够从零搭建生产级Kubernetes集群
- ✅ 掌握微服务应用的容器化和部署
- ✅ 理解服务网格和流量管理
- ✅ 建立完整的CI/CD流水线
- ✅ 实施全面的监控告警体系
- ✅ 进行性能优化和成本控制
- ✅ 具备独立运维生产集群的能力

**前置知识**：

- 熟练掌握第1-11章的所有内容
- 具备Linux系统管理经验
- 了解Docker容器技术
- 熟悉至少一门编程语言
- 理解微服务架构理念

**技能等级要求**：

```yaml
初级工程师:
  - 能够按照文档搭建测试集群
  - 会部署简单的无状态应用
  - 了解基本的故障排查方法

中级工程师:
  - 能够搭建生产级高可用集群
  - 掌握有状态应用的部署
  - 会配置CI/CD流水线
  - 能够进行性能调优

高级工程师/架构师:
  - 能够设计大规模集群架构
  - 掌握服务网格和高级特性
  - 会制定技术选型和最佳实践
  - 能够解决复杂的生产问题
```

---

## 本章内容概览

```
第12章: Kubernetes项目实战与最佳实践
├── 12.1 从零搭建生产级Kubernetes集群
│   ├── 集群规划与架构设计
│   ├── 高可用控制平面部署
│   ├── 网络方案选择与配置
│   ├── 存储方案选择与配置
│   └── 安全加固与最佳实践
│
├── 12.2 微服务应用容器化
│   ├── Dockerfile最佳实践
│   ├── 多阶段构建优化
│   ├── 镜像安全扫描
│   └── 私有镜像仓库搭建
│
├── 12.3 应用部署与配置管理
│   ├── Helm Chart编写
│   ├── 多环境配置管理
│   ├── Secret和ConfigMap管理
│   └── 应用版本管理
│
├── 12.4 服务网格与流量管理
│   ├── Istio服务网格部署
│   ├── 灰度发布与金丝雀部署
│   ├── 流量路由与熔断
│   └── 分布式追踪
│
├── 12.5 CI/CD流水线构建
│   ├── GitOps工作流
│   ├── Jenkins/GitLab CI集成
│   ├── 自动化测试
│   └── 自动化部署
│
├── 12.6 监控告警与日志分析
│   ├── Prometheus监控体系
│   ├── Grafana仪表板
│   ├── EFK日志聚合
│   └── 告警规则配置
│
├── 12.7 性能优化与成本控制
│   ├── 资源配额优化
│   ├── HPA/VPA自动扩缩容
│   ├── 集群成本分析
│   └── 性能调优实践
│
└── 12.8 本章总结
    ├── 项目实战回顾
    ├── 生产经验总结
    └── 持续学习路径
```

---

## 12.1 从零搭建生产级Kubernetes集群

搭建一个生产级的Kubernetes集群是一项系统工程，需要综合考虑高可用性、安全性、性能和可维护性。本节将带你从零开始，构建一个符合生产标准的Kubernetes集群。

### 12.1.1 集群规划与架构设计

在开始部署之前，必须进行充分的规划和设计。

#### 集群规模评估

**根据业务需求确定集群规模**：

```yaml
小型集群 (开发/测试环境):
  节点数量: 3-5个节点
  Master节点: 1个
  Worker节点: 2-4个
  适用场景:
    - 开发测试环境
    - 小型应用 (<50个Pod)
    - 学习和实验

中型集群 (生产环境):
  节点数量: 10-50个节点
  Master节点: 3个 (高可用)
  Worker节点: 7-47个
  适用场景:
    - 中小型企业生产环境
    - 应用数量: 50-500个Pod
    - 日均请求: 百万级

大型集群 (大规模生产):
  节点数量: 50-5000个节点
  Master节点: 3-5个 (高可用 + 负载均衡)
  Worker节点: 47-4995个
  适用场景:
    - 大型互联网公司
    - 应用数量: 500+个Pod
    - 日均请求: 千万级以上
```

**资源规划示例**：

```yaml
# 中型生产集群资源规划
集群总览:
  节点总数: 15个
  Master节点: 3个
  Worker节点: 12个

Master节点配置:
  CPU: 4核
  内存: 16GB
  磁盘:
    - 系统盘: 100GB SSD
    - etcd数据盘: 200GB SSD (IOPS >3000)
  网络: 万兆网卡

Worker节点配置:
  CPU: 16核
  内存: 64GB
  磁盘:
    - 系统盘: 100GB SSD
    - 容器存储: 500GB SSD
    - 数据存储: 1TB SSD (根据需求)
  网络: 万兆网卡

网络规划:
  Pod CIDR: 10.244.0.0/16 (支持65536个Pod)
  Service CIDR: 10.96.0.0/12 (支持1048576个Service)
  节点网络: 192.168.1.0/24
```

#### 高可用架构设计

**生产级高可用架构**：

```
                    ┌─────────────────────────────────────┐
                    │      外部负载均衡器 (HAProxy/LVS)    │
                    │      VIP: 192.168.1.100             │
                    └──────────────┬──────────────────────┘
                                   │
                    ┌──────────────┴──────────────────────┐
                    │                                      │
         ┌──────────┴──────────┐            ┌────────────┴─────────┐
         │                     │            │                      │
    ┌────┴────┐          ┌────┴────┐  ┌────┴────┐
    │ Master1 │          │ Master2 │  │ Master3 │
    │         │          │         │  │         │
    │ API     │          │ API     │  │ API     │
    │ Server  │          │ Server  │  │ Server  │
    │         │          │         │  │         │
    │ Sched   │          │ Sched   │  │ Sched   │
    │ uler    │          │ uler    │  │ uler    │
    │         │          │         │  │         │
    │ Ctrl    │          │ Ctrl    │  │ Ctrl    │
    │ Manager │          │ Manager │  │ Manager │
    └────┬────┘          └────┬────┘  └────┬────┘
         │                    │            │
         └────────────────────┴────────────┘
                              │
                    ┌─────────┴─────────┐
                    │   etcd集群 (3节点) │
                    │   - etcd1          │
                    │   - etcd2          │
                    │   - etcd3          │
                    └─────────┬─────────┘
                              │
         ┌────────────────────┴────────────────────┐
         │                                          │
    ┌────┴────┐  ┌──────────┐  ┌──────────┐  ┌────┴────┐
    │ Worker1 │  │ Worker2  │  │ Worker3  │  │ Worker12│
    │         │  │          │  │          │  │         │
    │ kubelet │  │ kubelet  │  │ kubelet  │  │ kubelet │
    │ kube-   │  │ kube-    │  │ kube-    │  │ kube-   │
    │ proxy   │  │ proxy    │  │ proxy    │  │ proxy   │
    │         │  │          │  │          │  │         │
    │ 容器    │  │ 容器     │  │ 容器     │  │ 容器    │
    │ 运行时  │  │ 运行时   │  │ 运行时   │  │ 运行时  │
    └─────────┘  └──────────┘  └──────────┘  └─────────┘
```

**高可用关键点**：

1. **控制平面高可用**：
   - 3个Master节点，奇数个避免脑裂
   - 通过负载均衡器提供统一入口
   - API Server无状态，可水平扩展

2. **etcd集群高可用**：
   - 3个etcd节点，支持1个节点故障
   - 独立部署或与Master节点混部
   - 使用SSD磁盘，确保IOPS >3000

3. **负载均衡器高可用**：
   - HAProxy + Keepalived实现VIP漂移
   - 或使用云厂商的负载均衡服务

#### 节点角色规划

**节点分类与职责**：

```yaml
Master节点 (控制平面):
  组件:
    - kube-apiserver: API服务器
    - kube-scheduler: 调度器
    - kube-controller-manager: 控制器管理器
    - etcd: 分布式键值存储
  特点:
    - 不运行业务Pod (通过污点实现)
    - 需要高可用和高性能
    - 资源需求相对稳定

Worker节点 (工作负载):
  组件:
    - kubelet: 节点代理
    - kube-proxy: 网络代理
    - 容器运行时: containerd/CRI-O
  特点:
    - 运行业务Pod
    - 可根据负载动态扩缩容
    - 资源需求波动较大

边缘节点 (可选):
  用途:
    - 运行Ingress Controller
    - 运行监控组件
    - 运行日志收集组件
  特点:
    - 固定IP或域名
    - 较高的网络带宽
    - 独立的资源配额
```

#### 网络方案选择

**常见CNI插件对比**：

| CNI插件 | 性能 | 功能 | 复杂度 | 适用场景 |
|---------|------|------|--------|----------|
| **Flannel** | ⭐⭐⭐ | ⭐⭐ | ⭐ 简单 | 小型集群、学习环境 |
| **Calico** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ 中等 | 生产环境、需要NetworkPolicy |
| **Cilium** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ 复杂 | 大规模集群、需要eBPF |
| **Weave** | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ 简单 | 中小型集群 |

**推荐方案：Calico**

```yaml
选择理由:
  1. 性能优秀: 基于BGP路由，性能接近原生网络
  2. 功能完善: 支持NetworkPolicy、IPAM、BGP等
  3. 社区活跃: 文档完善，问题容易解决
  4. 生产验证: 大量企业生产环境使用

网络模式选择:
  - IPIP模式: 跨子网通信，兼容性好
  - BGP模式: 同子网通信，性能最佳
  - VXLAN模式: 云环境推荐
```

#### 存储方案选择

**存储类型对比**：

```yaml
本地存储 (Local PV):
  优点:
    - 性能最好 (直接访问本地磁盘)
    - 延迟最低
    - 成本最低
  缺点:
    - 不支持Pod迁移
    - 数据可靠性依赖节点
  适用场景:
    - 缓存数据
    - 临时数据
    - 高性能数据库 (配合副本)

网络存储 (NFS/Ceph/GlusterFS):
  优点:
    - 支持Pod迁移
    - 数据高可用
    - 支持多节点共享
  缺点:
    - 性能较差
    - 网络依赖
    - 运维复杂
  适用场景:
    - 共享文件存储
    - 日志收集
    - 配置文件

云存储 (EBS/云盘):
  优点:
    - 高可用
    - 易于管理
    - 按需扩容
  缺点:
    - 成本较高
    - 性能受限于云厂商
    - 厂商锁定
  适用场景:
    - 云上部署
    - 快速上线
    - 中小规模应用
```

**推荐方案：混合存储**

```yaml
存储分层策略:
  高性能层 (Local PV + Rook-Ceph):
    - 数据库: MySQL/PostgreSQL/MongoDB
    - 缓存: Redis/Memcached
    - 消息队列: Kafka/RabbitMQ
  
  标准层 (Ceph RBD):
    - 应用数据
    - 用户上传文件
    - 日志数据
  
  归档层 (对象存储):
    - 备份数据
    - 历史日志
    - 冷数据
```

### 12.1.2 高可用控制平面部署

使用kubeadm部署高可用Kubernetes集群。

#### 环境准备

**节点信息**：

```bash
# 节点规划
192.168.1.11  k8s-master1  # Master节点1
192.168.1.12  k8s-master2  # Master节点2
192.168.1.13  k8s-master3  # Master节点3
192.168.1.21  k8s-worker1  # Worker节点1
192.168.1.22  k8s-worker2  # Worker节点2
192.168.1.23  k8s-worker3  # Worker节点3
192.168.1.100 k8s-api-vip  # API Server VIP
```

**系统要求**：

```bash
# 操作系统
Ubuntu 22.04 LTS / CentOS 8 Stream / Rocky Linux 8

# 内核版本
>= 4.19

# 系统配置
- 关闭swap
- 关闭SELinux/AppArmor (或配置为Permissive)
- 配置防火墙规则
- 同步时间
```

#### 步骤1：所有节点基础配置

```bash
# 1. 配置主机名和hosts
cat >> /etc/hosts << EOF
192.168.1.11 k8s-master1
192.168.1.12 k8s-master2
192.168.1.13 k8s-master3
192.168.1.21 k8s-worker1
192.168.1.22 k8s-worker2
192.168.1.23 k8s-worker3
192.168.1.100 k8s-api-vip
EOF

# 2. 关闭swap
swapoff -a
sed -i '/swap/d' /etc/fstab

# 3. 加载内核模块
cat > /etc/modules-load.d/k8s.conf << EOF
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

# 4. 配置内核参数
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sysctl --system

# 5. 安装容器运行时 (containerd)
# Ubuntu
apt-get update
apt-get install -y containerd

# CentOS/Rocky
dnf install -y containerd

# 配置containerd
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml

# 修改配置使用systemd cgroup driver
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

# 重启containerd
systemctl restart containerd
systemctl enable containerd

# 6. 安装kubeadm、kubelet、kubectl
# Ubuntu
apt-get update
apt-get install -y apt-transport-https ca-certificates curl
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list
apt-get update
apt-get install -y kubelet=1.28.0-1.1 kubeadm=1.28.0-1.1 kubectl=1.28.0-1.1
apt-mark hold kubelet kubeadm kubectl

# CentOS/Rocky
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.28/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.28/rpm/repodata/repomd.xml.key
EOF

dnf install -y kubelet-1.28.0 kubeadm-1.28.0 kubectl-1.28.0
systemctl enable kubelet
```

#### 步骤2：部署负载均衡器

在所有Master节点上部署HAProxy + Keepalived：

```bash
# 安装HAProxy和Keepalived
apt-get install -y haproxy keepalived

# 配置HAProxy
cat > /etc/haproxy/haproxy.cfg << 'HAPROXY_EOF'
global
    log /dev/log local0
    log /dev/log local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    log     global
    mode    tcp
    option  tcplog
    option  dontlognull
    timeout connect 5000
    timeout client  50000
    timeout server  50000

frontend k8s-api
    bind *:6443
    mode tcp
    option tcplog
    default_backend k8s-api-backend

backend k8s-api-backend
    mode tcp
    option tcp-check
    balance roundrobin
    server k8s-master1 192.168.1.11:6443 check fall 3 rise 2
    server k8s-master2 192.168.1.12:6443 check fall 3 rise 2
    server k8s-master3 192.168.1.13:6443 check fall 3 rise 2
HAPROXY_EOF

# 配置Keepalived (Master1)
cat > /etc/keepalived/keepalived.conf << 'KEEPALIVED_EOF'
vrrp_script check_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
    weight 2
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass k8s_ha_pass
    }
    virtual_ipaddress {
        192.168.1.100
    }
    track_script {
        check_haproxy
    }
}
KEEPALIVED_EOF

# Master2和Master3的priority分别设置为99和98

# 启动服务
systemctl restart haproxy
systemctl enable haproxy
systemctl restart keepalived
systemctl enable keepalived
```

#### 步骤3：初始化第一个Master节点

```bash
# 在k8s-master1上执行

# 创建kubeadm配置文件
cat > kubeadm-config.yaml << 'KUBEADM_EOF'
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: "192.168.1.100:6443"
networking:
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"
apiServer:
  certSANs:
  - "192.168.1.100"
  - "k8s-api-vip"
  - "192.168.1.11"
  - "192.168.1.12"
  - "192.168.1.13"
  extraArgs:
    authorization-mode: "Node,RBAC"
etcd:
  local:
    dataDir: /var/lib/etcd
    extraArgs:
      listen-metrics-urls: "http://0.0.0.0:2381"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: systemd
KUBEADM_EOF

# 初始化集群
kubeadm init --config=kubeadm-config.yaml --upload-certs

# 配置kubectl
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# 记录输出的join命令，后续需要使用
```

#### 步骤4：加入其他Master节点

```bash
# 在k8s-master2和k8s-master3上执行
# 使用kubeadm init输出的join命令

kubeadm join 192.168.1.100:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash> \
    --control-plane --certificate-key <certificate-key>

# 配置kubectl
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
```

#### 步骤5：加入Worker节点

```bash
# 在所有Worker节点上执行

kubeadm join 192.168.1.100:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash>
```

#### 步骤6：安装网络插件

```bash
# 在任意Master节点上执行

# 安装Calico
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml

# 等待所有Pod运行
kubectl get pods -n kube-system -w

# 验证节点状态
kubectl get nodes
```

#### 步骤7：验证集群

```bash
# 1. 检查节点状态
kubectl get nodes
# 所有节点应该是Ready状态

# 2. 检查组件状态
kubectl get pods -n kube-system

# 3. 检查etcd集群
kubectl -n kube-system exec -it etcd-k8s-master1 -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list

# 4. 测试高可用
# 停止master1的kubelet
systemctl stop kubelet

# 在其他节点验证API Server仍然可用
kubectl get nodes

# 恢复master1
systemctl start kubelet
```


### 12.1.3 网络和存储配置

#### Calico网络配置优化

```bash
# 下载Calico配置文件
curl -O https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml

# 修改配置
# 1. 修改Pod CIDR (如果与kubeadm配置不同)
# 2. 启用IP-in-IP或VXLAN模式
# 3. 配置MTU大小

# 应用配置
kubectl apply -f calico.yaml

# 验证Calico状态
kubectl get pods -n kube-system -l k8s-app=calico-node
kubectl get pods -n kube-system -l k8s-app=calico-kube-controllers

# 查看Calico节点状态
kubectl exec -n kube-system calico-node-xxxxx -- calicoctl node status
```

**Calico NetworkPolicy示例**：

```yaml
# 限制Pod只能被特定命名空间访问
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-frontend
  namespace: backend
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend
    ports:
    - protocol: TCP
      port: 8080
```

#### 部署Rook-Ceph存储

```bash
# 1. 克隆Rook仓库
git clone --single-branch --branch v1.12.0 https://github.com/rook/rook.git
cd rook/deploy/examples

# 2. 部署Rook Operator
kubectl apply -f crds.yaml
kubectl apply -f common.yaml
kubectl apply -f operator.yaml

# 3. 验证Operator运行
kubectl -n rook-ceph get pod

# 4. 创建Ceph集群
kubectl apply -f cluster.yaml

# 5. 等待集群就绪
kubectl -n rook-ceph get cephcluster

# 6. 创建StorageClass
kubectl apply -f csi/rbd/storageclass.yaml

# 7. 验证StorageClass
kubectl get storageclass
```

**Ceph存储配置示例**：

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v17.2.6
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    count: 2
    allowMultiplePerNode: false
  dashboard:
    enabled: true
    ssl: true
  storage:
    useAllNodes: true
    useAllDevices: false
    deviceFilter: "^sd[b-z]"
    config:
      osdsPerDevice: "1"
```

### 12.1.4 安全加固与最佳实践

#### RBAC权限配置

```yaml
# 创建只读用户
apiVersion: v1
kind: ServiceAccount
metadata:
  name: readonly-user
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: readonly-role
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: readonly-binding
subjects:
- kind: ServiceAccount
  name: readonly-user
  namespace: default
roleRef:
  kind: ClusterRole
  name: readonly-role
  apiGroup: rbac.authorization.k8s.io
```

#### Pod Security Standards

```yaml
# 在命名空间级别启用Pod Security
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

#### 审计日志配置

```yaml
# API Server审计策略
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: Metadata
  resources:
  - group: ""
    resources: ["secrets", "configmaps"]
- level: RequestResponse
  resources:
  - group: ""
    resources: ["pods"]
  verbs: ["create", "delete", "update", "patch"]
- level: Metadata
  omitStages:
  - RequestReceived
```

#### 证书管理

```bash
# 检查证书有效期
kubeadm certs check-expiration

# 续期所有证书
kubeadm certs renew all

# 重启控制平面组件
systemctl restart kubelet
```

#### 安全加固检查清单

```yaml
基础安全:
  - [ ] 关闭不必要的端口
  - [ ] 配置防火墙规则
  - [ ] 禁用root SSH登录
  - [ ] 使用密钥认证
  - [ ] 定期更新系统补丁

Kubernetes安全:
  - [ ] 启用RBAC
  - [ ] 配置Pod Security Standards
  - [ ] 启用审计日志
  - [ ] 配置NetworkPolicy
  - [ ] 使用Secret管理敏感信息
  - [ ] 定期轮换证书
  - [ ] 限制API Server访问
  - [ ] 配置资源配额

容器安全:
  - [ ] 使用非root用户运行容器
  - [ ] 设置只读根文件系统
  - [ ] 禁用特权容器
  - [ ] 扫描镜像漏洞
  - [ ] 使用私有镜像仓库
  - [ ] 配置镜像拉取策略

网络安全:
  - [ ] 启用TLS加密
  - [ ] 配置Ingress TLS
  - [ ] 使用NetworkPolicy隔离
  - [ ] 限制出站流量
  - [ ] 配置DNS策略

数据安全:
  - [ ] 加密etcd数据
  - [ ] 加密Secret
  - [ ] 配置存储加密
  - [ ] 定期备份
  - [ ] 测试恢复流程
```

---

**本节小结**

在12.1节中，我们完成了生产级Kubernetes集群的搭建：

1. **集群规划**：根据业务需求确定集群规模，设计高可用架构，选择合适的网络和存储方案
2. **高可用部署**：使用kubeadm部署3节点Master集群，配置HAProxy+Keepalived实现负载均衡和VIP漂移
3. **网络配置**：部署Calico网络插件，配置NetworkPolicy实现网络隔离
4. **存储配置**：部署Rook-Ceph提供分布式存储，支持动态卷供应
5. **安全加固**：配置RBAC、Pod Security Standards、审计日志，建立完整的安全体系

通过本节的实践，你已经掌握了从零搭建生产级Kubernetes集群的完整流程。接下来，我们将在这个集群上部署微服务应用！

---


## 12.2 微服务应用容器化

在搭建好Kubernetes集群后，下一步是将微服务应用容器化。本节将以电商系统的四个核心服务为例，讲解如何编写高质量的Dockerfile，优化镜像大小和构建速度，并确保镜像安全。

### 12.2.1 Dockerfile最佳实践

#### 基础镜像选择

**镜像选择原则**：

```yaml
生产环境镜像选择:
  优先级1 - 官方镜像:
    - 来源可信，定期更新
    - 安全漏洞及时修复
    - 社区支持完善
  
  优先级2 - Alpine镜像:
    - 体积小 (5MB vs 100MB+)
    - 安全性高 (攻击面小)
    - 适合Go/Node.js等静态编译语言
  
  优先级3 - Distroless镜像:
    - 只包含应用和运行时依赖
    - 无shell，安全性最高
    - 适合Java/Python等
  
  避免使用:
    - latest标签 (不可追溯)
    - 过大的基础镜像 (ubuntu:latest 77MB)
    - 未维护的镜像
```

**常用基础镜像对比**：

| 镜像 | 大小 | 安全性 | 适用场景 |
|------|------|--------|----------|
| alpine:3.18 | 7MB | ⭐⭐⭐⭐⭐ | Go、Node.js、Python |
| debian:12-slim | 74MB | ⭐⭐⭐⭐ | 需要完整工具链 |
| ubuntu:22.04 | 77MB | ⭐⭐⭐ | 开发测试环境 |
| distroless/base | 20MB | ⭐⭐⭐⭐⭐ | 生产环境 |
| scratch | 0MB | ⭐⭐⭐⭐⭐ | 静态编译的Go程序 |

#### Go服务Dockerfile示例

**用户服务 (Go语言)**：

```dockerfile
# 不推荐的写法 ❌
FROM golang:1.21
WORKDIR /app
COPY . .
RUN go build -o user-service
CMD ["./user-service"]

# 问题:
# 1. 镜像过大 (golang:1.21 约1GB)
# 2. 包含编译工具和源代码
# 3. 安全风险高
```

**推荐的写法 ✅**：

```dockerfile
# 多阶段构建 - 用户服务
FROM golang:1.21-alpine AS builder

# 设置工作目录
WORKDIR /build

# 复制go mod文件并下载依赖 (利用缓存)
COPY go.mod go.sum ./
RUN go mod download

# 复制源代码
COPY . .

# 编译 - 静态链接，禁用CGO
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build     -ldflags="-w -s"     -o user-service     ./cmd/user-service

# 最终镜像
FROM alpine:3.18

# 安装CA证书 (HTTPS请求需要)
RUN apk --no-cache add ca-certificates tzdata

# 创建非root用户
RUN addgroup -g 1000 appuser &&     adduser -D -u 1000 -G appuser appuser

# 设置工作目录
WORKDIR /app

# 从构建阶段复制二进制文件
COPY --from=builder /build/user-service .

# 复制配置文件
COPY configs/config.yaml ./configs/

# 切换到非root用户
USER appuser

# 暴露端口
EXPOSE 8080

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3     CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1

# 启动命令
CMD ["./user-service"]
```

**Dockerfile最佳实践要点**：

```yaml
1. 使用多阶段构建:
   - 构建阶段: 包含编译工具
   - 运行阶段: 只包含运行时依赖
   - 镜像大小: 从1GB降到20MB

2. 优化层缓存:
   - 先复制依赖文件 (go.mod)
   - 再复制源代码
   - 依赖不变时可复用缓存

3. 最小化镜像:
   - 使用alpine或distroless
   - 删除不必要的文件
   - 使用.dockerignore

4. 安全加固:
   - 使用非root用户运行
   - 不包含shell (可选)
   - 定期更新基础镜像

5. 添加元数据:
   - LABEL维护者信息
   - LABEL版本信息
   - HEALTHCHECK健康检查
```

#### Java服务Dockerfile示例

**商品服务 (Java Spring Boot)**：

```dockerfile
# 多阶段构建 - 商品服务
FROM maven:3.9-eclipse-temurin-17 AS builder

WORKDIR /build

# 复制pom.xml并下载依赖
COPY pom.xml .
RUN mvn dependency:go-offline

# 复制源代码并构建
COPY src ./src
RUN mvn clean package -DskipTests

# 最终镜像 - 使用JRE而非JDK
FROM eclipse-temurin:17-jre-alpine

# 安装必要工具
RUN apk --no-cache add curl

# 创建非root用户
RUN addgroup -g 1000 appuser &&     adduser -D -u 1000 -G appuser appuser

WORKDIR /app

# 从构建阶段复制jar包
COPY --from=builder /build/target/product-service-*.jar app.jar

# 切换用户
USER appuser

# 暴露端口
EXPOSE 8080

# JVM参数优化
ENV JAVA_OPTS="-Xms512m -Xmx1024m -XX:+UseG1GC -XX:MaxGCPauseMillis=200"

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=40s --retries=3     CMD curl -f http://localhost:8080/actuator/health || exit 1

# 启动命令
ENTRYPOINT ["sh", "-c", "java $JAVA_OPTS -jar app.jar"]
```

#### Python服务Dockerfile示例

**订单服务 (Python Flask)**：

```dockerfile
# 多阶段构建 - 订单服务
FROM python:3.11-slim AS builder

WORKDIR /build

# 安装构建依赖
RUN apt-get update &&     apt-get install -y --no-install-recommends gcc &&     rm -rf /var/lib/apt/lists/*

# 复制requirements并安装依赖
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# 最终镜像
FROM python:3.11-slim

# 安装运行时依赖
RUN apt-get update &&     apt-get install -y --no-install-recommends curl &&     rm -rf /var/lib/apt/lists/*

# 创建非root用户
RUN useradd -m -u 1000 appuser

WORKDIR /app

# 从构建阶段复制Python包
COPY --from=builder /root/.local /home/appuser/.local

# 复制应用代码
COPY --chown=appuser:appuser . .

# 切换用户
USER appuser

# 设置Python路径
ENV PATH=/home/appuser/.local/bin:$PATH
ENV PYTHONUNBUFFERED=1

# 暴露端口
EXPOSE 5000

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3     CMD curl -f http://localhost:5000/health || exit 1

# 启动命令 - 使用gunicorn
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "4", "app:app"]
```

#### Node.js服务Dockerfile示例

**支付服务 (Node.js Express)**：

```dockerfile
# 多阶段构建 - 支付服务
FROM node:20-alpine AS builder

WORKDIR /build

# 复制package文件并安装依赖
COPY package*.json ./
RUN npm ci --only=production

# 最终镜像
FROM node:20-alpine

# 安装dumb-init (正确处理信号)
RUN apk --no-cache add dumb-init

# 创建非root用户
RUN addgroup -g 1000 appuser &&     adduser -D -u 1000 -G appuser appuser

WORKDIR /app

# 从构建阶段复制node_modules
COPY --from=builder /build/node_modules ./node_modules

# 复制应用代码
COPY --chown=appuser:appuser . .

# 切换用户
USER appuser

# 暴露端口
EXPOSE 3000

# 环境变量
ENV NODE_ENV=production

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3     CMD node healthcheck.js || exit 1

# 使用dumb-init启动
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "server.js"]
```

#### .dockerignore文件

```bash
# .dockerignore - 减少构建上下文大小

# Git相关
.git
.gitignore
.gitattributes

# 文档
README.md
CHANGELOG.md
docs/
*.md

# 测试文件
*_test.go
test/
tests/
__tests__/
*.test.js

# 开发工具配置
.vscode/
.idea/
*.swp
*.swo

# 依赖目录 (会在镜像中重新安装)
node_modules/
vendor/
target/

# 构建产物
dist/
build/
*.exe
*.dll
*.so

# 日志和临时文件
*.log
tmp/
temp/
.DS_Store

# 环境变量文件
.env
.env.local
*.pem
*.key
```


### 12.2.2 多阶段构建优化

多阶段构建是Docker的强大特性，可以显著减小镜像大小并提高安全性。

#### 构建缓存优化

**利用BuildKit加速构建**：

```bash
# 启用BuildKit
export DOCKER_BUILDKIT=1

# 使用缓存挂载加速依赖下载
docker build --build-arg BUILDKIT_INLINE_CACHE=1 -t myapp:latest .

# 使用外部缓存
docker build --cache-from myapp:latest -t myapp:v2 .
```

**Go服务优化示例**：

```dockerfile
# 优化的多阶段构建
FROM golang:1.21-alpine AS base
WORKDIR /build
RUN apk add --no-cache git ca-certificates tzdata

# 依赖阶段 - 单独缓存
FROM base AS dependencies
COPY go.mod go.sum ./
RUN --mount=type=cache,target=/go/pkg/mod     go mod download

# 构建阶段
FROM dependencies AS builder
COPY . .
RUN --mount=type=cache,target=/go/pkg/mod     --mount=type=cache,target=/root/.cache/go-build     CGO_ENABLED=0 go build -ldflags="-w -s" -o app .

# 测试阶段 (可选)
FROM builder AS tester
RUN go test -v ./...

# 最终镜像
FROM scratch
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo
COPY --from=builder /build/app /app
ENTRYPOINT ["/app"]
```

**构建命令**：

```bash
# 构建并跳过测试阶段
docker build --target builder -t myapp:latest .

# 构建并运行测试
docker build --target tester -t myapp:test .

# 构建最终镜像
docker build -t myapp:latest .
```

#### 镜像大小对比

**优化前后对比**：

```yaml
用户服务 (Go):
  优化前: 1.2GB (golang:1.21)
  优化后: 15MB (scratch + 静态编译)
  减少: 98.75%

商品服务 (Java):
  优化前: 450MB (openjdk:17)
  优化后: 180MB (eclipse-temurin:17-jre-alpine)
  减少: 60%

订单服务 (Python):
  优化前: 950MB (python:3.11)
  优化后: 120MB (python:3.11-slim + 多阶段)
  减少: 87.4%

支付服务 (Node.js):
  优化前: 1.1GB (node:20)
  优化后: 85MB (node:20-alpine)
  减少: 92.3%
```

#### 并行构建优化

**使用BuildKit并行构建**：

```dockerfile
# syntax=docker/dockerfile:1.4

FROM golang:1.21-alpine AS builder-user
WORKDIR /build
COPY services/user/ .
RUN go build -o user-service .

FROM golang:1.21-alpine AS builder-product
WORKDIR /build
COPY services/product/ .
RUN go build -o product-service .

# 并行构建多个服务
FROM alpine:3.18 AS user-service
COPY --from=builder-user /build/user-service /app/
ENTRYPOINT ["/app/user-service"]

FROM alpine:3.18 AS product-service
COPY --from=builder-product /build/product-service /app/
ENTRYPOINT ["/app/product-service"]
```

#### 构建参数和环境变量

```dockerfile
# 使用ARG传递构建参数
FROM golang:1.21-alpine AS builder

# 构建参数
ARG VERSION=dev
ARG BUILD_TIME
ARG GIT_COMMIT

WORKDIR /build
COPY . .

# 编译时注入版本信息
RUN CGO_ENABLED=0 go build     -ldflags="-X main.Version=${VERSION}               -X main.BuildTime=${BUILD_TIME}               -X main.GitCommit=${GIT_COMMIT}               -w -s"     -o app .

FROM alpine:3.18
COPY --from=builder /build/app /app

# 运行时环境变量
ENV APP_ENV=production     LOG_LEVEL=info

ENTRYPOINT ["/app"]
```

**构建命令**：

```bash
# 传递构建参数
docker build   --build-arg VERSION=v1.2.3   --build-arg BUILD_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")   --build-arg GIT_COMMIT=$(git rev-parse --short HEAD)   -t myapp:v1.2.3 .
```

### 12.2.3 镜像安全扫描

确保容器镜像的安全性是生产环境的关键要求。

#### 使用Trivy扫描镜像

**安装Trivy**：

```bash
# Ubuntu/Debian
wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list
sudo apt-get update
sudo apt-get install trivy

# macOS
brew install trivy

# 使用Docker运行
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock   aquasec/trivy image myapp:latest
```

**扫描镜像**：

```bash
# 扫描本地镜像
trivy image myapp:latest

# 只显示高危和严重漏洞
trivy image --severity HIGH,CRITICAL myapp:latest

# 输出JSON格式
trivy image --format json --output result.json myapp:latest

# 扫描并在发现漏洞时失败
trivy image --exit-code 1 --severity CRITICAL myapp:latest
```

**扫描结果示例**：

```
myapp:latest (alpine 3.18.0)
============================
Total: 2 (HIGH: 1, CRITICAL: 1)

┌───────────────┬────────────────┬──────────┬───────────────────┬───────────────┬────────────────────────────────────┐
│   Library     │ Vulnerability  │ Severity │ Installed Version │ Fixed Version │              Title                 │
├───────────────┼────────────────┼──────────┼───────────────────┼───────────────┼────────────────────────────────────┤
│ openssl       │ CVE-2023-12345 │ CRITICAL │ 3.1.0-r0          │ 3.1.1-r0      │ OpenSSL: Buffer overflow           │
│ libcrypto3    │ CVE-2023-12346 │ HIGH     │ 3.1.0-r0          │ 3.1.1-r0      │ OpenSSL: Memory leak               │
└───────────────┴────────────────┴──────────┴───────────────────┴───────────────┴────────────────────────────────────┘
```

#### 修复漏洞

```dockerfile
# 修复前
FROM alpine:3.18.0
RUN apk add --no-cache openssl

# 修复后 - 更新到最新版本
FROM alpine:3.18
RUN apk add --no-cache --upgrade openssl libcrypto3

# 或使用更新的基础镜像
FROM alpine:3.19
```

#### 集成到CI/CD

**GitLab CI示例**：

```yaml
# .gitlab-ci.yml
stages:
  - build
  - scan
  - deploy

build:
  stage: build
  script:
    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .
    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA

security-scan:
  stage: scan
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 0 --severity LOW,MEDIUM $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
    - trivy image --exit-code 1 --severity HIGH,CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  allow_failure: false

deploy:
  stage: deploy
  script:
    - kubectl set image deployment/myapp myapp=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  only:
    - main
```

#### 使用Cosign签名镜像

**安装Cosign**：

```bash
# 安装cosign
wget https://github.com/sigstore/cosign/releases/download/v2.2.0/cosign-linux-amd64
chmod +x cosign-linux-amd64
sudo mv cosign-linux-amd64 /usr/local/bin/cosign

# 生成密钥对
cosign generate-key-pair

# 签名镜像
cosign sign --key cosign.key myregistry.com/myapp:v1.0.0

# 验证签名
cosign verify --key cosign.pub myregistry.com/myapp:v1.0.0
```

#### 镜像安全最佳实践

```yaml
基础镜像安全:
  - [ ] 使用官方或可信镜像
  - [ ] 使用特定版本标签，避免latest
  - [ ] 定期更新基础镜像
  - [ ] 使用最小化镜像 (alpine/distroless)

构建安全:
  - [ ] 不在镜像中包含敏感信息
  - [ ] 使用.dockerignore排除敏感文件
  - [ ] 使用多阶段构建减少攻击面
  - [ ] 不在镜像中包含构建工具

运行时安全:
  - [ ] 使用非root用户运行
  - [ ] 设置只读根文件系统
  - [ ] 限制容器权限 (capabilities)
  - [ ] 使用安全上下文 (SecurityContext)

漏洞管理:
  - [ ] 定期扫描镜像漏洞
  - [ ] 及时修复高危漏洞
  - [ ] 建立漏洞响应流程
  - [ ] 使用镜像签名验证完整性
```


### 12.2.4 私有镜像仓库搭建

生产环境通常需要私有镜像仓库来管理内部镜像。

#### 使用Harbor搭建企业级镜像仓库

**Harbor特性**：

```yaml
核心功能:
  - 镜像存储和分发
  - 镜像复制 (多地域同步)
  - 漏洞扫描 (集成Trivy)
  - 镜像签名 (Notary)
  - RBAC权限管理
  - 审计日志
  - Webhook通知

企业特性:
  - 高可用部署
  - LDAP/AD集成
  - 配额管理
  - 垃圾回收
  - 镜像代理缓存
```

**使用Helm部署Harbor**：

```bash
# 添加Harbor Helm仓库
helm repo add harbor https://helm.goharbor.io
helm repo update

# 创建命名空间
kubectl create namespace harbor

# 创建values.yaml配置文件
cat > harbor-values.yaml << 'EOF'
expose:
  type: ingress
  tls:
    enabled: true
    certSource: secret
    secret:
      secretName: harbor-tls
  ingress:
    hosts:
      core: harbor.example.com
    className: nginx

externalURL: https://harbor.example.com

persistence:
  enabled: true
  resourcePolicy: "keep"
  persistentVolumeClaim:
    registry:
      storageClass: "rook-ceph-block"
      size: 500Gi
    database:
      storageClass: "rook-ceph-block"
      size: 10Gi
    redis:
      storageClass: "rook-ceph-block"
      size: 5Gi

harborAdminPassword: "ChangeMe123!"

database:
  type: internal

redis:
  type: internal

trivy:
  enabled: true

notary:
  enabled: true

metrics:
  enabled: true
  serviceMonitor:
    enabled: true
EOF

# 部署Harbor
helm install harbor harbor/harbor   -f harbor-values.yaml   -n harbor

# 等待所有Pod运行
kubectl get pods -n harbor -w

# 获取Harbor访问地址
echo "Harbor URL: https://harbor.example.com"
echo "Username: admin"
echo "Password: ChangeMe123!"
```

#### 配置Docker使用私有仓库

```bash
# 1. 登录Harbor
docker login harbor.example.com
# 输入用户名和密码

# 2. 标记镜像
docker tag myapp:latest harbor.example.com/library/myapp:latest

# 3. 推送镜像
docker push harbor.example.com/library/myapp:latest

# 4. 拉取镜像
docker pull harbor.example.com/library/myapp:latest
```

#### 配置Kubernetes使用私有仓库

**创建Docker Registry Secret**：

```bash
# 方法1: 使用kubectl create secret
kubectl create secret docker-registry harbor-secret   --docker-server=harbor.example.com   --docker-username=admin   --docker-password=ChangeMe123!   --docker-email=admin@example.com   -n default

# 方法2: 使用现有的docker config
kubectl create secret generic harbor-secret   --from-file=.dockerconfigjson=$HOME/.docker/config.json   --type=kubernetes.io/dockerconfigjson   -n default
```

**在Pod中使用Secret**：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: myapp
    image: harbor.example.com/library/myapp:latest
  imagePullSecrets:
  - name: harbor-secret
```

**在ServiceAccount中配置**：

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: default
  namespace: default
imagePullSecrets:
- name: harbor-secret
```

#### Harbor项目和用户管理

**创建项目**：

```bash
# 使用Harbor API创建项目
curl -X POST "https://harbor.example.com/api/v2.0/projects"   -H "Content-Type: application/json"   -u "admin:ChangeMe123!"   -d '{
    "project_name": "ecommerce",
    "public": false,
    "metadata": {
      "auto_scan": "true",
      "severity": "high"
    }
  }'
```

**配置镜像复制**：

```yaml
# 配置多地域镜像复制
复制规则:
  源仓库: harbor-beijing.example.com
  目标仓库: harbor-shanghai.example.com
  触发方式: 
    - 手动触发
    - 定时触发 (每天凌晨2点)
    - 事件触发 (镜像推送时)
  过滤规则:
    - 项目: ecommerce/*
    - 标签: v*
```

#### 镜像清理策略

```yaml
# Harbor垃圾回收配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: harbor-gc-config
  namespace: harbor
data:
  config.yaml: |
    # 保留策略
    retention:
      # 保留最近30天的镜像
      - selector:
          tagSelectors:
            - kind: doublestar
              decoration: matches
              pattern: "**"
        action: retain
        params:
          latestPushedK: 30
      
      # 删除未打标签的镜像
      - selector:
          untagged: true
        action: delete
    
    # 定时清理
    schedule:
      cron: "0 2 * * *"  # 每天凌晨2点
```

#### 镜像构建和推送流程

**完整的CI/CD流程**：

```bash
#!/bin/bash
# build-and-push.sh

set -e

# 配置
HARBOR_URL="harbor.example.com"
PROJECT="ecommerce"
IMAGE_NAME="user-service"
VERSION="${CI_COMMIT_TAG:-latest}"
FULL_IMAGE="${HARBOR_URL}/${PROJECT}/${IMAGE_NAME}:${VERSION}"

echo "Building image: ${FULL_IMAGE}"

# 1. 构建镜像
docker build   --build-arg VERSION=${VERSION}   --build-arg BUILD_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")   --build-arg GIT_COMMIT=$(git rev-parse --short HEAD)   -t ${FULL_IMAGE}   .

# 2. 扫描镜像
echo "Scanning image for vulnerabilities..."
trivy image --exit-code 1 --severity CRITICAL ${FULL_IMAGE}

# 3. 推送镜像
echo "Pushing image to Harbor..."
docker push ${FULL_IMAGE}

# 4. 签名镜像 (可选)
if [ -f "cosign.key" ]; then
  echo "Signing image..."
  cosign sign --key cosign.key ${FULL_IMAGE}
fi

# 5. 触发部署
echo "Image pushed successfully: ${FULL_IMAGE}"
```

---

**本节小结**

在12.2节中，我们完成了微服务应用的容器化：

1. **Dockerfile最佳实践**：
   - 选择合适的基础镜像（Alpine、Distroless）
   - 编写了Go、Java、Python、Node.js四种语言的优化Dockerfile
   - 使用非root用户、健康检查等安全措施
   - 配置.dockerignore减少构建上下文

2. **多阶段构建优化**：
   - 利用BuildKit加速构建和缓存
   - 镜像大小平均减少85%以上
   - 使用构建参数注入版本信息
   - 并行构建多个服务

3. **镜像安全扫描**：
   - 使用Trivy扫描镜像漏洞
   - 集成到CI/CD流程
   - 使用Cosign签名镜像
   - 建立漏洞修复流程

4. **私有镜像仓库**：
   - 使用Harbor搭建企业级镜像仓库
   - 配置镜像复制和清理策略
   - 集成漏洞扫描和镜像签名
   - 建立完整的镜像管理流程

通过本节的实践，你已经掌握了如何将微服务应用容器化，并建立了完整的镜像构建、扫描、存储和分发体系。接下来，我们将学习如何使用Helm部署这些容器化应用！

---


## 12.3 应用部署与配置管理

在完成应用容器化后，下一步是将应用部署到Kubernetes集群。Helm是Kubernetes的包管理工具，可以简化应用的部署和管理。本节将讲解如何使用Helm Chart部署微服务应用，并实现多环境配置管理。

### 12.3.1 Helm Chart编写

#### Helm基础概念

**Helm核心组件**：

```yaml
Chart (图表):
  - 包含Kubernetes资源定义的包
  - 类似于apt的deb包或yum的rpm包
  - 包含Chart.yaml、values.yaml和模板文件

Release (发布):
  - Chart的一个实例
  - 同一个Chart可以安装多次，每次都是一个新的Release
  - 例如：mysql-dev、mysql-prod

Repository (仓库):
  - 存储和分享Chart的地方
  - 类似于Docker Hub
  - 官方仓库：https://artifacthub.io/
```

#### 创建用户服务Helm Chart

**Chart目录结构**：

```bash
user-service/
├── Chart.yaml              # Chart元数据
├── values.yaml             # 默认配置值
├── values-dev.yaml         # 开发环境配置
├── values-prod.yaml        # 生产环境配置
├── templates/              # Kubernetes资源模板
│   ├── deployment.yaml     # Deployment模板
│   ├── service.yaml        # Service模板
│   ├── ingress.yaml        # Ingress模板
│   ├── configmap.yaml      # ConfigMap模板
│   ├── secret.yaml         # Secret模板
│   ├── hpa.yaml            # HPA模板
│   ├── serviceaccount.yaml # ServiceAccount模板
│   ├── _helpers.tpl        # 辅助模板函数
│   └── NOTES.txt           # 安装后的提示信息
└── .helmignore             # 忽略文件
```

**Chart.yaml**：

```yaml
apiVersion: v2
name: user-service
description: 用户服务 - 电商系统核心服务
type: application
version: 1.0.0        # Chart版本
appVersion: "v1.2.3"  # 应用版本

keywords:
  - ecommerce
  - user
  - microservice

maintainers:
  - name: DevOps Team
    email: devops@example.com

dependencies: []

annotations:
  category: Microservice
```

**values.yaml**：

```yaml
# 默认配置值
replicaCount: 2

image:
  repository: harbor.example.com/ecommerce/user-service
  pullPolicy: IfNotPresent
  tag: ""  # 默认使用Chart.yaml中的appVersion

imagePullSecrets:
  - name: harbor-secret

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true

service:
  type: ClusterIP
  port: 80
  targetPort: 8080
  annotations: {}

ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
  hosts:
    - host: user-api.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: user-service-tls
      hosts:
        - user-api.example.com

resources:
  limits:
    cpu: 1000m
    memory: 512Mi
  requests:
    cpu: 100m
    memory: 128Mi

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

livenessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: http
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

nodeSelector: {}

tolerations: []

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - user-service
          topologyKey: kubernetes.io/hostname

config:
  logLevel: info
  database:
    host: mysql.database.svc.cluster.local
    port: 3306
    name: users
  redis:
    host: redis.cache.svc.cluster.local
    port: 6379
  jwt:
    expiresIn: 3600

secrets:
  database:
    username: user_service
    password: ""  # 从外部注入
  redis:
    password: ""  # 从外部注入
  jwt:
    secret: ""    # 从外部注入
```

**templates/deployment.yaml**：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "user-service.fullname" . }}
  labels:
    {{- include "user-service.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "user-service.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
        checksum/secret: {{ include (print $.Template.BasePath "/secret.yaml") . | sha256sum }}
        {{- with .Values.podAnnotations }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      labels:
        {{- include "user-service.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "user-service.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
      - name: {{ .Chart.Name }}
        securityContext:
          {{- toYaml .Values.securityContext | nindent 12 }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        ports:
        - name: http
          containerPort: {{ .Values.service.targetPort }}
          protocol: TCP
        livenessProbe:
          {{- toYaml .Values.livenessProbe | nindent 12 }}
        readinessProbe:
          {{- toYaml .Values.readinessProbe | nindent 12 }}
        resources:
          {{- toYaml .Values.resources | nindent 12 }}
        env:
        - name: LOG_LEVEL
          value: {{ .Values.config.logLevel | quote }}
        - name: DB_HOST
          value: {{ .Values.config.database.host | quote }}
        - name: DB_PORT
          value: {{ .Values.config.database.port | quote }}
        - name: DB_NAME
          value: {{ .Values.config.database.name | quote }}
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: {{ include "user-service.fullname" . }}
              key: db-username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: {{ include "user-service.fullname" . }}
              key: db-password
        - name: REDIS_HOST
          value: {{ .Values.config.redis.host | quote }}
        - name: REDIS_PORT
          value: {{ .Values.config.redis.port | quote }}
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: {{ include "user-service.fullname" . }}
              key: redis-password
        - name: JWT_SECRET
          valueFrom:
            secretKeyRef:
              name: {{ include "user-service.fullname" . }}
              key: jwt-secret
        - name: JWT_EXPIRES_IN
          value: {{ .Values.config.jwt.expiresIn | quote }}
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /app/cache
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir: {}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
```

**templates/service.yaml**：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "user-service.fullname" . }}
  labels:
    {{- include "user-service.labels" . | nindent 4 }}
  {{- with .Values.service.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "user-service.selectorLabels" . | nindent 4 }}
```

**templates/_helpers.tpl**：

```yaml
{{/*
Expand the name of the chart.
*/}}
{{- define "user-service.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
*/}}
{{- define "user-service.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "user-service.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "user-service.labels" -}}
helm.sh/chart: {{ include "user-service.chart" . }}
{{ include "user-service.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "user-service.selectorLabels" -}}
app.kubernetes.io/name: {{ include "user-service.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Create the name of the service account to use
*/}}
{{- define "user-service.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "user-service.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}
```


#### Helm命令使用

```bash
# 1. 安装Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# 2. 验证Chart语法
helm lint user-service/

# 3. 渲染模板（不安装）
helm template user-service user-service/   --values user-service/values-dev.yaml   --set image.tag=v1.2.3

# 4. 安装Chart到开发环境
helm install user-service-dev user-service/   --namespace dev   --create-namespace   --values user-service/values-dev.yaml   --set image.tag=v1.2.3

# 5. 查看Release状态
helm list -n dev
helm status user-service-dev -n dev

# 6. 升级Release
helm upgrade user-service-dev user-service/   --namespace dev   --values user-service/values-dev.yaml   --set image.tag=v1.2.4

# 7. 回滚Release
helm rollback user-service-dev 1 -n dev

# 8. 卸载Release
helm uninstall user-service-dev -n dev

# 9. 查看历史版本
helm history user-service-dev -n dev
```

### 12.3.2 多环境配置管理

在掌握了Helm Chart的基本编写方法后，我们需要解决一个实际问题：如何管理不同环境的配置？开发、测试、生产环境往往需要不同的副本数、资源限制、镜像标签等配置。Helm提供了values文件机制来优雅地解决这个问题。

#### 开发环境配置

**values-dev.yaml**：

```yaml
# 开发环境配置
replicaCount: 1

image:
  tag: dev-latest
  pullPolicy: Always

resources:
  limits:
    cpu: 500m
    memory: 256Mi
  requests:
    cpu: 50m
    memory: 64Mi

autoscaling:
  enabled: false

ingress:
  enabled: true
  hosts:
    - host: user-api-dev.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: user-service-dev-tls
      hosts:
        - user-api-dev.example.com

config:
  logLevel: debug
  database:
    host: mysql-dev.database.svc.cluster.local
  redis:
    host: redis-dev.cache.svc.cluster.local

# 开发环境使用NodePort便于调试
service:
  type: NodePort
```

#### 生产环境配置

**values-prod.yaml**：

```yaml
# 生产环境配置
replicaCount: 3

image:
  tag: v1.2.3
  pullPolicy: IfNotPresent

resources:
  limits:
    cpu: 2000m
    memory: 1Gi
  requests:
    cpu: 500m
    memory: 512Mi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

ingress:
  enabled: true
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "1000"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  hosts:
    - host: user-api.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: user-service-prod-tls
      hosts:
        - user-api.example.com

config:
  logLevel: info
  database:
    host: mysql-prod.database.svc.cluster.local
  redis:
    host: redis-prod.cache.svc.cluster.local

# 生产环境Pod反亲和性
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
                - user-service
        topologyKey: kubernetes.io/hostname

# 生产环境节点选择器
nodeSelector:
  node-role: production
```

#### 使用Kustomize管理多环境

**目录结构**：

```bash
kustomize/
├── base/                   # 基础配置
│   ├── kustomization.yaml
│   ├── deployment.yaml
│   ├── service.yaml
│   └── configmap.yaml
├── overlays/               # 环境覆盖
│   ├── dev/
│   │   ├── kustomization.yaml
│   │   ├── replica-patch.yaml
│   │   └── resource-patch.yaml
│   ├── staging/
│   │   └── kustomization.yaml
│   └── prod/
│       ├── kustomization.yaml
│       ├── replica-patch.yaml
│       └── hpa.yaml
```

**base/kustomization.yaml**：

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - deployment.yaml
  - service.yaml
  - configmap.yaml

commonLabels:
  app: user-service
  managed-by: kustomize

namespace: default
```

**overlays/prod/kustomization.yaml**：

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

bases:
  - ../../base

namespace: production

commonLabels:
  environment: production

patchesStrategicMerge:
  - replica-patch.yaml

resources:
  - hpa.yaml

images:
  - name: user-service
    newName: harbor.example.com/ecommerce/user-service
    newTag: v1.2.3

configMapGenerator:
  - name: user-service-config
    behavior: merge
    literals:
      - LOG_LEVEL=info
      - ENVIRONMENT=production
```

**使用Kustomize部署**：

```bash
# 查看渲染结果
kubectl kustomize overlays/prod/

# 部署到生产环境
kubectl apply -k overlays/prod/

# 删除部署
kubectl delete -k overlays/prod/
```

#### 环境变量注入

**使用ConfigMap和Secret**：

```yaml
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-service-config
data:
  LOG_LEVEL: "info"
  DB_HOST: "mysql.database.svc.cluster.local"
  DB_PORT: "3306"
  REDIS_HOST: "redis.cache.svc.cluster.local"
  REDIS_PORT: "6379"

---
# secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: user-service-secret
type: Opaque
stringData:
  DB_USERNAME: "user_service"
  DB_PASSWORD: "SecurePassword123!"
  REDIS_PASSWORD: "RedisPass456!"
  JWT_SECRET: "jwt-secret-key-change-me"
```

**在Deployment中使用**：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
spec:
  template:
    spec:
      containers:
      - name: user-service
        envFrom:
        - configMapRef:
            name: user-service-config
        - secretRef:
            name: user-service-secret
        # 或者单独引用
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: user-service-secret
              key: DB_PASSWORD
```

### 12.3.3 Secret和ConfigMap管理

在多环境配置中，我们使用了Secret来存储敏感信息。但直接将Secret明文存储在Git仓库中是极其危险的。本小节将介绍如何安全地管理Secret和ConfigMap，包括加密存储、外部密钥管理系统集成，以及配置热更新等最佳实践。

#### 使用Sealed Secrets

**安装Sealed Secrets**：

```bash
# 安装Sealed Secrets Controller
kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.24.0/controller.yaml

# 安装kubeseal CLI
wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.24.0/kubeseal-0.24.0-linux-amd64.tar.gz
tar xfz kubeseal-0.24.0-linux-amd64.tar.gz
sudo install -m 755 kubeseal /usr/local/bin/kubeseal
```

**创建Sealed Secret**：

```bash
# 1. 创建普通Secret（不提交到Git）
kubectl create secret generic user-service-secret   --from-literal=DB_PASSWORD=SecurePassword123!   --from-literal=JWT_SECRET=jwt-secret-key   --dry-run=client -o yaml > secret.yaml

# 2. 加密Secret
kubeseal --format=yaml < secret.yaml > sealed-secret.yaml

# 3. 提交sealed-secret.yaml到Git（安全）
git add sealed-secret.yaml
git commit -m "Add sealed secret"

# 4. 部署Sealed Secret
kubectl apply -f sealed-secret.yaml

# Sealed Secrets Controller会自动解密并创建Secret
```

**sealed-secret.yaml示例**：

```yaml
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: user-service-secret
  namespace: default
spec:
  encryptedData:
    DB_PASSWORD: AgBx8F7j3k...  # 加密后的数据
    JWT_SECRET: AgCy9G8k4l...
  template:
    metadata:
      name: user-service-secret
    type: Opaque
```

#### 使用External Secrets Operator

**安装External Secrets Operator**：

```bash
helm repo add external-secrets https://charts.external-secrets.io
helm install external-secrets   external-secrets/external-secrets   -n external-secrets-system   --create-namespace
```

**配置AWS Secrets Manager**：

```yaml
# secretstore.yaml
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: aws-secretsmanager
  namespace: default
spec:
  provider:
    aws:
      service: SecretsManager
      region: us-east-1
      auth:
        jwt:
          serviceAccountRef:
            name: external-secrets-sa

---
# externalsecret.yaml
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: user-service-secret
  namespace: default
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: aws-secretsmanager
    kind: SecretStore
  target:
    name: user-service-secret
    creationPolicy: Owner
  data:
  - secretKey: DB_PASSWORD
    remoteRef:
      key: prod/user-service/db-password
  - secretKey: JWT_SECRET
    remoteRef:
      key: prod/user-service/jwt-secret
```


#### ConfigMap热更新

```yaml
# 使用Reloader自动重启Pod
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-service-config
  annotations:
    reloader.stakater.com/match: "true"
data:
  config.yaml: |
    log_level: info
    database:
      host: mysql.database.svc.cluster.local
      port: 3306

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  template:
    spec:
      containers:
      - name: user-service
        volumeMounts:
        - name: config
          mountPath: /app/config
      volumes:
      - name: config
        configMap:
          name: user-service-config
```

### 12.3.4 应用版本管理

完成了应用部署和配置管理后，我们还需要建立完善的版本管理体系。如何标记版本？如何安全地发布新版本？如何在出现问题时快速回滚？本小节将介绍语义化版本控制、蓝绿部署、金丝雀发布等生产级版本管理策略。

#### 语义化版本控制

**版本号规范**：

```yaml
版本格式: MAJOR.MINOR.PATCH

MAJOR (主版本号):
  - 不兼容的API变更
  - 重大架构调整
  - 示例: 1.0.0 -> 2.0.0

MINOR (次版本号):
  - 向后兼容的功能新增
  - 功能改进
  - 示例: 1.0.0 -> 1.1.0

PATCH (修订号):
  - 向后兼容的bug修复
  - 安全补丁
  - 示例: 1.0.0 -> 1.0.1

预发布版本:
  - 1.0.0-alpha.1
  - 1.0.0-beta.2
  - 1.0.0-rc.1

构建元数据:
  - 1.0.0+20231201
  - 1.0.0+sha.5114f85
```

#### 使用Git标签管理版本

```bash
# 创建版本标签
git tag -a v1.2.3 -m "Release version 1.2.3"
git push origin v1.2.3

# 查看所有标签
git tag -l

# 基于标签构建镜像
docker build -t harbor.example.com/ecommerce/user-service:v1.2.3 .
docker push harbor.example.com/ecommerce/user-service:v1.2.3
```

#### 蓝绿部署

**蓝绿部署策略**：

```yaml
# 部署绿色版本（新版本）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service-green
  labels:
    app: user-service
    version: green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user-service
      version: green
  template:
    metadata:
      labels:
        app: user-service
        version: green
    spec:
      containers:
      - name: user-service
        image: harbor.example.com/ecommerce/user-service:v1.2.4

---
# Service指向蓝色版本（当前版本）
apiVersion: v1
kind: Service
metadata:
  name: user-service
spec:
  selector:
    app: user-service
    version: blue  # 当前指向蓝色
  ports:
  - port: 80
    targetPort: 8080

# 切换流量到绿色版本
# kubectl patch service user-service -p '{"spec":{"selector":{"version":"green"}}}'

# 验证无误后删除蓝色版本
# kubectl delete deployment user-service-blue
```

#### 金丝雀发布

**使用Flagger实现金丝雀发布**：

```bash
# 安装Flagger
kubectl apply -k github.com/fluxcd/flagger//kustomize/linkerd

# 创建Canary资源
cat > canary.yaml << 'EOF'
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: user-service
  namespace: default
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service
  service:
    port: 80
    targetPort: 8080
  analysis:
    interval: 1m
    threshold: 5
    maxWeight: 50
    stepWeight: 10
    metrics:
    - name: request-success-rate
      thresholdRange:
        min: 99
      interval: 1m
    - name: request-duration
      thresholdRange:
        max: 500
      interval: 1m
    webhooks:
    - name: load-test
      url: http://flagger-loadtester/
      timeout: 5s
      metadata:
        cmd: "hey -z 1m -q 10 -c 2 http://user-service-canary/"
EOF

kubectl apply -f canary.yaml
```

**金丝雀发布流程**：

```yaml
发布流程:
  1. 初始状态:
     - Primary: v1.2.3 (100%流量)
     - Canary: v1.2.3 (0%流量)
  
  2. 部署新版本:
     - 更新Deployment镜像为v1.2.4
     - Flagger检测到变更
  
  3. 金丝雀分析:
     - 第1分钟: Canary 10%流量
     - 第2分钟: Canary 20%流量
     - 第3分钟: Canary 30%流量
     - 第4分钟: Canary 40%流量
     - 第5分钟: Canary 50%流量
  
  4. 成功条件:
     - 请求成功率 >= 99%
     - 请求延迟 <= 500ms
     - 连续5次检查通过
  
  5. 发布完成:
     - Primary更新为v1.2.4 (100%流量)
     - Canary缩容为0
  
  6. 失败回滚:
     - 任何指标不达标
     - 自动回滚到v1.2.3
     - 发送告警通知
```

#### 版本回滚

**使用Helm回滚**：

```bash
# 查看发布历史
helm history user-service -n production

# 回滚到上一个版本
helm rollback user-service -n production

# 回滚到指定版本
helm rollback user-service 3 -n production

# 查看回滚状态
kubectl rollout status deployment/user-service -n production
```

**使用kubectl回滚**：

```bash
# 查看Deployment历史
kubectl rollout history deployment/user-service -n production

# 查看特定版本详情
kubectl rollout history deployment/user-service --revision=2 -n production

# 回滚到上一个版本
kubectl rollout undo deployment/user-service -n production

# 回滚到指定版本
kubectl rollout undo deployment/user-service --to-revision=2 -n production

# 暂停发布
kubectl rollout pause deployment/user-service -n production

# 恢复发布
kubectl rollout resume deployment/user-service -n production
```

#### 版本管理最佳实践

```yaml
版本标记:
  - [ ] 使用语义化版本号
  - [ ] 镜像标签与Git标签一致
  - [ ] 避免使用latest标签
  - [ ] 记录版本变更日志

发布策略:
  - [ ] 开发环境: 直接部署
  - [ ] 测试环境: 蓝绿部署
  - [ ] 生产环境: 金丝雀发布
  - [ ] 关键服务: 人工审批

回滚准备:
  - [ ] 保留至少3个历史版本
  - [ ] 测试回滚流程
  - [ ] 准备回滚脚本
  - [ ] 建立回滚决策标准

监控验证:
  - [ ] 部署前健康检查
  - [ ] 部署中实时监控
  - [ ] 部署后验证测试
  - [ ] 记录部署日志
```

---

**本节小结**

在12.3节中，我们完成了应用部署与配置管理：

1. **Helm Chart编写**：
   - 创建了完整的Helm Chart结构
   - 编写了Deployment、Service、Ingress等模板
   - 使用_helpers.tpl定义辅助函数
   - 配置了健康检查、资源限制、安全上下文

2. **多环境配置管理**：
   - 使用values-dev.yaml和values-prod.yaml管理不同环境
   - 使用Kustomize实现配置覆盖
   - 通过ConfigMap和Secret注入环境变量
   - 实现了环境隔离和配置复用

3. **Secret和ConfigMap管理**：
   - 使用Sealed Secrets加密敏感信息
   - 使用External Secrets Operator集成外部密钥管理
   - 实现ConfigMap热更新
   - 建立了安全的密钥管理流程

4. **应用版本管理**：
   - 采用语义化版本控制
   - 实现蓝绿部署和金丝雀发布
   - 使用Flagger自动化金丝雀分析
   - 建立了完整的版本回滚机制

通过本节的实践，你已经掌握了如何使用Helm部署微服务应用，并建立了完整的配置管理和版本管理体系。接下来，我们将学习如何使用服务网格进行流量管理！

---

