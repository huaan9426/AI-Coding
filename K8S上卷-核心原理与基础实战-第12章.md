# 第12章 Kubernetes项目实战与最佳实践

## 本章概述

经过前11章的系统学习，你已经掌握了Kubernetes的核心概念、资源管理、网络存储、安全机制、监控可观测性、高级特性以及故障排查等完整知识体系。本章将通过真实的项目实战，将这些知识融会贯通，帮助你构建生产级的Kubernetes应用。

**为什么需要项目实战？**

理论知识和实际应用之间存在巨大鸿沟：

📚 **理论学习阶段**：
- 理解Pod、Service、Deployment等概念
- 知道如何编写YAML配置文件
- 了解网络、存储、安全的工作原理
- 掌握监控和故障排查方法

🚀 **生产实战阶段**：
- 如何设计高可用的应用架构？
- 如何处理有状态应用的部署？
- 如何实现零停机滚动更新？
- 如何保证应用的安全性和性能？
- 如何建立完整的CI/CD流程？
- 如何应对突发流量和故障？

**本章实战项目**：

我们将通过一个完整的微服务电商系统，涵盖Kubernetes生产实战的各个方面：

```
电商系统架构:
┌─────────────────────────────────────────────────────────────┐
│                        用户层                                 │
│  Web浏览器  ←→  移动App  ←→  第三方系统                      │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                     Ingress层                                 │
│  Nginx Ingress Controller + TLS证书 + 限流                   │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                    API网关层                                  │
│  Kong/APISIX - 认证、鉴权、限流、熔断                        │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                   微服务层                                    │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  │
│  │用户服务  │  │商品服务  │  │订单服务  │  │支付服务  │  │
│  │(Go)      │  │(Java)    │  │(Python)  │  │(Node.js) │  │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘  │
└────────────────────┬────────────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────────────┐
│                    数据层                                     │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  │
│  │MySQL     │  │Redis     │  │MongoDB   │  │RabbitMQ  │  │
│  │(主从)    │  │(集群)    │  │(副本集)  │  │(集群)    │  │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘  │
└─────────────────────────────────────────────────────────────┘
```

**本章主要内容**：

1. **从零搭建生产级Kubernetes集群** (12.1)
   - 集群规划与架构设计
   - 高可用控制平面部署
   - 网络和存储方案选择
   - 安全加固与最佳实践

2. **微服务应用容器化** (12.2)
   - Dockerfile最佳实践
   - 多阶段构建优化
   - 镜像安全扫描
   - 私有镜像仓库搭建

3. **应用部署与配置管理** (12.3)
   - Helm Chart编写
   - 多环境配置管理
   - Secret和ConfigMap管理
   - 应用版本管理

4. **服务网格与流量管理** (12.4)
   - Istio服务网格部署
   - 灰度发布与金丝雀部署
   - 流量路由与熔断
   - 分布式追踪

5. **CI/CD流水线构建** (12.5)
   - GitOps工作流
   - Jenkins/GitLab CI集成
   - 自动化测试
   - 自动化部署

6. **监控告警与日志分析** (12.6)
   - Prometheus监控体系
   - Grafana仪表板
   - EFK日志聚合
   - 告警规则配置

7. **性能优化与成本控制** (12.7)
   - 资源配额优化
   - HPA/VPA自动扩缩容
   - 集群成本分析
   - 性能调优实践

8. **本章总结** (12.8)
   - 项目实战回顾
   - 生产经验总结
   - 持续学习路径

**学习目标**：

- ✅ 能够从零搭建生产级Kubernetes集群
- ✅ 掌握微服务应用的容器化和部署
- ✅ 理解服务网格和流量管理
- ✅ 建立完整的CI/CD流水线
- ✅ 实施全面的监控告警体系
- ✅ 进行性能优化和成本控制
- ✅ 具备独立运维生产集群的能力

**前置知识**：

- 熟练掌握第1-11章的所有内容
- 具备Linux系统管理经验
- 了解Docker容器技术
- 熟悉至少一门编程语言
- 理解微服务架构理念

**技能等级要求**：

```yaml
初级工程师:
  - 能够按照文档搭建测试集群
  - 会部署简单的无状态应用
  - 了解基本的故障排查方法

中级工程师:
  - 能够搭建生产级高可用集群
  - 掌握有状态应用的部署
  - 会配置CI/CD流水线
  - 能够进行性能调优

高级工程师/架构师:
  - 能够设计大规模集群架构
  - 掌握服务网格和高级特性
  - 会制定技术选型和最佳实践
  - 能够解决复杂的生产问题
```

---

## 本章内容概览

```
第12章: Kubernetes项目实战与最佳实践
├── 12.1 从零搭建生产级Kubernetes集群
│   ├── 集群规划与架构设计
│   ├── 高可用控制平面部署
│   ├── 网络方案选择与配置
│   ├── 存储方案选择与配置
│   └── 安全加固与最佳实践
│
├── 12.2 微服务应用容器化
│   ├── Dockerfile最佳实践
│   ├── 多阶段构建优化
│   ├── 镜像安全扫描
│   └── 私有镜像仓库搭建
│
├── 12.3 应用部署与配置管理
│   ├── Helm Chart编写
│   ├── 多环境配置管理
│   ├── Secret和ConfigMap管理
│   └── 应用版本管理
│
├── 12.4 服务网格与流量管理
│   ├── Istio服务网格部署
│   ├── 灰度发布与金丝雀部署
│   ├── 流量路由与熔断
│   └── 分布式追踪
│
├── 12.5 CI/CD流水线构建
│   ├── GitOps工作流
│   ├── Jenkins/GitLab CI集成
│   ├── 自动化测试
│   └── 自动化部署
│
├── 12.6 监控告警与日志分析
│   ├── Prometheus监控体系
│   ├── Grafana仪表板
│   ├── EFK日志聚合
│   └── 告警规则配置
│
├── 12.7 性能优化与成本控制
│   ├── 资源配额优化
│   ├── HPA/VPA自动扩缩容
│   ├── 集群成本分析
│   └── 性能调优实践
│
└── 12.8 本章总结
    ├── 项目实战回顾
    ├── 生产经验总结
    └── 持续学习路径
```

---

## 12.1 从零搭建生产级Kubernetes集群

搭建一个生产级的Kubernetes集群是一项系统工程，需要综合考虑高可用性、安全性、性能和可维护性。本节将带你从零开始，构建一个符合生产标准的Kubernetes集群。

### 12.1.1 集群规划与架构设计

在开始部署之前，必须进行充分的规划和设计。

#### 集群规模评估

**根据业务需求确定集群规模**：

```yaml
小型集群 (开发/测试环境):
  节点数量: 3-5个节点
  Master节点: 1个
  Worker节点: 2-4个
  适用场景:
    - 开发测试环境
    - 小型应用 (<50个Pod)
    - 学习和实验

中型集群 (生产环境):
  节点数量: 10-50个节点
  Master节点: 3个 (高可用)
  Worker节点: 7-47个
  适用场景:
    - 中小型企业生产环境
    - 应用数量: 50-500个Pod
    - 日均请求: 百万级

大型集群 (大规模生产):
  节点数量: 50-5000个节点
  Master节点: 3-5个 (高可用 + 负载均衡)
  Worker节点: 47-4995个
  适用场景:
    - 大型互联网公司
    - 应用数量: 500+个Pod
    - 日均请求: 千万级以上
```

**资源规划示例**：

```yaml
# 中型生产集群资源规划
集群总览:
  节点总数: 15个
  Master节点: 3个
  Worker节点: 12个

Master节点配置:
  CPU: 4核
  内存: 16GB
  磁盘:
    - 系统盘: 100GB SSD
    - etcd数据盘: 200GB SSD (IOPS >3000)
  网络: 万兆网卡

Worker节点配置:
  CPU: 16核
  内存: 64GB
  磁盘:
    - 系统盘: 100GB SSD
    - 容器存储: 500GB SSD
    - 数据存储: 1TB SSD (根据需求)
  网络: 万兆网卡

网络规划:
  Pod CIDR: 10.244.0.0/16 (支持65536个Pod)
  Service CIDR: 10.96.0.0/12 (支持1048576个Service)
  节点网络: 192.168.1.0/24
```

#### 高可用架构设计

**生产级高可用架构**：

```
                    ┌─────────────────────────────────────┐
                    │      外部负载均衡器 (HAProxy/LVS)    │
                    │      VIP: 192.168.1.100             │
                    └──────────────┬──────────────────────┘
                                   │
                    ┌──────────────┴──────────────────────┐
                    │                                      │
         ┌──────────┴──────────┐            ┌────────────┴─────────┐
         │                     │            │                      │
    ┌────┴────┐          ┌────┴────┐  ┌────┴────┐
    │ Master1 │          │ Master2 │  │ Master3 │
    │         │          │         │  │         │
    │ API     │          │ API     │  │ API     │
    │ Server  │          │ Server  │  │ Server  │
    │         │          │         │  │         │
    │ Sched   │          │ Sched   │  │ Sched   │
    │ uler    │          │ uler    │  │ uler    │
    │         │          │         │  │         │
    │ Ctrl    │          │ Ctrl    │  │ Ctrl    │
    │ Manager │          │ Manager │  │ Manager │
    └────┬────┘          └────┬────┘  └────┬────┘
         │                    │            │
         └────────────────────┴────────────┘
                              │
                    ┌─────────┴─────────┐
                    │   etcd集群 (3节点) │
                    │   - etcd1          │
                    │   - etcd2          │
                    │   - etcd3          │
                    └─────────┬─────────┘
                              │
         ┌────────────────────┴────────────────────┐
         │                                          │
    ┌────┴────┐  ┌──────────┐  ┌──────────┐  ┌────┴────┐
    │ Worker1 │  │ Worker2  │  │ Worker3  │  │ Worker12│
    │         │  │          │  │          │  │         │
    │ kubelet │  │ kubelet  │  │ kubelet  │  │ kubelet │
    │ kube-   │  │ kube-    │  │ kube-    │  │ kube-   │
    │ proxy   │  │ proxy    │  │ proxy    │  │ proxy   │
    │         │  │          │  │          │  │         │
    │ 容器    │  │ 容器     │  │ 容器     │  │ 容器    │
    │ 运行时  │  │ 运行时   │  │ 运行时   │  │ 运行时  │
    └─────────┘  └──────────┘  └──────────┘  └─────────┘
```

**高可用关键点**：

1. **控制平面高可用**：
   - 3个Master节点，奇数个避免脑裂
   - 通过负载均衡器提供统一入口
   - API Server无状态，可水平扩展

2. **etcd集群高可用**：
   - 3个etcd节点，支持1个节点故障
   - 独立部署或与Master节点混部
   - 使用SSD磁盘，确保IOPS >3000

3. **负载均衡器高可用**：
   - HAProxy + Keepalived实现VIP漂移
   - 或使用云厂商的负载均衡服务

#### 节点角色规划

**节点分类与职责**：

```yaml
Master节点 (控制平面):
  组件:
    - kube-apiserver: API服务器
    - kube-scheduler: 调度器
    - kube-controller-manager: 控制器管理器
    - etcd: 分布式键值存储
  特点:
    - 不运行业务Pod (通过污点实现)
    - 需要高可用和高性能
    - 资源需求相对稳定

Worker节点 (工作负载):
  组件:
    - kubelet: 节点代理
    - kube-proxy: 网络代理
    - 容器运行时: containerd/CRI-O
  特点:
    - 运行业务Pod
    - 可根据负载动态扩缩容
    - 资源需求波动较大

边缘节点 (可选):
  用途:
    - 运行Ingress Controller
    - 运行监控组件
    - 运行日志收集组件
  特点:
    - 固定IP或域名
    - 较高的网络带宽
    - 独立的资源配额
```

#### 网络方案选择

**常见CNI插件对比**：

| CNI插件 | 性能 | 功能 | 复杂度 | 适用场景 |
|---------|------|------|--------|----------|
| **Flannel** | ⭐⭐⭐ | ⭐⭐ | ⭐ 简单 | 小型集群、学习环境 |
| **Calico** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ 中等 | 生产环境、需要NetworkPolicy |
| **Cilium** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ 复杂 | 大规模集群、需要eBPF |
| **Weave** | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ 简单 | 中小型集群 |

**推荐方案：Calico**

```yaml
选择理由:
  1. 性能优秀: 基于BGP路由，性能接近原生网络
  2. 功能完善: 支持NetworkPolicy、IPAM、BGP等
  3. 社区活跃: 文档完善，问题容易解决
  4. 生产验证: 大量企业生产环境使用

网络模式选择:
  - IPIP模式: 跨子网通信，兼容性好
  - BGP模式: 同子网通信，性能最佳
  - VXLAN模式: 云环境推荐
```

#### 存储方案选择

**存储类型对比**：

```yaml
本地存储 (Local PV):
  优点:
    - 性能最好 (直接访问本地磁盘)
    - 延迟最低
    - 成本最低
  缺点:
    - 不支持Pod迁移
    - 数据可靠性依赖节点
  适用场景:
    - 缓存数据
    - 临时数据
    - 高性能数据库 (配合副本)

网络存储 (NFS/Ceph/GlusterFS):
  优点:
    - 支持Pod迁移
    - 数据高可用
    - 支持多节点共享
  缺点:
    - 性能较差
    - 网络依赖
    - 运维复杂
  适用场景:
    - 共享文件存储
    - 日志收集
    - 配置文件

云存储 (EBS/云盘):
  优点:
    - 高可用
    - 易于管理
    - 按需扩容
  缺点:
    - 成本较高
    - 性能受限于云厂商
    - 厂商锁定
  适用场景:
    - 云上部署
    - 快速上线
    - 中小规模应用
```

**推荐方案：混合存储**

```yaml
存储分层策略:
  高性能层 (Local PV + Rook-Ceph):
    - 数据库: MySQL/PostgreSQL/MongoDB
    - 缓存: Redis/Memcached
    - 消息队列: Kafka/RabbitMQ
  
  标准层 (Ceph RBD):
    - 应用数据
    - 用户上传文件
    - 日志数据
  
  归档层 (对象存储):
    - 备份数据
    - 历史日志
    - 冷数据
```

### 12.1.2 高可用控制平面部署

使用kubeadm部署高可用Kubernetes集群。

#### 环境准备

**节点信息**：

```bash
# 节点规划
192.168.1.11  k8s-master1  # Master节点1
192.168.1.12  k8s-master2  # Master节点2
192.168.1.13  k8s-master3  # Master节点3
192.168.1.21  k8s-worker1  # Worker节点1
192.168.1.22  k8s-worker2  # Worker节点2
192.168.1.23  k8s-worker3  # Worker节点3
192.168.1.100 k8s-api-vip  # API Server VIP
```

**系统要求**：

```bash
# 操作系统
Ubuntu 22.04 LTS / CentOS 8 Stream / Rocky Linux 8

# 内核版本
>= 4.19

# 系统配置
- 关闭swap
- 关闭SELinux/AppArmor (或配置为Permissive)
- 配置防火墙规则
- 同步时间
```

#### 步骤1：所有节点基础配置

```bash
# 1. 配置主机名和hosts
cat >> /etc/hosts << EOF
192.168.1.11 k8s-master1
192.168.1.12 k8s-master2
192.168.1.13 k8s-master3
192.168.1.21 k8s-worker1
192.168.1.22 k8s-worker2
192.168.1.23 k8s-worker3
192.168.1.100 k8s-api-vip
EOF

# 2. 关闭swap
swapoff -a
sed -i '/swap/d' /etc/fstab

# 3. 加载内核模块
cat > /etc/modules-load.d/k8s.conf << EOF
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

# 4. 配置内核参数
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sysctl --system

# 5. 安装容器运行时 (containerd)
# Ubuntu
apt-get update
apt-get install -y containerd

# CentOS/Rocky
dnf install -y containerd

# 配置containerd
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml

# 修改配置使用systemd cgroup driver
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

# 重启containerd
systemctl restart containerd
systemctl enable containerd

# 6. 安装kubeadm、kubelet、kubectl
# Ubuntu
apt-get update
apt-get install -y apt-transport-https ca-certificates curl
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list
apt-get update
apt-get install -y kubelet=1.28.0-1.1 kubeadm=1.28.0-1.1 kubectl=1.28.0-1.1
apt-mark hold kubelet kubeadm kubectl

# CentOS/Rocky
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.28/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.28/rpm/repodata/repomd.xml.key
EOF

dnf install -y kubelet-1.28.0 kubeadm-1.28.0 kubectl-1.28.0
systemctl enable kubelet
```

#### 步骤2：部署负载均衡器

在所有Master节点上部署HAProxy + Keepalived：

```bash
# 安装HAProxy和Keepalived
apt-get install -y haproxy keepalived

# 配置HAProxy
cat > /etc/haproxy/haproxy.cfg << 'HAPROXY_EOF'
global
    log /dev/log local0
    log /dev/log local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    log     global
    mode    tcp
    option  tcplog
    option  dontlognull
    timeout connect 5000
    timeout client  50000
    timeout server  50000

frontend k8s-api
    bind *:6443
    mode tcp
    option tcplog
    default_backend k8s-api-backend

backend k8s-api-backend
    mode tcp
    option tcp-check
    balance roundrobin
    server k8s-master1 192.168.1.11:6443 check fall 3 rise 2
    server k8s-master2 192.168.1.12:6443 check fall 3 rise 2
    server k8s-master3 192.168.1.13:6443 check fall 3 rise 2
HAPROXY_EOF

# 配置Keepalived (Master1)
cat > /etc/keepalived/keepalived.conf << 'KEEPALIVED_EOF'
vrrp_script check_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
    weight 2
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass k8s_ha_pass
    }
    virtual_ipaddress {
        192.168.1.100
    }
    track_script {
        check_haproxy
    }
}
KEEPALIVED_EOF

# Master2和Master3的priority分别设置为99和98

# 启动服务
systemctl restart haproxy
systemctl enable haproxy
systemctl restart keepalived
systemctl enable keepalived
```

#### 步骤3：初始化第一个Master节点

```bash
# 在k8s-master1上执行

# 创建kubeadm配置文件
cat > kubeadm-config.yaml << 'KUBEADM_EOF'
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: "192.168.1.100:6443"
networking:
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"
apiServer:
  certSANs:
  - "192.168.1.100"
  - "k8s-api-vip"
  - "192.168.1.11"
  - "192.168.1.12"
  - "192.168.1.13"
  extraArgs:
    authorization-mode: "Node,RBAC"
etcd:
  local:
    dataDir: /var/lib/etcd
    extraArgs:
      listen-metrics-urls: "http://0.0.0.0:2381"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: systemd
KUBEADM_EOF

# 初始化集群
kubeadm init --config=kubeadm-config.yaml --upload-certs

# 配置kubectl
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# 记录输出的join命令，后续需要使用
```

#### 步骤4：加入其他Master节点

```bash
# 在k8s-master2和k8s-master3上执行
# 使用kubeadm init输出的join命令

kubeadm join 192.168.1.100:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash> \
    --control-plane --certificate-key <certificate-key>

# 配置kubectl
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
```

#### 步骤5：加入Worker节点

```bash
# 在所有Worker节点上执行

kubeadm join 192.168.1.100:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash>
```

#### 步骤6：安装网络插件

```bash
# 在任意Master节点上执行

# 安装Calico
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml

# 等待所有Pod运行
kubectl get pods -n kube-system -w

# 验证节点状态
kubectl get nodes
```

#### 步骤7：验证集群

```bash
# 1. 检查节点状态
kubectl get nodes
# 所有节点应该是Ready状态

# 2. 检查组件状态
kubectl get pods -n kube-system

# 3. 检查etcd集群
kubectl -n kube-system exec -it etcd-k8s-master1 -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list

# 4. 测试高可用
# 停止master1的kubelet
systemctl stop kubelet

# 在其他节点验证API Server仍然可用
kubectl get nodes

# 恢复master1
systemctl start kubelet
```


### 12.1.3 网络和存储配置

#### Calico网络配置优化

```bash
# 下载Calico配置文件
curl -O https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml

# 修改配置
# 1. 修改Pod CIDR (如果与kubeadm配置不同)
# 2. 启用IP-in-IP或VXLAN模式
# 3. 配置MTU大小

# 应用配置
kubectl apply -f calico.yaml

# 验证Calico状态
kubectl get pods -n kube-system -l k8s-app=calico-node
kubectl get pods -n kube-system -l k8s-app=calico-kube-controllers

# 查看Calico节点状态
kubectl exec -n kube-system calico-node-xxxxx -- calicoctl node status
```

**Calico NetworkPolicy示例**：

```yaml
# 限制Pod只能被特定命名空间访问
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-frontend
  namespace: backend
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend
    ports:
    - protocol: TCP
      port: 8080
```

#### 部署Rook-Ceph存储

```bash
# 1. 克隆Rook仓库
git clone --single-branch --branch v1.12.0 https://github.com/rook/rook.git
cd rook/deploy/examples

# 2. 部署Rook Operator
kubectl apply -f crds.yaml
kubectl apply -f common.yaml
kubectl apply -f operator.yaml

# 3. 验证Operator运行
kubectl -n rook-ceph get pod

# 4. 创建Ceph集群
kubectl apply -f cluster.yaml

# 5. 等待集群就绪
kubectl -n rook-ceph get cephcluster

# 6. 创建StorageClass
kubectl apply -f csi/rbd/storageclass.yaml

# 7. 验证StorageClass
kubectl get storageclass
```

**Ceph存储配置示例**：

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v17.2.6
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    count: 2
    allowMultiplePerNode: false
  dashboard:
    enabled: true
    ssl: true
  storage:
    useAllNodes: true
    useAllDevices: false
    deviceFilter: "^sd[b-z]"
    config:
      osdsPerDevice: "1"
```

### 12.1.4 安全加固与最佳实践

#### RBAC权限配置

```yaml
# 创建只读用户
apiVersion: v1
kind: ServiceAccount
metadata:
  name: readonly-user
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: readonly-role
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: readonly-binding
subjects:
- kind: ServiceAccount
  name: readonly-user
  namespace: default
roleRef:
  kind: ClusterRole
  name: readonly-role
  apiGroup: rbac.authorization.k8s.io
```

#### Pod Security Standards

```yaml
# 在命名空间级别启用Pod Security
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

#### 审计日志配置

```yaml
# API Server审计策略
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: Metadata
  resources:
  - group: ""
    resources: ["secrets", "configmaps"]
- level: RequestResponse
  resources:
  - group: ""
    resources: ["pods"]
  verbs: ["create", "delete", "update", "patch"]
- level: Metadata
  omitStages:
  - RequestReceived
```

#### 证书管理

```bash
# 检查证书有效期
kubeadm certs check-expiration

# 续期所有证书
kubeadm certs renew all

# 重启控制平面组件
systemctl restart kubelet
```

#### 安全加固检查清单

```yaml
基础安全:
  - [ ] 关闭不必要的端口
  - [ ] 配置防火墙规则
  - [ ] 禁用root SSH登录
  - [ ] 使用密钥认证
  - [ ] 定期更新系统补丁

Kubernetes安全:
  - [ ] 启用RBAC
  - [ ] 配置Pod Security Standards
  - [ ] 启用审计日志
  - [ ] 配置NetworkPolicy
  - [ ] 使用Secret管理敏感信息
  - [ ] 定期轮换证书
  - [ ] 限制API Server访问
  - [ ] 配置资源配额

容器安全:
  - [ ] 使用非root用户运行容器
  - [ ] 设置只读根文件系统
  - [ ] 禁用特权容器
  - [ ] 扫描镜像漏洞
  - [ ] 使用私有镜像仓库
  - [ ] 配置镜像拉取策略

网络安全:
  - [ ] 启用TLS加密
  - [ ] 配置Ingress TLS
  - [ ] 使用NetworkPolicy隔离
  - [ ] 限制出站流量
  - [ ] 配置DNS策略

数据安全:
  - [ ] 加密etcd数据
  - [ ] 加密Secret
  - [ ] 配置存储加密
  - [ ] 定期备份
  - [ ] 测试恢复流程
```

---

**本节小结**

在12.1节中，我们完成了生产级Kubernetes集群的搭建：

1. **集群规划**：根据业务需求确定集群规模，设计高可用架构，选择合适的网络和存储方案
2. **高可用部署**：使用kubeadm部署3节点Master集群，配置HAProxy+Keepalived实现负载均衡和VIP漂移
3. **网络配置**：部署Calico网络插件，配置NetworkPolicy实现网络隔离
4. **存储配置**：部署Rook-Ceph提供分布式存储，支持动态卷供应
5. **安全加固**：配置RBAC、Pod Security Standards、审计日志，建立完整的安全体系

通过本节的实践，你已经掌握了从零搭建生产级Kubernetes集群的完整流程。接下来，我们将在这个集群上部署微服务应用！

---


## 12.2 微服务应用容器化

在搭建好Kubernetes集群后，下一步是将微服务应用容器化。本节将以电商系统的四个核心服务为例，讲解如何编写高质量的Dockerfile，优化镜像大小和构建速度，并确保镜像安全。

### 12.2.1 Dockerfile最佳实践

#### 基础镜像选择

**镜像选择原则**：

```yaml
生产环境镜像选择:
  优先级1 - 官方镜像:
    - 来源可信，定期更新
    - 安全漏洞及时修复
    - 社区支持完善
  
  优先级2 - Alpine镜像:
    - 体积小 (5MB vs 100MB+)
    - 安全性高 (攻击面小)
    - 适合Go/Node.js等静态编译语言
  
  优先级3 - Distroless镜像:
    - 只包含应用和运行时依赖
    - 无shell，安全性最高
    - 适合Java/Python等
  
  避免使用:
    - latest标签 (不可追溯)
    - 过大的基础镜像 (ubuntu:latest 77MB)
    - 未维护的镜像
```

**常用基础镜像对比**：

| 镜像 | 大小 | 安全性 | 适用场景 |
|------|------|--------|----------|
| alpine:3.18 | 7MB | ⭐⭐⭐⭐⭐ | Go、Node.js、Python |
| debian:12-slim | 74MB | ⭐⭐⭐⭐ | 需要完整工具链 |
| ubuntu:22.04 | 77MB | ⭐⭐⭐ | 开发测试环境 |
| distroless/base | 20MB | ⭐⭐⭐⭐⭐ | 生产环境 |
| scratch | 0MB | ⭐⭐⭐⭐⭐ | 静态编译的Go程序 |

#### Go服务Dockerfile示例

**用户服务 (Go语言)**：

```dockerfile
# 不推荐的写法 ❌
FROM golang:1.21
WORKDIR /app
COPY . .
RUN go build -o user-service
CMD ["./user-service"]

# 问题:
# 1. 镜像过大 (golang:1.21 约1GB)
# 2. 包含编译工具和源代码
# 3. 安全风险高
```

**推荐的写法 ✅**：

```dockerfile
# 多阶段构建 - 用户服务
FROM golang:1.21-alpine AS builder

# 设置工作目录
WORKDIR /build

# 复制go mod文件并下载依赖 (利用缓存)
COPY go.mod go.sum ./
RUN go mod download

# 复制源代码
COPY . .

# 编译 - 静态链接，禁用CGO
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build     -ldflags="-w -s"     -o user-service     ./cmd/user-service

# 最终镜像
FROM alpine:3.18

# 安装CA证书 (HTTPS请求需要)
RUN apk --no-cache add ca-certificates tzdata

# 创建非root用户
RUN addgroup -g 1000 appuser &&     adduser -D -u 1000 -G appuser appuser

# 设置工作目录
WORKDIR /app

# 从构建阶段复制二进制文件
COPY --from=builder /build/user-service .

# 复制配置文件
COPY configs/config.yaml ./configs/

# 切换到非root用户
USER appuser

# 暴露端口
EXPOSE 8080

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3     CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1

# 启动命令
CMD ["./user-service"]
```

**Dockerfile最佳实践要点**：

```yaml
1. 使用多阶段构建:
   - 构建阶段: 包含编译工具
   - 运行阶段: 只包含运行时依赖
   - 镜像大小: 从1GB降到20MB

2. 优化层缓存:
   - 先复制依赖文件 (go.mod)
   - 再复制源代码
   - 依赖不变时可复用缓存

3. 最小化镜像:
   - 使用alpine或distroless
   - 删除不必要的文件
   - 使用.dockerignore

4. 安全加固:
   - 使用非root用户运行
   - 不包含shell (可选)
   - 定期更新基础镜像

5. 添加元数据:
   - LABEL维护者信息
   - LABEL版本信息
   - HEALTHCHECK健康检查
```

#### Java服务Dockerfile示例

**商品服务 (Java Spring Boot)**：

```dockerfile
# 多阶段构建 - 商品服务
FROM maven:3.9-eclipse-temurin-17 AS builder

WORKDIR /build

# 复制pom.xml并下载依赖
COPY pom.xml .
RUN mvn dependency:go-offline

# 复制源代码并构建
COPY src ./src
RUN mvn clean package -DskipTests

# 最终镜像 - 使用JRE而非JDK
FROM eclipse-temurin:17-jre-alpine

# 安装必要工具
RUN apk --no-cache add curl

# 创建非root用户
RUN addgroup -g 1000 appuser &&     adduser -D -u 1000 -G appuser appuser

WORKDIR /app

# 从构建阶段复制jar包
COPY --from=builder /build/target/product-service-*.jar app.jar

# 切换用户
USER appuser

# 暴露端口
EXPOSE 8080

# JVM参数优化
ENV JAVA_OPTS="-Xms512m -Xmx1024m -XX:+UseG1GC -XX:MaxGCPauseMillis=200"

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=40s --retries=3     CMD curl -f http://localhost:8080/actuator/health || exit 1

# 启动命令
ENTRYPOINT ["sh", "-c", "java $JAVA_OPTS -jar app.jar"]
```

#### Python服务Dockerfile示例

**订单服务 (Python Flask)**：

```dockerfile
# 多阶段构建 - 订单服务
FROM python:3.11-slim AS builder

WORKDIR /build

# 安装构建依赖
RUN apt-get update &&     apt-get install -y --no-install-recommends gcc &&     rm -rf /var/lib/apt/lists/*

# 复制requirements并安装依赖
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# 最终镜像
FROM python:3.11-slim

# 安装运行时依赖
RUN apt-get update &&     apt-get install -y --no-install-recommends curl &&     rm -rf /var/lib/apt/lists/*

# 创建非root用户
RUN useradd -m -u 1000 appuser

WORKDIR /app

# 从构建阶段复制Python包
COPY --from=builder /root/.local /home/appuser/.local

# 复制应用代码
COPY --chown=appuser:appuser . .

# 切换用户
USER appuser

# 设置Python路径
ENV PATH=/home/appuser/.local/bin:$PATH
ENV PYTHONUNBUFFERED=1

# 暴露端口
EXPOSE 5000

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3     CMD curl -f http://localhost:5000/health || exit 1

# 启动命令 - 使用gunicorn
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "4", "app:app"]
```

#### Node.js服务Dockerfile示例

**支付服务 (Node.js Express)**：

```dockerfile
# 多阶段构建 - 支付服务
FROM node:20-alpine AS builder

WORKDIR /build

# 复制package文件并安装依赖
COPY package*.json ./
RUN npm ci --only=production

# 最终镜像
FROM node:20-alpine

# 安装dumb-init (正确处理信号)
RUN apk --no-cache add dumb-init

# 创建非root用户
RUN addgroup -g 1000 appuser &&     adduser -D -u 1000 -G appuser appuser

WORKDIR /app

# 从构建阶段复制node_modules
COPY --from=builder /build/node_modules ./node_modules

# 复制应用代码
COPY --chown=appuser:appuser . .

# 切换用户
USER appuser

# 暴露端口
EXPOSE 3000

# 环境变量
ENV NODE_ENV=production

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3     CMD node healthcheck.js || exit 1

# 使用dumb-init启动
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "server.js"]
```

#### .dockerignore文件

```bash
# .dockerignore - 减少构建上下文大小

# Git相关
.git
.gitignore
.gitattributes

# 文档
README.md
CHANGELOG.md
docs/
*.md

# 测试文件
*_test.go
test/
tests/
__tests__/
*.test.js

# 开发工具配置
.vscode/
.idea/
*.swp
*.swo

# 依赖目录 (会在镜像中重新安装)
node_modules/
vendor/
target/

# 构建产物
dist/
build/
*.exe
*.dll
*.so

# 日志和临时文件
*.log
tmp/
temp/
.DS_Store

# 环境变量文件
.env
.env.local
*.pem
*.key
```


### 12.2.2 多阶段构建优化

多阶段构建是Docker的强大特性，可以显著减小镜像大小并提高安全性。

#### 构建缓存优化

**利用BuildKit加速构建**：

```bash
# 启用BuildKit
export DOCKER_BUILDKIT=1

# 使用缓存挂载加速依赖下载
docker build --build-arg BUILDKIT_INLINE_CACHE=1 -t myapp:latest .

# 使用外部缓存
docker build --cache-from myapp:latest -t myapp:v2 .
```

**Go服务优化示例**：

```dockerfile
# 优化的多阶段构建
FROM golang:1.21-alpine AS base
WORKDIR /build
RUN apk add --no-cache git ca-certificates tzdata

# 依赖阶段 - 单独缓存
FROM base AS dependencies
COPY go.mod go.sum ./
RUN --mount=type=cache,target=/go/pkg/mod     go mod download

# 构建阶段
FROM dependencies AS builder
COPY . .
RUN --mount=type=cache,target=/go/pkg/mod     --mount=type=cache,target=/root/.cache/go-build     CGO_ENABLED=0 go build -ldflags="-w -s" -o app .

# 测试阶段 (可选)
FROM builder AS tester
RUN go test -v ./...

# 最终镜像
FROM scratch
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo
COPY --from=builder /build/app /app
ENTRYPOINT ["/app"]
```

**构建命令**：

```bash
# 构建并跳过测试阶段
docker build --target builder -t myapp:latest .

# 构建并运行测试
docker build --target tester -t myapp:test .

# 构建最终镜像
docker build -t myapp:latest .
```

#### 镜像大小对比

**优化前后对比**：

```yaml
用户服务 (Go):
  优化前: 1.2GB (golang:1.21)
  优化后: 15MB (scratch + 静态编译)
  减少: 98.75%

商品服务 (Java):
  优化前: 450MB (openjdk:17)
  优化后: 180MB (eclipse-temurin:17-jre-alpine)
  减少: 60%

订单服务 (Python):
  优化前: 950MB (python:3.11)
  优化后: 120MB (python:3.11-slim + 多阶段)
  减少: 87.4%

支付服务 (Node.js):
  优化前: 1.1GB (node:20)
  优化后: 85MB (node:20-alpine)
  减少: 92.3%
```

#### 并行构建优化

**使用BuildKit并行构建**：

```dockerfile
# syntax=docker/dockerfile:1.4

FROM golang:1.21-alpine AS builder-user
WORKDIR /build
COPY services/user/ .
RUN go build -o user-service .

FROM golang:1.21-alpine AS builder-product
WORKDIR /build
COPY services/product/ .
RUN go build -o product-service .

# 并行构建多个服务
FROM alpine:3.18 AS user-service
COPY --from=builder-user /build/user-service /app/
ENTRYPOINT ["/app/user-service"]

FROM alpine:3.18 AS product-service
COPY --from=builder-product /build/product-service /app/
ENTRYPOINT ["/app/product-service"]
```

#### 构建参数和环境变量

```dockerfile
# 使用ARG传递构建参数
FROM golang:1.21-alpine AS builder

# 构建参数
ARG VERSION=dev
ARG BUILD_TIME
ARG GIT_COMMIT

WORKDIR /build
COPY . .

# 编译时注入版本信息
RUN CGO_ENABLED=0 go build     -ldflags="-X main.Version=${VERSION}               -X main.BuildTime=${BUILD_TIME}               -X main.GitCommit=${GIT_COMMIT}               -w -s"     -o app .

FROM alpine:3.18
COPY --from=builder /build/app /app

# 运行时环境变量
ENV APP_ENV=production     LOG_LEVEL=info

ENTRYPOINT ["/app"]
```

**构建命令**：

```bash
# 传递构建参数
docker build   --build-arg VERSION=v1.2.3   --build-arg BUILD_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")   --build-arg GIT_COMMIT=$(git rev-parse --short HEAD)   -t myapp:v1.2.3 .
```

### 12.2.3 镜像安全扫描

确保容器镜像的安全性是生产环境的关键要求。

#### 使用Trivy扫描镜像

**安装Trivy**：

```bash
# Ubuntu/Debian
wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list
sudo apt-get update
sudo apt-get install trivy

# macOS
brew install trivy

# 使用Docker运行
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock   aquasec/trivy image myapp:latest
```

**扫描镜像**：

```bash
# 扫描本地镜像
trivy image myapp:latest

# 只显示高危和严重漏洞
trivy image --severity HIGH,CRITICAL myapp:latest

# 输出JSON格式
trivy image --format json --output result.json myapp:latest

# 扫描并在发现漏洞时失败
trivy image --exit-code 1 --severity CRITICAL myapp:latest
```

**扫描结果示例**：

```
myapp:latest (alpine 3.18.0)
============================
Total: 2 (HIGH: 1, CRITICAL: 1)

┌───────────────┬────────────────┬──────────┬───────────────────┬───────────────┬────────────────────────────────────┐
│   Library     │ Vulnerability  │ Severity │ Installed Version │ Fixed Version │              Title                 │
├───────────────┼────────────────┼──────────┼───────────────────┼───────────────┼────────────────────────────────────┤
│ openssl       │ CVE-2023-12345 │ CRITICAL │ 3.1.0-r0          │ 3.1.1-r0      │ OpenSSL: Buffer overflow           │
│ libcrypto3    │ CVE-2023-12346 │ HIGH     │ 3.1.0-r0          │ 3.1.1-r0      │ OpenSSL: Memory leak               │
└───────────────┴────────────────┴──────────┴───────────────────┴───────────────┴────────────────────────────────────┘
```

#### 修复漏洞

```dockerfile
# 修复前
FROM alpine:3.18.0
RUN apk add --no-cache openssl

# 修复后 - 更新到最新版本
FROM alpine:3.18
RUN apk add --no-cache --upgrade openssl libcrypto3

# 或使用更新的基础镜像
FROM alpine:3.19
```

#### 集成到CI/CD

**GitLab CI示例**：

```yaml
# .gitlab-ci.yml
stages:
  - build
  - scan
  - deploy

build:
  stage: build
  script:
    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .
    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA

security-scan:
  stage: scan
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 0 --severity LOW,MEDIUM $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
    - trivy image --exit-code 1 --severity HIGH,CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  allow_failure: false

deploy:
  stage: deploy
  script:
    - kubectl set image deployment/myapp myapp=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  only:
    - main
```

#### 使用Cosign签名镜像

**安装Cosign**：

```bash
# 安装cosign
wget https://github.com/sigstore/cosign/releases/download/v2.2.0/cosign-linux-amd64
chmod +x cosign-linux-amd64
sudo mv cosign-linux-amd64 /usr/local/bin/cosign

# 生成密钥对
cosign generate-key-pair

# 签名镜像
cosign sign --key cosign.key myregistry.com/myapp:v1.0.0

# 验证签名
cosign verify --key cosign.pub myregistry.com/myapp:v1.0.0
```

#### 镜像安全最佳实践

```yaml
基础镜像安全:
  - [ ] 使用官方或可信镜像
  - [ ] 使用特定版本标签，避免latest
  - [ ] 定期更新基础镜像
  - [ ] 使用最小化镜像 (alpine/distroless)

构建安全:
  - [ ] 不在镜像中包含敏感信息
  - [ ] 使用.dockerignore排除敏感文件
  - [ ] 使用多阶段构建减少攻击面
  - [ ] 不在镜像中包含构建工具

运行时安全:
  - [ ] 使用非root用户运行
  - [ ] 设置只读根文件系统
  - [ ] 限制容器权限 (capabilities)
  - [ ] 使用安全上下文 (SecurityContext)

漏洞管理:
  - [ ] 定期扫描镜像漏洞
  - [ ] 及时修复高危漏洞
  - [ ] 建立漏洞响应流程
  - [ ] 使用镜像签名验证完整性
```


### 12.2.4 私有镜像仓库搭建

生产环境通常需要私有镜像仓库来管理内部镜像。

#### 使用Harbor搭建企业级镜像仓库

**Harbor特性**：

```yaml
核心功能:
  - 镜像存储和分发
  - 镜像复制 (多地域同步)
  - 漏洞扫描 (集成Trivy)
  - 镜像签名 (Notary)
  - RBAC权限管理
  - 审计日志
  - Webhook通知

企业特性:
  - 高可用部署
  - LDAP/AD集成
  - 配额管理
  - 垃圾回收
  - 镜像代理缓存
```

**使用Helm部署Harbor**：

```bash
# 添加Harbor Helm仓库
helm repo add harbor https://helm.goharbor.io
helm repo update

# 创建命名空间
kubectl create namespace harbor

# 创建values.yaml配置文件
cat > harbor-values.yaml << 'EOF'
expose:
  type: ingress
  tls:
    enabled: true
    certSource: secret
    secret:
      secretName: harbor-tls
  ingress:
    hosts:
      core: harbor.example.com
    className: nginx

externalURL: https://harbor.example.com

persistence:
  enabled: true
  resourcePolicy: "keep"
  persistentVolumeClaim:
    registry:
      storageClass: "rook-ceph-block"
      size: 500Gi
    database:
      storageClass: "rook-ceph-block"
      size: 10Gi
    redis:
      storageClass: "rook-ceph-block"
      size: 5Gi

harborAdminPassword: "ChangeMe123!"

database:
  type: internal

redis:
  type: internal

trivy:
  enabled: true

notary:
  enabled: true

metrics:
  enabled: true
  serviceMonitor:
    enabled: true
EOF

# 部署Harbor
helm install harbor harbor/harbor   -f harbor-values.yaml   -n harbor

# 等待所有Pod运行
kubectl get pods -n harbor -w

# 获取Harbor访问地址
echo "Harbor URL: https://harbor.example.com"
echo "Username: admin"
echo "Password: ChangeMe123!"
```

#### 配置Docker使用私有仓库

```bash
# 1. 登录Harbor
docker login harbor.example.com
# 输入用户名和密码

# 2. 标记镜像
docker tag myapp:latest harbor.example.com/library/myapp:latest

# 3. 推送镜像
docker push harbor.example.com/library/myapp:latest

# 4. 拉取镜像
docker pull harbor.example.com/library/myapp:latest
```

#### 配置Kubernetes使用私有仓库

**创建Docker Registry Secret**：

```bash
# 方法1: 使用kubectl create secret
kubectl create secret docker-registry harbor-secret   --docker-server=harbor.example.com   --docker-username=admin   --docker-password=ChangeMe123!   --docker-email=admin@example.com   -n default

# 方法2: 使用现有的docker config
kubectl create secret generic harbor-secret   --from-file=.dockerconfigjson=$HOME/.docker/config.json   --type=kubernetes.io/dockerconfigjson   -n default
```

**在Pod中使用Secret**：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: myapp
    image: harbor.example.com/library/myapp:latest
  imagePullSecrets:
  - name: harbor-secret
```

**在ServiceAccount中配置**：

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: default
  namespace: default
imagePullSecrets:
- name: harbor-secret
```

#### Harbor项目和用户管理

**创建项目**：

```bash
# 使用Harbor API创建项目
curl -X POST "https://harbor.example.com/api/v2.0/projects"   -H "Content-Type: application/json"   -u "admin:ChangeMe123!"   -d '{
    "project_name": "ecommerce",
    "public": false,
    "metadata": {
      "auto_scan": "true",
      "severity": "high"
    }
  }'
```

**配置镜像复制**：

```yaml
# 配置多地域镜像复制
复制规则:
  源仓库: harbor-beijing.example.com
  目标仓库: harbor-shanghai.example.com
  触发方式: 
    - 手动触发
    - 定时触发 (每天凌晨2点)
    - 事件触发 (镜像推送时)
  过滤规则:
    - 项目: ecommerce/*
    - 标签: v*
```

#### 镜像清理策略

```yaml
# Harbor垃圾回收配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: harbor-gc-config
  namespace: harbor
data:
  config.yaml: |
    # 保留策略
    retention:
      # 保留最近30天的镜像
      - selector:
          tagSelectors:
            - kind: doublestar
              decoration: matches
              pattern: "**"
        action: retain
        params:
          latestPushedK: 30
      
      # 删除未打标签的镜像
      - selector:
          untagged: true
        action: delete
    
    # 定时清理
    schedule:
      cron: "0 2 * * *"  # 每天凌晨2点
```

#### 镜像构建和推送流程

**完整的CI/CD流程**：

```bash
#!/bin/bash
# build-and-push.sh

set -e

# 配置
HARBOR_URL="harbor.example.com"
PROJECT="ecommerce"
IMAGE_NAME="user-service"
VERSION="${CI_COMMIT_TAG:-latest}"
FULL_IMAGE="${HARBOR_URL}/${PROJECT}/${IMAGE_NAME}:${VERSION}"

echo "Building image: ${FULL_IMAGE}"

# 1. 构建镜像
docker build   --build-arg VERSION=${VERSION}   --build-arg BUILD_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")   --build-arg GIT_COMMIT=$(git rev-parse --short HEAD)   -t ${FULL_IMAGE}   .

# 2. 扫描镜像
echo "Scanning image for vulnerabilities..."
trivy image --exit-code 1 --severity CRITICAL ${FULL_IMAGE}

# 3. 推送镜像
echo "Pushing image to Harbor..."
docker push ${FULL_IMAGE}

# 4. 签名镜像 (可选)
if [ -f "cosign.key" ]; then
  echo "Signing image..."
  cosign sign --key cosign.key ${FULL_IMAGE}
fi

# 5. 触发部署
echo "Image pushed successfully: ${FULL_IMAGE}"
```

---

**本节小结**

在12.2节中，我们完成了微服务应用的容器化：

1. **Dockerfile最佳实践**：
   - 选择合适的基础镜像（Alpine、Distroless）
   - 编写了Go、Java、Python、Node.js四种语言的优化Dockerfile
   - 使用非root用户、健康检查等安全措施
   - 配置.dockerignore减少构建上下文

2. **多阶段构建优化**：
   - 利用BuildKit加速构建和缓存
   - 镜像大小平均减少85%以上
   - 使用构建参数注入版本信息
   - 并行构建多个服务

3. **镜像安全扫描**：
   - 使用Trivy扫描镜像漏洞
   - 集成到CI/CD流程
   - 使用Cosign签名镜像
   - 建立漏洞修复流程

4. **私有镜像仓库**：
   - 使用Harbor搭建企业级镜像仓库
   - 配置镜像复制和清理策略
   - 集成漏洞扫描和镜像签名
   - 建立完整的镜像管理流程

通过本节的实践，你已经掌握了如何将微服务应用容器化，并建立了完整的镜像构建、扫描、存储和分发体系。接下来，我们将学习如何使用Helm部署这些容器化应用！

---


## 12.3 应用部署与配置管理

在完成应用容器化后，下一步是将应用部署到Kubernetes集群。Helm是Kubernetes的包管理工具，可以简化应用的部署和管理。本节将讲解如何使用Helm Chart部署微服务应用，并实现多环境配置管理。

### 12.3.1 Helm Chart编写

#### Helm基础概念

**Helm核心组件**：

```yaml
Chart (图表):
  - 包含Kubernetes资源定义的包
  - 类似于apt的deb包或yum的rpm包
  - 包含Chart.yaml、values.yaml和模板文件

Release (发布):
  - Chart的一个实例
  - 同一个Chart可以安装多次，每次都是一个新的Release
  - 例如：mysql-dev、mysql-prod

Repository (仓库):
  - 存储和分享Chart的地方
  - 类似于Docker Hub
  - 官方仓库：https://artifacthub.io/
```

#### 创建用户服务Helm Chart

**Chart目录结构**：

```bash
user-service/
├── Chart.yaml              # Chart元数据
├── values.yaml             # 默认配置值
├── values-dev.yaml         # 开发环境配置
├── values-prod.yaml        # 生产环境配置
├── templates/              # Kubernetes资源模板
│   ├── deployment.yaml     # Deployment模板
│   ├── service.yaml        # Service模板
│   ├── ingress.yaml        # Ingress模板
│   ├── configmap.yaml      # ConfigMap模板
│   ├── secret.yaml         # Secret模板
│   ├── hpa.yaml            # HPA模板
│   ├── serviceaccount.yaml # ServiceAccount模板
│   ├── _helpers.tpl        # 辅助模板函数
│   └── NOTES.txt           # 安装后的提示信息
└── .helmignore             # 忽略文件
```

**Chart.yaml**：

```yaml
apiVersion: v2
name: user-service
description: 用户服务 - 电商系统核心服务
type: application
version: 1.0.0        # Chart版本
appVersion: "v1.2.3"  # 应用版本

keywords:
  - ecommerce
  - user
  - microservice

maintainers:
  - name: DevOps Team
    email: devops@example.com

dependencies: []

annotations:
  category: Microservice
```

**values.yaml**：

```yaml
# 默认配置值
replicaCount: 2

image:
  repository: harbor.example.com/ecommerce/user-service
  pullPolicy: IfNotPresent
  tag: ""  # 默认使用Chart.yaml中的appVersion

imagePullSecrets:
  - name: harbor-secret

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true

service:
  type: ClusterIP
  port: 80
  targetPort: 8080
  annotations: {}

ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
  hosts:
    - host: user-api.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: user-service-tls
      hosts:
        - user-api.example.com

resources:
  limits:
    cpu: 1000m
    memory: 512Mi
  requests:
    cpu: 100m
    memory: 128Mi

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

livenessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: http
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

nodeSelector: {}

tolerations: []

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - user-service
          topologyKey: kubernetes.io/hostname

config:
  logLevel: info
  database:
    host: mysql.database.svc.cluster.local
    port: 3306
    name: users
  redis:
    host: redis.cache.svc.cluster.local
    port: 6379
  jwt:
    expiresIn: 3600

secrets:
  database:
    username: user_service
    password: ""  # 从外部注入
  redis:
    password: ""  # 从外部注入
  jwt:
    secret: ""    # 从外部注入
```

**templates/deployment.yaml**：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "user-service.fullname" . }}
  labels:
    {{- include "user-service.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "user-service.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
        checksum/secret: {{ include (print $.Template.BasePath "/secret.yaml") . | sha256sum }}
        {{- with .Values.podAnnotations }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      labels:
        {{- include "user-service.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "user-service.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
      - name: {{ .Chart.Name }}
        securityContext:
          {{- toYaml .Values.securityContext | nindent 12 }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        ports:
        - name: http
          containerPort: {{ .Values.service.targetPort }}
          protocol: TCP
        livenessProbe:
          {{- toYaml .Values.livenessProbe | nindent 12 }}
        readinessProbe:
          {{- toYaml .Values.readinessProbe | nindent 12 }}
        resources:
          {{- toYaml .Values.resources | nindent 12 }}
        env:
        - name: LOG_LEVEL
          value: {{ .Values.config.logLevel | quote }}
        - name: DB_HOST
          value: {{ .Values.config.database.host | quote }}
        - name: DB_PORT
          value: {{ .Values.config.database.port | quote }}
        - name: DB_NAME
          value: {{ .Values.config.database.name | quote }}
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: {{ include "user-service.fullname" . }}
              key: db-username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: {{ include "user-service.fullname" . }}
              key: db-password
        - name: REDIS_HOST
          value: {{ .Values.config.redis.host | quote }}
        - name: REDIS_PORT
          value: {{ .Values.config.redis.port | quote }}
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: {{ include "user-service.fullname" . }}
              key: redis-password
        - name: JWT_SECRET
          valueFrom:
            secretKeyRef:
              name: {{ include "user-service.fullname" . }}
              key: jwt-secret
        - name: JWT_EXPIRES_IN
          value: {{ .Values.config.jwt.expiresIn | quote }}
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /app/cache
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir: {}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
```

**templates/service.yaml**：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "user-service.fullname" . }}
  labels:
    {{- include "user-service.labels" . | nindent 4 }}
  {{- with .Values.service.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "user-service.selectorLabels" . | nindent 4 }}
```

**templates/_helpers.tpl**：

```yaml
{{/*
Expand the name of the chart.
*/}}
{{- define "user-service.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
*/}}
{{- define "user-service.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "user-service.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "user-service.labels" -}}
helm.sh/chart: {{ include "user-service.chart" . }}
{{ include "user-service.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "user-service.selectorLabels" -}}
app.kubernetes.io/name: {{ include "user-service.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Create the name of the service account to use
*/}}
{{- define "user-service.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "user-service.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}
```


#### Helm命令使用

```bash
# 1. 安装Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# 2. 验证Chart语法
helm lint user-service/

# 3. 渲染模板（不安装）
helm template user-service user-service/   --values user-service/values-dev.yaml   --set image.tag=v1.2.3

# 4. 安装Chart到开发环境
helm install user-service-dev user-service/   --namespace dev   --create-namespace   --values user-service/values-dev.yaml   --set image.tag=v1.2.3

# 5. 查看Release状态
helm list -n dev
helm status user-service-dev -n dev

# 6. 升级Release
helm upgrade user-service-dev user-service/   --namespace dev   --values user-service/values-dev.yaml   --set image.tag=v1.2.4

# 7. 回滚Release
helm rollback user-service-dev 1 -n dev

# 8. 卸载Release
helm uninstall user-service-dev -n dev

# 9. 查看历史版本
helm history user-service-dev -n dev
```

### 12.3.2 多环境配置管理

在掌握了Helm Chart的基本编写方法后，我们需要解决一个实际问题：如何管理不同环境的配置？开发、测试、生产环境往往需要不同的副本数、资源限制、镜像标签等配置。Helm提供了values文件机制来优雅地解决这个问题。

#### 开发环境配置

**values-dev.yaml**：

```yaml
# 开发环境配置
replicaCount: 1

image:
  tag: dev-latest
  pullPolicy: Always

resources:
  limits:
    cpu: 500m
    memory: 256Mi
  requests:
    cpu: 50m
    memory: 64Mi

autoscaling:
  enabled: false

ingress:
  enabled: true
  hosts:
    - host: user-api-dev.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: user-service-dev-tls
      hosts:
        - user-api-dev.example.com

config:
  logLevel: debug
  database:
    host: mysql-dev.database.svc.cluster.local
  redis:
    host: redis-dev.cache.svc.cluster.local

# 开发环境使用NodePort便于调试
service:
  type: NodePort
```

#### 生产环境配置

**values-prod.yaml**：

```yaml
# 生产环境配置
replicaCount: 3

image:
  tag: v1.2.3
  pullPolicy: IfNotPresent

resources:
  limits:
    cpu: 2000m
    memory: 1Gi
  requests:
    cpu: 500m
    memory: 512Mi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

ingress:
  enabled: true
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "1000"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  hosts:
    - host: user-api.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: user-service-prod-tls
      hosts:
        - user-api.example.com

config:
  logLevel: info
  database:
    host: mysql-prod.database.svc.cluster.local
  redis:
    host: redis-prod.cache.svc.cluster.local

# 生产环境Pod反亲和性
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
                - user-service
        topologyKey: kubernetes.io/hostname

# 生产环境节点选择器
nodeSelector:
  node-role: production
```

#### 使用Kustomize管理多环境

**目录结构**：

```bash
kustomize/
├── base/                   # 基础配置
│   ├── kustomization.yaml
│   ├── deployment.yaml
│   ├── service.yaml
│   └── configmap.yaml
├── overlays/               # 环境覆盖
│   ├── dev/
│   │   ├── kustomization.yaml
│   │   ├── replica-patch.yaml
│   │   └── resource-patch.yaml
│   ├── staging/
│   │   └── kustomization.yaml
│   └── prod/
│       ├── kustomization.yaml
│       ├── replica-patch.yaml
│       └── hpa.yaml
```

**base/kustomization.yaml**：

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - deployment.yaml
  - service.yaml
  - configmap.yaml

commonLabels:
  app: user-service
  managed-by: kustomize

namespace: default
```

**overlays/prod/kustomization.yaml**：

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

bases:
  - ../../base

namespace: production

commonLabels:
  environment: production

patchesStrategicMerge:
  - replica-patch.yaml

resources:
  - hpa.yaml

images:
  - name: user-service
    newName: harbor.example.com/ecommerce/user-service
    newTag: v1.2.3

configMapGenerator:
  - name: user-service-config
    behavior: merge
    literals:
      - LOG_LEVEL=info
      - ENVIRONMENT=production
```

**使用Kustomize部署**：

```bash
# 查看渲染结果
kubectl kustomize overlays/prod/

# 部署到生产环境
kubectl apply -k overlays/prod/

# 删除部署
kubectl delete -k overlays/prod/
```

#### 环境变量注入

**使用ConfigMap和Secret**：

```yaml
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-service-config
data:
  LOG_LEVEL: "info"
  DB_HOST: "mysql.database.svc.cluster.local"
  DB_PORT: "3306"
  REDIS_HOST: "redis.cache.svc.cluster.local"
  REDIS_PORT: "6379"

---
# secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: user-service-secret
type: Opaque
stringData:
  DB_USERNAME: "user_service"
  DB_PASSWORD: "SecurePassword123!"
  REDIS_PASSWORD: "RedisPass456!"
  JWT_SECRET: "jwt-secret-key-change-me"
```

**在Deployment中使用**：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
spec:
  template:
    spec:
      containers:
      - name: user-service
        envFrom:
        - configMapRef:
            name: user-service-config
        - secretRef:
            name: user-service-secret
        # 或者单独引用
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: user-service-secret
              key: DB_PASSWORD
```

### 12.3.3 Secret和ConfigMap管理

在多环境配置中，我们使用了Secret来存储敏感信息。但直接将Secret明文存储在Git仓库中是极其危险的。本小节将介绍如何安全地管理Secret和ConfigMap，包括加密存储、外部密钥管理系统集成，以及配置热更新等最佳实践。

#### 使用Sealed Secrets

**安装Sealed Secrets**：

```bash
# 安装Sealed Secrets Controller
kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.24.0/controller.yaml

# 安装kubeseal CLI
wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.24.0/kubeseal-0.24.0-linux-amd64.tar.gz
tar xfz kubeseal-0.24.0-linux-amd64.tar.gz
sudo install -m 755 kubeseal /usr/local/bin/kubeseal
```

**创建Sealed Secret**：

```bash
# 1. 创建普通Secret（不提交到Git）
kubectl create secret generic user-service-secret   --from-literal=DB_PASSWORD=SecurePassword123!   --from-literal=JWT_SECRET=jwt-secret-key   --dry-run=client -o yaml > secret.yaml

# 2. 加密Secret
kubeseal --format=yaml < secret.yaml > sealed-secret.yaml

# 3. 提交sealed-secret.yaml到Git（安全）
git add sealed-secret.yaml
git commit -m "Add sealed secret"

# 4. 部署Sealed Secret
kubectl apply -f sealed-secret.yaml

# Sealed Secrets Controller会自动解密并创建Secret
```

**sealed-secret.yaml示例**：

```yaml
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: user-service-secret
  namespace: default
spec:
  encryptedData:
    DB_PASSWORD: AgBx8F7j3k...  # 加密后的数据
    JWT_SECRET: AgCy9G8k4l...
  template:
    metadata:
      name: user-service-secret
    type: Opaque
```

#### 使用External Secrets Operator

**安装External Secrets Operator**：

```bash
helm repo add external-secrets https://charts.external-secrets.io
helm install external-secrets   external-secrets/external-secrets   -n external-secrets-system   --create-namespace
```

**配置AWS Secrets Manager**：

```yaml
# secretstore.yaml
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: aws-secretsmanager
  namespace: default
spec:
  provider:
    aws:
      service: SecretsManager
      region: us-east-1
      auth:
        jwt:
          serviceAccountRef:
            name: external-secrets-sa

---
# externalsecret.yaml
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: user-service-secret
  namespace: default
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: aws-secretsmanager
    kind: SecretStore
  target:
    name: user-service-secret
    creationPolicy: Owner
  data:
  - secretKey: DB_PASSWORD
    remoteRef:
      key: prod/user-service/db-password
  - secretKey: JWT_SECRET
    remoteRef:
      key: prod/user-service/jwt-secret
```


#### ConfigMap热更新

```yaml
# 使用Reloader自动重启Pod
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-service-config
  annotations:
    reloader.stakater.com/match: "true"
data:
  config.yaml: |
    log_level: info
    database:
      host: mysql.database.svc.cluster.local
      port: 3306

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  template:
    spec:
      containers:
      - name: user-service
        volumeMounts:
        - name: config
          mountPath: /app/config
      volumes:
      - name: config
        configMap:
          name: user-service-config
```

### 12.3.4 应用版本管理

完成了应用部署和配置管理后，我们还需要建立完善的版本管理体系。如何标记版本？如何安全地发布新版本？如何在出现问题时快速回滚？本小节将介绍语义化版本控制、蓝绿部署、金丝雀发布等生产级版本管理策略。

#### 语义化版本控制

**版本号规范**：

```yaml
版本格式: MAJOR.MINOR.PATCH

MAJOR (主版本号):
  - 不兼容的API变更
  - 重大架构调整
  - 示例: 1.0.0 -> 2.0.0

MINOR (次版本号):
  - 向后兼容的功能新增
  - 功能改进
  - 示例: 1.0.0 -> 1.1.0

PATCH (修订号):
  - 向后兼容的bug修复
  - 安全补丁
  - 示例: 1.0.0 -> 1.0.1

预发布版本:
  - 1.0.0-alpha.1
  - 1.0.0-beta.2
  - 1.0.0-rc.1

构建元数据:
  - 1.0.0+20231201
  - 1.0.0+sha.5114f85
```

#### 使用Git标签管理版本

```bash
# 创建版本标签
git tag -a v1.2.3 -m "Release version 1.2.3"
git push origin v1.2.3

# 查看所有标签
git tag -l

# 基于标签构建镜像
docker build -t harbor.example.com/ecommerce/user-service:v1.2.3 .
docker push harbor.example.com/ecommerce/user-service:v1.2.3
```

#### 蓝绿部署

**蓝绿部署策略**：

```yaml
# 部署绿色版本（新版本）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service-green
  labels:
    app: user-service
    version: green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user-service
      version: green
  template:
    metadata:
      labels:
        app: user-service
        version: green
    spec:
      containers:
      - name: user-service
        image: harbor.example.com/ecommerce/user-service:v1.2.4

---
# Service指向蓝色版本（当前版本）
apiVersion: v1
kind: Service
metadata:
  name: user-service
spec:
  selector:
    app: user-service
    version: blue  # 当前指向蓝色
  ports:
  - port: 80
    targetPort: 8080

# 切换流量到绿色版本
# kubectl patch service user-service -p '{"spec":{"selector":{"version":"green"}}}'

# 验证无误后删除蓝色版本
# kubectl delete deployment user-service-blue
```

#### 金丝雀发布

**使用Flagger实现金丝雀发布**：

```bash
# 安装Flagger
kubectl apply -k github.com/fluxcd/flagger//kustomize/linkerd

# 创建Canary资源
cat > canary.yaml << 'EOF'
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: user-service
  namespace: default
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service
  service:
    port: 80
    targetPort: 8080
  analysis:
    interval: 1m
    threshold: 5
    maxWeight: 50
    stepWeight: 10
    metrics:
    - name: request-success-rate
      thresholdRange:
        min: 99
      interval: 1m
    - name: request-duration
      thresholdRange:
        max: 500
      interval: 1m
    webhooks:
    - name: load-test
      url: http://flagger-loadtester/
      timeout: 5s
      metadata:
        cmd: "hey -z 1m -q 10 -c 2 http://user-service-canary/"
EOF

kubectl apply -f canary.yaml
```

**金丝雀发布流程**：

```yaml
发布流程:
  1. 初始状态:
     - Primary: v1.2.3 (100%流量)
     - Canary: v1.2.3 (0%流量)
  
  2. 部署新版本:
     - 更新Deployment镜像为v1.2.4
     - Flagger检测到变更
  
  3. 金丝雀分析:
     - 第1分钟: Canary 10%流量
     - 第2分钟: Canary 20%流量
     - 第3分钟: Canary 30%流量
     - 第4分钟: Canary 40%流量
     - 第5分钟: Canary 50%流量
  
  4. 成功条件:
     - 请求成功率 >= 99%
     - 请求延迟 <= 500ms
     - 连续5次检查通过
  
  5. 发布完成:
     - Primary更新为v1.2.4 (100%流量)
     - Canary缩容为0
  
  6. 失败回滚:
     - 任何指标不达标
     - 自动回滚到v1.2.3
     - 发送告警通知
```

#### 版本回滚

**使用Helm回滚**：

```bash
# 查看发布历史
helm history user-service -n production

# 回滚到上一个版本
helm rollback user-service -n production

# 回滚到指定版本
helm rollback user-service 3 -n production

# 查看回滚状态
kubectl rollout status deployment/user-service -n production
```

**使用kubectl回滚**：

```bash
# 查看Deployment历史
kubectl rollout history deployment/user-service -n production

# 查看特定版本详情
kubectl rollout history deployment/user-service --revision=2 -n production

# 回滚到上一个版本
kubectl rollout undo deployment/user-service -n production

# 回滚到指定版本
kubectl rollout undo deployment/user-service --to-revision=2 -n production

# 暂停发布
kubectl rollout pause deployment/user-service -n production

# 恢复发布
kubectl rollout resume deployment/user-service -n production
```

#### 版本管理最佳实践

```yaml
版本标记:
  - [ ] 使用语义化版本号
  - [ ] 镜像标签与Git标签一致
  - [ ] 避免使用latest标签
  - [ ] 记录版本变更日志

发布策略:
  - [ ] 开发环境: 直接部署
  - [ ] 测试环境: 蓝绿部署
  - [ ] 生产环境: 金丝雀发布
  - [ ] 关键服务: 人工审批

回滚准备:
  - [ ] 保留至少3个历史版本
  - [ ] 测试回滚流程
  - [ ] 准备回滚脚本
  - [ ] 建立回滚决策标准

监控验证:
  - [ ] 部署前健康检查
  - [ ] 部署中实时监控
  - [ ] 部署后验证测试
  - [ ] 记录部署日志
```

---

**本节小结**

在12.3节中，我们完成了应用部署与配置管理：

1. **Helm Chart编写**：
   - 创建了完整的Helm Chart结构
   - 编写了Deployment、Service、Ingress等模板
   - 使用_helpers.tpl定义辅助函数
   - 配置了健康检查、资源限制、安全上下文

2. **多环境配置管理**：
   - 使用values-dev.yaml和values-prod.yaml管理不同环境
   - 使用Kustomize实现配置覆盖
   - 通过ConfigMap和Secret注入环境变量
   - 实现了环境隔离和配置复用

3. **Secret和ConfigMap管理**：
   - 使用Sealed Secrets加密敏感信息
   - 使用External Secrets Operator集成外部密钥管理
   - 实现ConfigMap热更新
   - 建立了安全的密钥管理流程

4. **应用版本管理**：
   - 采用语义化版本控制
   - 实现蓝绿部署和金丝雀发布
   - 使用Flagger自动化金丝雀分析
   - 建立了完整的版本回滚机制

通过本节的实践，你已经掌握了如何使用Helm部署微服务应用，并建立了完整的配置管理和版本管理体系。接下来，我们将学习如何使用服务网格进行流量管理！

---


## 12.4 服务网格与流量管理

在完成了应用的部署和配置管理后，我们面临着微服务架构带来的新挑战：如何管理服务间的通信？如何实现灰度发布？如何进行流量控制和故障注入？传统的方式需要在每个服务中编写大量的治理代码。服务网格（Service Mesh）提供了一种优雅的解决方案，通过Sidecar代理模式，将服务治理能力从应用代码中剥离出来。

**为什么需要服务网格？**

```yaml
微服务通信挑战:
  服务发现:
    - 如何找到服务实例？
    - 如何处理服务动态变化？
  
  负载均衡:
    - 如何在多个实例间分配流量？
    - 如何实现智能路由？
  
  故障处理:
    - 如何处理服务超时？
    - 如何实现熔断和重试？
  
  安全通信:
    - 如何加密服务间通信？
    - 如何实现服务认证和授权？
  
  可观测性:
    - 如何追踪请求链路？
    - 如何收集服务指标？

传统解决方案的问题:
  - 每个服务都要实现相同的治理逻辑
  - 不同语言需要不同的SDK
  - 升级困难，需要修改所有服务
  - 治理代码与业务代码耦合

服务网格的优势:
  - 治理能力与应用解耦
  - 语言无关，支持任何语言
  - 统一管理，集中配置
  - 无需修改应用代码
```

**Istio架构概览**：

```
Istio服务网格架构:
┌─────────────────────────────────────────────────────────────┐
│                        控制平面 (Control Plane)              │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐     │
│  │   Istiod     │  │   Pilot      │  │   Citadel    │     │
│  │ (统一组件)   │  │ (流量管理)   │  │ (证书管理)   │     │
│  └──────────────┘  └──────────────┘  └──────────────┘     │
└────────────────────────┬────────────────────────────────────┘
                         │ 配置下发
┌────────────────────────┴────────────────────────────────────┐
│                        数据平面 (Data Plane)                 │
│  ┌─────────────────┐  ┌─────────────────┐                  │
│  │  Pod A          │  │  Pod B          │                  │
│  │  ┌───────────┐  │  │  ┌───────────┐  │                  │
│  │  │  App      │  │  │  │  App      │  │                  │
│  │  └─────┬─────┘  │  │  └─────┬─────┘  │                  │
│  │  ┌─────┴─────┐  │  │  ┌─────┴─────┐  │                  │
│  │  │  Envoy    │◄─┼──┼─►│  Envoy    │  │                  │
│  │  │  Sidecar  │  │  │  │  Sidecar  │  │                  │
│  │  └───────────┘  │  │  └───────────┘  │                  │
│  └─────────────────┘  └─────────────────┘                  │
└─────────────────────────────────────────────────────────────┘
```

### 12.4.1 Istio服务网格部署

#### Istio核心组件

**Istiod（控制平面）**：

```yaml
Istiod组件功能:
  Pilot (服务发现与流量管理):
    - 服务发现: 从Kubernetes获取服务信息
    - 流量管理: 将VirtualService、DestinationRule转换为Envoy配置
    - 配置分发: 通过xDS协议下发配置到Envoy
  
  Citadel (安全与证书管理):
    - 证书签发: 为每个服务签发mTLS证书
    - 证书轮换: 自动更新证书
    - 身份认证: 基于SPIFFE的服务身份
  
  Galley (配置验证):
    - 配置验证: 验证Istio配置的正确性
    - 配置转换: 将用户配置转换为内部格式

Envoy (数据平面):
  - 高性能C++代理
  - 支持HTTP/1.1、HTTP/2、gRPC、TCP
  - 动态配置更新
  - 丰富的可观测性
```

#### 安装Istio

**下载Istio**：

```bash
# 下载最新版本Istio
curl -L https://istio.io/downloadIstio | sh -

# 进入Istio目录
cd istio-1.20.0

# 将istioctl添加到PATH
export PATH=$PWD/bin:$PATH

# 验证安装
istioctl version
```

**安装Istio到Kubernetes集群**：

```bash
# 1. 使用demo配置文件安装（适合学习和测试）
istioctl install --set profile=demo -y

# 输出示例：
# ✔ Istio core installed
# ✔ Istiod installed
# ✔ Ingress gateways installed
# ✔ Egress gateways installed
# ✔ Installation complete

# 2. 生产环境推荐配置
istioctl install --set profile=production -y

# 3. 自定义配置
cat > istio-config.yaml << 'EOF'
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-control-plane
spec:
  profile: default
  
  # 控制平面配置
  components:
    pilot:
      k8s:
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 1000m
            memory: 4Gi
        hpaSpec:
          minReplicas: 2
          maxReplicas: 5
    
    # Ingress Gateway配置
    ingressGateways:
    - name: istio-ingressgateway
      enabled: true
      k8s:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 2000m
            memory: 1Gi
        hpaSpec:
          minReplicas: 2
          maxReplicas: 5
        service:
          type: LoadBalancer
          ports:
          - port: 80
            targetPort: 8080
            name: http2
          - port: 443
            targetPort: 8443
            name: https
  
  # 全局配置
  meshConfig:
    # 访问日志
    accessLogFile: /dev/stdout
    accessLogEncoding: JSON
    
    # 默认配置
    defaultConfig:
      # 追踪配置
      tracing:
        sampling: 100.0
        zipkin:
          address: jaeger-collector.istio-system:9411
      
      # 代理资源限制
      proxyMetadata:
        CPU_LIMIT: "2000m"
        MEMORY_LIMIT: "1Gi"
    
    # 出站流量策略
    outboundTrafficPolicy:
      mode: REGISTRY_ONLY  # 只允许访问注册的服务
    
    # 启用自动mTLS
    enableAutoMtls: true
EOF

istioctl install -f istio-config.yaml -y
```

**验证Istio安装**：

```bash
# 检查Istio组件状态
kubectl get pods -n istio-system

# 输出示例：
# NAME                                    READY   STATUS    RESTARTS   AGE
# istio-ingressgateway-5c7f8f4d4f-abc12   1/1     Running   0          2m
# istiod-6b9c8d8f9c-xyz34                 1/1     Running   0          2m

# 检查Istio配置
istioctl verify-install

# 查看Istio版本
istioctl version
```

#### 为应用注入Sidecar

**自动注入（推荐）**：

```bash
# 为命名空间启用自动注入
kubectl label namespace default istio-injection=enabled

# 验证标签
kubectl get namespace -L istio-injection

# 重新部署应用以注入Sidecar
kubectl rollout restart deployment/user-service

# 验证Sidecar注入
kubectl get pod user-service-xxx -o jsonpath='{.spec.containers[*].name}'
# 输出: user-service istio-proxy
```

**手动注入**：

```bash
# 手动注入Sidecar到YAML
istioctl kube-inject -f deployment.yaml | kubectl apply -f -

# 或者先生成注入后的YAML
istioctl kube-inject -f deployment.yaml > deployment-injected.yaml
kubectl apply -f deployment-injected.yaml
```

**验证Sidecar注入**：

```bash
# 查看Pod详情
kubectl describe pod user-service-xxx

# 应该看到两个容器：
# Containers:
#   user-service:
#     Image: harbor.example.com/ecommerce/user-service:v1.2.3
#   istio-proxy:
#     Image: docker.io/istio/proxyv2:1.20.0

# 查看Sidecar日志
kubectl logs user-service-xxx -c istio-proxy

# 检查Envoy配置
istioctl proxy-config cluster user-service-xxx
istioctl proxy-config listener user-service-xxx
istioctl proxy-config route user-service-xxx
```

#### 部署示例应用

**部署BookInfo示例应用**：

```bash
# 部署BookInfo应用
kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml

# 验证部署
kubectl get pods

# 输出示例：
# NAME                              READY   STATUS    RESTARTS   AGE
# details-v1-5f4d584748-abc12       2/2     Running   0          1m
# productpage-v1-564d4686f-xyz34    2/2     Running   0          1m
# ratings-v1-686ccfb5d8-def56       2/2     Running   0          1m
# reviews-v1-86896b7648-ghi78       2/2     Running   0          1m
# reviews-v2-b7dcd394c-jkl90        2/2     Running   0          1m
# reviews-v3-5c5cc7b6d-mno12        2/2     Running   0          1m

# 验证服务
kubectl get services

# 测试应用内部访问
kubectl exec -it productpage-v1-xxx -c productpage -- curl -s http://details:9080/details/0
```

**配置Ingress Gateway**：


```bash
# 创建Gateway资源
cat > bookinfo-gateway.yaml << 'EOF'
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: bookinfo-gateway
spec:
  selector:
    istio: ingressgateway  # 使用默认的Ingress Gateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "bookinfo.example.com"
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: bookinfo
spec:
  hosts:
  - "bookinfo.example.com"
  gateways:
  - bookinfo-gateway
  http:
  - match:
    - uri:
        exact: /productpage
    - uri:
        prefix: /static
    - uri:
        exact: /login
    - uri:
        exact: /logout
    - uri:
        prefix: /api/v1/products
    route:
    - destination:
        host: productpage
        port:
          number: 9080
EOF

kubectl apply -f bookinfo-gateway.yaml

# 获取Ingress Gateway地址
export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT

# 测试访问
curl -s http://$GATEWAY_URL/productpage | grep -o "<title>.*</title>"
```

#### Istio流量管理核心概念

**Gateway（网关）**：

```yaml
# Gateway定义入口流量
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: ecommerce-gateway
  namespace: default
spec:
  selector:
    istio: ingressgateway
  servers:
  # HTTP配置
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*.example.com"
    # HTTP重定向到HTTPS
    tls:
      httpsRedirect: true
  
  # HTTPS配置
  - port:
      number: 443
      name: https
      protocol: HTTPS
    hosts:
    - "*.example.com"
    tls:
      mode: SIMPLE
      credentialName: example-com-cert  # Secret名称
```

**VirtualService（虚拟服务）**：

```yaml
# VirtualService定义路由规则
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  # 路由规则1: 基于URI前缀
  - match:
    - uri:
        prefix: /api/v1/users
    route:
    - destination:
        host: user-service
        subset: v1
      weight: 90
    - destination:
        host: user-service
        subset: v2
      weight: 10
  
  # 路由规则2: 基于请求头
  - match:
    - headers:
        x-user-type:
          exact: premium
    route:
    - destination:
        host: user-service
        subset: v2
  
  # 默认路由
  - route:
    - destination:
        host: user-service
        subset: v1
```

**DestinationRule（目标规则）**：

```yaml
# DestinationRule定义服务子集和策略
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: user-service
spec:
  host: user-service
  
  # 流量策略
  trafficPolicy:
    # 负载均衡
    loadBalancer:
      simple: LEAST_REQUEST  # 最少请求
    
    # 连接池
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100
        maxRequestsPerConnection: 2
    
    # 异常检测（熔断）
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
      minHealthPercent: 40
  
  # 定义服务子集
  subsets:
  - name: v1
    labels:
      version: v1
    trafficPolicy:
      loadBalancer:
        simple: ROUND_ROBIN
  
  - name: v2
    labels:
      version: v2
    trafficPolicy:
      loadBalancer:
        simple: RANDOM
```

**ServiceEntry（服务条目）**：

```yaml
# ServiceEntry允许访问外部服务
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: external-api
spec:
  hosts:
  - api.external.com
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  location: MESH_EXTERNAL
  resolution: DNS
```

#### Istio可观测性

**Kiali（服务网格可视化）**：

```bash
# 安装Kiali
kubectl apply -f samples/addons/kiali.yaml

# 等待Kiali就绪
kubectl rollout status deployment/kiali -n istio-system

# 访问Kiali Dashboard
istioctl dashboard kiali

# 或者使用端口转发
kubectl port-forward -n istio-system svc/kiali 20001:20001

# 浏览器访问: http://localhost:20001
```

**Prometheus（指标收集）**：

```bash
# 安装Prometheus
kubectl apply -f samples/addons/prometheus.yaml

# 访问Prometheus
istioctl dashboard prometheus
```

**Grafana（指标可视化）**：

```bash
# 安装Grafana
kubectl apply -f samples/addons/grafana.yaml

# 访问Grafana
istioctl dashboard grafana

# Grafana预置了Istio仪表板：
# - Istio Mesh Dashboard
# - Istio Service Dashboard
# - Istio Workload Dashboard
# - Istio Performance Dashboard
```

**Jaeger（分布式追踪）**：

```bash
# 安装Jaeger
kubectl apply -f samples/addons/jaeger.yaml

# 访问Jaeger UI
istioctl dashboard jaeger
```

#### Istio安全

**启用mTLS（双向TLS）**：

```yaml
# 全局启用严格mTLS
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: istio-system
spec:
  mtls:
    mode: STRICT  # 严格模式，所有流量必须使用mTLS

---
# 命名空间级别配置
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: default
spec:
  mtls:
    mode: PERMISSIVE  # 宽容模式，允许明文和mTLS

---
# 服务级别配置
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: user-service
  namespace: default
spec:
  selector:
    matchLabels:
      app: user-service
  mtls:
    mode: STRICT
  portLevelMtls:
    8080:
      mode: DISABLE  # 特定端口禁用mTLS
```

**授权策略**：

```yaml
# 拒绝所有访问（默认拒绝）
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: deny-all
  namespace: default
spec:
  {}

---
# 允许特定服务访问
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-order-to-user
  namespace: default
spec:
  selector:
    matchLabels:
      app: user-service
  action: ALLOW
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/default/sa/order-service"]
    to:
    - operation:
        methods: ["GET", "POST"]
        paths: ["/api/v1/users/*"]

---
# 基于JWT的授权
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: require-jwt
  namespace: default
spec:
  selector:
    matchLabels:
      app: user-service
  action: ALLOW
  rules:
  - from:
    - source:
        requestPrincipals: ["*"]
    when:
    - key: request.auth.claims[role]
      values: ["admin", "user"]
```

**RequestAuthentication（请求认证）**：

```yaml
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-auth
  namespace: default
spec:
  selector:
    matchLabels:
      app: user-service
  jwtRules:
  - issuer: "https://auth.example.com"
    jwksUri: "https://auth.example.com/.well-known/jwks.json"
    audiences:
    - "user-service"
    forwardOriginalToken: true
```


### 12.4.2 灰度发布与金丝雀部署

在完成Istio的部署和基础配置后，我们可以利用其强大的流量管理能力实现灰度发布和金丝雀部署。与12.3.4节中使用Flagger的方式不同，Istio提供了更细粒度的流量控制能力，可以基于权重、请求头、Cookie等多种条件进行流量分配。

#### 基于权重的流量分配

**场景：用户服务v1到v2的渐进式发布**

```bash
# 1. 部署v1版本
cat > user-service-v1.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service-v1
  labels:
    app: user-service
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user-service
      version: v1
  template:
    metadata:
      labels:
        app: user-service
        version: v1
    spec:
      containers:
      - name: user-service
        image: harbor.example.com/ecommerce/user-service:v1.2.3
        ports:
        - containerPort: 8080
        env:
        - name: VERSION
          value: "v1"
---
apiVersion: v1
kind: Service
metadata:
  name: user-service
  labels:
    app: user-service
spec:
  selector:
    app: user-service
  ports:
  - port: 80
    targetPort: 8080
    name: http
EOF

kubectl apply -f user-service-v1.yaml

# 2. 部署v2版本
cat > user-service-v2.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service-v2
  labels:
    app: user-service
    version: v2
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user-service
      version: v2
  template:
    metadata:
      labels:
        app: user-service
        version: v2
    spec:
      containers:
      - name: user-service
        image: harbor.example.com/ecommerce/user-service:v1.3.0
        ports:
        - containerPort: 8080
        env:
        - name: VERSION
          value: "v2"
EOF

kubectl apply -f user-service-v2.yaml
```

**配置流量分配**：

```yaml
# 3. 创建DestinationRule定义版本子集
cat > user-service-destinationrule.yaml << 'EOF'
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: user-service
spec:
  host: user-service
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
EOF

kubectl apply -f user-service-destinationrule.yaml

# 4. 创建VirtualService配置流量权重
cat > user-service-virtualservice.yaml << 'EOF'
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  - route:
    - destination:
        host: user-service
        subset: v1
      weight: 90  # 90%流量到v1
    - destination:
        host: user-service
        subset: v2
      weight: 10  # 10%流量到v2
EOF

kubectl apply -f user-service-virtualservice.yaml
```

**渐进式流量切换**：

```bash
# 阶段1: 10% v2流量（已配置）
# 观察5分钟，检查错误率和延迟

# 阶段2: 增加到30% v2流量
kubectl patch virtualservice user-service --type merge -p '
{
  "spec": {
    "http": [{
      "route": [
        {"destination": {"host": "user-service", "subset": "v1"}, "weight": 70},
        {"destination": {"host": "user-service", "subset": "v2"}, "weight": 30}
      ]
    }]
  }
}'

# 阶段3: 增加到50% v2流量
kubectl patch virtualservice user-service --type merge -p '
{
  "spec": {
    "http": [{
      "route": [
        {"destination": {"host": "user-service", "subset": "v1"}, "weight": 50},
        {"destination": {"host": "user-service", "subset": "v2"}, "weight": 50}
      ]
    }]
  }
}'

# 阶段4: 全部切换到v2
kubectl patch virtualservice user-service --type merge -p '
{
  "spec": {
    "http": [{
      "route": [
        {"destination": {"host": "user-service", "subset": "v2"}, "weight": 100}
      ]
    }]
  }
}'

# 阶段5: 删除v1版本
kubectl delete deployment user-service-v1
```

#### 基于请求头的路由

**场景：内部测试用户访问新版本**

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  # 规则1: 测试用户访问v2
  - match:
    - headers:
        x-user-type:
          exact: tester
    route:
    - destination:
        host: user-service
        subset: v2
  
  # 规则2: VIP用户访问v2
  - match:
    - headers:
        x-user-level:
          exact: vip
    route:
    - destination:
        host: user-service
        subset: v2
  
  # 规则3: 特定用户ID访问v2
  - match:
    - headers:
        x-user-id:
          regex: "^(1001|1002|1003)$"
    route:
    - destination:
        host: user-service
        subset: v2
  
  # 默认规则: 其他用户访问v1
  - route:
    - destination:
        host: user-service
        subset: v1
```

**测试请求头路由**：

```bash
# 普通用户访问v1
curl http://user-service/api/v1/users

# 测试用户访问v2
curl -H "x-user-type: tester" http://user-service/api/v1/users

# VIP用户访问v2
curl -H "x-user-level: vip" http://user-service/api/v1/users

# 特定用户ID访问v2
curl -H "x-user-id: 1001" http://user-service/api/v1/users
```

#### 基于Cookie的路由

**场景：通过Cookie实现A/B测试**

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  # 规则1: Cookie中包含version=v2的用户访问v2
  - match:
    - headers:
        cookie:
          regex: ".*version=v2.*"
    route:
    - destination:
        host: user-service
        subset: v2
  
  # 规则2: 随机50%用户访问v2（通过一致性哈希）
  - route:
    - destination:
        host: user-service
        subset: v1
      weight: 50
    - destination:
        host: user-service
        subset: v2
      weight: 50
    # 基于Cookie的一致性哈希，确保同一用户始终访问同一版本
    headers:
      request:
        set:
          x-version: v2
      response:
        set:
          Set-Cookie: "version=v2; Path=/; Max-Age=86400"
```

#### 基于URI的路由

**场景：新API版本灰度发布**

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  # 规则1: /api/v2路径访问v2版本
  - match:
    - uri:
        prefix: /api/v2
    rewrite:
      uri: /api/v1  # 重写URI
    route:
    - destination:
        host: user-service
        subset: v2
  
  # 规则2: /api/v1路径访问v1版本
  - match:
    - uri:
        prefix: /api/v1
    route:
    - destination:
        host: user-service
        subset: v1
  
  # 规则3: 特定端点访问v2
  - match:
    - uri:
        exact: /api/v1/users/profile
    route:
    - destination:
        host: user-service
        subset: v2
  
  # 默认规则
  - route:
    - destination:
        host: user-service
        subset: v1
```

#### 金丝雀部署最佳实践

**完整的金丝雀发布流程**：

```yaml
# 1. 初始状态：100% v1
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  - route:
    - destination:
        host: user-service
        subset: v1
      weight: 100

---
# 2. 金丝雀阶段1：5%流量到v2（内部测试）
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  # 内部测试用户100%访问v2
  - match:
    - headers:
        x-user-type:
          exact: internal
    route:
    - destination:
        host: user-service
        subset: v2
  
  # 其他用户5%访问v2
  - route:
    - destination:
        host: user-service
        subset: v1
      weight: 95
    - destination:
        host: user-service
        subset: v2
      weight: 5

---
# 3. 金丝雀阶段2：20%流量到v2（小范围用户）
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  - route:
    - destination:
        host: user-service
        subset: v1
      weight: 80
    - destination:
        host: user-service
        subset: v2
      weight: 20

---
# 4. 金丝雀阶段3：50%流量到v2（大范围验证）
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  - route:
    - destination:
        host: user-service
        subset: v1
      weight: 50
    - destination:
        host: user-service
        subset: v2
      weight: 50

---
# 5. 完成发布：100%流量到v2
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  - route:
    - destination:
        host: user-service
        subset: v2
      weight: 100
```

**监控指标**：

```bash
# 使用Prometheus查询成功率
sum(rate(istio_requests_total{destination_service="user-service.default.svc.cluster.local",response_code!~"5.*"}[5m])) 
/ 
sum(rate(istio_requests_total{destination_service="user-service.default.svc.cluster.local"}[5m]))

# 查询延迟P99
histogram_quantile(0.99, 
  sum(rate(istio_request_duration_milliseconds_bucket{destination_service="user-service.default.svc.cluster.local"}[5m])) by (le)
)

# 查询错误率
sum(rate(istio_requests_total{destination_service="user-service.default.svc.cluster.local",response_code=~"5.*"}[5m]))
```

**自动化金丝雀脚本**：

```bash
#!/bin/bash
# canary-deploy.sh

SERVICE_NAME="user-service"
NAMESPACE="default"

# 金丝雀阶段配置
STAGES=(5 20 50 100)
WAIT_TIME=300  # 每个阶段等待5分钟

# 成功标准
MAX_ERROR_RATE=0.01  # 1%错误率
MAX_P99_LATENCY=500  # 500ms P99延迟

function check_metrics() {
    local subset=$1
    
    # 检查错误率
    error_rate=$(kubectl exec -n istio-system deploy/prometheus -c prometheus --         promtool query instant 'sum(rate(istio_requests_total{destination_service="'$SERVICE_NAME'.'$NAMESPACE'.svc.cluster.local",destination_version="'$subset'",response_code=~"5.*"}[5m])) / sum(rate(istio_requests_total{destination_service="'$SERVICE_NAME'.'$NAMESPACE'.svc.cluster.local",destination_version="'$subset'"}[5m]))'         | grep -oP '\d+\.\d+' | head -1)
    
    if (( $(echo "$error_rate > $MAX_ERROR_RATE" | bc -l) )); then
        echo "❌ 错误率过高: $error_rate"
        return 1
    fi
    
    echo "✅ 错误率正常: $error_rate"
    return 0
}

function update_traffic() {
    local v1_weight=$1
    local v2_weight=$2
    
    echo "📊 更新流量分配: v1=$v1_weight%, v2=$v2_weight%"
    
    kubectl patch virtualservice $SERVICE_NAME -n $NAMESPACE --type merge -p "
{
  "spec": {
    "http": [{
      "route": [
        {"destination": {"host": "$SERVICE_NAME", "subset": "v1"}, "weight": $v1_weight},
        {"destination": {"host": "$SERVICE_NAME", "subset": "v2"}, "weight": $v2_weight}
      ]
    }]
  }
}"
}

function rollback() {
    echo "🔄 检测到问题，回滚到v1"
    update_traffic 100 0
    exit 1
}

# 主流程
echo "🚀 开始金丝雀发布: $SERVICE_NAME"

for stage in "${STAGES[@]}"; do
    v2_weight=$stage
    v1_weight=$((100 - stage))
    
    update_traffic $v1_weight $v2_weight
    
    echo "⏳ 等待 $WAIT_TIME 秒观察指标..."
    sleep $WAIT_TIME
    
    if ! check_metrics "v2"; then
        rollback
    fi
    
    echo "✅ 阶段 $stage% 验证通过"
done

echo "🎉 金丝雀发布完成！"
```


### 12.4.3 流量路由与熔断

在实现了灰度发布后，我们还需要处理更复杂的流量管理场景：如何实现智能路由？如何处理服务超时？如何防止故障扩散？Istio的流量管理和弹性能力可以帮助我们构建健壮的微服务系统。

#### 高级流量路由

**基于地理位置的路由**：

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  # 规则1: 亚洲用户路由到亚洲区域
  - match:
    - headers:
        x-user-region:
          exact: asia
    route:
    - destination:
        host: user-service
        subset: asia
  
  # 规则2: 欧洲用户路由到欧洲区域
  - match:
    - headers:
        x-user-region:
          exact: europe
    route:
    - destination:
        host: user-service
        subset: europe
  
  # 默认路由到最近的区域
  - route:
    - destination:
        host: user-service
        subset: default

---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: user-service
spec:
  host: user-service
  subsets:
  - name: asia
    labels:
      region: asia
  - name: europe
    labels:
      region: europe
  - name: default
    labels:
      region: us
```

**基于负载的智能路由**：

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: user-service
spec:
  host: user-service
  trafficPolicy:
    # 负载均衡策略
    loadBalancer:
      consistentHash:
        httpHeaderName: x-user-id  # 基于用户ID的一致性哈希
    
    # 或者使用最少请求策略
    # loadBalancer:
    #   simple: LEAST_REQUEST
    
    # 或者使用加权最少请求
    # loadBalancer:
    #   simple: LEAST_CONN
  
  subsets:
  - name: v1
    labels:
      version: v1
    trafficPolicy:
      loadBalancer:
        simple: ROUND_ROBIN
  
  - name: v2
    labels:
      version: v2
    trafficPolicy:
      loadBalancer:
        simple: RANDOM
```

**镜像流量（影子流量）**：

```yaml
# 将生产流量镜像到测试版本，用于验证新版本
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  - route:
    - destination:
        host: user-service
        subset: v1
      weight: 100
    # 镜像流量到v2版本
    mirror:
      host: user-service
      subset: v2
    mirrorPercentage:
      value: 100.0  # 镜像100%的流量
```

#### 超时与重试

**配置请求超时**：

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  - route:
    - destination:
        host: user-service
    # 请求超时设置
    timeout: 3s
    
    # 重试策略
    retries:
      attempts: 3
      perTryTimeout: 1s
      retryOn: 5xx,reset,connect-failure,refused-stream
```

**不同端点的超时配置**：

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  # 查询接口：短超时
  - match:
    - uri:
        prefix: /api/v1/users/search
    route:
    - destination:
        host: user-service
    timeout: 1s
    retries:
      attempts: 2
      perTryTimeout: 500ms
  
  # 导出接口：长超时
  - match:
    - uri:
        prefix: /api/v1/users/export
    route:
    - destination:
        host: user-service
    timeout: 30s
    retries:
      attempts: 1
      perTryTimeout: 30s
  
  # 默认配置
  - route:
    - destination:
        host: user-service
    timeout: 5s
    retries:
      attempts: 3
      perTryTimeout: 2s
```

#### 熔断器配置

**连接池和熔断**：

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: user-service
spec:
  host: user-service
  trafficPolicy:
    # 连接池配置
    connectionPool:
      tcp:
        maxConnections: 100        # 最大TCP连接数
        connectTimeout: 30ms       # 连接超时
        tcpKeepalive:
          time: 7200s
          interval: 75s
      
      http:
        http1MaxPendingRequests: 50    # HTTP/1.1最大挂起请求
        http2MaxRequests: 100          # HTTP/2最大请求数
        maxRequestsPerConnection: 2    # 每个连接最大请求数
        maxRetries: 3                  # 最大重试次数
        idleTimeout: 3600s             # 空闲超时
    
    # 异常检测（熔断）
    outlierDetection:
      consecutiveErrors: 5           # 连续错误次数
      interval: 30s                  # 检测间隔
      baseEjectionTime: 30s          # 基础驱逐时间
      maxEjectionPercent: 50         # 最大驱逐百分比
      minHealthPercent: 40           # 最小健康百分比
      
      # 基于成功率的驱逐
      splitExternalLocalOriginErrors: true
      consecutiveLocalOriginFailures: 5
      consecutiveGatewayErrors: 5
```

**熔断器工作原理**：

```yaml
熔断器状态机:
  关闭状态 (Closed):
    - 正常处理请求
    - 统计错误次数
    - 错误次数达到阈值 → 打开状态
  
  打开状态 (Open):
    - 快速失败，不发送请求
    - 等待baseEjectionTime
    - 时间到达 → 半开状态
  
  半开状态 (Half-Open):
    - 允许部分请求通过
    - 如果成功 → 关闭状态
    - 如果失败 → 打开状态

驱逐策略:
  1. 连续错误驱逐:
     - 连续5次错误 → 驱逐30秒
     - 再次错误 → 驱逐60秒（2倍）
     - 最多驱逐300秒（10倍）
  
  2. 成功率驱逐:
     - 统计窗口内成功率 < 阈值
     - 驱逐该实例
  
  3. 延迟驱逐:
     - P99延迟 > 阈值
     - 驱逐该实例
```

**测试熔断器**：

```bash
# 部署测试客户端
kubectl apply -f - << 'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: fortio
  labels:
    app: fortio
spec:
  containers:
  - name: fortio
    image: fortio/fortio:latest
    ports:
    - containerPort: 8080
      name: http
EOF

# 测试正常请求
kubectl exec fortio -c fortio -- fortio load -c 2 -qps 0 -n 20 -loglevel Warning http://user-service/api/v1/users

# 触发熔断：并发50个连接
kubectl exec fortio -c fortio -- fortio load -c 50 -qps 0 -n 1000 -loglevel Warning http://user-service/api/v1/users

# 查看熔断统计
kubectl exec fortio -c fortio -- fortio load -c 50 -qps 0 -n 1000 -loglevel Warning http://user-service/api/v1/users | grep "Code 503"

# 查看Envoy统计信息
kubectl exec user-service-xxx -c istio-proxy -- pilot-agent request GET stats | grep user-service | grep pending
kubectl exec user-service-xxx -c istio-proxy -- pilot-agent request GET stats | grep user-service | grep overflow
```

#### 故障注入

**延迟注入**：

```yaml
# 模拟网络延迟
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  - fault:
      delay:
        percentage:
          value: 10.0  # 10%的请求
        fixedDelay: 5s  # 延迟5秒
    route:
    - destination:
        host: user-service
```

**错误注入**：

```yaml
# 模拟服务错误
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  - fault:
      abort:
        percentage:
          value: 10.0  # 10%的请求
        httpStatus: 500  # 返回500错误
    route:
    - destination:
        host: user-service
```

**混合故障注入**：

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  # 对特定用户注入故障
  - match:
    - headers:
        x-user-id:
          exact: "test-user"
    fault:
      delay:
        percentage:
          value: 100.0
        fixedDelay: 2s
      abort:
        percentage:
          value: 20.0
        httpStatus: 503
    route:
    - destination:
        host: user-service
  
  # 正常流量
  - route:
    - destination:
        host: user-service
```

#### 流量管理最佳实践

**生产环境配置示例**：

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: user-service-production
  namespace: production
spec:
  host: user-service
  
  trafficPolicy:
    # 负载均衡：最少请求
    loadBalancer:
      simple: LEAST_REQUEST
    
    # 连接池：保守配置
    connectionPool:
      tcp:
        maxConnections: 200
        connectTimeout: 30ms
      http:
        http1MaxPendingRequests: 100
        http2MaxRequests: 200
        maxRequestsPerConnection: 5
        maxRetries: 3
        idleTimeout: 1800s
    
    # 熔断：快速失败
    outlierDetection:
      consecutiveErrors: 3
      interval: 10s
      baseEjectionTime: 30s
      maxEjectionPercent: 30
      minHealthPercent: 50
    
    # TLS配置
    tls:
      mode: ISTIO_MUTUAL
  
  subsets:
  - name: stable
    labels:
      version: stable
    trafficPolicy:
      loadBalancer:
        simple: ROUND_ROBIN
  
  - name: canary
    labels:
      version: canary
    trafficPolicy:
      loadBalancer:
        simple: RANDOM
      # 金丝雀版本更严格的熔断
      outlierDetection:
        consecutiveErrors: 2
        interval: 5s
        baseEjectionTime: 60s

---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service-production
  namespace: production
spec:
  hosts:
  - user-service
  http:
  # 健康检查端点：不重试
  - match:
    - uri:
        exact: /health
    route:
    - destination:
        host: user-service
    timeout: 1s
    retries:
      attempts: 0
  
  # 查询接口：短超时，多重试
  - match:
    - uri:
        prefix: /api/v1/users
      method:
        exact: GET
    route:
    - destination:
        host: user-service
        subset: stable
      weight: 95
    - destination:
        host: user-service
        subset: canary
      weight: 5
    timeout: 3s
    retries:
      attempts: 3
      perTryTimeout: 1s
      retryOn: 5xx,reset,connect-failure
  
  # 写入接口：长超时，少重试
  - match:
    - uri:
        prefix: /api/v1/users
      method:
        regex: "POST|PUT|DELETE"
    route:
    - destination:
        host: user-service
        subset: stable
    timeout: 10s
    retries:
      attempts: 1
      perTryTimeout: 10s
      retryOn: reset,connect-failure
  
  # 默认配置
  - route:
    - destination:
        host: user-service
        subset: stable
    timeout: 5s
    retries:
      attempts: 2
      perTryTimeout: 2s
```

**监控熔断和重试**：

```bash
# Prometheus查询熔断次数
sum(envoy_cluster_upstream_rq_pending_overflow{cluster_name=~".*user-service.*"})

# 查询重试次数
sum(rate(envoy_cluster_upstream_rq_retry{cluster_name=~".*user-service.*"}[5m]))

# 查询超时次数
sum(rate(envoy_cluster_upstream_rq_timeout{cluster_name=~".*user-service.*"}[5m]))

# 查询驱逐的实例数
sum(envoy_cluster_outlier_detection_ejections_active{cluster_name=~".*user-service.*"})
```


### 12.4.4 分布式追踪

在微服务架构中，一个用户请求可能会经过多个服务，如何追踪请求的完整链路？如何定位性能瓶颈？分布式追踪系统可以帮助我们可视化请求流转过程，分析服务依赖关系，快速定位问题。

#### 分布式追踪基础概念

**OpenTracing标准**：

```yaml
核心概念:
  Trace (追踪):
    - 一次完整的请求链路
    - 由多个Span组成
    - 示例: 用户下单请求
  
  Span (跨度):
    - 一次服务调用
    - 包含操作名称、开始时间、持续时间
    - 示例: 调用用户服务查询用户信息
  
  SpanContext (跨度上下文):
    - Trace ID: 全局唯一的追踪ID
    - Span ID: 当前Span的ID
    - Parent Span ID: 父Span的ID
    - Baggage: 跨服务传递的键值对
  
  Tags (标签):
    - Span的元数据
    - 示例: http.method=GET, http.status_code=200
  
  Logs (日志):
    - Span内的事件记录
    - 示例: 数据库查询开始、查询结束

追踪链路示例:
  Trace: 用户下单
  ├── Span1: API Gateway (100ms)
  │   ├── Span2: 用户服务 (20ms)
  │   │   └── Span3: MySQL查询 (10ms)
  │   ├── Span4: 商品服务 (30ms)
  │   │   └── Span5: Redis查询 (5ms)
  │   └── Span6: 订单服务 (40ms)
  │       ├── Span7: MySQL写入 (15ms)
  │       └── Span8: RabbitMQ发送 (10ms)
```

#### 部署Jaeger

**使用Operator部署Jaeger**：

```bash
# 1. 安装Jaeger Operator
kubectl create namespace observability
kubectl create -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.51.0/jaeger-operator.yaml -n observability

# 2. 等待Operator就绪
kubectl wait --for=condition=available --timeout=300s deployment/jaeger-operator -n observability

# 3. 部署Jaeger实例
cat > jaeger-instance.yaml << 'EOF'
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger
  namespace: istio-system
spec:
  strategy: production  # 生产模式
  
  # 存储配置
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: http://elasticsearch:9200
        index-prefix: jaeger
    esIndexCleaner:
      enabled: true
      numberOfDays: 7  # 保留7天数据
      schedule: "55 23 * * *"
  
  # Collector配置
  collector:
    maxReplicas: 5
    resources:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 512Mi
  
  # Query配置
  query:
    replicas: 2
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 200m
        memory: 256Mi
  
  # Agent配置（Sidecar模式）
  agent:
    strategy: DaemonSet
    resources:
      limits:
        cpu: 200m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 64Mi
EOF

kubectl apply -f jaeger-instance.yaml

# 4. 验证部署
kubectl get pods -n istio-system -l app.kubernetes.io/instance=jaeger

# 5. 访问Jaeger UI
kubectl port-forward -n istio-system svc/jaeger-query 16686:16686

# 浏览器访问: http://localhost:16686
```

**简化部署（开发/测试环境）**：

```bash
# 使用all-in-one模式
kubectl apply -f - << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: istio-system
  labels:
    app: jaeger
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.51
        env:
        - name: COLLECTOR_ZIPKIN_HOST_PORT
          value: ":9411"
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        ports:
        - containerPort: 5775
          protocol: UDP
        - containerPort: 6831
          protocol: UDP
        - containerPort: 6832
          protocol: UDP
        - containerPort: 5778
          protocol: TCP
        - containerPort: 16686
          protocol: TCP
        - containerPort: 9411
          protocol: TCP
        - containerPort: 4317
          protocol: TCP
        - containerPort: 4318
          protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-collector
  namespace: istio-system
  labels:
    app: jaeger
spec:
  ports:
  - name: jaeger-collector-http
    port: 14268
    targetPort: 14268
  - name: jaeger-collector-grpc
    port: 14250
    targetPort: 14250
  - name: zipkin
    port: 9411
    targetPort: 9411
  selector:
    app: jaeger
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-query
  namespace: istio-system
  labels:
    app: jaeger
spec:
  ports:
  - name: query-http
    port: 16686
    targetPort: 16686
  selector:
    app: jaeger
EOF
```

#### 配置Istio追踪

**启用追踪采样**：

```bash
# 配置Istio追踪
cat > istio-tracing-config.yaml << 'EOF'
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-tracing
spec:
  meshConfig:
    # 启用访问日志
    accessLogFile: /dev/stdout
    accessLogEncoding: JSON
    
    # 追踪配置
    enableTracing: true
    defaultConfig:
      tracing:
        sampling: 100.0  # 采样率100%（生产环境建议1-10%）
        max_path_tag_length: 256
        zipkin:
          address: jaeger-collector.istio-system:9411
    
    # 扩展提供者配置
    extensionProviders:
    - name: jaeger
      zipkin:
        service: jaeger-collector.istio-system
        port: 9411
EOF

istioctl install -f istio-tracing-config.yaml -y

# 或者使用Telemetry API配置
cat > telemetry-tracing.yaml << 'EOF'
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: mesh-default
  namespace: istio-system
spec:
  tracing:
  - providers:
    - name: jaeger
    randomSamplingPercentage: 100.0
    customTags:
      cluster_id:
        literal:
          value: "production-cluster"
      region:
        environment:
          name: REGION
          defaultValue: "us-east-1"
EOF

kubectl apply -f telemetry-tracing.yaml
```

**应用级追踪配置**：

```yaml
# 为特定命名空间配置追踪
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: namespace-tracing
  namespace: default
spec:
  tracing:
  - providers:
    - name: jaeger
    randomSamplingPercentage: 50.0  # 50%采样率
    customTags:
      namespace:
        literal:
          value: "default"
      app_version:
        literal:
          value: "v1.2.3"

---
# 为特定服务配置追踪
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: user-service-tracing
  namespace: default
spec:
  selector:
    matchLabels:
      app: user-service
  tracing:
  - providers:
    - name: jaeger
    randomSamplingPercentage: 100.0  # 关键服务100%采样
    customTags:
      service_name:
        literal:
          value: "user-service"
      version:
        literal:
          value: "v1.2.3"
```

#### 应用代码集成

**Go应用集成OpenTelemetry**：

```go
package main

import (
    "context"
    "log"
    "net/http"
    
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/exporters/jaeger"
    "go.opentelemetry.io/otel/propagation"
    "go.opentelemetry.io/otel/sdk/resource"
    "go.opentelemetry.io/otel/sdk/trace"
    semconv "go.opentelemetry.io/otel/semconv/v1.17.0"
    oteltrace "go.opentelemetry.io/otel/trace"
)

func initTracer() (*trace.TracerProvider, error) {
    // 创建Jaeger exporter
    exporter, err := jaeger.New(jaeger.WithCollectorEndpoint(
        jaeger.WithEndpoint("http://jaeger-collector.istio-system:14268/api/traces"),
    ))
    if err != nil {
        return nil, err
    }
    
    // 创建TracerProvider
    tp := trace.NewTracerProvider(
        trace.WithBatcher(exporter),
        trace.WithResource(resource.NewWithAttributes(
            semconv.SchemaURL,
            semconv.ServiceName("user-service"),
            semconv.ServiceVersion("v1.2.3"),
        )),
    )
    
    otel.SetTracerProvider(tp)
    otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(
        propagation.TraceContext{},
        propagation.Baggage{},
    ))
    
    return tp, nil
}

func getUserHandler(w http.ResponseWriter, r *http.Request) {
    // 从请求中提取追踪上下文
    ctx := otel.GetTextMapPropagator().Extract(r.Context(), propagation.HeaderCarrier(r.Header))
    
    // 创建Span
    tracer := otel.Tracer("user-service")
    ctx, span := tracer.Start(ctx, "getUserHandler")
    defer span.End()
    
    // 添加标签
    span.SetAttributes(
        semconv.HTTPMethod(r.Method),
        semconv.HTTPRoute(r.URL.Path),
    )
    
    // 调用数据库
    user, err := getUserFromDB(ctx, "user123")
    if err != nil {
        span.RecordError(err)
        http.Error(w, err.Error(), http.StatusInternalServerError)
        return
    }
    
    // 添加事件
    span.AddEvent("user_retrieved", oteltrace.WithAttributes(
        semconv.EnduserID(user.ID),
    ))
    
    w.Write([]byte(user.Name))
}

func getUserFromDB(ctx context.Context, userID string) (*User, error) {
    tracer := otel.Tracer("user-service")
    ctx, span := tracer.Start(ctx, "getUserFromDB")
    defer span.End()
    
    span.SetAttributes(
        semconv.DBSystem("mysql"),
        semconv.DBName("users"),
        semconv.DBStatement("SELECT * FROM users WHERE id = ?"),
    )
    
    // 数据库查询逻辑
    // ...
    
    return &User{ID: userID, Name: "John"}, nil
}

func main() {
    tp, err := initTracer()
    if err != nil {
        log.Fatal(err)
    }
    defer func() {
        if err := tp.Shutdown(context.Background()); err != nil {
            log.Printf("Error shutting down tracer provider: %v", err)
        }
    }()
    
    http.HandleFunc("/api/v1/users", getUserHandler)
    log.Fatal(http.ListenAndServe(":8080", nil))
}
```

**Java应用集成OpenTelemetry**：

```java
import io.opentelemetry.api.GlobalOpenTelemetry;
import io.opentelemetry.api.trace.Span;
import io.opentelemetry.api.trace.Tracer;
import io.opentelemetry.context.Scope;
import io.opentelemetry.exporter.jaeger.JaegerGrpcSpanExporter;
import io.opentelemetry.sdk.OpenTelemetrySdk;
import io.opentelemetry.sdk.trace.SdkTracerProvider;
import io.opentelemetry.sdk.trace.export.BatchSpanProcessor;

public class UserService {
    private static final Tracer tracer = GlobalOpenTelemetry.getTracer("user-service");
    
    public static void initTracer() {
        JaegerGrpcSpanExporter jaegerExporter = JaegerGrpcSpanExporter.builder()
            .setEndpoint("http://jaeger-collector.istio-system:14250")
            .build();
        
        SdkTracerProvider tracerProvider = SdkTracerProvider.builder()
            .addSpanProcessor(BatchSpanProcessor.builder(jaegerExporter).build())
            .build();
        
        OpenTelemetrySdk.builder()
            .setTracerProvider(tracerProvider)
            .buildAndRegisterGlobal();
    }
    
    public User getUser(String userId) {
        Span span = tracer.spanBuilder("getUser")
            .setAttribute("user.id", userId)
            .startSpan();
        
        try (Scope scope = span.makeCurrent()) {
            // 业务逻辑
            User user = getUserFromDB(userId);
            
            span.addEvent("user_retrieved");
            span.setAttribute("user.name", user.getName());
            
            return user;
        } catch (Exception e) {
            span.recordException(e);
            throw e;
        } finally {
            span.end();
        }
    }
    
    private User getUserFromDB(String userId) {
        Span span = tracer.spanBuilder("getUserFromDB")
            .setAttribute("db.system", "mysql")
            .setAttribute("db.name", "users")
            .startSpan();
        
        try (Scope scope = span.makeCurrent()) {
            // 数据库查询
            return new User(userId, "John");
        } finally {
            span.end();
        }
    }
}
```

#### 追踪数据分析

**使用Jaeger UI分析追踪**：

```yaml
Jaeger UI功能:
  搜索追踪:
    - 按服务名称搜索
    - 按操作名称搜索
    - 按标签搜索
    - 按时间范围搜索
    - 按持续时间搜索
  
  追踪详情:
    - 时间线视图: 显示Span的时序关系
    - 依赖关系图: 显示服务调用关系
    - Span详情: 显示标签、日志、错误信息
  
  性能分析:
    - 识别慢查询
    - 发现性能瓶颈
    - 分析服务依赖
    - 统计错误率
  
  服务依赖图:
    - 可视化服务拓扑
    - 显示调用频率
    - 显示平均延迟
    - 识别关键路径
```

**常见追踪查询**：

```bash
# 1. 查找慢请求（持续时间>1秒）
Service: user-service
Min Duration: 1s

# 2. 查找错误请求
Service: user-service
Tags: error=true

# 3. 查找特定用户的请求
Service: user-service
Tags: user.id=12345

# 4. 查找特定操作
Service: user-service
Operation: getUserFromDB

# 5. 查找数据库查询
Tags: db.system=mysql
```

#### 追踪与监控集成

**Grafana集成Jaeger**：

```yaml
# Grafana数据源配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: istio-system
data:
  jaeger.yaml: |
    apiVersion: 1
    datasources:
    - name: Jaeger
      type: jaeger
      access: proxy
      url: http://jaeger-query:16686
      isDefault: false
      editable: true
```

**Prometheus与Jaeger关联**：

```yaml
# 在Grafana中关联Prometheus指标和Jaeger追踪
# 1. 配置Exemplars
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: istio-system
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      external_labels:
        cluster: 'production'
    
    scrape_configs:
    - job_name: 'istio-mesh'
      kubernetes_sd_configs:
      - role: endpoints
      
      # 启用Exemplars
      metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'istio_request_duration_milliseconds_bucket'
        action: keep
      
      # 提取trace_id
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_trace_id]
        target_label: trace_id
```


---

**本节小结**

在12.4节中，我们深入学习了服务网格与流量管理：

1. **Istio服务网格部署**：
   - 理解了Istio的架构：控制平面（Istiod）和数据平面（Envoy）
   - 使用istioctl安装Istio到Kubernetes集群
   - 为应用注入Sidecar代理，实现流量拦截
   - 掌握了Gateway、VirtualService、DestinationRule等核心资源
   - 配置了mTLS双向认证和授权策略
   - 部署了Kiali、Prometheus、Grafana等可观测性组件

2. **灰度发布与金丝雀部署**：
   - 实现了基于权重的流量分配（10% → 30% → 50% → 100%）
   - 配置了基于请求头、Cookie、URI的智能路由
   - 实现了A/B测试和金丝雀发布
   - 编写了自动化金丝雀发布脚本
   - 建立了完整的发布验证和回滚机制

3. **流量路由与熔断**：
   - 配置了高级流量路由（地理位置、负载、镜像流量）
   - 实现了请求超时和重试策略
   - 配置了连接池和熔断器
   - 使用故障注入测试系统弹性
   - 建立了生产级流量管理最佳实践

4. **分布式追踪**：
   - 理解了OpenTracing标准（Trace、Span、SpanContext）
   - 部署了Jaeger分布式追踪系统
   - 配置了Istio追踪采样和自定义标签
   - 在Go和Java应用中集成OpenTelemetry
   - 使用Jaeger UI分析追踪数据和性能瓶颈
   - 实现了追踪与监控的集成

通过本节的实践，你已经掌握了如何使用Istio服务网格管理微服务流量，实现灰度发布、熔断降级、分布式追踪等高级特性。服务网格将服务治理能力从应用代码中剥离，极大地简化了微服务的运维复杂度。接下来，我们将学习如何构建CI/CD流水线，实现应用的自动化构建和部署！

---

## 12.5 CI/CD流水线构建

在云原生时代，持续集成和持续部署（CI/CD）是实现DevOps的核心实践。通过自动化的流水线，我们可以快速、可靠地将代码变更部署到生产环境。本节将介绍如何在Kubernetes环境中构建完整的CI/CD流水线。

### 12.5.1 GitLab CI/CD配置

GitLab CI/CD是一个内置在GitLab中的强大工具，通过`.gitlab-ci.yml`文件定义流水线。

#### 12.5.1.1 GitLab Runner部署

首先在Kubernetes集群中部署GitLab Runner：

```yaml
# gitlab-runner-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gitlab-runner-config
  namespace: gitlab
data:
  config.toml: |
    concurrent = 10
    check_interval = 3
    
    [[runners]]
      name = "kubernetes-runner"
      url = "https://gitlab.example.com"
      token = "YOUR_RUNNER_TOKEN"
      executor = "kubernetes"
      
      [runners.kubernetes]
        namespace = "gitlab"
        image = "alpine:latest"
        privileged = true
        cpu_request = "100m"
        memory_request = "128Mi"
        service_cpu_request = "100m"
        service_memory_request = "128Mi"
        helper_cpu_request = "50m"
        helper_memory_request = "64Mi"
        
        # Pod安全配置
        [runners.kubernetes.pod_security_context]
          run_as_non_root = true
          run_as_user = 1000
          fs_group = 1000
        
        # 资源限制
        [[runners.kubernetes.volumes.empty_dir]]
          name = "docker-certs"
          mount_path = "/certs/client"
          medium = "Memory"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitlab-runner
  namespace: gitlab
spec:
  replicas: 2
  selector:
    matchLabels:
      app: gitlab-runner
  template:
    metadata:
      labels:
        app: gitlab-runner
    spec:
      serviceAccountName: gitlab-runner
      containers:
      - name: gitlab-runner
        image: gitlab/gitlab-runner:alpine-v16.5.0
        command:
        - /bin/bash
        - -c
        - |
          cp /configmap/config.toml /etc/gitlab-runner/config.toml
          gitlab-runner run --user=gitlab-runner --working-directory=/home/gitlab-runner
        volumeMounts:
        - name: config
          mountPath: /configmap
        - name: runner-data
          mountPath: /home/gitlab-runner
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        livenessProbe:
          exec:
            command:
            - /usr/bin/pgrep
            - gitlab-runner
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - /usr/bin/pgrep
            - gitlab-runner
          initialDelaySeconds: 10
          periodSeconds: 5
      volumes:
      - name: config
        configMap:
          name: gitlab-runner-config
      - name: runner-data
        emptyDir: {}
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gitlab-runner
  namespace: gitlab
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: gitlab-runner
  namespace: gitlab
rules:
- apiGroups: [""]
  resources: ["pods", "pods/exec", "pods/log", "secrets", "configmaps"]
  verbs: ["get", "list", "watch", "create", "delete", "update", "patch"]
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get", "list", "create", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: gitlab-runner
  namespace: gitlab
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: gitlab-runner
subjects:
- kind: ServiceAccount
  name: gitlab-runner
  namespace: gitlab
```

部署GitLab Runner：

```bash
# 创建命名空间
kubectl create namespace gitlab

# 部署Runner
kubectl apply -f gitlab-runner-configmap.yaml

# 验证部署
kubectl get pods -n gitlab
kubectl logs -n gitlab deployment/gitlab-runner
```

#### 12.5.1.2 完整的CI/CD流水线配置

创建一个完整的`.gitlab-ci.yml`文件，包含构建、测试、镜像构建、安全扫描和部署阶段：

```yaml
# .gitlab-ci.yml
variables:
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"
  DOCKER_HOST: tcp://docker:2376
  DOCKER_TLS_VERIFY: 1
  DOCKER_CERT_PATH: "$DOCKER_TLS_CERTDIR/client"
  
  # 镜像仓库配置
  REGISTRY: registry.example.com
  IMAGE_NAME: $REGISTRY/$CI_PROJECT_PATH
  IMAGE_TAG: $CI_COMMIT_SHORT_SHA
  
  # Kubernetes配置
  KUBE_NAMESPACE: production
  DEPLOYMENT_NAME: user-service

# 定义流水线阶段
stages:
  - build
  - test
  - security
  - package
  - deploy

# 构建阶段
build:
  stage: build
  image: golang:1.21-alpine
  script:
    - echo "Building application..."
    - go mod download
    - go build -o app ./cmd/server
    - echo "Build completed successfully"
  artifacts:
    paths:
      - app
    expire_in: 1 hour
  cache:
    key: ${CI_COMMIT_REF_SLUG}
    paths:
      - .go/pkg/mod/
  only:
    - main
    - develop
    - merge_requests
  tags:
    - kubernetes

# 单元测试
unit-test:
  stage: test
  image: golang:1.21-alpine
  script:
    - echo "Running unit tests..."
    - go test -v -race -coverprofile=coverage.out ./...
    - go tool cover -func=coverage.out
  coverage: '/total:.*?(\d+\.\d+)%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.out
  only:
    - main
    - develop
    - merge_requests
  tags:
    - kubernetes

# 集成测试
integration-test:
  stage: test
  image: golang:1.21-alpine
  services:
    - name: postgres:15-alpine
      alias: postgres
    - name: redis:7-alpine
      alias: redis
  variables:
    POSTGRES_DB: testdb
    POSTGRES_USER: testuser
    POSTGRES_PASSWORD: testpass
    DATABASE_URL: "postgres://testuser:testpass@postgres:5432/testdb?sslmode=disable"
    REDIS_URL: "redis://redis:6379/0"
  script:
    - echo "Running integration tests..."
    - go test -v -tags=integration ./tests/integration/...
  only:
    - main
    - develop
  tags:
    - kubernetes

# 代码质量检查
code-quality:
  stage: test
  image: golangci/golangci-lint:v1.55-alpine
  script:
    - echo "Running code quality checks..."
    - golangci-lint run --timeout 5m
  allow_failure: true
  only:
    - main
    - develop
    - merge_requests
  tags:
    - kubernetes

# 安全扫描 - 依赖检查
dependency-scan:
  stage: security
  image: aquasec/trivy:latest
  script:
    - echo "Scanning dependencies for vulnerabilities..."
    - trivy fs --severity HIGH,CRITICAL --exit-code 0 .
  artifacts:
    reports:
      dependency_scanning: gl-dependency-scanning-report.json
  only:
    - main
    - develop
  tags:
    - kubernetes

# 构建Docker镜像
docker-build:
  stage: package
  image: docker:24-dind
  services:
    - docker:24-dind
  before_script:
    - echo "$REGISTRY_PASSWORD" | docker login -u "$REGISTRY_USER" --password-stdin $REGISTRY
  script:
    - echo "Building Docker image..."
    - |
      docker build \
        --build-arg VERSION=$CI_COMMIT_SHORT_SHA \
        --build-arg BUILD_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ") \
        --build-arg VCS_REF=$CI_COMMIT_SHA \
        --tag $IMAGE_NAME:$IMAGE_TAG \
        --tag $IMAGE_NAME:latest \
        .
    - docker push $IMAGE_NAME:$IMAGE_TAG
    - docker push $IMAGE_NAME:latest
    - echo "Image pushed: $IMAGE_NAME:$IMAGE_TAG"
  only:
    - main
    - develop
  tags:
    - kubernetes

# 镜像安全扫描
image-scan:
  stage: security
  image: aquasec/trivy:latest
  script:
    - echo "Scanning Docker image for vulnerabilities..."
    - trivy image --severity HIGH,CRITICAL --exit-code 1 $IMAGE_NAME:$IMAGE_TAG
  dependencies:
    - docker-build
  only:
    - main
  tags:
    - kubernetes

# 部署到开发环境
deploy-dev:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - echo "Deploying to development environment..."
    - kubectl config use-context dev-cluster
    - |
      kubectl set image deployment/$DEPLOYMENT_NAME \
        $DEPLOYMENT_NAME=$IMAGE_NAME:$IMAGE_TAG \
        -n dev
    - kubectl rollout status deployment/$DEPLOYMENT_NAME -n dev --timeout=5m
    - echo "Deployment to dev completed"
  environment:
    name: development
    url: https://dev.example.com
    on_stop: stop-dev
  only:
    - develop
  tags:
    - kubernetes

# 部署到生产环境（需要手动触发）
deploy-prod:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - echo "Deploying to production environment..."
    - kubectl config use-context prod-cluster
    - |
      # 创建备份
      kubectl get deployment $DEPLOYMENT_NAME -n $KUBE_NAMESPACE -o yaml > backup-$(date +%Y%m%d-%H%M%S).yaml
      
      # 更新镜像
      kubectl set image deployment/$DEPLOYMENT_NAME \
        $DEPLOYMENT_NAME=$IMAGE_NAME:$IMAGE_TAG \
        -n $KUBE_NAMESPACE
      
      # 等待滚动更新完成
      kubectl rollout status deployment/$DEPLOYMENT_NAME -n $KUBE_NAMESPACE --timeout=10m
      
      # 验证部署
      kubectl get pods -n $KUBE_NAMESPACE -l app=$DEPLOYMENT_NAME
    - echo "Production deployment completed successfully"
  environment:
    name: production
    url: https://api.example.com
    on_stop: rollback-prod
  when: manual
  only:
    - main
  tags:
    - kubernetes

# 生产环境回滚
rollback-prod:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - echo "Rolling back production deployment..."
    - kubectl config use-context prod-cluster
    - kubectl rollout undo deployment/$DEPLOYMENT_NAME -n $KUBE_NAMESPACE
    - kubectl rollout status deployment/$DEPLOYMENT_NAME -n $KUBE_NAMESPACE --timeout=5m
    - echo "Rollback completed"
  environment:
    name: production
    action: stop
  when: manual
  only:
    - main
  tags:
    - kubernetes

# 停止开发环境
stop-dev:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - echo "Scaling down development deployment..."
    - kubectl config use-context dev-cluster
    - kubectl scale deployment/$DEPLOYMENT_NAME --replicas=0 -n dev
  environment:
    name: development
    action: stop
  when: manual
  only:
    - develop
  tags:
    - kubernetes
```

#### 12.5.1.3 多环境配置管理

使用GitLab CI/CD变量管理不同环境的配置：

```yaml
# .gitlab-ci.yml (环境变量部分)
.deploy_template: &deploy_template
  image: bitnami/kubectl:latest
  before_script:
    - echo "$KUBE_CONFIG" | base64 -d > ~/.kube/config
    - kubectl version --client
  script:
    - |
      # 使用envsubst替换环境变量
      envsubst < k8s/deployment.yaml | kubectl apply -f -
      envsubst < k8s/service.yaml | kubectl apply -f -
      envsubst < k8s/ingress.yaml | kubectl apply -f -
      
      # 等待部署完成
      kubectl rollout status deployment/$DEPLOYMENT_NAME -n $KUBE_NAMESPACE --timeout=10m

deploy-staging:
  <<: *deploy_template
  stage: deploy
  variables:
    KUBE_NAMESPACE: staging
    REPLICAS: "2"
    CPU_REQUEST: "100m"
    MEMORY_REQUEST: "128Mi"
    CPU_LIMIT: "500m"
    MEMORY_LIMIT: "512Mi"
  environment:
    name: staging
    url: https://staging.example.com
  only:
    - develop

deploy-production:
  <<: *deploy_template
  stage: deploy
  variables:
    KUBE_NAMESPACE: production
    REPLICAS: "5"
    CPU_REQUEST: "500m"
    MEMORY_REQUEST: "1Gi"
    CPU_LIMIT: "2000m"
    MEMORY_LIMIT: "4Gi"
  environment:
    name: production
    url: https://api.example.com
  when: manual
  only:
    - main
```

对应的Kubernetes部署模板：

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ${DEPLOYMENT_NAME}
  namespace: ${KUBE_NAMESPACE}
  labels:
    app: ${DEPLOYMENT_NAME}
    version: ${IMAGE_TAG}
spec:
  replicas: ${REPLICAS}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: ${DEPLOYMENT_NAME}
  template:
    metadata:
      labels:
        app: ${DEPLOYMENT_NAME}
        version: ${IMAGE_TAG}
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: ${DEPLOYMENT_NAME}
      containers:
      - name: ${DEPLOYMENT_NAME}
        image: ${IMAGE_NAME}:${IMAGE_TAG}
        imagePullPolicy: Always
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        - name: grpc
          containerPort: 9090
          protocol: TCP
        env:
        - name: ENVIRONMENT
          value: "${KUBE_NAMESPACE}"
        - name: LOG_LEVEL
          value: "info"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: ${DEPLOYMENT_NAME}-secrets
              key: database-url
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: ${DEPLOYMENT_NAME}-secrets
              key: redis-url
        resources:
          requests:
            cpu: ${CPU_REQUEST}
            memory: ${MEMORY_REQUEST}
          limits:
            cpu: ${CPU_LIMIT}
            memory: ${MEMORY_LIMIT}
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /app/cache
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir: {}
      imagePullSecrets:
      - name: registry-credentials
```

#### 12.5.1.4 Pipeline优化技巧

**1. 使用缓存加速构建**

```yaml
# .gitlab-ci.yml
variables:
  CACHE_COMPRESSION_LEVEL: "fast"

.go_cache: &go_cache
  cache:
    key:
      files:
        - go.sum
    paths:
      - .go/pkg/mod/
    policy: pull-push

build:
  <<: *go_cache
  stage: build
  before_script:
    - export GOPATH=$CI_PROJECT_DIR/.go
  script:
    - go build -o app ./cmd/server
```

**2. 并行执行测试**

```yaml
# .gitlab-ci.yml
test:unit:
  stage: test
  parallel:
    matrix:
      - GO_VERSION: ["1.20", "1.21", "1.22"]
  image: golang:${GO_VERSION}-alpine
  script:
    - go test -v ./...
```

**3. 使用include模块化配置**

```yaml
# .gitlab-ci.yml
include:
  - local: '.gitlab/ci/build.yml'
  - local: '.gitlab/ci/test.yml'
  - local: '.gitlab/ci/deploy.yml'
  - template: Security/SAST.gitlab-ci.yml
  - template: Security/Dependency-Scanning.gitlab-ci.yml
```

```yaml
# .gitlab/ci/build.yml
.build_template:
  stage: build
  image: golang:1.21-alpine
  before_script:
    - apk add --no-cache git make
  script:
    - make build
  artifacts:
    paths:
      - bin/
    expire_in: 1 hour

build:linux:
  extends: .build_template
  variables:
    GOOS: linux
    GOARCH: amd64

build:darwin:
  extends: .build_template
  variables:
    GOOS: darwin
    GOARCH: arm64
```

#### 12.5.1.5 监控和通知

配置流水线状态通知：

```yaml
# .gitlab-ci.yml
notify:success:
  stage: .post
  image: curlimages/curl:latest
  script:
    - |
      curl -X POST https://hooks.slack.com/services/YOUR/WEBHOOK/URL \
        -H 'Content-Type: application/json' \
        -d '{
          "text": "✅ Pipeline succeeded for '"$CI_PROJECT_NAME"'",
          "attachments": [{
            "color": "good",
            "fields": [
              {"title": "Branch", "value": "'"$CI_COMMIT_REF_NAME"'", "short": true},
              {"title": "Commit", "value": "'"$CI_COMMIT_SHORT_SHA"'", "short": true},
              {"title": "Author", "value": "'"$GITLAB_USER_NAME"'", "short": true},
              {"title": "Pipeline", "value": "'"$CI_PIPELINE_URL"'", "short": false}
            ]
          }]
        }'
  when: on_success
  only:
    - main

notify:failure:
  stage: .post
  image: curlimages/curl:latest
  script:
    - |
      curl -X POST https://hooks.slack.com/services/YOUR/WEBHOOK/URL \
        -H 'Content-Type: application/json' \
        -d '{
          "text": "❌ Pipeline failed for '"$CI_PROJECT_NAME"'",
          "attachments": [{
            "color": "danger",
            "fields": [
              {"title": "Branch", "value": "'"$CI_COMMIT_REF_NAME"'", "short": true},
              {"title": "Commit", "value": "'"$CI_COMMIT_SHORT_SHA"'", "short": true},
              {"title": "Author", "value": "'"$GITLAB_USER_NAME"'", "short": true},
              {"title": "Pipeline", "value": "'"$CI_PIPELINE_URL"'", "short": false}
            ]
          }]
        }'
  when: on_failure
  only:
    - main
```


### 12.5.2 Jenkins Pipeline实践

Jenkins是最流行的开源CI/CD工具之一，通过Pipeline as Code实现声明式或脚本式的流水线定义。

#### 12.5.2.1 Jenkins在Kubernetes中的部署

使用Helm部署Jenkins：

```bash
# 添加Jenkins Helm仓库
helm repo add jenkins https://charts.jenkins.io
helm repo update

# 创建values.yaml配置文件
cat > jenkins-values.yaml << 'HELMEOF'
controller:
  # Jenkins控制器配置
  image: "jenkins/jenkins"
  tag: "2.426.1-lts"
  
  # 资源配置
  resources:
    requests:
      cpu: "500m"
      memory: "2Gi"
    limits:
      cpu: "2000m"
      memory: "4Gi"
  
  # JVM参数
  javaOpts: "-Xms2g -Xmx2g -XX:+UseG1GC"
  
  # 持久化存储
  persistence:
    enabled: true
    storageClass: "standard"
    size: "50Gi"
  
  # 服务配置
  serviceType: ClusterIP
  servicePort: 8080
  
  # Ingress配置
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: "letsencrypt-prod"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
    hostName: jenkins.example.com
    tls:
      - secretName: jenkins-tls
        hosts:
          - jenkins.example.com
  
  # 安全配置
  adminUser: "admin"
  adminPassword: "changeme"
  
  # 预安装插件
  installPlugins:
    - kubernetes:4029.v5712230ccb_f8
    - workflow-aggregator:596.v8c21c963d92d
    - git:5.2.0
    - configuration-as-code:1670.v564dc8b_982d0
    - docker-workflow:572.v950f58993843
    - pipeline-stage-view:2.33
    - blueocean:1.27.9
    - prometheus:2.3.2
    - slack:664.vc9a_90f8b_c24a_
    - sonar:2.16.1
    - kubernetes-credentials-provider:1.262.v2670ef7ea_0c5
  
  # Jenkins配置即代码（JCasC）
  JCasC:
    defaultConfig: true
    configScripts:
      welcome-message: |
        jenkins:
          systemMessage: "Welcome to Jenkins on Kubernetes!"
      
      kubernetes-cloud: |
        jenkins:
          clouds:
            - kubernetes:
                name: "kubernetes"
                serverUrl: "https://kubernetes.default"
                namespace: "jenkins"
                jenkinsUrl: "http://jenkins:8080"
                jenkinsTunnel: "jenkins-agent:50000"
                containerCapStr: "10"
                connectTimeout: 5
                readTimeout: 15
                retentionTimeout: 5
                maxRequestsPerHostStr: 32
                
                templates:
                  - name: "jnlp-agent"
                    namespace: "jenkins"
                    label: "jenkins-agent"
                    nodeUsageMode: NORMAL
                    containers:
                      - name: "jnlp"
                        image: "jenkins/inbound-agent:latest"
                        alwaysPullImage: false
                        workingDir: "/home/jenkins/agent"
                        ttyEnabled: true
                        resourceRequestCpu: "500m"
                        resourceRequestMemory: "512Mi"
                        resourceLimitCpu: "1000m"
                        resourceLimitMemory: "1Gi"
                    
                    volumes:
                      - emptyDirVolume:
                          memory: false
                          mountPath: "/tmp"
                    
                    yaml: |
                      apiVersion: v1
                      kind: Pod
                      spec:
                        securityContext:
                          runAsUser: 1000
                          fsGroup: 1000

agent:
  enabled: true
  image: "jenkins/inbound-agent"
  tag: "latest"
  
  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "1Gi"

# RBAC配置
rbac:
  create: true
  readSecrets: true

serviceAccount:
  create: true
  name: jenkins

# 监控配置
prometheus:
  enabled: true
  serviceMonitorNamespace: "monitoring"
  serviceMonitorAdditionalLabels:
    release: prometheus
HELMEOF

# 部署Jenkins
helm install jenkins jenkins/jenkins \
  --namespace jenkins \
  --create-namespace \
  --values jenkins-values.yaml \
  --wait

# 获取管理员密码
kubectl get secret --namespace jenkins jenkins -o jsonpath="{.data.jenkins-admin-password}" | base64 --decode

# 查看部署状态
kubectl get pods -n jenkins
kubectl get svc -n jenkins
```

#### 12.5.2.2 声明式Pipeline

创建一个完整的声明式Pipeline（Jenkinsfile）：

```groovy
// Jenkinsfile
pipeline {
    agent {
        kubernetes {
            yaml """
apiVersion: v1
kind: Pod
metadata:
  labels:
    jenkins: agent
spec:
  serviceAccountName: jenkins
  containers:
  - name: golang
    image: golang:1.21-alpine
    command:
    - cat
    tty: true
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 2000m
        memory: 2Gi
  - name: docker
    image: docker:24-dind
    command:
    - dockerd
    - --host=unix:///var/run/docker.sock
    - --host=tcp://0.0.0.0:2375
    securityContext:
      privileged: true
    volumeMounts:
    - name: docker-sock
      mountPath: /var/run
  - name: kubectl
    image: bitnami/kubectl:latest
    command:
    - cat
    tty: true
  - name: trivy
    image: aquasec/trivy:latest
    command:
    - cat
    tty: true
  volumes:
  - name: docker-sock
    emptyDir: {}
"""
        }
    }
    
    options {
        // Pipeline选项
        buildDiscarder(logRotator(numToKeepStr: '10'))
        disableConcurrentBuilds()
        timeout(time: 1, unit: 'HOURS')
        timestamps()
    }
    
    environment {
        // 环境变量
        REGISTRY = 'registry.example.com'
        IMAGE_NAME = "${REGISTRY}/${env.JOB_NAME}"
        IMAGE_TAG = "${env.BUILD_NUMBER}-${env.GIT_COMMIT.take(8)}"
        KUBE_NAMESPACE = 'production'
        DEPLOYMENT_NAME = 'user-service'
        
        // 凭证
        REGISTRY_CREDENTIALS = credentials('docker-registry')
        KUBE_CONFIG = credentials('kubeconfig')
        SONAR_TOKEN = credentials('sonarqube-token')
    }
    
    stages {
        stage('Checkout') {
            steps {
                script {
                    echo "Checking out code from ${env.GIT_BRANCH}"
                    checkout scm
                    
                    // 获取Git信息
                    env.GIT_COMMIT_MSG = sh(
                        script: 'git log -1 --pretty=%B',
                        returnStdout: true
                    ).trim()
                    env.GIT_AUTHOR = sh(
                        script: 'git log -1 --pretty=%an',
                        returnStdout: true
                    ).trim()
                }
            }
        }
        
        stage('Build') {
            steps {
                container('golang') {
                    sh """
                        echo 'Building application...'
                        go mod download
                        go build -v -o app ./cmd/server
                        echo 'Build completed successfully'
                    """
                }
            }
        }
        
        stage('Test') {
            parallel {
                stage('Unit Tests') {
                    steps {
                        container('golang') {
                            sh """
                                echo 'Running unit tests...'
                                go test -v -race -coverprofile=coverage.out ./...
                                go tool cover -func=coverage.out
                            """
                        }
                    }
                    post {
                        always {
                            // 发布测试报告
                            junit '**/test-results/*.xml'
                            publishHTML([
                                reportDir: 'coverage',
                                reportFiles: 'index.html',
                                reportName: 'Coverage Report'
                            ])
                        }
                    }
                }
                
                stage('Code Quality') {
                    steps {
                        container('golang') {
                            sh """
                                echo 'Running code quality checks...'
                                go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest
                                golangci-lint run --timeout 5m
                            """
                        }
                    }
                }
                
                stage('Security Scan') {
                    steps {
                        container('trivy') {
                            sh """
                                echo 'Scanning dependencies for vulnerabilities...'
                                trivy fs --severity HIGH,CRITICAL --exit-code 0 .
                            """
                        }
                    }
                }
            }
        }
        
        stage('SonarQube Analysis') {
            when {
                branch 'main'
            }
            steps {
                container('golang') {
                    script {
                        def scannerHome = tool 'SonarScanner'
                        withSonarQubeEnv('SonarQube') {
                            sh """
                                ${scannerHome}/bin/sonar-scanner \
                                    -Dsonar.projectKey=${env.JOB_NAME} \
                                    -Dsonar.sources=. \
                                    -Dsonar.go.coverage.reportPaths=coverage.out \
                                    -Dsonar.exclusions=**/*_test.go,**/vendor/**
                            """
                        }
                    }
                }
            }
        }
        
        stage('Quality Gate') {
            when {
                branch 'main'
            }
            steps {
                timeout(time: 5, unit: 'MINUTES') {
                    waitForQualityGate abortPipeline: true
                }
            }
        }
        
        stage('Build Docker Image') {
            when {
                anyOf {
                    branch 'main'
                    branch 'develop'
                }
            }
            steps {
                container('docker') {
                    sh """
                        echo 'Building Docker image...'
                        echo "$REGISTRY_CREDENTIALS_PSW" | docker login -u "$REGISTRY_CREDENTIALS_USR" --password-stdin $REGISTRY
                        
                        docker build \
                            --build-arg VERSION=$IMAGE_TAG \
                            --build-arg BUILD_DATE=\$(date -u +"%Y-%m-%dT%H:%M:%SZ") \
                            --build-arg VCS_REF=$GIT_COMMIT \
                            --tag $IMAGE_NAME:$IMAGE_TAG \
                            --tag $IMAGE_NAME:latest \
                            .
                        
                        docker push $IMAGE_NAME:$IMAGE_TAG
                        docker push $IMAGE_NAME:latest
                        
                        echo "Image pushed: $IMAGE_NAME:$IMAGE_TAG"
                    """
                }
            }
        }
        
        stage('Image Security Scan') {
            when {
                anyOf {
                    branch 'main'
                    branch 'develop'
                }
            }
            steps {
                container('trivy') {
                    sh """
                        echo 'Scanning Docker image for vulnerabilities...'
                        trivy image --severity HIGH,CRITICAL --exit-code 1 $IMAGE_NAME:$IMAGE_TAG
                    """
                }
            }
        }
        
        stage('Deploy to Dev') {
            when {
                branch 'develop'
            }
            steps {
                container('kubectl') {
                    sh """
                        echo 'Deploying to development environment...'
                        echo "$KUBE_CONFIG" > ~/.kube/config
                        
                        kubectl set image deployment/$DEPLOYMENT_NAME \
                            $DEPLOYMENT_NAME=$IMAGE_NAME:$IMAGE_TAG \
                            -n dev
                        
                        kubectl rollout status deployment/$DEPLOYMENT_NAME -n dev --timeout=5m
                        echo 'Deployment to dev completed'
                    """
                }
            }
        }
        
        stage('Deploy to Production') {
            when {
                branch 'main'
            }
            steps {
                script {
                    // 需要手动批准
                    timeout(time: 1, unit: 'HOURS') {
                        input message: 'Deploy to production?',
                              ok: 'Deploy',
                              submitter: 'admin,ops-team'
                    }
                }
                
                container('kubectl') {
                    sh """
                        echo 'Deploying to production environment...'
                        echo "$KUBE_CONFIG" > ~/.kube/config
                        
                        # 创建备份
                        kubectl get deployment $DEPLOYMENT_NAME -n $KUBE_NAMESPACE -o yaml > backup-\$(date +%Y%m%d-%H%M%S).yaml
                        
                        # 更新镜像
                        kubectl set image deployment/$DEPLOYMENT_NAME \
                            $DEPLOYMENT_NAME=$IMAGE_NAME:$IMAGE_TAG \
                            -n $KUBE_NAMESPACE
                        
                        # 等待滚动更新完成
                        kubectl rollout status deployment/$DEPLOYMENT_NAME -n $KUBE_NAMESPACE --timeout=10m
                        
                        # 验证部署
                        kubectl get pods -n $KUBE_NAMESPACE -l app=$DEPLOYMENT_NAME
                        
                        echo 'Production deployment completed successfully'
                    """
                }
            }
        }
    }
    
    post {
        always {
            // 清理工作空间
            cleanWs()
        }
        
        success {
            script {
                // 发送成功通知
                slackSend(
                    color: 'good',
                    message: """
                        ✅ Pipeline succeeded for ${env.JOB_NAME}
                        Branch: ${env.GIT_BRANCH}
                        Commit: ${env.GIT_COMMIT.take(8)}
                        Author: ${env.GIT_AUTHOR}
                        Build: ${env.BUILD_URL}
                    """.stripIndent()
                )
            }
        }
        
        failure {
            script {
                // 发送失败通知
                slackSend(
                    color: 'danger',
                    message: """
                        ❌ Pipeline failed for ${env.JOB_NAME}
                        Branch: ${env.GIT_BRANCH}
                        Commit: ${env.GIT_COMMIT.take(8)}
                        Author: ${env.GIT_AUTHOR}
                        Build: ${env.BUILD_URL}
                    """.stripIndent()
                )
            }
        }
    }
}
```

#### 12.5.2.3 脚本式Pipeline

对于更复杂的场景，可以使用脚本式Pipeline：

```groovy
// Jenkinsfile (Scripted Pipeline)
node('kubernetes') {
    def app
    def imageTag = "${env.BUILD_NUMBER}-${env.GIT_COMMIT.take(8)}"
    def imageName = "registry.example.com/${env.JOB_NAME}:${imageTag}"
    
    try {
        stage('Checkout') {
            checkout scm
            
            // 获取Git信息
            env.GIT_COMMIT_MSG = sh(
                script: 'git log -1 --pretty=%B',
                returnStdout: true
            ).trim()
        }
        
        stage('Build') {
            container('golang') {
                sh """
                    go mod download
                    go build -v -o app ./cmd/server
                """
            }
        }
        
        stage('Test') {
            parallel(
                'Unit Tests': {
                    container('golang') {
                        sh 'go test -v -race ./...'
                    }
                },
                'Integration Tests': {
                    container('golang') {
                        sh 'go test -v -tags=integration ./tests/integration/...'
                    }
                },
                'Lint': {
                    container('golang') {
                        sh 'golangci-lint run'
                    }
                }
            )
        }
        
        stage('Build Image') {
            container('docker') {
                app = docker.build(imageName)
            }
        }
        
        stage('Push Image') {
            container('docker') {
                docker.withRegistry('https://registry.example.com', 'docker-registry') {
                    app.push(imageTag)
                    app.push('latest')
                }
            }
        }
        
        stage('Deploy') {
            if (env.BRANCH_NAME == 'main') {
                timeout(time: 1, unit: 'HOURS') {
                    input message: 'Deploy to production?',
                          ok: 'Deploy'
                }
                
                container('kubectl') {
                    sh """
                        kubectl set image deployment/user-service \
                            user-service=${imageName} \
                            -n production
                        kubectl rollout status deployment/user-service -n production
                    """
                }
            }
        }
        
        currentBuild.result = 'SUCCESS'
        
    } catch (Exception e) {
        currentBuild.result = 'FAILURE'
        throw e
        
    } finally {
        // 发送通知
        def color = currentBuild.result == 'SUCCESS' ? 'good' : 'danger'
        def emoji = currentBuild.result == 'SUCCESS' ? '✅' : '❌'
        
        slackSend(
            color: color,
            message: """
                ${emoji} ${currentBuild.result}: ${env.JOB_NAME} #${env.BUILD_NUMBER}
                Branch: ${env.BRANCH_NAME}
                Commit: ${env.GIT_COMMIT_MSG}
                Duration: ${currentBuild.durationString}
                Build: ${env.BUILD_URL}
            """.stripIndent()
        )
    }
}
```

#### 12.5.2.4 共享库（Shared Library）

创建可复用的Pipeline共享库：

```groovy
// vars/buildGoApp.groovy
def call(Map config = [:]) {
    pipeline {
        agent {
            kubernetes {
                yaml libraryResource('podTemplates/golang.yaml')
            }
        }
        
        environment {
            GO_VERSION = config.goVersion ?: '1.21'
            APP_NAME = config.appName ?: env.JOB_NAME
            REGISTRY = config.registry ?: 'registry.example.com'
        }
        
        stages {
            stage('Checkout') {
                steps {
                    checkout scm
                }
            }
            
            stage('Build') {
                steps {
                    container('golang') {
                        sh """
                            go mod download
                            go build -v -o ${APP_NAME} ${config.mainPath ?: './cmd/server'}
                        """
                    }
                }
            }
            
            stage('Test') {
                steps {
                    container('golang') {
                        sh 'go test -v -race -coverprofile=coverage.out ./...'
                    }
                }
            }
            
            stage('Build & Push Image') {
                steps {
                    script {
                        def imageTag = "${env.BUILD_NUMBER}-${env.GIT_COMMIT.take(8)}"
                        buildAndPushImage(
                            imageName: "${REGISTRY}/${APP_NAME}",
                            imageTag: imageTag,
                            dockerfile: config.dockerfile ?: 'Dockerfile'
                        )
                    }
                }
            }
            
            stage('Deploy') {
                steps {
                    script {
                        deployToKubernetes(
                            namespace: config.namespace ?: 'default',
                            deployment: config.deployment ?: APP_NAME,
                            image: "${REGISTRY}/${APP_NAME}:${imageTag}"
                        )
                    }
                }
            }
        }
    }
}
```

```groovy
// vars/buildAndPushImage.groovy
def call(Map config) {
    container('docker') {
        sh """
            docker build -f ${config.dockerfile} \
                -t ${config.imageName}:${config.imageTag} \
                -t ${config.imageName}:latest \
                .
            
            docker push ${config.imageName}:${config.imageTag}
            docker push ${config.imageName}:latest
        """
    }
}
```

```groovy
// vars/deployToKubernetes.groovy
def call(Map config) {
    container('kubectl') {
        sh """
            kubectl set image deployment/${config.deployment} \
                ${config.deployment}=${config.image} \
                -n ${config.namespace}
            
            kubectl rollout status deployment/${config.deployment} \
                -n ${config.namespace} \
                --timeout=5m
        """
    }
}
```

使用共享库：

```groovy
// Jenkinsfile
@Library('jenkins-shared-library') _

buildGoApp(
    appName: 'user-service',
    goVersion: '1.21',
    mainPath: './cmd/server',
    namespace: 'production',
    deployment: 'user-service'
)
```

#### 12.5.2.5 多分支Pipeline

配置多分支Pipeline自动发现和构建：

```groovy
// Jenkinsfile
pipeline {
    agent any
    
    stages {
        stage('Build') {
            steps {
                echo "Building branch: ${env.BRANCH_NAME}"
                sh 'make build'
            }
        }
        
        stage('Test') {
            steps {
                sh 'make test'
            }
        }
        
        stage('Deploy') {
            steps {
                script {
                    switch(env.BRANCH_NAME) {
                        case 'main':
                            echo 'Deploying to production'
                            deployToEnvironment('production')
                            break
                        case 'develop':
                            echo 'Deploying to staging'
                            deployToEnvironment('staging')
                            break
                        case ~/^feature\/.*/:
                            echo 'Deploying to feature environment'
                            deployToEnvironment("feature-${env.BRANCH_NAME.replaceAll('/', '-')}")
                            break
                        case ~/^PR-.*/:
                            echo 'Deploying to PR environment'
                            deployToEnvironment("pr-${env.CHANGE_ID}")
                            break
                        default:
                            echo 'Skipping deployment for this branch'
                    }
                }
            }
        }
    }
}

def deployToEnvironment(String environment) {
    sh """
        kubectl set image deployment/app \
            app=registry.example.com/app:${env.BUILD_NUMBER} \
            -n ${environment}
    """
}
```


### 12.5.3 ArgoCD GitOps部署

ArgoCD是一个声明式的GitOps持续交付工具，它将Git仓库作为应用配置的唯一真实来源，自动同步和部署应用到Kubernetes集群。

#### 12.5.3.1 ArgoCD架构与安装

**ArgoCD架构组件：**

```
┌─────────────────────────────────────────────────────────┐
│                      ArgoCD 架构                         │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  ┌──────────────┐         ┌──────────────┐             │
│  │  Git Repo    │────────>│  ArgoCD API  │             │
│  │  (Source)    │         │   Server     │             │
│  └──────────────┘         └──────┬───────┘             │
│                                   │                      │
│                          ┌────────▼────────┐            │
│                          │  Application    │            │
│                          │   Controller    │            │
│                          └────────┬────────┘            │
│                                   │                      │
│                          ┌────────▼────────┐            │
│                          │  Repo Server    │            │
│                          │  (Manifest Gen) │            │
│                          └────────┬────────┘            │
│                                   │                      │
│                          ┌────────▼────────┐            │
│                          │   Kubernetes    │            │
│                          │     Cluster     │            │
│                          └─────────────────┘            │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

使用Helm安装ArgoCD：

```bash
# 创建命名空间
kubectl create namespace argocd

# 添加ArgoCD Helm仓库
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

# 创建values.yaml配置
cat > argocd-values.yaml << 'EOF'
global:
  image:
    repository: quay.io/argoproj/argocd
    tag: v2.9.3

server:
  # 服务器配置
  replicas: 2
  
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
  
  # Ingress配置
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: "letsencrypt-prod"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
    hosts:
      - argocd.example.com
    tls:
      - secretName: argocd-tls
        hosts:
          - argocd.example.com
  
  # 配置管理
  config:
    # 仓库凭证
    repositories: |
      - url: https://github.com/your-org/k8s-manifests
        passwordSecret:
          name: github-credentials
          key: password
        usernameSecret:
          name: github-credentials
          key: username
      - url: https://gitlab.example.com/your-org/k8s-manifests
        passwordSecret:
          name: gitlab-credentials
          key: password
        usernameSecret:
          name: gitlab-credentials
          key: username
    
    # 资源自定义
    resource.customizations: |
      argoproj.io/Application:
        health.lua: |
          hs = {}
          hs.status = "Progressing"
          hs.message = ""
          if obj.status ~= nil then
            if obj.status.health ~= nil then
              hs.status = obj.status.health.status
              if obj.status.health.message ~= nil then
                hs.message = obj.status.health.message
              end
            end
          end
          return hs

controller:
  # 控制器配置
  replicas: 2
  
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 2000m
      memory: 2Gi
  
  # 应用同步配置
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      namespace: monitoring

repoServer:
  # 仓库服务器配置
  replicas: 2
  
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  
  # 启用Helm支持
  env:
    - name: ARGOCD_EXEC_TIMEOUT
      value: "5m"

redis:
  # Redis配置
  enabled: true
  
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi

# RBAC配置
configs:
  rbac:
    policy.default: role:readonly
    policy.csv: |
      # 管理员角色
      p, role:admin, applications, *, */*, allow
      p, role:admin, clusters, *, *, allow
      p, role:admin, repositories, *, *, allow
      p, role:admin, projects, *, *, allow
      
      # 开发者角色
      p, role:developer, applications, get, */*, allow
      p, role:developer, applications, sync, */*, allow
      p, role:developer, applications, override, */*, allow
      
      # 只读角色
      p, role:readonly, applications, get, */*, allow
      p, role:readonly, projects, get, *, allow
      
      # 用户组绑定
      g, admin-group, role:admin
      g, dev-group, role:developer
      g, viewer-group, role:readonly

# 通知配置
notifications:
  enabled: true
  
  argocdUrl: https://argocd.example.com
  
  notifiers:
    service.slack: |
      token: $slack-token
    
    service.webhook.github: |
      url: https://api.github.com
      headers:
      - name: Authorization
        value: token $github-token
  
  templates:
    template.app-deployed: |
      message: |
        Application {{.app.metadata.name}} is now running new version.
      slack:
        attachments: |
          [{
            "title": "{{ .app.metadata.name}}",
            "title_link":"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}",
            "color": "#18be52",
            "fields": [
            {
              "title": "Sync Status",
              "value": "{{.app.status.sync.status}}",
              "short": true
            },
            {
              "title": "Repository",
              "value": "{{.app.spec.source.repoURL}}",
              "short": true
            }
            ]
          }]
    
    template.app-health-degraded: |
      message: |
        Application {{.app.metadata.name}} has degraded.
      slack:
        attachments: |
          [{
            "title": "{{ .app.metadata.name}}",
            "title_link":"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}",
            "color": "#f4c030",
            "fields": [
            {
              "title": "Health Status",
              "value": "{{.app.status.health.status}}",
              "short": true
            },
            {
              "title": "Repository",
              "value": "{{.app.spec.source.repoURL}}",
              "short": true
            }
            ]
          }]
  
  triggers:
    trigger.on-deployed: |
      - when: app.status.operationState.phase in ['Succeeded']
        send: [app-deployed]
    
    trigger.on-health-degraded: |
      - when: app.status.health.status == 'Degraded'
        send: [app-health-degraded]
EOF

# 安装ArgoCD
helm install argocd argo/argo-cd \
  --namespace argocd \
  --values argocd-values.yaml \
  --wait

# 获取初始管理员密码
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

# 安装ArgoCD CLI
curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
chmod +x /usr/local/bin/argocd

# 登录ArgoCD
argocd login argocd.example.com --username admin --password <password>

# 修改管理员密码
argocd account update-password
```

#### 12.5.3.2 创建ArgoCD应用

**方式一：使用kubectl创建Application资源**

```yaml
# application.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: user-service
  namespace: argocd
  # 添加finalizer确保级联删除
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  # 项目配置
  project: default
  
  # Git仓库配置
  source:
    repoURL: https://github.com/your-org/k8s-manifests
    targetRevision: main
    path: apps/user-service
    
    # Helm配置（如果使用Helm）
    helm:
      valueFiles:
        - values-prod.yaml
      parameters:
        - name: image.tag
          value: "v1.2.3"
        - name: replicaCount
          value: "3"
      
      # 使用values文件覆盖
      values: |
        image:
          repository: registry.example.com/user-service
          tag: v1.2.3
        
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi
    
    # Kustomize配置（如果使用Kustomize）
    kustomize:
      namePrefix: prod-
      nameSuffix: -v1
      images:
        - registry.example.com/user-service:v1.2.3
      commonLabels:
        environment: production
  
  # 目标集群配置
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  
  # 同步策略
  syncPolicy:
    # 自动同步
    automated:
      prune: true        # 自动删除不在Git中的资源
      selfHeal: true     # 自动修复漂移
      allowEmpty: false  # 不允许空应用
    
    # 同步选项
    syncOptions:
      - CreateNamespace=true    # 自动创建命名空间
      - PrunePropagationPolicy=foreground  # 删除策略
      - PruneLast=true          # 最后删除资源
      - RespectIgnoreDifferences=true
    
    # 重试策略
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
  
  # 忽略差异
  ignoreDifferences:
    - group: apps
      kind: Deployment
      jsonPointers:
        - /spec/replicas  # 忽略副本数差异（HPA管理）
    - group: ""
      kind: Secret
      jsonPointers:
        - /data  # 忽略Secret数据差异
  
  # 健康检查
  revisionHistoryLimit: 10
```

部署应用：

```bash
# 创建应用
kubectl apply -f application.yaml

# 查看应用状态
argocd app get user-service

# 同步应用
argocd app sync user-service

# 查看同步历史
argocd app history user-service

# 回滚到指定版本
argocd app rollback user-service 5
```

**方式二：使用ArgoCD CLI创建**

```bash
# 创建应用
argocd app create user-service \
  --repo https://github.com/your-org/k8s-manifests \
  --path apps/user-service \
  --dest-server https://kubernetes.default.svc \
  --dest-namespace production \
  --sync-policy automated \
  --auto-prune \
  --self-heal

# 使用Helm创建应用
argocd app create user-service \
  --repo https://github.com/your-org/helm-charts \
  --path charts/user-service \
  --dest-server https://kubernetes.default.svc \
  --dest-namespace production \
  --helm-set image.tag=v1.2.3 \
  --helm-set replicaCount=3 \
  --values values-prod.yaml

# 使用Kustomize创建应用
argocd app create user-service \
  --repo https://github.com/your-org/k8s-manifests \
  --path apps/user-service/overlays/production \
  --dest-server https://kubernetes.default.svc \
  --dest-namespace production \
  --kustomize-image registry.example.com/user-service:v1.2.3
```

#### 12.5.3.3 多环境管理

使用Kustomize管理多环境配置：

```bash
# 目录结构
k8s-manifests/
├── base/
│   ├── deployment.yaml
│   ├── service.yaml
│   ├── ingress.yaml
│   └── kustomization.yaml
└── overlays/
    ├── dev/
    │   ├── kustomization.yaml
    │   └── patches/
    │       └── deployment-patch.yaml
    ├── staging/
    │   ├── kustomization.yaml
    │   └── patches/
    │       └── deployment-patch.yaml
    └── production/
        ├── kustomization.yaml
        └── patches/
            └── deployment-patch.yaml
```

```yaml
# base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - deployment.yaml
  - service.yaml
  - ingress.yaml

commonLabels:
  app: user-service
  managed-by: argocd
```

```yaml
# overlays/production/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: production

bases:
  - ../../base

namePrefix: prod-

commonLabels:
  environment: production

images:
  - name: user-service
    newName: registry.example.com/user-service
    newTag: v1.2.3

replicas:
  - name: user-service
    count: 5

patches:
  - path: patches/deployment-patch.yaml

configMapGenerator:
  - name: app-config
    literals:
      - LOG_LEVEL=info
      - ENVIRONMENT=production

secretGenerator:
  - name: app-secrets
    envs:
      - secrets.env
```

```yaml
# overlays/production/patches/deployment-patch.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
spec:
  template:
    spec:
      containers:
      - name: user-service
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi
        env:
        - name: ENVIRONMENT
          value: production
```

为每个环境创建ArgoCD应用：

```yaml
# apps/dev-user-service.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: dev-user-service
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/k8s-manifests
    targetRevision: develop
    path: overlays/dev
  destination:
    server: https://kubernetes.default.svc
    namespace: dev
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
---
# apps/staging-user-service.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: staging-user-service
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/k8s-manifests
    targetRevision: main
    path: overlays/staging
  destination:
    server: https://kubernetes.default.svc
    namespace: staging
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
---
# apps/prod-user-service.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: prod-user-service
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/k8s-manifests
    targetRevision: main
    path: overlays/production
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    # 生产环境不自动同步，需要手动批准
    syncOptions:
      - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
```

#### 12.5.3.4 App of Apps模式

使用App of Apps模式管理多个应用：

```yaml
# apps/root-app.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: root-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/k8s-manifests
    targetRevision: main
    path: apps
  destination:
    server: https://kubernetes.default.svc
    namespace: argocd
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

```yaml
# apps/microservices.yaml
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: microservices
  namespace: argocd
spec:
  generators:
    # 列表生成器
    - list:
        elements:
          - name: user-service
            namespace: production
            replicaCount: "5"
          - name: order-service
            namespace: production
            replicaCount: "3"
          - name: payment-service
            namespace: production
            replicaCount: "3"
  
  template:
    metadata:
      name: '{{name}}'
    spec:
      project: default
      source:
        repoURL: https://github.com/your-org/k8s-manifests
        targetRevision: main
        path: 'apps/{{name}}'
        helm:
          parameters:
            - name: replicaCount
              value: '{{replicaCount}}'
      destination:
        server: https://kubernetes.default.svc
        namespace: '{{namespace}}'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
```

#### 12.5.3.5 集成CI/CD流水线

**GitLab CI/CD集成ArgoCD：**

```yaml
# .gitlab-ci.yml
stages:
  - build
  - deploy

build:
  stage: build
  script:
    - docker build -t $IMAGE_NAME:$IMAGE_TAG .
    - docker push $IMAGE_NAME:$IMAGE_TAG

deploy:
  stage: deploy
  image: argoproj/argocd:latest
  script:
    # 登录ArgoCD
    - argocd login argocd.example.com --username admin --password $ARGOCD_PASSWORD --insecure
    
    # 更新镜像标签
    - |
      argocd app set user-service \
        --helm-set image.tag=$IMAGE_TAG
    
    # 同步应用
    - argocd app sync user-service --prune
    
    # 等待同步完成
    - argocd app wait user-service --health --timeout 600
  only:
    - main
```

**Jenkins Pipeline集成ArgoCD：**

```groovy
// Jenkinsfile
pipeline {
    agent any
    
    environment {
        ARGOCD_SERVER = 'argocd.example.com'
        ARGOCD_APP = 'user-service'
    }
    
    stages {
        stage('Build & Push Image') {
            steps {
                script {
                    def imageTag = "${env.BUILD_NUMBER}-${env.GIT_COMMIT.take(8)}"
                    sh """
                        docker build -t registry.example.com/user-service:${imageTag} .
                        docker push registry.example.com/user-service:${imageTag}
                    """
                    env.IMAGE_TAG = imageTag
                }
            }
        }
        
        stage('Update ArgoCD') {
            steps {
                withCredentials([string(credentialsId: 'argocd-token', variable: 'ARGOCD_AUTH_TOKEN')]) {
                    sh """
                        # 登录ArgoCD
                        argocd login ${ARGOCD_SERVER} --auth-token ${ARGOCD_AUTH_TOKEN} --insecure
                        
                        # 更新镜像标签
                        argocd app set ${ARGOCD_APP} --helm-set image.tag=${IMAGE_TAG}
                        
                        # 同步应用
                        argocd app sync ${ARGOCD_APP} --prune
                        
                        # 等待同步完成
                        argocd app wait ${ARGOCD_APP} --health --timeout 600
                    """
                }
            }
        }
    }
}
```

**使用Git提交触发部署：**

```bash
# 更新镜像标签
cd k8s-manifests
git checkout main

# 方式1：直接修改values文件
yq eval '.image.tag = "v1.2.4"' -i apps/user-service/values-prod.yaml

# 方式2：使用kustomize设置镜像
cd overlays/production
kustomize edit set image user-service=registry.example.com/user-service:v1.2.4

# 提交变更
git add .
git commit -m "Update user-service to v1.2.4"
git push origin main

# ArgoCD会自动检测变更并同步
```

#### 12.5.3.6 高级特性

**1. 同步波次（Sync Waves）**

控制资源的同步顺序：

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  annotations:
    argocd.argoproj.io/sync-wave: "-1"  # 最先创建
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
  annotations:
    argocd.argoproj.io/sync-wave: "0"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
  namespace: production
  annotations:
    argocd.argoproj.io/sync-wave: "1"
---
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: production
  annotations:
    argocd.argoproj.io/sync-wave: "2"
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: user-service
  namespace: production
  annotations:
    argocd.argoproj.io/sync-wave: "3"  # 最后创建
```

**2. 资源钩子（Resource Hooks）**

在同步前后执行任务：

```yaml
# pre-sync-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: db-migration
  namespace: production
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
spec:
  template:
    spec:
      containers:
      - name: migrate
        image: registry.example.com/db-migrator:latest
        command:
        - /bin/sh
        - -c
        - |
          echo "Running database migrations..."
          migrate -path /migrations -database $DATABASE_URL up
      restartPolicy: Never
  backoffLimit: 3
---
# post-sync-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: smoke-test
  namespace: production
  annotations:
    argocd.argoproj.io/hook: PostSync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
spec:
  template:
    spec:
      containers:
      - name: test
        image: curlimages/curl:latest
        command:
        - /bin/sh
        - -c
        - |
          echo "Running smoke tests..."
          curl -f http://user-service:8080/health || exit 1
      restartPolicy: Never
  backoffLimit: 3
```

**3. 同步窗口（Sync Windows）**

限制应用同步的时间窗口：

```yaml
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: production
  namespace: argocd
spec:
  description: Production project
  
  # 同步窗口
  syncWindows:
    # 允许窗口：工作日9:00-18:00
    - kind: allow
      schedule: '0 9 * * 1-5'
      duration: 9h
      applications:
        - '*'
      manualSync: true
    
    # 拒绝窗口：周末和节假日
    - kind: deny
      schedule: '0 0 * * 0,6'
      duration: 24h
      applications:
        - '*'
  
  sourceRepos:
    - '*'
  
  destinations:
    - namespace: production
      server: https://kubernetes.default.svc
```


### 12.5.4 镜像安全扫描与签名

容器镜像安全是云原生安全的重要组成部分。本节介绍如何使用Trivy进行漏洞扫描，以及使用Cosign进行镜像签名和验证。

#### 12.5.4.1 Trivy漏洞扫描

Trivy是一个全面的容器镜像漏洞扫描工具，支持扫描OS包、应用依赖、配置文件等。

**安装Trivy：**

```bash
# 方式1：使用包管理器安装
# Ubuntu/Debian
wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list
sudo apt-get update
sudo apt-get install trivy

# CentOS/RHEL
sudo rpm -ivh https://github.com/aquasecurity/trivy/releases/download/v0.48.0/trivy_0.48.0_Linux-64bit.rpm

# 方式2：使用Docker运行
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock aquasec/trivy:latest image nginx:latest

# 方式3：在Kubernetes中部署
kubectl apply -f https://raw.githubusercontent.com/aquasecurity/trivy-operator/main/deploy/static/trivy-operator.yaml
```

**基本扫描命令：**

```bash
# 扫描镜像
trivy image nginx:latest

# 只显示高危和严重漏洞
trivy image --severity HIGH,CRITICAL nginx:latest

# 扫描并输出JSON格式
trivy image --format json --output result.json nginx:latest

# 扫描文件系统
trivy fs /path/to/project

# 扫描Git仓库
trivy repo https://github.com/your-org/your-repo

# 扫描Kubernetes集群
trivy k8s --report summary cluster

# 扫描IaC配置文件
trivy config ./terraform/

# 扫描SBOM（软件物料清单）
trivy sbom ./sbom.json
```

**集成到CI/CD流水线：**

```yaml
# .gitlab-ci.yml
security-scan:
  stage: security
  image: aquasec/trivy:latest
  script:
    # 更新漏洞数据库
    - trivy image --download-db-only
    
    # 扫描镜像
    - |
      trivy image \
        --severity HIGH,CRITICAL \
        --exit-code 1 \
        --no-progress \
        --format json \
        --output trivy-report.json \
        $IMAGE_NAME:$IMAGE_TAG
    
    # 生成HTML报告
    - |
      trivy image \
        --severity HIGH,CRITICAL \
        --format template \
        --template "@contrib/html.tpl" \
        --output trivy-report.html \
        $IMAGE_NAME:$IMAGE_TAG
  
  artifacts:
    reports:
      container_scanning: trivy-report.json
    paths:
      - trivy-report.html
    expire_in: 30 days
  
  allow_failure: false
  only:
    - main
    - develop
```

**Trivy配置文件：**

```yaml
# trivy.yaml
scan:
  # 扫描级别
  severity:
    - CRITICAL
    - HIGH
    - MEDIUM
  
  # 忽略未修复的漏洞
  ignore-unfixed: true
  
  # 漏洞类型
  vuln-type:
    - os
    - library
  
  # 安全检查
  security-checks:
    - vuln
    - config
    - secret
  
  # 超时设置
  timeout: 10m

# 忽略特定漏洞
vulnerability:
  ignore:
    - id: CVE-2023-12345
      reason: "False positive - not applicable to our use case"
      expires: "2024-12-31"
    
    - id: CVE-2023-67890
      reason: "Waiting for upstream fix"
      expires: "2024-06-30"

# 自定义策略
policy:
  # 禁止使用的包
  deny-packages:
    - name: "log4j"
      version: "< 2.17.0"
      reason: "Known RCE vulnerability"
  
  # 必需的标签
  required-labels:
    - "maintainer"
    - "version"
```

使用配置文件：

```bash
trivy image --config trivy.yaml nginx:latest
```

#### 12.5.4.2 Cosign镜像签名

Cosign是Sigstore项目的一部分，用于容器镜像的签名和验证。

**安装Cosign：**

```bash
# 下载并安装
wget https://github.com/sigstore/cosign/releases/download/v2.2.2/cosign-linux-amd64
chmod +x cosign-linux-amd64
sudo mv cosign-linux-amd64 /usr/local/bin/cosign

# 验证安装
cosign version
```

**生成密钥对：**

```bash
# 生成密钥对（会提示输入密码）
cosign generate-key-pair

# 这会生成两个文件：
# - cosign.key (私钥，需要安全保管)
# - cosign.pub (公钥，用于验证)

# 将私钥存储到Kubernetes Secret
kubectl create secret generic cosign-key \
  --from-file=cosign.key=cosign.key \
  --namespace=default

# 将公钥存储到ConfigMap
kubectl create configmap cosign-pub \
  --from-file=cosign.pub=cosign.pub \
  --namespace=default
```

**签名镜像：**

```bash
# 使用密钥对签名
cosign sign --key cosign.key registry.example.com/user-service:v1.2.3

# 使用密钥对签名并添加注释
cosign sign --key cosign.key \
  -a "author=ops-team" \
  -a "build-id=12345" \
  -a "git-commit=abc123" \
  registry.example.com/user-service:v1.2.3

# 使用Keyless签名（无需管理密钥）
cosign sign registry.example.com/user-service:v1.2.3

# 批量签名
for tag in v1.2.3 v1.2.4 v1.2.5; do
  cosign sign --key cosign.key registry.example.com/user-service:$tag
done
```

**验证签名：**

```bash
# 验证镜像签名
cosign verify --key cosign.pub registry.example.com/user-service:v1.2.3

# 验证并显示注释
cosign verify --key cosign.pub \
  -a "author=ops-team" \
  registry.example.com/user-service:v1.2.3

# 验证Keyless签名
cosign verify \
  --certificate-identity=user@example.com \
  --certificate-oidc-issuer=https://accounts.google.com \
  registry.example.com/user-service:v1.2.3
```

**集成到CI/CD流水线：**

```yaml
# .gitlab-ci.yml
sign-image:
  stage: security
  image: gcr.io/projectsigstore/cosign:latest
  before_script:
    # 从GitLab CI/CD变量获取私钥
    - echo "$COSIGN_PRIVATE_KEY" > cosign.key
    - echo "$COSIGN_PASSWORD" > cosign.password
  
  script:
    # 签名镜像
    - |
      cosign sign --key cosign.key \
        -a "pipeline-id=$CI_PIPELINE_ID" \
        -a "commit-sha=$CI_COMMIT_SHA" \
        -a "commit-author=$GITLAB_USER_NAME" \
        -a "build-date=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
        $IMAGE_NAME:$IMAGE_TAG
    
    # 生成SBOM并签名
    - |
      cosign attach sbom --sbom sbom.json $IMAGE_NAME:$IMAGE_TAG
      cosign sign --key cosign.key --attachment sbom $IMAGE_NAME:$IMAGE_TAG
  
  after_script:
    # 清理敏感文件
    - rm -f cosign.key cosign.password
  
  only:
    - main
    - tags

verify-image:
  stage: deploy
  image: gcr.io/projectsigstore/cosign:latest
  before_script:
    - echo "$COSIGN_PUBLIC_KEY" > cosign.pub
  
  script:
    # 验证镜像签名
    - |
      cosign verify --key cosign.pub \
        -a "pipeline-id=$CI_PIPELINE_ID" \
        $IMAGE_NAME:$IMAGE_TAG
    
    # 验证SBOM签名
    - cosign verify --key cosign.pub --attachment sbom $IMAGE_NAME:$IMAGE_TAG
  
  only:
    - main
```

#### 12.5.4.3 策略执行（Policy Enforcement）

使用Kyverno或OPA Gatekeeper强制执行镜像签名验证策略。

**使用Kyverno验证镜像签名：**

```bash
# 安装Kyverno
helm repo add kyverno https://kyverno.github.io/kyverno/
helm repo update
helm install kyverno kyverno/kyverno --namespace kyverno --create-namespace
```

```yaml
# verify-image-policy.yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: verify-image-signature
spec:
  validationFailureAction: Enforce
  background: false
  
  rules:
    - name: verify-signature
      match:
        any:
          - resources:
              kinds:
                - Pod
              namespaces:
                - production
                - staging
      
      verifyImages:
        - imageReferences:
            - "registry.example.com/*"
          
          attestors:
            - count: 1
              entries:
                - keys:
                    publicKeys: |-
                      -----BEGIN PUBLIC KEY-----
                      MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE...
                      -----END PUBLIC KEY-----
          
          # 验证注释
          attestations:
            - predicateType: https://cosign.sigstore.dev/attestation/v1
              conditions:
                - all:
                    - key: "{{ author }}"
                      operator: Equals
                      value: "ops-team"
          
          # 验证镜像仓库
          repository: "registry.example.com"
          
          # 验证镜像摘要
          required: true
          
          # 验证失败消息
          message: "Image signature verification failed. Only signed images from registry.example.com are allowed."
```

部署策略：

```bash
kubectl apply -f verify-image-policy.yaml

# 测试策略
kubectl run test-pod --image=registry.example.com/user-service:v1.2.3 -n production

# 如果镜像未签名，会看到错误：
# Error from server: admission webhook "mutate.kyverno.svc" denied the request:
# Image signature verification failed. Only signed images from registry.example.com are allowed.
```

**使用OPA Gatekeeper验证镜像签名：**

```bash
# 安装Gatekeeper
kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml
```

```yaml
# constraint-template.yaml
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8srequireimagesignature
spec:
  crd:
    spec:
      names:
        kind: K8sRequireImageSignature
      validation:
        openAPIV3Schema:
          type: object
          properties:
            allowedRegistries:
              type: array
              items:
                type: string
  
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequireimagesignature
        
        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not is_signed(container.image)
          msg := sprintf("Image %v is not signed", [container.image])
        }
        
        is_signed(image) {
          # 这里需要集成Cosign验证逻辑
          # 实际实现中需要调用外部服务验证签名
          startswith(image, "registry.example.com/")
        }
---
# constraint.yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequireImageSignature
metadata:
  name: require-image-signature
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - production
      - staging
  
  parameters:
    allowedRegistries:
      - "registry.example.com"
```

#### 12.5.4.4 SBOM生成与管理

软件物料清单（SBOM）记录了软件组件的详细信息，是供应链安全的重要组成部分。

**使用Syft生成SBOM：**

```bash
# 安装Syft
curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin

# 生成SBOM
syft registry.example.com/user-service:v1.2.3 -o json > sbom.json

# 生成多种格式
syft registry.example.com/user-service:v1.2.3 -o spdx-json > sbom-spdx.json
syft registry.example.com/user-service:v1.2.3 -o cyclonedx-json > sbom-cyclonedx.json

# 扫描本地目录
syft dir:/path/to/project -o json > sbom.json

# 扫描Docker镜像
syft docker:nginx:latest -o json > sbom.json
```

**集成到CI/CD流水线：**

```yaml
# .gitlab-ci.yml
generate-sbom:
  stage: security
  image: anchore/syft:latest
  script:
    # 生成SBOM
    - syft $IMAGE_NAME:$IMAGE_TAG -o spdx-json > sbom-spdx.json
    - syft $IMAGE_NAME:$IMAGE_TAG -o cyclonedx-json > sbom-cyclonedx.json
    
    # 附加SBOM到镜像
    - cosign attach sbom --sbom sbom-spdx.json $IMAGE_NAME:$IMAGE_TAG
    
    # 签名SBOM
    - cosign sign --key cosign.key --attachment sbom $IMAGE_NAME:$IMAGE_TAG
  
  artifacts:
    paths:
      - sbom-spdx.json
      - sbom-cyclonedx.json
    expire_in: 1 year
  
  only:
    - main
    - tags
```

**使用Grype扫描SBOM：**

```bash
# 安装Grype
curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin

# 扫描SBOM
grype sbom:sbom.json

# 只显示高危和严重漏洞
grype sbom:sbom.json --fail-on high

# 输出JSON格式
grype sbom:sbom.json -o json > vulnerabilities.json
```

#### 12.5.4.5 完整的安全流水线

整合所有安全工具的完整CI/CD流水线：

```yaml
# .gitlab-ci.yml
variables:
  DOCKER_DRIVER: overlay2
  REGISTRY: registry.example.com
  IMAGE_NAME: $REGISTRY/$CI_PROJECT_PATH
  IMAGE_TAG: $CI_COMMIT_SHORT_SHA

stages:
  - build
  - scan
  - sign
  - deploy

# 构建镜像
build:
  stage: build
  image: docker:24-dind
  services:
    - docker:24-dind
  script:
    - docker build -t $IMAGE_NAME:$IMAGE_TAG .
    - docker push $IMAGE_NAME:$IMAGE_TAG
  only:
    - main
    - develop

# 代码扫描
code-scan:
  stage: scan
  image: aquasec/trivy:latest
  script:
    # 扫描源代码
    - trivy fs --severity HIGH,CRITICAL --exit-code 0 .
    
    # 扫描配置文件
    - trivy config --severity HIGH,CRITICAL --exit-code 0 .
    
    # 扫描密钥
    - trivy fs --scanners secret --exit-code 1 .
  allow_failure: false

# 依赖扫描
dependency-scan:
  stage: scan
  image: aquasec/trivy:latest
  script:
    - trivy image --severity HIGH,CRITICAL --exit-code 0 $IMAGE_NAME:$IMAGE_TAG
  artifacts:
    reports:
      dependency_scanning: gl-dependency-scanning-report.json

# 镜像扫描
image-scan:
  stage: scan
  image: aquasec/trivy:latest
  script:
    # 扫描镜像漏洞
    - |
      trivy image \
        --severity HIGH,CRITICAL \
        --exit-code 1 \
        --format json \
        --output trivy-report.json \
        $IMAGE_NAME:$IMAGE_TAG
    
    # 生成HTML报告
    - |
      trivy image \
        --severity HIGH,CRITICAL \
        --format template \
        --template "@contrib/html.tpl" \
        --output trivy-report.html \
        $IMAGE_NAME:$IMAGE_TAG
  
  artifacts:
    reports:
      container_scanning: trivy-report.json
    paths:
      - trivy-report.html
    expire_in: 30 days
  
  allow_failure: false

# 生成SBOM
generate-sbom:
  stage: scan
  image: anchore/syft:latest
  script:
    # 生成SBOM
    - syft $IMAGE_NAME:$IMAGE_TAG -o spdx-json > sbom-spdx.json
    - syft $IMAGE_NAME:$IMAGE_TAG -o cyclonedx-json > sbom-cyclonedx.json
  
  artifacts:
    paths:
      - sbom-spdx.json
      - sbom-cyclonedx.json
    expire_in: 1 year

# 扫描SBOM
scan-sbom:
  stage: scan
  image: anchore/grype:latest
  dependencies:
    - generate-sbom
  script:
    - grype sbom:sbom-spdx.json --fail-on high -o json > grype-report.json
  
  artifacts:
    paths:
      - grype-report.json
    expire_in: 30 days
  
  allow_failure: false

# 签名镜像
sign-image:
  stage: sign
  image: gcr.io/projectsigstore/cosign:latest
  before_script:
    - echo "$COSIGN_PRIVATE_KEY" > cosign.key
    - echo "$COSIGN_PASSWORD" > cosign.password
  
  script:
    # 签名镜像
    - |
      COSIGN_PASSWORD=$(cat cosign.password) cosign sign --key cosign.key \
        -a "pipeline-id=$CI_PIPELINE_ID" \
        -a "commit-sha=$CI_COMMIT_SHA" \
        -a "commit-author=$GITLAB_USER_NAME" \
        -a "build-date=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
        -a "trivy-scan=passed" \
        $IMAGE_NAME:$IMAGE_TAG
    
    # 附加并签名SBOM
    - cosign attach sbom --sbom sbom-spdx.json $IMAGE_NAME:$IMAGE_TAG
    - COSIGN_PASSWORD=$(cat cosign.password) cosign sign --key cosign.key --attachment sbom $IMAGE_NAME:$IMAGE_TAG
  
  after_script:
    - rm -f cosign.key cosign.password
  
  dependencies:
    - generate-sbom
  
  only:
    - main

# 验证并部署
deploy:
  stage: deploy
  image: bitnami/kubectl:latest
  before_script:
    - echo "$COSIGN_PUBLIC_KEY" > cosign.pub
    - echo "$KUBE_CONFIG" > ~/.kube/config
  
  script:
    # 验证镜像签名
    - |
      cosign verify --key cosign.pub \
        -a "pipeline-id=$CI_PIPELINE_ID" \
        $IMAGE_NAME:$IMAGE_TAG
    
    # 部署到Kubernetes
    - |
      kubectl set image deployment/user-service \
        user-service=$IMAGE_NAME:$IMAGE_TAG \
        -n production
    
    - kubectl rollout status deployment/user-service -n production --timeout=10m
  
  environment:
    name: production
    url: https://api.example.com
  
  when: manual
  only:
    - main
```

**本节小结**

通过本节的学习，你已经掌握了：

1. **GitLab CI/CD配置**
   - 部署GitLab Runner到Kubernetes
   - 编写完整的.gitlab-ci.yml流水线
   - 多环境配置管理
   - Pipeline优化技巧

2. **Jenkins Pipeline实践**
   - 在Kubernetes中部署Jenkins
   - 声明式和脚本式Pipeline
   - 共享库（Shared Library）
   - 多分支Pipeline

3. **ArgoCD GitOps部署**
   - ArgoCD架构与安装
   - 创建和管理Application
   - 多环境管理
   - App of Apps模式
   - 集成CI/CD流水线

4. **镜像安全扫描与签名**
   - 使用Trivy进行漏洞扫描
   - 使用Cosign进行镜像签名和验证
   - 策略执行（Kyverno/OPA Gatekeeper）
   - SBOM生成与管理

通过这些实践，你已经建立了一个完整的、安全的CI/CD流水线，实现了从代码提交到生产部署的全自动化流程。接下来，我们将学习如何进行Kubernetes集群的备份与恢复！

---

