# 第3章: 工作负载控制器

## 章节概述

在第2章中,我们深入学习了Kubernetes的最小调度单元 **Pod**。但在生产环境中,我们很少直接创建和管理Pod,而是使用更高层次的 **工作负载控制器 (Workload Controllers)** 来自动化管理Pod的生命周期。

**本章核心目标**:
- 掌握5大工作负载控制器的原理与使用场景
- 学习滚动更新、版本回滚、金丝雀发布等部署策略
- 实战有状态应用(数据库集群)和无状态应用(Web服务)的部署
- 理解DaemonSet守护进程和Job批处理任务的最佳实践

**为什么需要工作负载控制器?**

```
直接使用Pod的问题:
┌────────────────────────────────────────┐
│  问题1: 无法自动重启                    │
│  Pod被删除或节点故障 → Pod永久丢失      │
├────────────────────────────────────────┤
│  问题2: 无法水平扩展                    │
│  流量增加 → 手动创建多个Pod副本          │
├────────────────────────────────────────┤
│  问题3: 无法滚动更新                    │
│  镜像更新 → 手动删除旧Pod + 创建新Pod    │
├────────────────────────────────────────┤
│  问题4: 无法版本回滚                    │
│  更新失败 → 手动恢复到之前的配置          │
└────────────────────────────────────────┘

工作负载控制器的解决方案:
┌────────────────────────────────────────┐
│  ✅ Deployment  → 无状态应用自动化管理   │
│  ✅ StatefulSet → 有状态应用有序管理     │
│  ✅ DaemonSet   → 每节点守护进程         │
│  ✅ Job/CronJob → 批处理/定时任务        │
└────────────────────────────────────────┘
```

**控制器模式 (Controller Pattern)**:

所有工作负载控制器都基于Kubernetes的 **控制循环 (Control Loop)** 模式工作:

```
控制循环原理:
┌──────────────────────────────────────────┐
│  1. 监听资源变化 (Watch API Server)      │
│     → 控制器订阅资源对象的增删改事件      │
└──────────────────────────────────────────┘
                ↓
┌──────────────────────────────────────────┐
│  2. 对比期望状态与实际状态                │
│     期望状态 (Spec): 用户声明的配置       │
│     实际状态 (Status): 集群当前状态       │
└──────────────────────────────────────────┘
                ↓
┌──────────────────────────────────────────┐
│  3. 执行协调 (Reconcile)                 │
│     实际状态 ≠ 期望状态 → 采取行动        │
│     - 创建Pod (副本数不足)               │
│     - 删除Pod (副本数过多)               │
│     - 更新Pod (版本变更)                 │
└──────────────────────────────────────────┘
                ↓
┌──────────────────────────────────────────┐
│  4. 更新资源状态                          │
│     将协调结果写回资源的Status字段        │
└──────────────────────────────────────────┘
                ↓
         (循环往复,持续监控)
```

**5大工作负载控制器对比**:

| 控制器 | 适用场景 | 副本管理 | Pod标识 | 启动顺序 | 典型应用 |
|-------|---------|---------|---------|---------|---------|
| **Deployment** | 无状态应用 | ✅ 支持 | 随机 | 并行 | Web服务、API服务、微服务 |
| **StatefulSet** | 有状态应用 | ✅ 支持 | 稳定 (hostname) | 有序 (0→N) | 数据库、Zookeeper、Kafka |
| **DaemonSet** | 守护进程 | ❌ 每节点1个 | 随机 | 并行 | 日志采集、监控Agent、CNI插件 |
| **Job** | 一次性任务 | ❌ 运行到完成 | 随机 | 并行/串行 | 数据处理、批量计算、数据库迁移 |
| **CronJob** | 定时任务 | ❌ 定时创建Job | 随机 | 定时触发 | 定时备份、报表生成、数据同步 |

**本章学习路线**:

```
3.1 ReplicaSet (副本控制基础)
  ↓ 理解副本控制原理
3.2 Deployment (无状态应用管理)
  ↓ 掌握滚动更新、版本回滚、金丝雀发布
3.3 StatefulSet (有状态应用管理)
  ↓ 掌握稳定网络标识、有序部署、持久化存储
3.4 DaemonSet (守护进程管理)
  ↓ 掌握节点选择器、容忍污点
3.5 Job & CronJob (批处理任务)
  ↓ 掌握并行策略、任务重试、定时调度
3.6 实战项目 (综合应用)
  ↓ 部署Web应用、Redis集群、日志采集、定时备份
3.7 本章小结
```

现在让我们开始深入学习每个工作负载控制器!


---

## 3.1 ReplicaSet: 副本控制器

### 3.1.1 ReplicaSet 基础概念

**定义**: ReplicaSet确保指定数量的Pod副本在任何时候都处于运行状态。

```
ReplicaSet工作原理:
┌──────────────────────────────────────────────┐
│  ReplicaSet: web-rs                          │
│  replicas: 3 (期望副本数)                     │
├──────────────────────────────────────────────┤
│  标签选择器: app=web                          │
│  Pod模板: nginx:1.25                          │
└──────────────────────────────────────────────┘
                ↓
        控制循环持续监控
                ↓
┌──────────────────────────────────────────────┐
│  场景1: 实际副本数 < 期望副本数               │
│  当前: 2个Pod运行, 1个Pod失败                │
│  行动: 自动创建1个新Pod                       │
└──────────────────────────────────────────────┘
                ↓
┌──────────────────────────────────────────────┐
│  场景2: 实际副本数 > 期望副本数               │
│  当前: 4个Pod运行 (手动创建了1个额外Pod)      │
│  行动: 自动删除1个Pod                         │
└──────────────────────────────────────────────┘
                ↓
┌──────────────────────────────────────────────┐
│  场景3: Node故障                              │
│  当前: Node-2故障, 该节点上的2个Pod不可用     │
│  行动: 在其他节点创建2个新Pod                 │
└──────────────────────────────────────────────┘
```

**基础示例**:

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
  namespace: default
  labels:
    app: nginx
    tier: frontend
spec:
  # 副本数量
  replicas: 3

  # 标签选择器 (匹配哪些Pod由此ReplicaSet管理)
  selector:
    matchLabels:
      app: nginx
      tier: frontend

  # Pod模板 (创建新Pod时使用此模板)
  template:
    metadata:
      labels:
        app: nginx
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx:1.25-alpine
        ports:
        - containerPort: 80
          name: http
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
```

**创建与验证**:

```bash
# 创建ReplicaSet
kubectl apply -f nginx-rs.yaml

# 查看ReplicaSet状态
kubectl get replicaset nginx-rs
# NAME       DESIRED   CURRENT   READY   AGE
# nginx-rs   3         3         3       30s

# 查看ReplicaSet详情
kubectl describe replicaset nginx-rs
# Name:         nginx-rs
# Namespace:    default
# Selector:     app=nginx,tier=frontend
# Replicas:     3 current / 3 desired
# Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed

# 查看ReplicaSet创建的Pod
kubectl get pods -l app=nginx,tier=frontend
# NAME             READY   STATUS    RESTARTS   AGE
# nginx-rs-7k8xz   1/1     Running   0          1m
# nginx-rs-m9p4j   1/1     Running   0          1m
# nginx-rs-xt6w2   1/1     Running   0          1m
```


### 3.1.2 标签选择器 (Label Selector)

ReplicaSet通过 **标签选择器** 识别哪些Pod由它管理。

**选择器类型**:

```yaml
# 方式1: matchLabels (精确匹配)
selector:
  matchLabels:
    app: nginx
    env: production
# 匹配规则: app=nginx AND env=production

---
# 方式2: matchExpressions (表达式匹配)
selector:
  matchExpressions:
  - key: app
    operator: In
    values:
    - nginx
    - httpd
  - key: tier
    operator: NotIn
    values:
    - backend
  - key: env
    operator: Exists
# 匹配规则: (app IN [nginx, httpd]) AND (tier NOT IN [backend]) AND (env key存在)
```

**操作符说明**:

| 操作符 | 说明 | 示例 |
|-------|------|------|
| `In` | 标签值在列表中 | `tier In [frontend, middleware]` |
| `NotIn` | 标签值不在列表中 | `env NotIn [test, dev]` |
| `Exists` | 标签key存在 (不关心value) | `version Exists` |
| `DoesNotExist` | 标签key不存在 | `deprecated DoesNotExist` |

**⚠️ 重要约束**: ReplicaSet的 `selector` 一旦创建就 **不可修改**!

```bash
# ❌ 错误: 尝试修改selector (会失败)
kubectl edit replicaset nginx-rs
# 修改 selector → 报错: field is immutable

# ✅ 正确: 如需修改selector,必须删除重建
kubectl delete replicaset nginx-rs
kubectl apply -f nginx-rs-new.yaml
```


### 3.1.3 ReplicaSet 自愈能力测试

**测试1: 手动删除Pod (验证自动重建)**

```bash
# 查看当前Pod
kubectl get pods -l app=nginx
# NAME             READY   STATUS    RESTARTS   AGE
# nginx-rs-7k8xz   1/1     Running   0          5m
# nginx-rs-m9p4j   1/1     Running   0          5m
# nginx-rs-xt6w2   1/1     Running   0          5m

# 手动删除1个Pod
kubectl delete pod nginx-rs-7k8xz
# pod "nginx-rs-7k8xz" deleted

# 立即查看 (ReplicaSet已自动创建新Pod)
kubectl get pods -l app=nginx
# NAME             READY   STATUS              RESTARTS   AGE
# nginx-rs-m9p4j   1/1     Running             0          5m
# nginx-rs-xt6w2   1/1     Running             0          5m
# nginx-rs-abc123  0/1     ContainerCreating   0          2s  ← 新Pod

# 等待几秒后
kubectl get pods -l app=nginx
# NAME             READY   STATUS    RESTARTS   AGE
# nginx-rs-m9p4j   1/1     Running   0          5m
# nginx-rs-xt6w2   1/1     Running   0          5m
# nginx-rs-abc123  1/1     Running   0          10s  ← 新Pod已就绪
```

**测试2: 修改副本数 (水平扩缩容)**

```bash
# 方式1: 使用kubectl scale命令
kubectl scale replicaset nginx-rs --replicas=5
# replicaset.apps/nginx-rs scaled

kubectl get pods -l app=nginx
# NAME             READY   STATUS    RESTARTS   AGE
# nginx-rs-m9p4j   1/1     Running   0          10m
# nginx-rs-xt6w2   1/1     Running   0          10m
# nginx-rs-abc123  1/1     Running   0          5m
# nginx-rs-def456  1/1     Running   0          5s  ← 新增
# nginx-rs-ghi789  1/1     Running   0          5s  ← 新增

# 方式2: 使用kubectl edit修改YAML
kubectl edit replicaset nginx-rs
# 修改 spec.replicas: 5 → 2

kubectl get pods -l app=nginx
# NAME             READY   STATUS        RESTARTS   AGE
# nginx-rs-m9p4j   1/1     Running       0          15m
# nginx-rs-xt6w2   1/1     Running       0          15m
# nginx-rs-abc123  1/1     Terminating   0          10m  ← 正在删除
# nginx-rs-def456  1/1     Terminating   0          5m   ← 正在删除
# nginx-rs-ghi789  1/1     Terminating   0          5m   ← 正在删除
```

**测试3: Node故障模拟 (验证跨节点重建)**

```bash
# 查看Pod分布
kubectl get pods -l app=nginx -o wide
# NAME             READY   STATUS    NODE
# nginx-rs-m9p4j   1/1     Running   node-1
# nginx-rs-xt6w2   1/1     Running   node-2
# nginx-rs-abc123  1/1     Running   node-2

# 模拟Node-2故障 (将节点标记为不可调度)
kubectl cordon node-2
# node/node-2 cordoned

kubectl drain node-2 --ignore-daemonsets --delete-emptydir-data
# node/node-2 drained

# ReplicaSet自动在node-1创建新Pod
kubectl get pods -l app=nginx -o wide
# NAME             READY   STATUS    NODE
# nginx-rs-m9p4j   1/1     Running   node-1
# nginx-rs-new1    1/1     Running   node-1  ← 新Pod
# nginx-rs-new2    1/1     Running   node-1  ← 新Pod
```


### 3.1.4 ReplicaSet vs ReplicationController

ReplicaSet是 **ReplicationController (RC)** 的升级版本,主要区别:

| 特性 | ReplicationController | ReplicaSet |
|------|----------------------|------------|
| **API版本** | v1 | apps/v1 |
| **选择器** | 仅支持等值选择器 (`app=nginx`) | 支持集合选择器 (`app in [nginx, httpd]`) |
| **推荐使用** | ❌ 已弃用 | ✅ 推荐 |
| **控制器** | 独立使用 | 通常由Deployment管理 |

**⚠️ 重要**: 在生产环境中,**不要直接使用ReplicaSet**,而是使用 **Deployment** (下一节),因为Deployment提供了更强大的功能:
- 滚动更新 (RollingUpdate)
- 版本回滚 (Rollback)
- 暂停/恢复更新
- 声明式更新


### 3.1.5 ReplicaSet 最佳实践

**1. 始终通过Deployment管理ReplicaSet**:

```yaml
# ❌ 不推荐: 直接创建ReplicaSet
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.25

# ✅ 推荐: 使用Deployment (会自动创建ReplicaSet)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
```

**2. 确保selector与template.labels匹配**:

```yaml
# ❌ 错误: selector与labels不匹配 (ReplicaSet无法创建)
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx        # selector要求 app=nginx
  template:
    metadata:
      labels:
        app: web        # ❌ template中是 app=web (不匹配!)
    spec:
      containers:
      - name: nginx
        image: nginx:1.25

# ✅ 正确: selector与labels匹配
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx      # ✅ 匹配selector
```

**3. 避免手动创建匹配selector的Pod**:

```bash
# 场景: ReplicaSet期望3个副本
kubectl get replicaset nginx-rs
# NAME       DESIRED   CURRENT   READY
# nginx-rs   3         3         3

# ❌ 错误操作: 手动创建匹配标签的Pod
kubectl run nginx-manual --image=nginx:1.25 --labels="app=nginx,tier=frontend"
# Pod创建成功,但会立即被ReplicaSet删除!

# 原因: ReplicaSet检测到副本数超过期望值 (4 > 3)
# ReplicaSet会随机删除1个Pod (可能是新创建的,也可能是旧Pod)
```

**4. 使用唯一的标签组合**:

```yaml
# ✅ 推荐: 使用多个标签组合,避免冲突
selector:
  matchLabels:
    app: nginx
    tier: frontend
    env: production
    version: v1.25
```


### 3.1.6 ReplicaSet 常用操作

```bash
# 查看ReplicaSet列表
kubectl get replicaset
kubectl get rs  # 简写

# 查看ReplicaSet详情
kubectl describe replicaset <rs-name>

# 查看ReplicaSet管理的Pod
kubectl get pods --selector=app=nginx

# 水平扩缩容
kubectl scale replicaset <rs-name> --replicas=5

# 删除ReplicaSet (级联删除Pod)
kubectl delete replicaset <rs-name>

# 删除ReplicaSet但保留Pod
kubectl delete replicaset <rs-name> --cascade=orphan

# 查看ReplicaSet的YAML定义
kubectl get replicaset <rs-name> -o yaml

# 查看ReplicaSet事件
kubectl get events --field-selector involvedObject.name=<rs-name>
```


---

**第3.1节完成 | 核心概念: ReplicaSet副本控制原理、标签选择器、自愈能力**

接下来将继续编写 3.2 Deployment 部分...

## 3.2 Deployment: 无状态应用部署管理

### 3.2.1 Deployment 基础概念

**定义**: Deployment为Pod和ReplicaSet提供声明式更新,是Kubernetes中最常用的工作负载控制器。

**Deployment vs ReplicaSet**:

```
Deployment的层次结构:
┌─────────────────────────────────────────────┐
│  Deployment: nginx-deploy                   │
│  管理ReplicaSet的生命周期                    │
│  - 滚动更新                                  │
│  - 版本回滚                                  │
│  - 暂停/恢复                                 │
└─────────────────────────────────────────────┘
                ↓ 创建并管理
┌─────────────────────────────────────────────┐
│  ReplicaSet: nginx-deploy-7d4b9c5f8d        │
│  管理Pod副本数量                             │
│  - 自动重启失败的Pod                         │
│  - 水平扩缩容                                │
└─────────────────────────────────────────────┘
                ↓ 创建并管理
┌─────────────────────────────────────────────┐
│  Pod: nginx-deploy-7d4b9c5f8d-abc123        │
│  Pod: nginx-deploy-7d4b9c5f8d-def456        │
│  Pod: nginx-deploy-7d4b9c5f8d-ghi789        │
└─────────────────────────────────────────────┘
```

**基础示例**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  namespace: default
  labels:
    app: nginx
spec:
  # 副本数量
  replicas: 3

  # 标签选择器
  selector:
    matchLabels:
      app: nginx

  # 更新策略 (关键!)
  strategy:
    type: RollingUpdate        # 滚动更新 (默认)
    rollingUpdate:
      maxUnavailable: 1        # 更新时最多1个Pod不可用
      maxSurge: 1              # 更新时最多超出1个Pod

  # 历史版本保留数量 (用于回滚)
  revisionHistoryLimit: 10

  # Pod模板
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.25-alpine
        ports:
        - containerPort: 80
          name: http
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        # 健康检查
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
```

**创建与验证**:

```bash
# 创建Deployment
kubectl apply -f nginx-deploy.yaml
# deployment.apps/nginx-deploy created

# 查看Deployment状态
kubectl get deployment nginx-deploy
# NAME            READY   UP-TO-DATE   AVAILABLE   AGE
# nginx-deploy    3/3     3            3           30s

# 查看Deployment创建的ReplicaSet
kubectl get replicaset -l app=nginx
# NAME                       DESIRED   CURRENT   READY   AGE
# nginx-deploy-7d4b9c5f8d    3         3         3       1m

# 查看Pod
kubectl get pods -l app=nginx
# NAME                            READY   STATUS    RESTARTS   AGE
# nginx-deploy-7d4b9c5f8d-abc123  1/1     Running   0          1m
# nginx-deploy-7d4b9c5f8d-def456  1/1     Running   0          1m
# nginx-deploy-7d4b9c5f8d-ghi789  1/1     Running   0          1m

# 查看Deployment详情
kubectl describe deployment nginx-deploy
# Name:                   nginx-deploy
# Namespace:              default
# Selector:               app=nginx
# Replicas:               3 desired | 3 updated | 3 total | 3 available
# StrategyType:           RollingUpdate
# MinReadySeconds:        0
# RollingUpdateStrategy:  1 max unavailable, 1 max surge
# Pod Template:
#   Labels:  app=nginx
#   Containers:
#    nginx:
#     Image:        nginx:1.25-alpine
```


### 3.2.2 滚动更新 (Rolling Update)

**滚动更新原理**:

```
滚动更新流程 (假设: replicas=4, maxUnavailable=1, maxSurge=1):
┌──────────────────────────────────────────────┐
│  阶段0: 初始状态 (旧版本v1)                   │
│  Pod-1(v1)  Pod-2(v1)  Pod-3(v1)  Pod-4(v1)  │
│  [Running]  [Running]  [Running]  [Running]  │
└──────────────────────────────────────────────┘
                ↓ 用户触发更新 (v1 → v2)
┌──────────────────────────────────────────────┐
│  阶段1: 创建1个新Pod (maxSurge=1)            │
│  Pod-1(v1)  Pod-2(v1)  Pod-3(v1)  Pod-4(v1)  │
│  [Running]  [Running]  [Running]  [Running]  │
│  Pod-5(v2)                                    │
│  [Creating]  ← 新版本Pod                     │
└──────────────────────────────────────────────┘
                ↓ Pod-5就绪
┌──────────────────────────────────────────────┐
│  阶段2: 删除1个旧Pod (maxUnavailable=1)      │
│  Pod-1(v1)  Pod-2(v1)  Pod-3(v1)  Pod-4(v1)  │
│  [Terminating] [Running] [Running] [Running] │
│  Pod-5(v2)                                    │
│  [Running]                                    │
└──────────────────────────────────────────────┘
                ↓ Pod-1删除完成
┌──────────────────────────────────────────────┐
│  阶段3: 创建第2个新Pod                        │
│  Pod-2(v1)  Pod-3(v1)  Pod-4(v1)  Pod-5(v2)  │
│  [Running]  [Running]  [Running]  [Running]  │
│  Pod-6(v2)                                    │
│  [Creating]                                   │
└──────────────────────────────────────────────┘
                ↓ Pod-6就绪
┌──────────────────────────────────────────────┐
│  阶段4: 删除第2个旧Pod                        │
│  Pod-2(v1)  Pod-3(v1)  Pod-4(v1)  Pod-5(v2)  │
│  [Terminating] [Running] [Running] [Running] │
│  Pod-6(v2)                                    │
│  [Running]                                    │
└──────────────────────────────────────────────┘
                ↓ 重复此过程...
┌──────────────────────────────────────────────┐
│  最终状态: 所有Pod都是新版本v2                │
│  Pod-5(v2)  Pod-6(v2)  Pod-7(v2)  Pod-8(v2)  │
│  [Running]  [Running]  [Running]  [Running]  │
└──────────────────────────────────────────────┘
```

**触发滚动更新的方式**:

**方式1: 修改镜像版本**

```bash
# 使用kubectl set image命令
kubectl set image deployment/nginx-deploy nginx=nginx:1.26-alpine
# deployment.apps/nginx-deploy image updated

# 查看滚动更新状态
kubectl rollout status deployment/nginx-deploy
# Waiting for deployment "nginx-deploy" rollout to finish: 1 out of 3 new replicas have been updated...
# Waiting for deployment "nginx-deploy" rollout to finish: 1 old replicas are pending termination...
# deployment "nginx-deploy" successfully rolled out

# 查看ReplicaSet (会看到新旧两个ReplicaSet)
kubectl get replicaset -l app=nginx
# NAME                       DESIRED   CURRENT   READY   AGE
# nginx-deploy-7d4b9c5f8d    0         0         0       10m  ← 旧版本 (v1.25)
# nginx-deploy-6c9f8b7d5e    3         3         3       2m   ← 新版本 (v1.26)
```

**方式2: 编辑YAML配置**

```bash
# 使用kubectl edit命令
kubectl edit deployment nginx-deploy
# 修改 spec.template.spec.containers[0].image: nginx:1.26-alpine
# 保存退出后自动触发更新

# 或者直接修改YAML文件并apply
kubectl apply -f nginx-deploy-v2.yaml
```

**方式3: 使用kubectl patch命令**

```bash
kubectl patch deployment nginx-deploy \
  -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","image":"nginx:1.26-alpine"}]}}}}'
```

**滚动更新参数详解**:

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    # maxUnavailable: 更新时最多有多少个Pod不可用
    # - 数字: 绝对值 (例: 1 表示最多1个Pod不可用)
    # - 百分比: 相对值 (例: 25% 表示最多25%的Pod不可用)
    # - 默认值: 25%
    maxUnavailable: 1

    # maxSurge: 更新时最多超出期望副本数多少个Pod
    # - 数字: 绝对值 (例: 1 表示最多超出1个Pod)
    # - 百分比: 相对值 (例: 25% 表示最多超出25%的Pod)
    # - 默认值: 25%
    maxSurge: 1
```

**不同场景的参数配置**:

```yaml
# 场景1: 快速更新 (允许更多Pod同时更新)
strategy:
  rollingUpdate:
    maxUnavailable: 50%     # 允许一半Pod不可用
    maxSurge: 50%           # 允许超出50%的Pod

# 场景2: 保守更新 (逐个Pod更新,确保服务可用)
strategy:
  rollingUpdate:
    maxUnavailable: 0       # 不允许任何Pod不可用
    maxSurge: 1             # 每次只多创建1个Pod

# 场景3: 平衡更新 (默认配置)
strategy:
  rollingUpdate:
    maxUnavailable: 25%
    maxSurge: 25%
```


### 3.2.3 Recreate 更新策略

**定义**: 先删除所有旧Pod,再创建新Pod (会导致服务短暂不可用)。

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deploy
spec:
  replicas: 3
  strategy:
    type: Recreate        # Recreate策略
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:v2
```

**Recreate更新流程**:

```
Recreate更新流程 (replicas=3):
┌──────────────────────────────────────────────┐
│  阶段0: 初始状态 (旧版本v1)                   │
│  Pod-1(v1)  Pod-2(v1)  Pod-3(v1)             │
│  [Running]  [Running]  [Running]             │
└──────────────────────────────────────────────┘
                ↓ 触发Recreate更新
┌──────────────────────────────────────────────┐
│  阶段1: 删除所有旧Pod                         │
│  Pod-1(v1)  Pod-2(v1)  Pod-3(v1)             │
│  [Terminating] [Terminating] [Terminating]   │
│  ⚠️ 服务完全不可用!                           │
└──────────────────────────────────────────────┘
                ↓ 所有旧Pod删除完成
┌──────────────────────────────────────────────┐
│  阶段2: 创建所有新Pod                         │
│  Pod-4(v2)  Pod-5(v2)  Pod-6(v2)             │
│  [Creating] [Creating] [Creating]            │
└──────────────────────────────────────────────┘
                ↓ 新Pod就绪
┌──────────────────────────────────────────────┐
│  最终状态: 所有Pod都是新版本v2                │
│  Pod-4(v2)  Pod-5(v2)  Pod-6(v2)             │
│  [Running]  [Running]  [Running]             │
└──────────────────────────────────────────────┘
```

**RollingUpdate vs Recreate对比**:

| 特性 | RollingUpdate | Recreate |
|------|--------------|----------|
| **服务可用性** | ✅ 更新期间服务持续可用 | ❌ 更新期间服务不可用 |
| **更新速度** | 慢 (逐步替换) | 快 (一次性替换) |
| **资源占用** | 高 (新旧版本同时存在) | 低 (仅新版本) |
| **适用场景** | 生产环境 (大多数情况) | 开发/测试环境、不兼容更新 |
| **版本共存** | 允许 (新旧版本短暂共存) | 不允许 (严格版本一致) |


### 3.2.4 版本管理与回滚

Deployment会保留历史ReplicaSet,用于版本回滚。

**查看历史版本**:

```bash
# 查看Deployment的版本历史
kubectl rollout history deployment/nginx-deploy
# REVISION  CHANGE-CAUSE
# 1         <none>
# 2         kubectl set image deployment/nginx-deploy nginx=nginx:1.26-alpine
# 3         kubectl set image deployment/nginx-deploy nginx=nginx:1.27-alpine

# 查看特定版本的详情
kubectl rollout history deployment/nginx-deploy --revision=2
# deployment.apps/nginx-deploy with revision #2
# Pod Template:
#   Labels:	app=nginx
#   Containers:
#    nginx:
#     Image:	nginx:1.26-alpine
```

**添加CHANGE-CAUSE注释**:

```bash
# 方式1: 使用--record参数 (已弃用,但仍可用)
kubectl set image deployment/nginx-deploy nginx=nginx:1.27-alpine --record
# 或者
kubectl apply -f nginx-deploy.yaml --record

# 方式2: 手动添加annotation (推荐)
kubectl annotate deployment/nginx-deploy \
  kubernetes.io/change-cause="Update to nginx 1.27"

# 查看历史 (会显示CHANGE-CAUSE)
kubectl rollout history deployment/nginx-deploy
# REVISION  CHANGE-CAUSE
# 1         <none>
# 2         Update to nginx 1.26
# 3         Update to nginx 1.27
```

**回滚到上一个版本**:

```bash
# 回滚到上一个版本
kubectl rollout undo deployment/nginx-deploy
# deployment.apps/nginx-deploy rolled back

# 查看回滚状态
kubectl rollout status deployment/nginx-deploy
# deployment "nginx-deploy" successfully rolled out

# 验证镜像版本已回滚
kubectl get deployment nginx-deploy -o jsonpath='{.spec.template.spec.containers[0].image}'
# nginx:1.26-alpine  ← 已回滚到revision 2
```

**回滚到指定版本**:

```bash
# 回滚到revision 1
kubectl rollout undo deployment/nginx-deploy --to-revision=1

# 验证
kubectl rollout history deployment/nginx-deploy
# REVISION  CHANGE-CAUSE
# 2         Update to nginx 1.26
# 3         Update to nginx 1.27
# 4         <none>  ← 回滚后revision 1变成了revision 4
```

**保留历史版本数量**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  # 保留历史ReplicaSet数量 (默认10个)
  revisionHistoryLimit: 10

  # 设置为0则不保留历史版本 (无法回滚!)
  # revisionHistoryLimit: 0
```


### 3.2.5 暂停与恢复更新

在大型更新时,可以暂停更新以验证部分Pod,然后恢复更新。

```bash
# 场景: 更新镜像版本,但想先验证少量Pod
kubectl set image deployment/nginx-deploy nginx=nginx:1.27-alpine

# 立即暂停更新
kubectl rollout pause deployment/nginx-deploy
# deployment.apps/nginx-deploy paused

# 此时只有部分Pod被更新,查看状态
kubectl get pods -l app=nginx
# NAME                            READY   STATUS    RESTARTS   AGE
# nginx-deploy-7d4b9c5f8d-abc123  1/1     Running   0          10m  ← 旧版本
# nginx-deploy-7d4b9c5f8d-def456  1/1     Running   0          10m  ← 旧版本
# nginx-deploy-8e9a7c6f9g-xyz789  1/1     Running   0          30s  ← 新版本

# 验证新版本Pod是否正常
kubectl logs nginx-deploy-8e9a7c6f9g-xyz789
kubectl exec nginx-deploy-8e9a7c6f9g-xyz789 -- nginx -v
# nginx version: nginx/1.27.0

# 如果新版本正常,恢复更新
kubectl rollout resume deployment/nginx-deploy
# deployment.apps/nginx-deploy resumed

# 如果新版本有问题,回滚
kubectl rollout undo deployment/nginx-deploy
```


### 3.2.6 金丝雀发布 (Canary Deployment)

**定义**: 先发布少量新版本Pod到生产环境,验证正常后再全量发布。

**实现方式1: 使用暂停/恢复**

```bash
# 步骤1: 更新镜像并立即暂停
kubectl set image deployment/nginx-deploy nginx=nginx:1.27-alpine && \
kubectl rollout pause deployment/nginx-deploy

# 步骤2: 等待1个新版本Pod就绪
kubectl get pods -l app=nginx --watch

# 步骤3: 验证新版本Pod (金丝雀Pod)
# - 查看日志
# - 检查监控指标
# - 执行烟雾测试

# 步骤4a: 如果验证通过,恢复更新
kubectl rollout resume deployment/nginx-deploy

# 步骤4b: 如果验证失败,回滚
kubectl rollout undo deployment/nginx-deploy
```

**实现方式2: 使用多个Deployment**

```yaml
---
# 稳定版本Deployment (90%流量)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-stable
spec:
  replicas: 9              # 90% 流量
  selector:
    matchLabels:
      app: nginx
      version: stable
  template:
    metadata:
      labels:
        app: nginx
        version: stable
    spec:
      containers:
      - name: nginx
        image: nginx:1.26-alpine

---
# 金丝雀版本Deployment (10%流量)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-canary
spec:
  replicas: 1              # 10% 流量
  selector:
    matchLabels:
      app: nginx
      version: canary
  template:
    metadata:
      labels:
        app: nginx
        version: canary
    spec:
      containers:
      - name: nginx
        image: nginx:1.27-alpine

---
# Service (同时路由到stable和canary)
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  selector:
    app: nginx           # 匹配stable和canary的共同标签
  ports:
  - port: 80
    targetPort: 80
```

**金丝雀发布流程**:

```bash
# 1. 创建稳定版本和金丝雀版本
kubectl apply -f nginx-stable.yaml
kubectl apply -f nginx-canary.yaml
kubectl apply -f nginx-svc.yaml

# 2. 验证流量分布 (90% stable, 10% canary)
# 发送100个请求,统计响应的Pod版本
for i in {1..100}; do
  curl -s http://nginx-svc | grep "nginx/" | cut -d'/' -f2
done | sort | uniq -c
#  90 1.26.0  ← stable版本
#  10 1.27.0  ← canary版本

# 3. 如果canary版本正常,逐步增加canary副本数
kubectl scale deployment nginx-canary --replicas=3  # 30%流量
# 继续观察监控指标...
kubectl scale deployment nginx-canary --replicas=5  # 50%流量

# 4. 最终全量切换到新版本
kubectl scale deployment nginx-stable --replicas=0
kubectl scale deployment nginx-canary --replicas=10

# 5. 删除旧版本Deployment
kubectl delete deployment nginx-stable
kubectl patch deployment nginx-canary -p '{"metadata":{"name":"nginx-stable"}}'
```


### 3.2.7 蓝绿部署 (Blue-Green Deployment)

**定义**: 维护两套完全相同的生产环境 (蓝环境和绿环境),通过切换Service流量实现零宕机部署。

```yaml
---
# 蓝环境 (当前生产环境)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
      version: blue
  template:
    metadata:
      labels:
        app: nginx
        version: blue
    spec:
      containers:
      - name: nginx
        image: nginx:1.26-alpine

---
# 绿环境 (新版本,待切换)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
      version: green
  template:
    metadata:
      labels:
        app: nginx
        version: green
    spec:
      containers:
      - name: nginx
        image: nginx:1.27-alpine

---
# Service (初始指向蓝环境)
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  selector:
    app: nginx
    version: blue      # 初始指向蓝环境
  ports:
  - port: 80
    targetPort: 80
```

**蓝绿部署流程**:

```bash
# 1. 部署蓝环境和绿环境
kubectl apply -f nginx-blue.yaml
kubectl apply -f nginx-green.yaml
kubectl apply -f nginx-svc.yaml

# 2. 验证Service当前指向蓝环境
kubectl get service nginx-svc -o jsonpath='{.spec.selector}'
# {"app":"nginx","version":"blue"}

# 3. 验证绿环境功能正常
kubectl port-forward deployment/nginx-green 8080:80 &
curl http://localhost:8080
# 或者创建临时Service测试绿环境

# 4. 切换流量到绿环境 (一键切换!)
kubectl patch service nginx-svc -p '{"spec":{"selector":{"version":"green"}}}'
# service/nginx-svc patched

# 5. 验证流量已切换
kubectl describe service nginx-svc | grep Endpoints
# Endpoints: 10.244.1.10:80,10.244.1.11:80,10.244.1.12:80  ← 绿环境的Pod IP

# 6. 如果绿环境有问题,立即切换回蓝环境 (秒级回滚!)
kubectl patch service nginx-svc -p '{"spec":{"selector":{"version":"blue"}}}'

# 7. 确认绿环境稳定后,删除蓝环境
kubectl delete deployment nginx-blue
```

**蓝绿部署优缺点**:

✅ **优点**:
- 零宕机部署 (瞬间切换流量)
- 快速回滚 (秒级切回蓝环境)
- 充分验证 (绿环境可完整测试)

❌ **缺点**:
- 资源消耗高 (需要2倍资源)
- 数据库迁移复杂 (蓝绿环境需要兼容同一数据库)


---

**第3.2节完成 | 核心概念: Deployment滚动更新、版本回滚、金丝雀发布、蓝绿部署**

---

## 3.3 StatefulSet: 有状态应用管理

### 3.3.1 为什么需要StatefulSet?

**Deployment的局限性**:

Deployment非常适合无状态应用,但对于 **有状态应用 (Stateful Application)** 存在以下问题:

```
Deployment管理的Pod特征:
┌────────────────────────────────────────┐
│  ❌ 随机Pod名称: nginx-deploy-5d4b8c9f   │
│     → 重启后名称变化 → 无法通过固定DNS访问│
├────────────────────────────────────────┤
│  ❌ 并行启动/终止                        │
│     → 数据库主从需要有序启动              │
├────────────────────────────────────────┤
│  ❌ 共享存储或无持久化                   │
│     → 每个Pod需要独立的持久化卷           │
├────────────────────────────────────────┤
│  ❌ 无稳定网络标识                       │
│     → 集群节点间无法通过固定地址通信       │
└────────────────────────────────────────┘

有状态应用的需求:
┌────────────────────────────────────────┐
│  ✅ 稳定的网络标识 (固定Hostname/DNS)    │
│     → mysql-0.mysql.default.svc.cluster.local │
├────────────────────────────────────────┤
│  ✅ 有序部署/扩缩容/更新                 │
│     → 先启动mysql-0 → 再启动mysql-1 → ...│
├────────────────────────────────────────┤
│  ✅ 每个Pod独立的持久化存储              │
│     → mysql-0绑定data-0卷 (重启后仍绑定) │
├────────────────────────────────────────┤
│  ✅ 有序优雅终止                         │
│     → 先终止mysql-2 → mysql-1 → mysql-0  │
└────────────────────────────────────────┘
```

**StatefulSet的核心特性**:

| 特性 | Deployment | StatefulSet |
|-----|-----------|------------|
| **Pod命名** | 随机哈希 (nginx-7d4b8c9f-xyz) | 有序索引 (nginx-0, nginx-1, nginx-2) |
| **启动顺序** | 并行 (所有Pod同时启动) | 有序 (等nginx-0 Ready后再启动nginx-1) |
| **终止顺序** | 并行 (所有Pod同时终止) | 有序 (先终止nginx-2,再nginx-1,最后nginx-0) |
| **网络标识** | 随机 (Pod IP变化) | 稳定 (通过Headless Service固定DNS) |
| **存储** | 共享PVC或emptyDir | 每个Pod绑定独立PVC (重启后仍绑定) |
| **更新策略** | RollingUpdate自动 | RollingUpdate有序 / OnDelete手动 |

**典型应用场景**:

```yaml
# 数据库集群 (MySQL主从、PostgreSQL、MongoDB副本集)
mysql-0 (Master) ← 固定Hostname,独立data-0卷
mysql-1 (Replica) ← 固定Hostname,独立data-1卷
mysql-2 (Replica) ← 固定Hostname,独立data-2卷

# 分布式协调服务 (Zookeeper、etcd)
zk-0 (Leader) ← 需要稳定的server.0标识
zk-1 (Follower)
zk-2 (Follower)

# 消息队列集群 (Kafka、RabbitMQ)
kafka-0 (broker.id=0) ← 需要固定的Broker ID
kafka-1 (broker.id=1)
kafka-2 (broker.id=2)
```

---

### 3.3.2 StatefulSet基础配置

**完整StatefulSet示例** (Nginx有状态Web服务):

```yaml
# nginx-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-headless       # Headless Service (无ClusterIP)
  namespace: default
spec:
  clusterIP: None            # ← 关键: None表示Headless Service
  selector:
    app: nginx
  ports:
  - port: 80
    name: web

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx
  namespace: default
spec:
  serviceName: "nginx-headless"   # ← 关键: 关联Headless Service
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.25-alpine
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html

  # ← 关键: PVC模板 (每个Pod自动创建独立的PVC)
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "standard"    # 使用集群默认StorageClass
      resources:
        requests:
          storage: 1Gi
```

**核心字段说明**:

```yaml
spec:
  serviceName: "nginx-headless"
  # ↑ 必须字段: 关联的Headless Service名称
  #   用于为每个Pod生成稳定的DNS记录

  replicas: 3
  # ↑ 副本数量: 创建3个Pod (nginx-0, nginx-1, nginx-2)

  podManagementPolicy: OrderedReady
  # ↑ Pod管理策略:
  #   - OrderedReady (默认): 有序部署/更新/删除
  #   - Parallel: 并行操作 (失去有序性保证)

  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
    # ↑ 更新策略:
    #   - RollingUpdate: 有序滚动更新 (从最大索引开始: 2→1→0)
    #   - OnDelete: 手动删除Pod才触发更新

  volumeClaimTemplates:
  # ↑ PVC模板: 为每个Pod自动创建独立的PersistentVolumeClaim
  #   - nginx-0 → www-nginx-0
  #   - nginx-1 → www-nginx-1
  #   - nginx-2 → www-nginx-2
  #   注意: 删除StatefulSet不会删除PVC (需手动清理)
```

---

### 3.3.3 稳定的网络标识

**Headless Service原理**:

普通Service会为Pod分配虚拟IP (ClusterIP),而 **Headless Service** (`clusterIP: None`) 不分配ClusterIP,直接返回Pod IP列表:

```bash
# 部署StatefulSet
kubectl apply -f nginx-statefulset.yaml

# 查看Service (无ClusterIP)
kubectl get service nginx-headless
# NAME              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
# nginx-headless    ClusterIP   None         <none>        80/TCP    10s

# 查看Pod (有序命名)
kubectl get pods -l app=nginx
# NAME      READY   STATUS    RESTARTS   AGE
# nginx-0   1/1     Running   0          30s  ← 第1个Pod (索引0)
# nginx-1   1/1     Running   0          25s  ← 第2个Pod (等nginx-0 Ready后启动)
# nginx-2   1/1     Running   0          20s  ← 第3个Pod (等nginx-1 Ready后启动)

# 查看Pod IP
kubectl get pods -l app=nginx -o wide
# NAME      IP           NODE
# nginx-0   10.244.1.5   worker-1
# nginx-1   10.244.2.8   worker-2
# nginx-2   10.244.1.6   worker-1
```

**DNS记录结构**:

```bash
# 进入临时Pod测试DNS解析
kubectl run -it --rm dns-test --image=busybox:1.36 --restart=Never -- sh

# 1. 解析Headless Service (返回所有Pod IP)
nslookup nginx-headless.default.svc.cluster.local
# Server:    10.96.0.10
# Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local
#
# Name:      nginx-headless.default.svc.cluster.local
# Address 1: 10.244.1.5 nginx-0.nginx-headless.default.svc.cluster.local
# Address 2: 10.244.2.8 nginx-1.nginx-headless.default.svc.cluster.local
# Address 3: 10.244.1.6 nginx-2.nginx-headless.default.svc.cluster.local

# 2. 解析单个Pod的固定DNS (稳定的网络标识!)
nslookup nginx-0.nginx-headless.default.svc.cluster.local
# Address 1: 10.244.1.5 nginx-0.nginx-headless.default.svc.cluster.local

nslookup nginx-1.nginx-headless.default.svc.cluster.local
# Address 1: 10.244.2.8 nginx-1.nginx-headless.default.svc.cluster.local

# 3. 即使Pod重启,Hostname和DNS记录保持不变
kubectl delete pod nginx-1
# 等待Pod重建...
kubectl get pod nginx-1 -o wide
# NAME      IP           NODE
# nginx-1   10.244.2.9   worker-2  ← IP可能变化
nslookup nginx-1.nginx-headless.default.svc.cluster.local
# Address 1: 10.244.2.9 nginx-1.nginx-headless.default.svc.cluster.local
# ↑ DNS记录自动更新到新IP,但Hostname不变!
```

**Pod DNS命名规范**:

```
<pod-name>.<service-name>.<namespace>.svc.<cluster-domain>
  nginx-0  . nginx-headless . default   .svc. cluster.local

完整DNS示例:
- nginx-0.nginx-headless.default.svc.cluster.local
- nginx-1.nginx-headless.default.svc.cluster.local
- nginx-2.nginx-headless.default.svc.cluster.local

短域名 (同Namespace内可省略后缀):
- nginx-0.nginx-headless
- nginx-1.nginx-headless
```

**在Pod内部访问其他Pod**:

```bash
# 进入nginx-0容器
kubectl exec -it nginx-0 -- sh

# 通过固定DNS访问nginx-1
wget -qO- http://nginx-1.nginx-headless

# 访问nginx-2
wget -qO- http://nginx-2.nginx-headless

# 遍历访问所有Pod
for i in 0 1 2; do
  echo "访问 nginx-$i:"
  wget -qO- http://nginx-$i.nginx-headless | head -n 1
done
```

---

### 3.3.4 有序部署与扩缩容

**有序启动流程**:

```bash
# 创建StatefulSet
kubectl apply -f nginx-statefulset.yaml

# 实时监控Pod创建过程
kubectl get pods -l app=nginx -w
# NAME      READY   STATUS              RESTARTS   AGE
# nginx-0   0/1     Pending             0          0s     ← 首先创建nginx-0
# nginx-0   0/1     ContainerCreating   0          1s
# nginx-0   1/1     Running             0          5s     ← nginx-0 Ready
# nginx-1   0/1     Pending             0          0s     ← nginx-0 Ready后创建nginx-1
# nginx-1   0/1     ContainerCreating   0          1s
# nginx-1   1/1     Running             0          4s     ← nginx-1 Ready
# nginx-2   0/1     Pending             0          0s     ← nginx-1 Ready后创建nginx-2
# nginx-2   0/1     ContainerCreating   0          1s
# nginx-2   1/1     Running             0          3s     ← nginx-2 Ready
```

**有序启动原理**:

```
OrderedReady策略 (默认):
┌─────────────────────────────────────┐
│  for i := 0; i < replicas; i++ {    │
│    创建 Pod-i                        │
│    等待 Pod-i Ready                  │
│  }                                   │
└─────────────────────────────────────┘

Ready条件:
1. Pod状态为 Running
2. Readiness Probe通过 (如果配置)
3. 所有容器都Running

如果Pod-0启动失败 → Pod-1不会创建
```

**扩容操作** (增加副本数):

```bash
# 扩容到5个副本
kubectl scale statefulset nginx --replicas=5

# 监控扩容过程 (有序创建新Pod)
kubectl get pods -l app=nginx -w
# nginx-0   1/1   Running   0   5m
# nginx-1   1/1   Running   0   5m
# nginx-2   1/1   Running   0   5m
# nginx-3   0/1   Pending   0   0s   ← 等nginx-2 Ready后创建
# nginx-3   1/1   Running   0   5s
# nginx-4   0/1   Pending   0   0s   ← 等nginx-3 Ready后创建
# nginx-4   1/1   Running   0   4s

# 查看自动创建的PVC
kubectl get pvc
# NAME          STATUS   VOLUME          CAPACITY   STORAGECLASS
# www-nginx-0   Bound    pvc-abc123      1Gi        standard
# www-nginx-1   Bound    pvc-def456      1Gi        standard
# www-nginx-2   Bound    pvc-ghi789      1Gi        standard
# www-nginx-3   Bound    pvc-jkl012      1Gi        standard  ← 新创建
# www-nginx-4   Bound    pvc-mno345      1Gi        standard  ← 新创建
```

**缩容操作** (减少副本数):

```bash
# 缩容到2个副本
kubectl scale statefulset nginx --replicas=2

# 监控缩容过程 (有序删除Pod: 从大到小)
kubectl get pods -l app=nginx -w
# nginx-0   1/1   Running       0   10m
# nginx-1   1/1   Running       0   10m
# nginx-2   1/1   Running       0   10m
# nginx-3   1/1   Running       0   5m
# nginx-4   1/1   Running       0   5m
# nginx-4   1/1   Terminating   0   5m   ← 先删除最大索引nginx-4
# nginx-4   0/1   Terminating   0   5m
# nginx-3   1/1   Terminating   0   5m   ← nginx-4删除后再删除nginx-3
# nginx-3   0/1   Terminating   0   5m

# ⚠️ 注意: PVC不会自动删除! (数据持久保留)
kubectl get pvc
# NAME          STATUS   VOLUME          CAPACITY
# www-nginx-0   Bound    pvc-abc123      1Gi
# www-nginx-1   Bound    pvc-def456      1Gi
# www-nginx-2   Bound    pvc-ghi789      1Gi
# www-nginx-3   Bound    pvc-jkl012      1Gi      ← 仍存在!
# www-nginx-4   Bound    pvc-mno345      1Gi      ← 仍存在!

# 再次扩容到5个副本 → 会重新绑定原有PVC (数据恢复!)
kubectl scale statefulset nginx --replicas=5
kubectl get pods -l app=nginx
# nginx-3和nginx-4会自动绑定www-nginx-3和www-nginx-4卷
```

**并行部署模式** (适用于不需要有序的场景):

```yaml
spec:
  podManagementPolicy: Parallel   # ← 改为并行模式
  # 所有Pod会同时创建/删除 (类似Deployment)
```

---

### 3.3.5 持久化存储绑定

**PVC自动创建机制**:

```yaml
volumeClaimTemplates:
- metadata:
    name: www            # PVC名称前缀
  spec:
    accessModes: ["ReadWriteOnce"]
    resources:
      requests:
        storage: 1Gi
```

**自动生成的PVC命名规则**:

```
<volumeClaimTemplate-name>-<statefulset-name>-<pod-index>
        www               -      nginx       -       0

示例:
- www-nginx-0  ← nginx-0的数据卷
- www-nginx-1  ← nginx-1的数据卷
- www-nginx-2  ← nginx-2的数据卷
```

**持久化数据验证**:

```bash
# 1. 在nginx-0中写入数据
kubectl exec nginx-0 -- sh -c 'echo "Data from nginx-0" > /usr/share/nginx/html/index.html'

# 2. 验证数据存在
kubectl exec nginx-0 -- cat /usr/share/nginx/html/index.html
# Data from nginx-0

# 3. 删除Pod (模拟故障)
kubectl delete pod nginx-0

# 4. 等待Pod自动重建
kubectl wait --for=condition=ready pod/nginx-0 --timeout=60s

# 5. 验证数据仍存在 (绑定相同的PVC)
kubectl exec nginx-0 -- cat /usr/share/nginx/html/index.html
# Data from nginx-0  ← 数据未丢失!

# 6. 查看PVC绑定关系
kubectl get pod nginx-0 -o jsonpath='{.spec.volumes[?(@.name=="www")].persistentVolumeClaim.claimName}'
# www-nginx-0
```

**手动清理PVC** (缩容后):

```bash
# 查看所有PVC
kubectl get pvc -l app=nginx

# 删除不再使用的PVC (释放存储空间)
kubectl delete pvc www-nginx-3 www-nginx-4

# 批量删除所有PVC
kubectl delete pvc -l app=nginx
```

---

### 3.3.6 更新策略

**RollingUpdate滚动更新** (默认):

```yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0   # 分区索引 (默认0,更新所有Pod)
```

**滚动更新流程**:

```bash
# 更新镜像版本
kubectl set image statefulset/nginx nginx=nginx:1.26-alpine

# 监控更新过程 (从大索引到小索引: 2→1→0)
kubectl rollout status statefulset/nginx
# waiting for statefulset rolling update to complete 0 pods updated...
# Waiting for 1 pods to be ready...
# Waiting for partition rollout to finish: 2 of 3 updated...
# Waiting for partition rollout to finish: 1 of 3 updated...
# statefulset rolling update complete 3 pods updated...

kubectl get pods -l app=nginx -w
# nginx-2   1/1   Running       0   10m
# nginx-2   1/1   Terminating   0   10m   ← 先更新最大索引nginx-2
# nginx-2   0/1   Pending       0   0s
# nginx-2   0/1   ContainerCreating   0   1s
# nginx-2   1/1   Running       0   5s    ← nginx-2更新完成
# nginx-1   1/1   Terminating   0   10m   ← 再更新nginx-1
# nginx-1   0/1   Pending       0   0s
# nginx-1   1/1   Running       0   5s
# nginx-0   1/1   Terminating   0   10m   ← 最后更新nginx-0
# nginx-0   1/1   Running       0   5s
```

**Partition分区更新** (金丝雀发布):

```yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 2   # ← 只更新索引 >= 2 的Pod
```

**分区更新示例**:

```bash
# 1. 设置partition=2 (只更新nginx-2)
kubectl patch statefulset nginx -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":2}}}}'

# 2. 更新镜像
kubectl set image statefulset/nginx nginx=nginx:1.26-alpine

# 3. 只有nginx-2会更新
kubectl get pods -l app=nginx -o custom-columns=NAME:.metadata.name,IMAGE:.spec.containers[0].image
# NAME      IMAGE
# nginx-0   nginx:1.25-alpine   ← 旧版本
# nginx-1   nginx:1.25-alpine   ← 旧版本
# nginx-2   nginx:1.26-alpine   ← 新版本 (金丝雀)

# 4. 验证nginx-2正常后,逐步降低partition
kubectl patch statefulset nginx -p '{"spec":{"updateStrategy":{"rollingUpdate":{"partition":1}}}}'
# 更新nginx-2和nginx-1

kubectl patch statefulset nginx -p '{"spec":{"updateStrategy":{"rollingUpdate":{"partition":0}}}}'
# 更新所有Pod
```

**OnDelete手动更新策略**:

```yaml
spec:
  updateStrategy:
    type: OnDelete   # ← 手动删除Pod才触发更新
```

```bash
# 更新镜像 (不会自动更新Pod)
kubectl set image statefulset/nginx nginx=nginx:1.26-alpine

# 手动删除Pod触发更新
kubectl delete pod nginx-2
# Pod重建后会使用新镜像 nginx:1.26-alpine

kubectl delete pod nginx-1
# nginx-1更新

kubectl delete pod nginx-0
# nginx-0更新
```

---

### 3.3.7 实战项目: MySQL主从集群

**架构设计**:

```
MySQL StatefulSet集群:
┌────────────────────────────────────────┐
│  mysql-0 (Master)                      │
│    ↓ 固定DNS: mysql-0.mysql           │
│    ↓ 数据卷: data-mysql-0 (10Gi)       │
│    ↓ 端口: 3306                        │
├────────────────────────────────────────┤
│  mysql-1 (Replica)                     │
│    ← 复制自mysql-0                     │
│    ↓ 固定DNS: mysql-1.mysql           │
│    ↓ 数据卷: data-mysql-1 (10Gi)       │
├────────────────────────────────────────┤
│  mysql-2 (Replica)                     │
│    ← 复制自mysql-0                     │
│    ↓ 固定DNS: mysql-2.mysql           │
│    ↓ 数据卷: data-mysql-2 (10Gi)       │
└────────────────────────────────────────┘
       ↑
   应用访问:
   - 写入: mysql-0.mysql (Master)
   - 读取: mysql-read Service (负载均衡到所有副本)
```

**完整配置清单** (`mysql-statefulset.yaml`):

```yaml
# 1. ConfigMap: MySQL配置文件
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-config
  namespace: default
data:
  master.cnf: |
    # MySQL Master配置
    [mysqld]
    log-bin=mysql-bin      # 启用二进制日志
    server-id=1            # Master的server-id
    binlog-format=ROW      # 行级别复制
    max_connections=500
    character-set-server=utf8mb4
    collation-server=utf8mb4_unicode_ci

  replica.cnf: |
    # MySQL Replica配置
    [mysqld]
    server-id=100          # Replica的server-id (会被Init容器动态修改)
    log-bin=mysql-bin
    relay-log=mysql-relay
    read_only=1            # 只读模式
    super_read_only=1
    max_connections=500
    character-set-server=utf8mb4
    collation-server=utf8mb4_unicode_ci

---
# 2. Headless Service: 用于StatefulSet DNS
apiVersion: v1
kind: Service
metadata:
  name: mysql
  namespace: default
spec:
  clusterIP: None         # Headless Service
  selector:
    app: mysql
  ports:
  - port: 3306
    name: mysql

---
# 3. Service: 读请求负载均衡 (所有Pod)
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  namespace: default
spec:
  selector:
    app: mysql
  ports:
  - port: 3306
    name: mysql
  type: ClusterIP

---
# 4. StatefulSet: MySQL集群
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: default
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      # Init容器: 根据Pod序号选择配置文件
      initContainers:
      - name: init-mysql
        image: mysql:8.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # 获取Pod序号 (从hostname中提取: mysql-0 → 0)
          [[ $(hostname) =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}

          # Pod-0使用master.cnf, 其他使用replica.cnf
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/replica.cnf /mnt/conf.d/
            # 动态设置server-id (避免冲突)
            sed -i "s/server-id=100/server-id=$((100 + $ordinal))/" /mnt/conf.d/replica.cnf
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map

      # Init容器: 从Master克隆数据 (仅Replica需要)
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          [[ $(hostname) =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}

          # Pod-0 (Master) 跳过克隆
          [[ $ordinal -eq 0 ]] && exit 0

          # 从Master克隆数据
          ncat --recv-only mysql-0.mysql 3307 | xbstream -x -C /var/lib/mysql
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d

      containers:
      # 主容器: MySQL
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "rootpassword"     # 生产环境应使用Secret
        - name: MYSQL_DATABASE
          value: "myapp"
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping", "-uroot", "-prootpassword"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command: ["mysql", "-uroot", "-prootpassword", "-e", "SELECT 1"]
          initialDelaySeconds: 10
          periodSeconds: 5

      # Sidecar容器: 提供备份接口 (用于新Replica克隆数据)
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - containerPort: 3307
          name: xtrabackup
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql
          # 等待MySQL启动
          until mysql -uroot -prootpassword -e "SELECT 1"; do
            sleep 1
          done
          # 提供备份服务
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xtar --host=127.0.0.1 --user=root --password=rootpassword"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
          limits:
            cpu: 200m
            memory: 200Mi

      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql-config

  # PVC模板: 每个Pod独立的10Gi数据卷
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "standard"
      resources:
        requests:
          storage: 10Gi
```

**部署MySQL集群**:

```bash
# 1. 部署集群
kubectl apply -f mysql-statefulset.yaml

# 2. 监控部署过程
kubectl get pods -l app=mysql -w
# NAME      READY   STATUS     AGE
# mysql-0   0/2     Init:0/2   5s    ← 初始化Master
# mysql-0   0/2     Init:1/2   15s   ← 克隆数据(跳过)
# mysql-0   0/2     PodInitializing   20s
# mysql-0   2/2     Running    25s   ← Master Ready
# mysql-1   0/2     Pending    0s    ← 开始初始化Replica-1
# mysql-1   0/2     Init:0/2   5s
# mysql-1   0/2     Init:1/2   15s   ← 从mysql-0克隆数据
# mysql-1   2/2     Running    45s
# mysql-2   0/2     Pending    0s    ← 开始初始化Replica-2
# mysql-2   2/2     Running    50s

# 3. 查看服务
kubectl get service
# NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)
# mysql        ClusterIP   None            <none>        3306/TCP   ← Headless
# mysql-read   ClusterIP   10.96.123.45    <none>        3306/TCP   ← 读服务

# 4. 查看PVC (每个Pod独立的10Gi卷)
kubectl get pvc
# NAME           STATUS   VOLUME          CAPACITY   STORAGECLASS
# data-mysql-0   Bound    pvc-abc123      10Gi       standard
# data-mysql-1   Bound    pvc-def456      10Gi       standard
# data-mysql-2   Bound    pvc-ghi789      10Gi       standard
```

**配置主从复制** (首次部署后需手动设置):

```bash
# 1. 在Master (mysql-0) 创建复制用户
kubectl exec mysql-0 -c mysql -- mysql -uroot -prootpassword -e "
  CREATE USER 'replicator'@'%' IDENTIFIED BY 'replicapass';
  GRANT REPLICATION SLAVE ON *.* TO 'replicator'@'%';
  FLUSH PRIVILEGES;
"

# 2. 获取Master的binlog位置
kubectl exec mysql-0 -c mysql -- mysql -uroot -prootpassword -e "SHOW MASTER STATUS\G"
# *************************** 1. row ***************************
#              File: mysql-bin.000003
#          Position: 157

# 3. 在Replica (mysql-1和mysql-2) 配置复制
for i in 1 2; do
  kubectl exec mysql-$i -c mysql -- mysql -uroot -prootpassword -e "
    CHANGE MASTER TO
      MASTER_HOST='mysql-0.mysql',
      MASTER_USER='replicator',
      MASTER_PASSWORD='replicapass',
      MASTER_LOG_FILE='mysql-bin.000003',
      MASTER_LOG_POS=157;
    START SLAVE;
  "
done

# 4. 验证复制状态
kubectl exec mysql-1 -c mysql -- mysql -uroot -prootpassword -e "SHOW SLAVE STATUS\G" | grep -E "Slave_IO_Running|Slave_SQL_Running"
# Slave_IO_Running: Yes
# Slave_SQL_Running: Yes
```

**测试主从复制**:

```bash
# 1. 在Master写入数据
kubectl exec mysql-0 -c mysql -- mysql -uroot -prootpassword myapp -e "
  CREATE TABLE users (id INT PRIMARY KEY, name VARCHAR(50));
  INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob');
"

# 2. 在Replica-1读取数据
kubectl exec mysql-1 -c mysql -- mysql -uroot -prootpassword myapp -e "SELECT * FROM users;"
# +----+-------+
# | id | name  |
# +----+-------+
# |  1 | Alice |
# |  2 | Bob   |
# +----+-------+

# 3. 在Replica-2读取数据
kubectl exec mysql-2 -c mysql -- mysql -uroot -prootpassword myapp -e "SELECT * FROM users;"
# (相同结果)
```

**应用连接配置**:

```yaml
# 应用Pod配置
spec:
  containers:
  - name: app
    env:
    # 写操作: 连接Master
    - name: MYSQL_WRITE_HOST
      value: "mysql-0.mysql.default.svc.cluster.local"

    # 读操作: 连接读服务 (负载均衡到所有Pod)
    - name: MYSQL_READ_HOST
      value: "mysql-read.default.svc.cluster.local"

    - name: MYSQL_PORT
      value: "3306"
    - name: MYSQL_USER
      value: "root"
    - name: MYSQL_PASSWORD
      valueFrom:
        secretKeyRef:
          name: mysql-secret
          key: password
```

**故障恢复测试**:

```bash
# 1. 删除Replica Pod (模拟故障)
kubectl delete pod mysql-1

# 2. Pod自动重建并绑定原PVC
kubectl wait --for=condition=ready pod/mysql-1 --timeout=120s

# 3. 验证数据未丢失
kubectl exec mysql-1 -c mysql -- mysql -uroot -prootpassword myapp -e "SELECT * FROM users;"
# (数据完整)

# 4. 验证复制状态自动恢复
kubectl exec mysql-1 -c mysql -- mysql -uroot -prootpassword -e "SHOW SLAVE STATUS\G" | grep Running
# Slave_IO_Running: Yes
# Slave_SQL_Running: Yes
```

---

### 3.3.8 StatefulSet生产最佳实践

**1. 资源配置建议**:

```yaml
resources:
  requests:
    cpu: 1000m          # 保证1核CPU
    memory: 2Gi         # 保证2GB内存
  limits:
    cpu: 2000m          # 最大2核CPU
    memory: 4Gi         # 最大4GB内存

# 生产环境建议配置Guaranteed QoS (requests = limits)
```

**2. 健康检查配置**:

```yaml
livenessProbe:
  exec:
    command: ["mysqladmin", "ping"]
  initialDelaySeconds: 30    # 等待数据库初始化
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  exec:
    command: ["mysql", "-e", "SELECT 1"]
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3
```

**3. PVC扩容**:

```bash
# 1. 检查StorageClass是否支持扩容
kubectl get storageclass standard -o jsonpath='{.allowVolumeExpansion}'
# true

# 2. 扩容PVC (需要Pod重启生效)
kubectl patch pvc data-mysql-0 -p '{"spec":{"resources":{"requests":{"storage":"20Gi"}}}}'

# 3. 重启Pod使扩容生效
kubectl delete pod mysql-0
kubectl wait --for=condition=ready pod/mysql-0 --timeout=120s

# 4. 验证扩容结果
kubectl exec mysql-0 -c mysql -- df -h /var/lib/mysql
# Filesystem      Size  Used Avail Use% Mounted on
# /dev/vda1        20G  5.0G   15G  25% /var/lib/mysql
```

**4. 备份策略**:

```bash
# 使用CronJob定期备份Master数据
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mysql-backup
spec:
  schedule: "0 2 * * *"   # 每天凌晨2点
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: mysql:8.0
            command:
            - /bin/bash
            - -c
            - |
              mysqldump -h mysql-0.mysql -uroot -p$MYSQL_ROOT_PASSWORD --all-databases \
                > /backup/mysql-$(date +%Y%m%d-%H%M%S).sql
            env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-secret
                  key: root-password
            volumeMounts:
            - name: backup
              mountPath: /backup
          volumes:
          - name: backup
            persistentVolumeClaim:
              claimName: mysql-backup-pvc
          restartPolicy: OnFailure
```

**5. 监控指标**:

```bash
# 关键监控指标:
- Pod Ready状态
- 主从复制延迟: SHOW SLAVE STATUS → Seconds_Behind_Master
- 慢查询数量: SHOW GLOBAL STATUS LIKE 'Slow_queries'
- 连接数: SHOW GLOBAL STATUS LIKE 'Threads_connected'
- PVC使用率: df -h /var/lib/mysql
```

**6. 安全加固**:

```yaml
# 1. 使用Secret存储密码
apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
type: Opaque
data:
  root-password: cm9vdHBhc3N3b3Jk  # base64: rootpassword

# 2. 限制root只能本地登录
CREATE USER 'admin'@'%' IDENTIFIED BY 'strongpass';
GRANT ALL PRIVILEGES ON myapp.* TO 'admin'@'%';

# 3. 启用审计日志
[mysqld]
plugin-load-add=audit_log.so
audit_log_file=/var/lib/mysql/audit.log
```

---

**第3.3节完成 | 核心概念: StatefulSet稳定网络标识、有序部署、持久化存储、MySQL集群实战**

---

## 3.4 DaemonSet: 守护进程管理

### 3.4.1 什么是DaemonSet?

**DaemonSet定义**:

DaemonSet确保 **每个符合条件的Node节点上都运行一个Pod副本**。当新节点加入集群时,DaemonSet自动为其调度Pod;当节点从集群移除时,Pod也会被自动清理。

**与Deployment的核心区别**:

```
Deployment:
┌──────────────────────────────────┐
│  replicas: 3                     │
│  → 总共运行3个Pod                 │
│  → Scheduler选择3个最佳节点       │
│  → 可能多个Pod在同一节点          │
└──────────────────────────────────┘

DaemonSet:
┌──────────────────────────────────┐
│  每个Node节点运行1个Pod           │
│  → 10个节点 = 10个Pod             │
│  → 100个节点 = 100个Pod           │
│  → 绕过Scheduler (直接指定节点)    │
└──────────────────────────────────┘

集群拓扑示例:
Node-1: [DaemonSet Pod]  [App Pod]  [App Pod]
Node-2: [DaemonSet Pod]  [App Pod]
Node-3: [DaemonSet Pod]  [App Pod]  [App Pod]  [App Pod]
        ↑ 每个节点固定1个DaemonSet Pod
```

**典型应用场景**:

| 场景 | 说明 | 示例 |
|-----|------|------|
| **日志收集** | 收集节点上所有容器的日志 | Fluentd, Filebeat, Logstash |
| **监控Agent** | 采集节点和Pod的指标数据 | Prometheus Node Exporter, Datadog Agent |
| **网络插件** | 为每个节点提供网络功能 | Calico, Flannel, Weave |
| **存储插件** | 为节点提供存储驱动 | Ceph, GlusterFS |
| **安全工具** | 入侵检测、漏洞扫描 | Falco, Cilium |
| **性能分析** | 系统性能监控 | eBPF-based工具 |

---

### 3.4.2 DaemonSet基础配置

**完整DaemonSet示例** (Fluentd日志采集):

```yaml
# fluentd-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
  labels:
    app: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      # 容忍Master节点的污点 (默认DaemonSet不调度到Master)
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        effect: NoSchedule

      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
          limits:
            cpu: 500m
            memory: 500Mi
        volumeMounts:
        # 挂载节点日志目录 (关键!)
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true

      # 挂载节点路径 (使用hostPath卷)
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
          type: Directory
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
          type: DirectoryOrCreate

      # 高优先级 (确保尽早调度)
      priorityClassName: system-node-critical

      # 使用主机网络 (可选,某些监控工具需要)
      # hostNetwork: true
      # hostPID: true
```

**核心字段说明**:

```yaml
spec:
  # ❌ 没有replicas字段! (副本数由节点数量决定)

  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1   # 每次最多1个节点的Pod不可用
    # 或使用 OnDelete策略 (手动删除Pod才更新)

  revisionHistoryLimit: 10
  # 保留的历史版本数量 (用于回滚)

  template:
    spec:
      tolerations:
      # 容忍污点,允许在特殊节点上运行
      # (例如Master节点默认有NoSchedule污点)

      volumes:
      - name: varlog
        hostPath:
          path: /var/log
          type: Directory
      # hostPath卷: 访问节点本地文件系统
      # 类型:
      #   - Directory: 必须存在的目录
      #   - DirectoryOrCreate: 不存在则创建
      #   - File: 必须存在的文件
      #   - Socket: Unix Socket文件

      priorityClassName: system-node-critical
      # 高优先级: system-node-critical (节点级) 或 system-cluster-critical (集群级)
```

---

### 3.4.3 节点选择与污点容忍

**1. 节点选择器 (nodeSelector)**:

只在特定标签的节点上运行DaemonSet Pod:

```yaml
spec:
  template:
    spec:
      nodeSelector:
        disk: ssd          # 只在有 disk=ssd 标签的节点运行
        # 或选择特定区域
        # topology.kubernetes.io/zone: us-east-1a
```

**节点标签管理**:

```bash
# 为节点添加标签
kubectl label nodes worker-1 disk=ssd
kubectl label nodes worker-2 disk=ssd

# 查看节点标签
kubectl get nodes --show-labels

# 删除标签
kubectl label nodes worker-1 disk-
```

**2. 节点亲和性 (nodeAffinity)**:

更灵活的节点选择:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          # 硬性要求: 必须满足 (requiredDuringSchedulingIgnoredDuringExecution)
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - worker-1
                - worker-2
                - worker-3

          # 软性要求: 优先满足 (preferredDuringSchedulingIgnoredDuringExecution)
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: disk
                operator: In
                values:
                - ssd
```

**3. 污点容忍 (Tolerations)**:

允许DaemonSet在有污点的节点上运行:

```yaml
spec:
  template:
    spec:
      tolerations:
      # 容忍Master节点污点 (Kubernetes 1.24+)
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule

      # 容忍Master节点污点 (Kubernetes < 1.24)
      - key: node-role.kubernetes.io/master
        effect: NoSchedule

      # 容忍节点NotReady状态 (节点故障时继续运行)
      - key: node.kubernetes.io/not-ready
        effect: NoExecute
        tolerationSeconds: 300   # 容忍5分钟

      # 容忍节点Unreachable状态
      - key: node.kubernetes.io/unreachable
        effect: NoExecute
        tolerationSeconds: 300

      # 容忍所有污点 (不推荐生产环境使用)
      - operator: Exists
```

**污点与容忍原理**:

```bash
# 为节点添加污点
kubectl taint nodes worker-1 dedicated=gpu:NoSchedule
# 效果:
#   - NoSchedule: 不允许新Pod调度 (已有Pod不受影响)
#   - PreferNoSchedule: 尽量不调度 (资源不足时可能调度)
#   - NoExecute: 不允许新Pod调度 + 驱逐已有Pod

# 查看节点污点
kubectl describe node worker-1 | grep Taints
# Taints: dedicated=gpu:NoSchedule

# 删除污点
kubectl taint nodes worker-1 dedicated:NoSchedule-
```

---

### 3.4.4 更新策略

**1. RollingUpdate滚动更新** (默认):

```yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1     # 每次最多1个节点的Pod不可用
      # maxUnavailable可以设置为:
      #   - 数字: 1, 2, 3 (绝对值)
      #   - 百分比: "30%" (相对于总节点数)
```

**滚动更新流程**:

```bash
# 更新DaemonSet镜像
kubectl set image daemonset/fluentd fluentd=fluent/fluentd:v2 -n kube-system

# 监控更新进度
kubectl rollout status daemonset/fluentd -n kube-system
# Waiting for daemon set "fluentd" rollout to finish: 1 out of 3 new pods updated...
# Waiting for daemon set "fluentd" rollout to finish: 2 out of 3 new pods updated...
# daemon set "fluentd" successfully rolled out

# 实时监控Pod更新
kubectl get pods -n kube-system -l app=fluentd -w
# NAME            READY   STATUS              AGE
# fluentd-abc12   1/1     Running             10m   ← 旧Pod (worker-1)
# fluentd-def34   1/1     Running             10m   ← 旧Pod (worker-2)
# fluentd-ghi56   1/1     Running             10m   ← 旧Pod (worker-3)
# fluentd-def34   1/1     Terminating         10m   ← 终止第1个旧Pod
# fluentd-xyz99   0/1     ContainerCreating   0s    ← 创建第1个新Pod
# fluentd-xyz99   1/1     Running             5s    ← 新Pod Ready
# fluentd-ghi56   1/1     Terminating         10m   ← 终止第2个旧Pod
# fluentd-uvw88   0/1     ContainerCreating   0s
# ...
```

**maxUnavailable参数影响**:

```
maxUnavailable: 1 (保守更新)
┌──────────────────────────────────┐
│  每次只更新1个节点               │
│  → 更新速度慢,影响最小            │
│  → 适用于关键服务 (日志/监控)     │
└──────────────────────────────────┘

maxUnavailable: 3 (激进更新)
┌──────────────────────────────────┐
│  并行更新3个节点                 │
│  → 更新速度快,但风险高            │
│  → 适用于测试环境或非关键服务     │
└──────────────────────────────────┘
```

**2. OnDelete手动更新策略**:

```yaml
spec:
  updateStrategy:
    type: OnDelete   # 手动删除Pod才触发更新
```

```bash
# 更新DaemonSet配置 (不会自动更新Pod)
kubectl apply -f fluentd-daemonset.yaml

# 手动删除Pod触发更新
kubectl delete pod fluentd-abc12 -n kube-system
# Pod重建后会使用新配置

# 分批删除实现灰度更新
kubectl delete pod fluentd-def34 -n kube-system
# 验证无问题后继续删除其他Pod
```

**3. 回滚操作**:

```bash
# 查看历史版本
kubectl rollout history daemonset/fluentd -n kube-system
# REVISION  CHANGE-CAUSE
# 1         Initial deployment
# 2         Update to v1.15
# 3         Update to v1.16

# 回滚到上一版本
kubectl rollout undo daemonset/fluentd -n kube-system

# 回滚到指定版本
kubectl rollout undo daemonset/fluentd --to-revision=2 -n kube-system

# 验证回滚结果
kubectl rollout status daemonset/fluentd -n kube-system
```

---

### 3.4.5 实战项目: Prometheus Node Exporter部署

**项目目标**: 在所有节点上部署Prometheus Node Exporter,采集节点级别的系统指标 (CPU/内存/磁盘/网络)。

**完整配置** (`node-exporter-daemonset.yaml`):

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
  labels:
    app: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter

  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1

  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      # 容忍Master节点污点 (在Master上也部署)
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        effect: NoSchedule

      # 使用主机网络 (直接监听节点9100端口)
      hostNetwork: true
      hostPID: true

      containers:
      - name: node-exporter
        image: prom/node-exporter:v1.7.0
        args:
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        - --path.rootfs=/host/root
        - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
        ports:
        - containerPort: 9100
          name: metrics
          protocol: TCP
        resources:
          requests:
            cpu: 50m
            memory: 50Mi
          limits:
            cpu: 200m
            memory: 100Mi
        volumeMounts:
        # 挂载节点文件系统 (只读)
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: root
          mountPath: /host/root
          mountPropagation: HostToContainer
          readOnly: true

      volumes:
      - name: proc
        hostPath:
          path: /proc
          type: Directory
      - name: sys
        hostPath:
          path: /sys
          type: Directory
      - name: root
        hostPath:
          path: /
          type: Directory

      # 高优先级 (确保监控Agent优先调度)
      priorityClassName: system-node-critical

---
# Service: 暴露所有Node Exporter (Headless Service)
apiVersion: v1
kind: Service
metadata:
  name: node-exporter
  namespace: monitoring
  labels:
    app: node-exporter
spec:
  clusterIP: None    # Headless Service (Prometheus通过DNS发现所有Pod)
  selector:
    app: node-exporter
  ports:
  - port: 9100
    name: metrics
```

**部署与验证**:

```bash
# 1. 部署Node Exporter DaemonSet
kubectl apply -f node-exporter-daemonset.yaml

# 2. 查看所有节点的Pod
kubectl get pods -n monitoring -o wide
# NAME                  READY   STATUS    NODE
# node-exporter-abc12   1/1     Running   master-1
# node-exporter-def34   1/1     Running   worker-1
# node-exporter-ghi56   1/1     Running   worker-2
# node-exporter-jkl78   1/1     Running   worker-3

# 3. 验证每个节点都有且仅有1个Pod
kubectl get daemonset -n monitoring
# NAME            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
# node-exporter   4         4         4       4            4           <none>          1m

# 4. 测试指标采集
kubectl port-forward -n monitoring daemonset/node-exporter 9100:9100 &
curl http://localhost:9100/metrics | head -n 20
# # HELP node_cpu_seconds_total Seconds the CPUs spent in each mode.
# # TYPE node_cpu_seconds_total counter
# node_cpu_seconds_total{cpu="0",mode="idle"} 1234567.89
# node_cpu_seconds_total{cpu="0",mode="system"} 12345.67
# node_memory_MemTotal_bytes 16777216000
# ...

# 5. 添加新节点验证自动部署
# (新节点加入后会自动调度Node Exporter Pod)
kubectl get nodes
# NAME       STATUS   ROLES           AGE
# master-1   Ready    control-plane   10d
# worker-1   Ready    <none>          10d
# worker-2   Ready    <none>          10d
# worker-3   Ready    <none>          10d
# worker-4   Ready    <none>          1m   ← 新节点

kubectl get pods -n monitoring -l app=node-exporter
# node-exporter-xyz99   1/1     Running   worker-4  ← 自动创建!
```

**Prometheus配置 (服务发现)**:

```yaml
# prometheus-config.yaml
scrape_configs:
- job_name: 'kubernetes-nodes'
  kubernetes_sd_configs:
  - role: endpoints
    namespaces:
      names:
      - monitoring
  relabel_configs:
  - source_labels: [__meta_kubernetes_service_name]
    action: keep
    regex: node-exporter
  - source_labels: [__meta_kubernetes_endpoint_node_name]
    target_label: node
```

---

### 3.4.6 常见问题与调试

**问题1: DaemonSet Pod未调度到所有节点**

```bash
# 检查未调度原因
kubectl get pods -n kube-system -l app=fluentd -o wide
# 发现某些节点缺少Pod

# 检查节点污点
kubectl describe node worker-1 | grep Taints
# Taints: disk=full:NoSchedule

# 解决方案1: 添加容忍到DaemonSet
kubectl edit daemonset fluentd -n kube-system
# 添加:
# tolerations:
# - key: disk
#   operator: Exists

# 解决方案2: 移除节点污点
kubectl taint nodes worker-1 disk:NoSchedule-
```

**问题2: 节点资源不足导致Pod Pending**

```bash
# 检查Pod状态
kubectl describe pod fluentd-abc12 -n kube-system
# Events:
#   Warning  FailedScheduling  Insufficient cpu

# 解决方案: 降低资源请求
kubectl edit daemonset fluentd -n kube-system
# resources:
#   requests:
#     cpu: 50m      # 降低CPU请求
#     memory: 100Mi # 降低内存请求
```

**问题3: hostPath权限问题**

```bash
# Pod日志显示权限错误
kubectl logs fluentd-abc12 -n kube-system
# Permission denied: /var/log/containers

# 解决方案: 添加SecurityContext
kubectl edit daemonset fluentd -n kube-system
# securityContext:
#   privileged: true   # 授予特权模式
#   # 或使用更精细的权限控制:
#   runAsUser: 0       # 以root用户运行
```

**问题4: 更新卡住**

```bash
# 检查更新状态
kubectl rollout status daemonset/fluentd -n kube-system
# Waiting for daemon set "fluentd" rollout to finish: 2 out of 3 new pods updated...

# 检查Pod事件
kubectl describe pod fluentd-xyz99 -n kube-system
# Events:
#   Warning  BackOff  Back-off restarting failed container

# 解决方案1: 回滚到上一版本
kubectl rollout undo daemonset/fluentd -n kube-system

# 解决方案2: 强制删除卡住的Pod
kubectl delete pod fluentd-xyz99 -n kube-system --force --grace-period=0
```

---

### 3.4.7 DaemonSet生产最佳实践

**1. 资源限制**:

```yaml
resources:
  requests:
    cpu: 100m       # 保证最小资源
    memory: 128Mi
  limits:
    cpu: 500m       # 防止资源耗尽
    memory: 512Mi

# ⚠️ 注意: DaemonSet Pod不应设置过高的requests,否则节点资源不足时无法调度
```

**2. 健康检查**:

```yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
```

**3. 日志收集**:

```bash
# 将DaemonSet日志输出到stdout (便于集中收集)
kubectl logs -n kube-system daemonset/fluentd --tail=50 -f

# 或使用kubectl-stern插件 (同时查看所有Pod日志)
stern -n kube-system fluentd
```

**4. 监控告警**:

```yaml
# Prometheus告警规则
groups:
- name: daemonset
  rules:
  - alert: DaemonSetNotScheduled
    expr: |
      kube_daemonset_status_number_available{daemonset="fluentd"}
      <
      kube_daemonset_status_desired_number_scheduled{daemonset="fluentd"}
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "DaemonSet {{ $labels.daemonset }} 未完全调度"
      description: "期望 {{ $value }} 个Pod,实际运行数量不足"
```

**5. 节点维护**:

```bash
# 方案1: 驱逐节点上的所有Pod (DaemonSet Pod除外)
kubectl drain worker-1 --ignore-daemonsets
# ↑ --ignore-daemonsets: 保留DaemonSet Pod

# 方案2: 临时禁用DaemonSet在特定节点
kubectl patch daemonset fluentd -n kube-system -p '{"spec":{"template":{"spec":{"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"kubernetes.io/hostname","operator":"NotIn","values":["worker-1"]}]}]}}}}}}}'

# 恢复节点
kubectl uncordon worker-1
```

**6. 安全加固**:

```yaml
# 最小权限原则
securityContext:
  runAsNonRoot: true      # 非root用户运行
  runAsUser: 1000
  fsGroup: 1000
  capabilities:
    drop:
    - ALL
    add:
    - NET_BIND_SERVICE   # 仅添加必要的能力

# 只读根文件系统
volumeMounts:
- name: varlog
  mountPath: /var/log
  readOnly: true         # 只读挂载
```

**7. 高可用配置**:

```yaml
# 优先级类 (确保关键DaemonSet不被驱逐)
priorityClassName: system-node-critical

# 资源预留 (确保节点始终有资源运行DaemonSet)
# 在kubelet配置中预留资源:
# --kube-reserved=cpu=100m,memory=200Mi
# --system-reserved=cpu=100m,memory=200Mi
```

---

**第3.4节完成 | 核心概念: DaemonSet每节点一个Pod、节点选择、污点容忍、Node Exporter实战**

---

## 3.5 Job与CronJob: 批处理任务管理

### 3.5.1 为什么需要Job?

**Deployment/StatefulSet的局限性**:

前面学习的控制器都是 **长期运行 (Long-Running)** 的应用,但在生产环境中,我们还需要运行 **一次性任务 (One-Time Tasks)** 和 **定时任务 (Scheduled Tasks)**:

```
长期运行的应用 (Deployment/StatefulSet):
┌──────────────────────────────────┐
│  Web服务、API服务、数据库         │
│  → 期望状态: 始终运行N个副本       │
│  → 失败重启: 自动重启             │
│  → 生命周期: 永久运行             │
└──────────────────────────────────┘

一次性任务 (Job):
┌──────────────────────────────────┐
│  数据库迁移、批量计算、数据导入    │
│  → 期望状态: 运行到成功完成        │
│  → 失败重试: 重新执行 (可配置次数) │
│  → 生命周期: 完成后停止            │
└──────────────────────────────────┘

定时任务 (CronJob):
┌──────────────────────────────────┐
│  定时备份、报表生成、数据同步      │
│  → 期望状态: 按Cron表达式触发     │
│  → 失败重试: 下次调度时重试        │
│  → 生命周期: 定时创建Job           │
└──────────────────────────────────┘
```

**典型应用场景**:

| 场景 | 说明 | 使用控制器 |
|-----|------|----------|
| **数据库迁移** | 升级数据库Schema (一次性) | Job |
| **批量数据处理** | ETL处理、数据清洗 | Job (并行) |
| **定时备份** | 每天凌晨备份数据库 | CronJob |
| **报表生成** | 每周生成业务报表 | CronJob |
| **数据同步** | 定时从外部API拉取数据 | CronJob |
| **日志归档** | 每月归档旧日志 | CronJob |
| **证书续期** | 定期检查并续期SSL证书 | CronJob |

---

### 3.5.2 Job基础配置

**完整Job示例** (圆周率计算任务):

```yaml
# pi-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-job
  namespace: default
spec:
  # ← 核心字段: 完成成功的Pod数量
  completions: 3          # 需要3个Pod成功完成
  parallelism: 2          # 最多并行运行2个Pod

  # ← 重试策略
  backoffLimit: 4         # 最多重试4次 (默认6次)

  # ← 超时设置
  activeDeadlineSeconds: 600   # 600秒内必须完成 (10分钟)

  template:
    metadata:
      labels:
        app: pi-job
    spec:
      # ← 重启策略 (Job必须设置为Never或OnFailure)
      restartPolicy: Never
      # restartPolicy选项:
      #   - Never: 失败后创建新Pod (推荐)
      #   - OnFailure: 失败后重启容器
      #   - Always: ❌ Job不支持!

      containers:
      - name: pi
        image: perl:5.34
        command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
```

**核心字段说明**:

```yaml
spec:
  completions: 3
  # 需要成功完成的Pod数量
  # - 不设置: 1个Pod成功即完成
  # - 设置为N: 需要N个Pod成功完成

  parallelism: 2
  # 最大并行运行的Pod数量
  # - 不设置: 默认1 (串行执行)
  # - 设置为N: 最多N个Pod并行运行

  backoffLimit: 4
  # 失败重试次数限制
  # - 默认: 6次
  # - 0: 不重试
  # - 设置为N: 最多重试N次后标记为Failed

  activeDeadlineSeconds: 600
  # 任务超时时间 (秒)
  # - 超时后Job状态变为Failed
  # - 所有运行中的Pod会被终止

  ttlSecondsAfterFinished: 100
  # 任务完成后的保留时间 (秒)
  # - 100秒后自动删除Job和Pod
  # - 不设置: 永久保留 (需手动清理)
```

---

### 3.5.3 Job执行模式

**模式1: 单次执行 (默认)**

```yaml
# 不设置completions和parallelism
spec:
  template:
    spec:
      containers:
      - name: task
        image: busybox
        command: ["sh", "-c", "echo 'Task completed!'"]
```

**执行流程**:
```bash
kubectl apply -f job.yaml

# 1个Pod运行 → 成功 → Job Complete
Pod-1: Running → Succeeded
Job: Complete
```

---

**模式2: 固定完成次数 (completions)**

```yaml
spec:
  completions: 5        # 需要5个Pod成功完成
  parallelism: 2        # 最多并行2个Pod
```

**执行流程**:
```bash
kubectl apply -f job.yaml

# 并行运行2个Pod,完成后再启动新Pod,直到5个成功
Pod-1: Running → Succeeded  ┐
Pod-2: Running → Succeeded  ┘ (并行)
Pod-3: Running → Succeeded  ┐
Pod-4: Running → Succeeded  ┘ (并行)
Pod-5: Running → Succeeded    (最后1个)

Job: Complete (5/5 succeeded)
```

---

**模式3: 工作队列模式 (Work Queue)**

```yaml
spec:
  completions: 10       # 需要处理10个任务
  parallelism: 3        # 并行3个Worker
```

**应用场景**: 从消息队列 (RabbitMQ/Redis) 拉取任务并行处理

```bash
# Worker Pod逻辑:
while true; do
  task=$(get_task_from_queue)
  [[ -z "$task" ]] && break
  process_task "$task"
done
```

---

**模式4: 带索引的并行任务 (Indexed Jobs, K8s 1.24+)**

```yaml
spec:
  completions: 5
  parallelism: 2
  completionMode: Indexed    # ← 启用索引模式
```

**特性**: 每个Pod会获得唯一的索引 (通过环境变量 `JOB_COMPLETION_INDEX`)

```bash
# Pod-0: JOB_COMPLETION_INDEX=0
# Pod-1: JOB_COMPLETION_INDEX=1
# Pod-2: JOB_COMPLETION_INDEX=2
# ...

# 应用示例: 处理分片数据
SHARD_ID=$JOB_COMPLETION_INDEX
process_shard $SHARD_ID
```

---

### 3.5.4 Job状态与监控

**Job生命周期**:

```bash
# 创建Job
kubectl apply -f pi-job.yaml

# 查看Job状态
kubectl get jobs
# NAME     COMPLETIONS   DURATION   AGE
# pi-job   0/3           5s         5s    ← 进行中 (0个完成,需要3个)

# 等待片刻后
kubectl get jobs
# NAME     COMPLETIONS   DURATION   AGE
# pi-job   2/3           30s        30s   ← 2个完成

# 最终完成
kubectl get jobs
# NAME     COMPLETIONS   DURATION   AGE
# pi-job   3/3           45s        45s   ← 全部完成 ✅

# 查看Pod状态
kubectl get pods -l app=pi-job
# NAME           READY   STATUS      RESTARTS   AGE
# pi-job-abc12   0/1     Completed   0          1m   ← 第1个成功
# pi-job-def34   0/1     Completed   0          1m   ← 第2个成功
# pi-job-ghi56   0/1     Completed   0          50s  ← 第3个成功
```

**Job状态字段**:

```bash
kubectl get job pi-job -o yaml | grep -A 10 "status:"
# status:
#   active: 1              # 运行中的Pod数量
#   succeeded: 2           # 成功完成的Pod数量
#   failed: 0              # 失败的Pod数量
#   completionTime: "2024-12-18T10:30:45Z"
#   startTime: "2024-12-18T10:30:00Z"
#   conditions:
#   - type: Complete
#     status: "True"
```

**监控Job进度**:

```bash
# 实时监控Job
kubectl get jobs -w

# 查看Pod日志
kubectl logs -l app=pi-job

# 查看失败Pod的日志
kubectl logs -l app=pi-job --selector='status.phase=Failed'

# 查看Job事件
kubectl describe job pi-job
# Events:
#   Normal  SuccessfulCreate  Created pod: pi-job-abc12
#   Normal  SuccessfulCreate  Created pod: pi-job-def34
#   Normal  Completed         Job completed
```

---

### 3.5.5 Job失败处理

**失败场景1: Pod执行失败**

```yaml
spec:
  backoffLimit: 3        # 最多重试3次
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: task
        image: busybox
        command: ["sh", "-c", "exit 1"]  # 模拟失败
```

**重试行为**:

```bash
kubectl apply -f failing-job.yaml

kubectl get pods -w
# NAME              READY   STATUS             RESTARTS   AGE
# failing-job-1     0/1     Error              0          5s    ← 第1次失败
# failing-job-2     0/1     Error              0          10s   ← 第2次重试失败
# failing-job-3     0/1     Error              0          20s   ← 第3次重试失败
# failing-job-4     0/1     Error              0          40s   ← 第4次重试失败

kubectl get job failing-job
# NAME          COMPLETIONS   DURATION   AGE
# failing-job   0/1           1m         1m

kubectl describe job failing-job
# Status:
#   Failed: 4
#   Conditions:
#     Type: Failed
#     Reason: BackoffLimitExceeded
```

**退避重试 (Backoff)**:

```
重试间隔指数增长:
┌────────────────────────────────┐
│  第1次失败: 立即重试 (0s)       │
│  第2次失败: 等待10s后重试       │
│  第3次失败: 等待20s后重试       │
│  第4次失败: 等待40s后重试       │
│  第5次失败: 等待80s后重试       │
│  ...                           │
│  最大间隔: 6分钟                │
└────────────────────────────────┘
```

---

**失败场景2: Pod Pending (资源不足)**

```bash
# Pod无法调度 (资源不足)
kubectl describe pod failing-job-xyz
# Events:
#   Warning  FailedScheduling  0/3 nodes available: insufficient cpu

# Job状态显示active=1 (Pod仍在Pending)
kubectl get job failing-job
# NAME          COMPLETIONS   DURATION   AGE
# failing-job   0/1           5m         5m

# 解决方案1: 降低资源请求
kubectl delete job failing-job
# 修改YAML降低resources.requests

# 解决方案2: 扩容集群节点
```

---

**失败场景3: 超时**

```yaml
spec:
  activeDeadlineSeconds: 60   # 60秒超时
  template:
    spec:
      containers:
      - name: slow-task
        image: busybox
        command: ["sh", "-c", "sleep 120"]  # 睡眠120秒 (超过60秒限制)
```

```bash
kubectl apply -f timeout-job.yaml

# 60秒后Job被终止
kubectl get job timeout-job
# NAME          COMPLETIONS   DURATION   AGE
# timeout-job   0/1           60s        60s

kubectl describe job timeout-job
# Status:
#   Failed: 1
#   Conditions:
#     Type: Failed
#     Reason: DeadlineExceeded
```

---

### 3.5.6 CronJob定时任务

**CronJob基础配置**:

```yaml
# backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: database-backup
  namespace: default
spec:
  # ← Cron调度表达式 (每天凌晨2点)
  schedule: "0 2 * * *"

  # ← 时区设置 (K8s 1.25+)
  timeZone: "Asia/Shanghai"

  # ← 并发策略
  concurrencyPolicy: Forbid
  # 选项:
  #   - Allow: 允许并发执行
  #   - Forbid: 禁止并发 (上次Job未完成则跳过)
  #   - Replace: 替换运行中的Job

  # ← 保留历史Job数量
  successfulJobsHistoryLimit: 3   # 保留3个成功的Job
  failedJobsHistoryLimit: 1       # 保留1个失败的Job

  # ← 启动截止时间
  startingDeadlineSeconds: 300    # 错过调度后300秒内仍可启动

  # ← Job模板 (与普通Job配置相同)
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 86400   # 24小时后清理
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: backup
            image: postgres:15-alpine
            command:
            - /bin/bash
            - -c
            - |
              pg_dump -h postgres.default.svc.cluster.local -U admin mydb \
                > /backup/backup-$(date +%Y%m%d-%H%M%S).sql
              echo "Backup completed at $(date)"
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
            volumeMounts:
            - name: backup
              mountPath: /backup
          volumes:
          - name: backup
            persistentVolumeClaim:
              claimName: backup-pvc
```

**Cron表达式详解**:

```
格式: "分 时 日 月 周"
      ┬  ┬  ┬  ┬  ┬
      │  │  │  │  └─ 周几 (0-7, 0和7都表示周日)
      │  │  │  └──── 月份 (1-12)
      │  │  └─────── 日期 (1-31)
      │  └────────── 小时 (0-23)
      └───────────── 分钟 (0-59)

常用示例:
"0 2 * * *"        # 每天凌晨2点
"30 */2 * * *"     # 每2小时的第30分钟
"0 0 * * 0"        # 每周日凌晨0点
"0 0 1 * *"        # 每月1号凌晨0点
"*/5 * * * *"      # 每5分钟
"0 9-17 * * 1-5"   # 工作日9-17点整点

特殊字符:
*   任意值
,   多个值 (1,3,5)
-   范围 (1-5)
/   步长 (*/5 表示每5个单位)
```

---

### 3.5.7 CronJob并发控制

**concurrencyPolicy策略对比**:

**策略1: Allow (允许并发)**

```yaml
spec:
  schedule: "*/1 * * * *"    # 每分钟执行
  concurrencyPolicy: Allow   # 允许并发
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: task
            command: ["sh", "-c", "sleep 120"]  # 任务耗时2分钟
```

**执行结果**:
```
时间轴:
00:00 → Job-1启动 (运行中...)
00:01 → Job-2启动 (运行中...) ← Job-1仍在运行
00:02 → Job-3启动 (运行中...) ← Job-1和Job-2仍在运行
00:02 → Job-1完成
00:03 → Job-2完成
...

⚠️ 风险: 并发任务可能导致资源竞争或数据冲突
```

---

**策略2: Forbid (禁止并发, 推荐)**

```yaml
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Forbid   # 禁止并发
```

**执行结果**:
```
时间轴:
00:00 → Job-1启动 (运行中...)
00:01 → 跳过 (Job-1仍在运行)
00:02 → 跳过 (Job-1仍在运行)
00:02 → Job-1完成
00:03 → Job-2启动

✅ 优点: 避免并发冲突
❌ 缺点: 可能错过调度
```

---

**策略3: Replace (替换运行中的Job)**

```yaml
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Replace   # 替换旧Job
```

**执行结果**:
```
时间轴:
00:00 → Job-1启动 (运行中...)
00:01 → 终止Job-1 → 启动Job-2
00:02 → 终止Job-2 → 启动Job-3

⚠️ 风险: 任务可能被频繁中断而无法完成
```

---

### 3.5.8 实战项目: 数据库定时备份

**项目目标**: 每天凌晨2点备份PostgreSQL数据库,保留最近7天的备份,失败时发送告警。

**完整配置清单**:

```yaml
# 1. Secret: 数据库密码
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: default
type: Opaque
data:
  password: cG9zdGdyZXMxMjM=   # base64: postgres123

---
# 2. PVC: 备份存储卷
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: default
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard

---
# 3. CronJob: 定时备份任务
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: default
spec:
  # 每天凌晨2点执行
  schedule: "0 2 * * *"
  timeZone: "Asia/Shanghai"

  # 禁止并发执行
  concurrencyPolicy: Forbid

  # 保留历史Job
  successfulJobsHistoryLimit: 7    # 保留7个成功备份
  failedJobsHistoryLimit: 3        # 保留3个失败记录

  # 错过调度后10分钟内仍可执行
  startingDeadlineSeconds: 600

  jobTemplate:
    spec:
      # 备份完成后24小时自动清理Job
      ttlSecondsAfterFinished: 86400

      template:
        metadata:
          labels:
            app: postgres-backup
        spec:
          restartPolicy: OnFailure

          containers:
          - name: backup
            image: postgres:15-alpine
            command:
            - /bin/bash
            - -c
            - |
              set -e

              # 设置备份文件名
              BACKUP_DATE=$(date +%Y%m%d-%H%M%S)
              BACKUP_FILE="/backup/postgres-${BACKUP_DATE}.sql"

              echo "$(date): Starting backup..."

              # 执行备份
              pg_dump -h postgres.default.svc.cluster.local \
                      -U postgres \
                      -d myapp \
                      --format=custom \
                      --compress=9 \
                      --file="${BACKUP_FILE}"

              # 验证备份文件
              if [ ! -f "${BACKUP_FILE}" ]; then
                echo "ERROR: Backup file not created!"
                exit 1
              fi

              BACKUP_SIZE=$(du -h "${BACKUP_FILE}" | cut -f1)
              echo "$(date): Backup completed! File: ${BACKUP_FILE} (${BACKUP_SIZE})"

              # 删除7天前的备份
              echo "$(date): Cleaning old backups (>7 days)..."
              find /backup -name "postgres-*.sql" -mtime +7 -delete

              # 显示剩余备份
              echo "$(date): Remaining backups:"
              ls -lh /backup/postgres-*.sql 2>/dev/null || echo "No backups found"

              echo "$(date): Backup job completed successfully!"

            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password

            volumeMounts:
            - name: backup
              mountPath: /backup

            resources:
              requests:
                cpu: 200m
                memory: 256Mi
              limits:
                cpu: 1000m
                memory: 1Gi

          volumes:
          - name: backup
            persistentVolumeClaim:
              claimName: backup-pvc

---
# 4. CronJob: 备份验证任务 (每天早上检查备份是否存在)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-verify
  namespace: default
spec:
  schedule: "0 3 * * *"   # 凌晨3点 (备份后1小时)
  timeZone: "Asia/Shanghai"
  concurrencyPolicy: Forbid

  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 3600
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: verify
            image: postgres:15-alpine
            command:
            - /bin/bash
            - -c
            - |
              set -e

              # 检查今天的备份是否存在
              TODAY=$(date +%Y%m%d)
              BACKUP_COUNT=$(ls /backup/postgres-${TODAY}-*.sql 2>/dev/null | wc -l)

              if [ "$BACKUP_COUNT" -eq 0 ]; then
                echo "ERROR: No backup found for today ($TODAY)!"
                exit 1
              fi

              # 检查最新备份的大小
              LATEST_BACKUP=$(ls -t /backup/postgres-${TODAY}-*.sql | head -1)
              BACKUP_SIZE=$(stat -c%s "$LATEST_BACKUP")

              # 备份至少应有1MB
              if [ "$BACKUP_SIZE" -lt 1048576 ]; then
                echo "WARNING: Backup file too small ($BACKUP_SIZE bytes)"
                exit 1
              fi

              echo "✅ Backup verification passed!"
              echo "   File: $LATEST_BACKUP"
              echo "   Size: $(du -h $LATEST_BACKUP | cut -f1)"

            volumeMounts:
            - name: backup
              mountPath: /backup
              readOnly: true

          volumes:
          - name: backup
            persistentVolumeClaim:
              claimName: backup-pvc
```

**部署与验证**:

```bash
# 1. 部署所有资源
kubectl apply -f postgres-backup.yaml

# 2. 查看CronJob
kubectl get cronjobs
# NAME              SCHEDULE    SUSPEND   ACTIVE   LAST SCHEDULE   AGE
# postgres-backup   0 2 * * *   False     0        <none>          10s
# backup-verify     0 3 * * *   False     0        <none>          10s

# 3. 手动触发备份 (测试)
kubectl create job --from=cronjob/postgres-backup manual-backup-1

# 4. 监控Job执行
kubectl get jobs -w
# NAME              COMPLETIONS   DURATION   AGE
# manual-backup-1   0/1           5s         5s
# manual-backup-1   1/1           30s        30s   ← 完成

# 5. 查看备份日志
kubectl logs -l app=postgres-backup
# 2024-12-18 02:00:01: Starting backup...
# 2024-12-18 02:00:25: Backup completed! File: /backup/postgres-20241218-020001.sql (45M)
# 2024-12-18 02:00:26: Cleaning old backups (>7 days)...
# 2024-12-18 02:00:26: Remaining backups:
# -rw-r--r-- 1 postgres postgres 45M Dec 18 02:00 postgres-20241218-020001.sql
# 2024-12-18 02:00:26: Backup job completed successfully!

# 6. 验证备份文件
kubectl run -it --rm backup-check --image=postgres:15-alpine --restart=Never -- sh
# 在容器中执行:
ls -lh /backup
# -rw-r--r-- 1 postgres postgres 45M Dec 18 02:00 postgres-20241218-020001.sql

# 7. 查看CronJob历史
kubectl get jobs --selector=cronjob-name=postgres-backup
# NAME                        COMPLETIONS   DURATION   AGE
# postgres-backup-28405320    1/1           25s        1d
# postgres-backup-28406480    1/1           28s        23h
# postgres-backup-28407640    1/1           26s        22h

# 8. 暂停CronJob (维护时)
kubectl patch cronjob postgres-backup -p '{"spec":{"suspend":true}}'

# 9. 恢复CronJob
kubectl patch cronjob postgres-backup -p '{"spec":{"suspend":false}}'
```

**备份恢复测试**:

```bash
# 从备份恢复数据库
kubectl run -it --rm restore-test --image=postgres:15-alpine --restart=Never -- bash
# 在容器中执行:
export PGPASSWORD=postgres123
pg_restore -h postgres.default.svc.cluster.local \
           -U postgres \
           -d myapp_restored \
           --create \
           /backup/postgres-20241218-020001.sql

# 验证恢复的数据
psql -h postgres.default.svc.cluster.local -U postgres -d myapp_restored -c "SELECT count(*) FROM users;"
```

---

### 3.5.9 Job与CronJob最佳实践

**1. 幂等性设计**:

```bash
# ❌ 非幂等 (多次执行产生重复数据)
INSERT INTO users (name) VALUES ('Alice');

# ✅ 幂等 (多次执行结果相同)
INSERT INTO users (name) VALUES ('Alice')
ON CONFLICT (name) DO NOTHING;
```

**2. 超时与重试配置**:

```yaml
spec:
  # Job级别超时
  activeDeadlineSeconds: 3600   # 1小时

  # 重试策略
  backoffLimit: 3               # 最多重试3次

  template:
    spec:
      # Pod级别配置
      restartPolicy: OnFailure

      containers:
      - name: task
        # 容器健康检查
        livenessProbe:
          exec:
            command: ["test", "-f", "/tmp/alive"]
          initialDelaySeconds: 30
          periodSeconds: 10
```

**3. 资源限制**:

```yaml
resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 2000m
    memory: 2Gi

# ⚠️ 批处理任务通常资源消耗较高,建议设置合理的limits
```

**4. 日志与监控**:

```yaml
# 将日志输出到stdout (便于kubectl logs查看)
command:
- /bin/bash
- -c
- |
  echo "$(date): Task started"
  # 任务逻辑...
  echo "$(date): Task completed"

# Prometheus监控指标
kube_job_status_succeeded{job_name="postgres-backup"}
kube_job_status_failed{job_name="postgres-backup"}
```

**5. 清理策略**:

```yaml
# 自动清理已完成的Job
spec:
  ttlSecondsAfterFinished: 86400   # 24小时后删除

# CronJob历史记录限制
spec:
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
```

**6. 错误处理**:

```bash
#!/bin/bash
set -euo pipefail   # 遇到错误立即退出

# 捕获错误并记录
trap 'echo "ERROR: Task failed at line $LINENO"; exit 1' ERR

# 任务逻辑
echo "Starting task..."
# ...
```

**7. 并发任务隔离**:

```yaml
# 为并发任务添加唯一标识
env:
- name: TASK_ID
  valueFrom:
    fieldRef:
      fieldPath: metadata.name   # 使用Pod名称作为唯一标识

# 避免文件名冲突
command:
- sh
- -c
- |
  OUTPUT_FILE="/data/result-${TASK_ID}.txt"
  # 处理逻辑...
```

---

**第3.5节完成 | 核心概念: Job一次性任务、CronJob定时调度、并发控制、数据库备份实战**

---

## 3.6 实战项目: 微服务应用完整部署

本节将综合运用本章学习的所有控制器,部署一个完整的微服务电商应用。

### 3.6.1 项目架构设计

**应用组件**:

```
电商微服务架构:
┌──────────────────────────────────────────────┐
│  Frontend (Deployment)                       │
│    ↓ Nginx + React SPA                       │
│    ↓ 3副本 + HPA自动扩缩容                    │
├──────────────────────────────────────────────┤
│  API Gateway (Deployment)                    │
│    ↓ Node.js Gateway                         │
│    ↓ 2副本 + 金丝雀发布                       │
├──────────────────────────────────────────────┤
│  Business Services (Deployment)              │
│    ├─ User Service (3副本)                   │
│    ├─ Product Service (3副本)                │
│    └─ Order Service (5副本, 核心服务)         │
├──────────────────────────────────────────────┤
│  Redis Cluster (StatefulSet)                 │
│    ↓ 6节点集群 (3 master + 3 replica)        │
│    ↓ 持久化存储 + 主从自动切换                │
├──────────────────────────────────────────────┤
│  MySQL (StatefulSet)                         │
│    ↓ 1 master + 2 replicas                   │
│    ↓ 主从复制 + 自动备份                     │
├──────────────────────────────────────────────┤
│  Infrastructure (DaemonSet)                  │
│    ├─ Fluentd (日志收集)                     │
│    ├─ Node Exporter (监控)                   │
│    └─ Filebeat (APM日志)                     │
├──────────────────────────────────────────────┤
│  Batch Jobs (CronJob)                        │
│    ├─ DB Backup (每天2点)                    │
│    ├─ Report Generation (每周一8点)          │
│    └─ Cache Warmup (每小时)                  │
└──────────────────────────────────────────────┘
```

### 3.6.2 Namespace与资源配额

```yaml
# namespace-and-quotas.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ecommerce
  labels:
    env: production
    app: ecommerce

---
# 资源配额: 限制Namespace总资源
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: ecommerce
spec:
  hard:
    requests.cpu: "20"         # 总CPU请求: 20核
    requests.memory: 40Gi      # 总内存请求: 40GB
    limits.cpu: "40"           # 总CPU限制: 40核
    limits.memory: 80Gi        # 总内存限制: 80GB
    pods: "100"                # 最多100个Pod

---
# Pod默认资源限制
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: ecommerce
spec:
  limits:
  - default:              # 默认limits
      cpu: 500m
      memory: 512Mi
    defaultRequest:       # 默认requests
      cpu: 100m
      memory: 128Mi
    max:                  # 单个Pod最大资源
      cpu: 2000m
      memory: 4Gi
    min:                  # 单个Pod最小资源
      cpu: 50m
      memory: 64Mi
    type: Container
```

### 3.6.3 前端服务部署

```yaml
# frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: ecommerce
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
      tier: web
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0      # 零宕机更新
  template:
    metadata:
      labels:
        app: frontend
        tier: web
        version: v1.2.0
    spec:
      containers:
      - name: nginx
        image: myregistry.com/ecommerce-frontend:v1.2.0
        ports:
        - containerPort: 80
          name: http
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config

---
# Service: ClusterIP (内部访问)
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: ecommerce
spec:
  type: ClusterIP
  selector:
    app: frontend
    tier: web
  ports:
  - port: 80
    targetPort: 80
    name: http

---
# HPA: 自动扩缩容 (3-10副本)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-hpa
  namespace: ecommerce
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70    # CPU >70% 扩容
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80    # 内存 >80% 扩容
```

### 3.6.4 业务服务部署 (以Order Service为例)

```yaml
# order-service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
  namespace: ecommerce
spec:
  replicas: 5                # 核心服务多副本
  selector:
    matchLabels:
      app: order-service
  template:
    metadata:
      labels:
        app: order-service
        tier: backend
        version: v2.1.0
    spec:
      affinity:
        # Pod反亲和性: 尽量分散到不同节点
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - order-service
              topologyKey: kubernetes.io/hostname

      containers:
      - name: order-service
        image: myregistry.com/order-service:v2.1.0
        ports:
        - containerPort: 8080
          name: http
        env:
        # 数据库连接
        - name: DB_HOST
          value: "mysql-0.mysql.ecommerce.svc.cluster.local"
        - name: DB_PORT
          value: "3306"
        - name: DB_USER
          value: "orderuser"
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: order-password

        # Redis连接
        - name: REDIS_CLUSTER_NODES
          value: "redis-0.redis.ecommerce:6379,redis-1.redis.ecommerce:6379,redis-2.redis.ecommerce:6379"

        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 1Gi

        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5

        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 5

---
# Service: Headless (用于StatefulSet风格DNS)
apiVersion: v1
kind: Service
metadata:
  name: order-service
  namespace: ecommerce
spec:
  selector:
    app: order-service
  ports:
  - port: 8080
    targetPort: 8080
    name: http
```

### 3.6.5 综合部署脚本

```bash
#!/bin/bash
# deploy-ecommerce.sh - 一键部署电商应用

set -e

NAMESPACE="ecommerce"

echo "========================================="
echo "  电商微服务应用部署脚本"
echo "========================================="

# 1. 创建Namespace和资源配额
echo "✅ 1/8: 创建Namespace和资源配额..."
kubectl apply -f namespace-and-quotas.yaml

# 2. 创建ConfigMap和Secret
echo "✅ 2/8: 创建配置..."
kubectl create configmap nginx-config \
  --from-file=nginx.conf \
  -n $NAMESPACE \
  --dry-run=client -o yaml | kubectl apply -f -

kubectl create secret generic mysql-secret \
  --from-literal=root-password='root123' \
  --from-literal=order-password='order123' \
  -n $NAMESPACE \
  --dry-run=client -o yaml | kubectl apply -f -

# 3. 部署MySQL StatefulSet
echo "✅ 3/8: 部署MySQL集群..."
kubectl apply -f mysql-statefulset.yaml
kubectl wait --for=condition=ready pod/mysql-0 -n $NAMESPACE --timeout=300s

# 4. 部署Redis Cluster StatefulSet
echo "✅ 4/8: 部署Redis集群..."
kubectl apply -f redis-statefulset.yaml
kubectl wait --for=condition=ready pod/redis-0 -n $NAMESPACE --timeout=180s

# 5. 部署业务服务 Deployments
echo "✅ 5/8: 部署业务服务..."
kubectl apply -f user-service-deployment.yaml
kubectl apply -f product-service-deployment.yaml
kubectl apply -f order-service-deployment.yaml

# 6. 部署前端和API Gateway
echo "✅ 6/8: 部署前端和网关..."
kubectl apply -f frontend-deployment.yaml
kubectl apply -f api-gateway-deployment.yaml

# 7. 部署DaemonSet (日志和监控)
echo "✅ 7/8: 部署基础设施服务..."
kubectl apply -f fluentd-daemonset.yaml
kubectl apply -f node-exporter-daemonset.yaml

# 8. 部署CronJob (定时任务)
echo "✅ 8/8: 部署定时任务..."
kubectl apply -f database-backup-cronjob.yaml
kubectl apply -f report-generation-cronjob.yaml

echo ""
echo "========================================="
echo "  部署完成! 正在检查状态..."
echo "========================================="

# 检查所有资源状态
kubectl get all -n $NAMESPACE

echo ""
echo "📊 Deployment状态:"
kubectl get deployments -n $NAMESPACE

echo ""
echo "📊 StatefulSet状态:"
kubectl get statefulsets -n $NAMESPACE

echo ""
echo "📊 DaemonSet状态:"
kubectl get daemonsets -n $NAMESPACE

echo ""
echo "📊 CronJob状态:"
kubectl get cronjobs -n $NAMESPACE

echo ""
echo "========================================="
echo "  部署成功! 🎉"
echo "========================================="
echo ""
echo "访问应用:"
echo "  - Frontend: http://<NodeIP>:30080"
echo "  - API Gateway: http://<NodeIP>:30081"
echo ""
echo "监控应用:"
echo "  kubectl get pods -n $NAMESPACE -w"
echo "  kubectl logs -f deployment/order-service -n $NAMESPACE"
echo ""
```

### 3.6.6 滚动更新与回滚演练

```bash
# 场景1: 金丝雀发布 (Order Service v2.1.0 → v2.2.0)

# 1. 创建Canary Deployment (10%流量)
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service-canary
  namespace: ecommerce
spec:
  replicas: 1              # 1个Canary Pod (总共6个Pod, 占比16%)
  selector:
    matchLabels:
      app: order-service
      track: canary
  template:
    metadata:
      labels:
        app: order-service
        track: canary
        version: v2.2.0
    spec:
      containers:
      - name: order-service
        image: myregistry.com/order-service:v2.2.0  # 新版本
        # (与stable版本配置相同)
EOF

# 2. 监控Canary Pod指标 (错误率/延迟)
kubectl logs -f -l app=order-service,track=canary -n ecommerce

# 3. 验证通过后逐步增加Canary副本
kubectl scale deployment order-service-canary --replicas=2 -n ecommerce
# 等待验证...
kubectl scale deployment order-service-canary --replicas=3 -n ecommerce

# 4. 全量切换: 更新Stable Deployment
kubectl set image deployment/order-service \
  order-service=myregistry.com/order-service:v2.2.0 \
  -n ecommerce

# 5. 删除Canary Deployment
kubectl delete deployment order-service-canary -n ecommerce

# ---

# 场景2: 紧急回滚
kubectl rollout undo deployment/order-service -n ecommerce

# 场景3: 回滚到指定版本
kubectl rollout history deployment/order-service -n ecommerce
kubectl rollout undo deployment/order-service --to-revision=3 -n ecommerce
```

---

**第3.6节完成 | 核心概念: 微服务架构完整部署、金丝雀发布、自动扩缩容**

---

## 3.7 本章小结

### 3.7.1 核心知识回顾

本章深入学习了Kubernetes的5大工作负载控制器,掌握了从无状态到有状态、从长期运行到批处理任务的完整应用部署方案。

**5大控制器对比总结**:

| 控制器 | 适用场景 | 核心特性 | 实战案例 |
|-------|---------|---------|---------|
| **Deployment** | 无状态应用 | 滚动更新、版本回滚、金丝雀/蓝绿部署 | Nginx Web服务、API Gateway |
| **StatefulSet** | 有状态应用 | 稳定网络标识、有序部署、独立持久化卷 | MySQL集群、Redis Cluster |
| **DaemonSet** | 守护进程 | 每节点一个Pod、节点选择、污点容忍 | Fluentd日志、Node Exporter |
| **Job** | 一次性任务 | 运行到完成、失败重试、并行执行 | 数据库迁移、批量数据处理 |
| **CronJob** | 定时任务 | Cron调度、并发控制、历史记录 | 数据库备份、报表生成 |

### 3.7.2 关键技术点梳理

**1. Deployment部署策略**:

```
滚动更新:
  maxUnavailable: 最多不可用Pod数
  maxSurge: 最多超出期望副本数
  → 平滑升级,避免宕机

金丝雀发布:
  方案1: 多Deployment + Service
  方案2: 单Deployment + Partition
  → 渐进式发布,降低风险

蓝绿部署:
  Blue/Green Deployment + Service切换
  → 瞬间切换,快速回滚
```

**2. StatefulSet关键机制**:

```
稳定网络标识:
  Pod命名: <name>-<index>
  DNS记录: <pod>.<service>.<namespace>.svc.cluster.local
  → 固定Hostname,重启后不变

有序部署:
  OrderedReady: Pod-0 Ready → Pod-1 Ready → ...
  Parallel: 并行创建 (失去有序性)
  → 确保依赖关系正确

持久化存储:
  volumeClaimTemplates: 自动为每个Pod创建PVC
  PVC命名: <template>-<statefulset>-<index>
  → Pod重启后绑定相同PVC
```

**3. DaemonSet调度控制**:

```
节点选择:
  nodeSelector: 简单标签匹配
  nodeAffinity: 复杂亲和性规则
  → 控制Pod运行的节点

污点容忍:
  Toleration: 容忍节点污点
  Effect: NoSchedule / PreferNoSchedule / NoExecute
  → 允许在特殊节点运行 (如Master)
```

**4. Job执行模式**:

```
单次执行: completions=1 (默认)
固定完成次数: completions=N, parallelism=M
工作队列: 从队列拉取任务并行处理
索引模式: JOB_COMPLETION_INDEX环境变量
```

**5. CronJob并发策略**:

```
Allow: 允许并发 (可能资源竞争)
Forbid: 禁止并发 (推荐,避免冲突)
Replace: 替换旧Job (可能被频繁中断)
```

### 3.7.3 生产环境最佳实践

**资源配置原则**:

```yaml
# 1. 合理设置资源请求和限制
resources:
  requests:
    cpu: 100m          # 调度依据 (必须满足)
    memory: 128Mi
  limits:
    cpu: 500m          # 运行时限制 (可超卖)
    memory: 512Mi

# 2. 选择QoS等级
# - Guaranteed (requests=limits): 核心服务
# - Burstable (requests<limits): 普通服务
# - BestEffort (无配置): 非关键服务
```

**健康检查配置**:

```yaml
# 3. 完整健康检查
startupProbe:           # 慢启动保护 (JVM应用)
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 0
  periodSeconds: 10
  failureThreshold: 30  # 300秒启动时间

livenessProbe:          # 检测死锁,失败重启
  httpGet:
    path: /healthz
    port: 8080
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:         # 控制流量路由
  httpGet:
    path: /ready
    port: 8080
  periodSeconds: 5
  timeoutSeconds: 3
```

**更新策略**:

```yaml
# 4. 零宕机滚动更新
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 0        # 确保无Pod不可用
    maxSurge: 1              # 逐个更新

# 5. 版本管理
revisionHistoryLimit: 10     # 保留10个历史版本
```

**监控与告警**:

```yaml
# 6. Prometheus监控指标
kube_deployment_status_replicas{deployment="order-service"}
kube_pod_container_status_restarts_total{pod=~"order-.*"}
kube_job_failed{job="database-backup"}
```

### 3.7.4 常用命令速查

**Deployment管理**:

```bash
# 创建
kubectl create deployment nginx --image=nginx:1.25 --replicas=3

# 扩缩容
kubectl scale deployment nginx --replicas=5

# 更新镜像
kubectl set image deployment/nginx nginx=nginx:1.26

# 查看更新状态
kubectl rollout status deployment/nginx

# 暂停/恢复更新
kubectl rollout pause deployment/nginx
kubectl rollout resume deployment/nginx

# 回滚
kubectl rollout undo deployment/nginx
kubectl rollout undo deployment/nginx --to-revision=3

# 查看历史
kubectl rollout history deployment/nginx
```

**StatefulSet管理**:

```bash
# 创建
kubectl apply -f mysql-statefulset.yaml

# 扩缩容
kubectl scale statefulset mysql --replicas=5

# 更新镜像 (有序更新)
kubectl set image statefulset/mysql mysql=mysql:8.1

# 删除Pod触发重建
kubectl delete pod mysql-2

# 手动清理PVC
kubectl delete pvc data-mysql-3
```

**Job/CronJob管理**:

```bash
# 创建Job
kubectl create job test --image=busybox -- echo "Hello"

# 从CronJob手动触发Job
kubectl create job --from=cronjob/backup manual-backup-1

# 查看Job状态
kubectl get jobs
kubectl describe job test

# 查看Job日志
kubectl logs -l job-name=test

# 暂停CronJob
kubectl patch cronjob backup -p '{"spec":{"suspend":true}}'

# 删除已完成的Job
kubectl delete job test
```

**调试命令**:

```bash
# 查看Pod事件
kubectl describe pod nginx-abc123

# 查看容器日志
kubectl logs nginx-abc123 -c nginx
kubectl logs -f deployment/nginx --all-containers=true

# 进入容器
kubectl exec -it nginx-abc123 -- bash

# 查看资源使用
kubectl top pods
kubectl top nodes

# 查看API资源
kubectl api-resources | grep batch
kubectl explain deployment.spec.strategy
```

### 3.7.5 故障排查清单

**Pod无法启动**:

```bash
# 1. 检查Pod状态
kubectl get pod nginx-abc123
# STATUS: Pending / ImagePullBackOff / CrashLoopBackOff

# 2. 查看详细事件
kubectl describe pod nginx-abc123
# Events: FailedScheduling / Failed / BackOff

# 3. 查看日志
kubectl logs nginx-abc123 --previous    # 上次运行的日志
```

**常见问题与解决方案**:

| 问题 | 原因 | 解决方案 |
|-----|------|---------|
| **Pending** | 资源不足 | 降低requests或扩容节点 |
| **ImagePullBackOff** | 镜像拉取失败 | 检查镜像名称/认证/网络 |
| **CrashLoopBackOff** | 容器启动失败 | 查看日志,检查配置 |
| **OOMKilled** | 内存溢出 | 增加memory limits |
| **Evicted** | 资源不足驱逐 | 检查节点资源,调整QoS |

### 3.7.6 学习建议

1. **实践为主**: 在本地搭建Kubernetes集群 (minikube/kind),动手部署所有示例
2. **阅读官方文档**: https://kubernetes.io/docs/concepts/workloads/
3. **参考源码**: 深入理解控制器实现原理
4. **生产经验**: 关注社区最佳实践和案例分享

### 3.7.7 下一章预告

**第4章: Service与Ingress - 服务发现与负载均衡**

将学习:
- Service类型 (ClusterIP/NodePort/LoadBalancer/ExternalName)
- Endpoint与EndpointSlice原理
- Ingress控制器 (Nginx/Traefik/Istio Gateway)
- 高级路由规则 (基于域名/路径/Header)
- TLS/SSL证书管理
- 金丝雀发布与流量镜像
- 实战: 完整的微服务流量治理方案

---

**🎉 第3章完成! 累计掌握: ReplicaSet、Deployment、StatefulSet、DaemonSet、Job、CronJob 6种控制器**

---
