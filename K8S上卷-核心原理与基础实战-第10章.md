# 第10章 Kubernetes高级特性与企业级实践

## 本章概述

在前面的章节中，我们系统学习了Kubernetes的核心概念、资源对象、网络存储、安全机制和可观测性等内容。本章将深入探讨Kubernetes的高级特性和企业级实践，帮助你构建生产级的、可扩展的、安全的Kubernetes平台。

**本章主要内容**：

1. **多租户与资源隔离**：如何在单个集群中安全地支持多个团队或应用
2. **集群联邦与多集群管理**：跨地域、跨云的多集群架构设计
3. **灾难恢复与业务连续性**：备份恢复策略和高可用架构
4. **安全加固与合规性**：企业级安全最佳实践和合规要求
5. **大规模集群运维**：管理数千节点的超大规模集群
6. **Kubernetes扩展开发**：自定义资源、Operator和准入控制器
7. **云原生最佳实践**：从传统应用到云原生的迁移路径
8. **本章总结**：高级特性的综合应用和未来展望

**学习目标**：

- 掌握多租户架构设计和资源隔离技术
- 理解多集群管理的挑战和解决方案
- 建立完善的灾难恢复和业务连续性体系
- 实施企业级安全加固和合规性管理
- 掌握大规模集群的运维技巧和性能优化
- 学会开发Kubernetes扩展和自定义控制器
- 了解云原生最佳实践和迁移策略

**前置知识**：

- 熟悉Kubernetes核心概念和资源对象（第1-4章）
- 理解网络、存储和安全机制（第5-7章）
- 掌握应用部署和可观测性（第8-9章）

让我们开始探索Kubernetes的高级特性和企业级实践！

---

## 10.1 多租户与资源隔离

### 10.1.1 多租户架构概述

**什么是多租户**

多租户（Multi-Tenancy）是指在单个Kubernetes集群中安全地支持多个用户、团队或应用的能力。每个租户拥有独立的资源配额、网络隔离和访问控制，互不干扰。

**多租户的价值**：

1. **成本优化**：共享集群基础设施，降低运维成本
2. **资源利用率**：通过资源共享提高整体利用率
3. **简化管理**：统一的集群管理和升级
4. **快速交付**：租户可以快速获得资源和环境

**多租户的挑战**：

1. **资源隔离**：防止租户之间的资源争抢
2. **网络隔离**：确保租户之间的网络安全
3. **安全隔离**：防止权限泄露和越权访问
4. **性能隔离**：避免"吵闹邻居"问题
5. **可见性隔离**：租户只能看到自己的资源

**多租户模型**

Kubernetes支持三种多租户模型：

```yaml
# 1. 软多租户（Soft Multi-Tenancy）
# 适用场景：同一组织内的不同团队
# 隔离级别：中等
# 信任级别：高
特点:
  - 使用Namespace进行逻辑隔离
  - 通过RBAC控制访问权限
  - ResourceQuota限制资源使用
  - NetworkPolicy实现网络隔离
  
优势:
  - 实现简单，运维成本低
  - 资源利用率高
  - 适合可信环境
  
劣势:
  - 隔离强度有限
  - 存在安全风险
  - 不适合多客户场景

---

# 2. 硬多租户（Hard Multi-Tenancy）
# 适用场景：不同组织或客户
# 隔离级别：高
# 信任级别：低
特点:
  - 每个租户独立集群
  - 完全的资源和网络隔离
  - 独立的控制平面
  - 强安全边界
  
优势:
  - 安全性最高
  - 完全隔离
  - 适合多客户SaaS
  
劣势:
  - 成本高
  - 运维复杂
  - 资源利用率低

---

# 3. 混合多租户（Hybrid Multi-Tenancy）
# 适用场景：大型企业或云服务商
# 隔离级别：可配置
# 信任级别：中等
特点:
  - 结合软硬多租户优势
  - 使用虚拟集群技术
  - 共享控制平面，隔离数据平面
  - 灵活的隔离策略
  
优势:
  - 平衡成本和安全
  - 灵活性高
  - 适合复杂场景
  
劣势:
  - 实现复杂
  - 需要额外工具支持
```

**多租户架构设计原则**

```yaml
# 多租户架构设计的核心原则

1. 最小权限原则（Principle of Least Privilege）:
   描述: 租户只能访问其必需的资源
   实现:
     - 使用RBAC精细化权限控制
     - 默认拒绝，显式授权
     - 定期审计权限使用情况
   
2. 纵深防御（Defense in Depth）:
   描述: 多层次的安全防护
   实现:
     - Namespace隔离
     - NetworkPolicy网络隔离
     - PodSecurityPolicy安全策略
     - ResourceQuota资源限制
     - 审计日志记录
   
3. 故障隔离（Fault Isolation）:
   描述: 一个租户的故障不影响其他租户
   实现:
     - 资源配额限制
     - PriorityClass优先级
     - Pod反亲和性
     - 独立的存储和网络
   
4. 公平性（Fairness）:
   描述: 资源分配公平合理
   实现:
     - ResourceQuota配额管理
     - LimitRange默认限制
     - 资源预留和超卖策略
     - 动态资源调整
   
5. 可观测性（Observability）:
   描述: 租户可以监控自己的资源
   实现:
     - 租户级别的监控指标
     - 独立的日志收集
     - 资源使用报表
     - 告警和通知机制
```

**多租户实现技术栈**

```yaml
# Kubernetes原生能力
核心组件:
  - Namespace: 逻辑隔离
  - RBAC: 访问控制
  - ResourceQuota: 资源配额
  - LimitRange: 资源限制
  - NetworkPolicy: 网络隔离
  - PodSecurityPolicy: 安全策略

# 增强工具
虚拟集群:
  - vcluster: 轻量级虚拟集群
  - Kamaji: 托管控制平面
  - Cluster API: 集群生命周期管理

策略引擎:
  - OPA/Gatekeeper: 策略即代码
  - Kyverno: Kubernetes原生策略引擎
  - Polaris: 最佳实践验证

网络隔离:
  - Calico: 高级网络策略
  - Cilium: eBPF网络和安全
  - Istio: 服务网格隔离

资源管理:
  - Hierarchical Namespace Controller: 层级命名空间
  - Capsule: 多租户Operator
  - Loft: 虚拟集群平台
```


### 10.1.2 Namespace级别的资源隔离

**Namespace隔离基础**

Namespace是Kubernetes中最基本的多租户隔离机制，它提供了逻辑上的资源分组和隔离。

**Namespace隔离的资源类型**：

```yaml
# Namespace级别的资源（受Namespace隔离）
namespace_scoped_resources:
  工作负载:
    - Pod
    - Deployment
    - StatefulSet
    - DaemonSet
    - Job
    - CronJob
  
  配置和存储:
    - ConfigMap
    - Secret
    - PersistentVolumeClaim
  
  服务发现:
    - Service
    - Endpoints
    - Ingress
  
  访问控制:
    - Role
    - RoleBinding
    - ServiceAccount
  
  资源管理:
    - ResourceQuota
    - LimitRange

# 集群级别的资源（不受Namespace隔离）
cluster_scoped_resources:
  - Node
  - PersistentVolume
  - StorageClass
  - ClusterRole
  - ClusterRoleBinding
  - Namespace
  - CustomResourceDefinition
```

**创建租户Namespace**

```yaml
# tenant-namespace.yaml
# 为租户创建完整的Namespace配置

apiVersion: v1
kind: Namespace
metadata:
  name: tenant-alpha
  labels:
    tenant: alpha
    environment: production
    cost-center: "engineering"
  annotations:
    description: "Alpha团队的生产环境"
    contact: "alpha-team@example.com"
    created-by: "platform-team"

---
# ResourceQuota - 限制租户的资源使用
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-alpha-quota
  namespace: tenant-alpha
spec:
  hard:
    # 计算资源限制
    requests.cpu: "100"           # 最多请求100核CPU
    requests.memory: 200Gi        # 最多请求200GB内存
    limits.cpu: "200"             # 最多限制200核CPU
    limits.memory: 400Gi          # 最多限制400GB内存
    
    # 存储资源限制
    requests.storage: 1Ti         # 最多请求1TB存储
    persistentvolumeclaims: "50"  # 最多50个PVC
    
    # 对象数量限制
    pods: "500"                   # 最多500个Pod
    services: "100"               # 最多100个Service
    services.loadbalancers: "10"  # 最多10个LoadBalancer
    services.nodeports: "20"      # 最多20个NodePort
    
    # 其他资源限制
    configmaps: "200"             # 最多200个ConfigMap
    secrets: "200"                # 最多200个Secret
    replicationcontrollers: "50"  # 最多50个RC
    
  # 作用域限制
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["high", "medium"]

---
# LimitRange - 设置默认资源限制
apiVersion: v1
kind: LimitRange
metadata:
  name: tenant-alpha-limits
  namespace: tenant-alpha
spec:
  limits:
  # Pod级别限制
  - type: Pod
    max:
      cpu: "16"
      memory: 32Gi
    min:
      cpu: "100m"
      memory: 128Mi
  
  # Container级别限制
  - type: Container
    default:              # 默认限制
      cpu: "1"
      memory: 1Gi
    defaultRequest:       # 默认请求
      cpu: "500m"
      memory: 512Mi
    max:                  # 最大限制
      cpu: "8"
      memory: 16Gi
    min:                  # 最小请求
      cpu: "100m"
      memory: 128Mi
    maxLimitRequestRatio: # 限制/请求比率
      cpu: "4"
      memory: "4"
  
  # PVC级别限制
  - type: PersistentVolumeClaim
    max:
      storage: 100Gi
    min:
      storage: 1Gi

---
# NetworkPolicy - 网络隔离
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tenant-alpha-network-policy
  namespace: tenant-alpha
spec:
  podSelector: {}  # 应用到所有Pod
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  # 允许同Namespace内的Pod互相访问
  - from:
    - podSelector: {}
  
  # 允许来自Ingress Controller的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
  
  egress:
  # 允许访问同Namespace内的Pod
  - to:
    - podSelector: {}
  
  # 允许DNS查询
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  
  # 允许访问外部服务（示例：数据库）
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432
  
  # 允许访问互联网（可选）
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
```

**RBAC权限配置**

```yaml
# tenant-rbac.yaml
# 为租户配置细粒度的访问控制

---
# ServiceAccount - 租户的服务账号
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tenant-alpha-admin
  namespace: tenant-alpha

---
# Role - 租户管理员角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: tenant-admin
  namespace: tenant-alpha
rules:
# 完全控制工作负载
- apiGroups: ["", "apps", "batch"]
  resources:
    - pods
    - pods/log
    - pods/exec
    - deployments
    - statefulsets
    - daemonsets
    - jobs
    - cronjobs
    - replicasets
  verbs: ["*"]

# 完全控制配置和存储
- apiGroups: [""]
  resources:
    - configmaps
    - secrets
    - persistentvolumeclaims
  verbs: ["*"]

# 完全控制服务发现
- apiGroups: ["", "networking.k8s.io"]
  resources:
    - services
    - endpoints
    - ingresses
  verbs: ["*"]

# 只读访问资源配额
- apiGroups: [""]
  resources:
    - resourcequotas
    - limitranges
  verbs: ["get", "list", "watch"]

# 只读访问事件
- apiGroups: [""]
  resources:
    - events
  verbs: ["get", "list", "watch"]

---
# Role - 租户开发者角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: tenant-developer
  namespace: tenant-alpha
rules:
# 管理工作负载（不包括删除）
- apiGroups: ["", "apps", "batch"]
  resources:
    - pods
    - deployments
    - statefulsets
    - jobs
    - cronjobs
  verbs: ["get", "list", "watch", "create", "update", "patch"]

# 查看日志和执行命令
- apiGroups: [""]
  resources:
    - pods/log
    - pods/exec
  verbs: ["get", "create"]

# 管理配置（不包括Secret）
- apiGroups: [""]
  resources:
    - configmaps
  verbs: ["get", "list", "watch", "create", "update", "patch"]

# 只读访问Secret
- apiGroups: [""]
  resources:
    - secrets
  verbs: ["get", "list"]

# 只读访问服务
- apiGroups: [""]
  resources:
    - services
    - endpoints
  verbs: ["get", "list", "watch"]

---
# Role - 租户只读角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: tenant-viewer
  namespace: tenant-alpha
rules:
# 只读访问所有资源
- apiGroups: ["", "apps", "batch", "networking.k8s.io"]
  resources: ["*"]
  verbs: ["get", "list", "watch"]

# 查看日志
- apiGroups: [""]
  resources:
    - pods/log
  verbs: ["get"]

---
# RoleBinding - 绑定管理员角色
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-alpha-admin-binding
  namespace: tenant-alpha
subjects:
- kind: User
  name: alice@example.com
  apiGroup: rbac.authorization.k8s.io
- kind: ServiceAccount
  name: tenant-alpha-admin
  namespace: tenant-alpha
roleRef:
  kind: Role
  name: tenant-admin
  apiGroup: rbac.authorization.k8s.io

---
# RoleBinding - 绑定开发者角色
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-alpha-developer-binding
  namespace: tenant-alpha
subjects:
- kind: Group
  name: alpha-developers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: tenant-developer
  apiGroup: rbac.authorization.k8s.io

---
# RoleBinding - 绑定只读角色
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-alpha-viewer-binding
  namespace: tenant-alpha
subjects:
- kind: Group
  name: alpha-viewers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: tenant-viewer
  apiGroup: rbac.authorization.k8s.io
```

**租户管理自动化脚本**

```go
// tenant-manager.go
// 自动化租户创建和管理的Go程序

package main

import (
	"context"
	"fmt"
	"log"

	corev1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
)

// TenantConfig 租户配置
type TenantConfig struct {
	Name        string
	Environment string
	CostCenter  string
	Contact     string
	
	// 资源配额
	CPURequest    string
	MemoryRequest string
	CPULimit      string
	MemoryLimit   string
	StorageQuota  string
	
	// 对象数量限制
	MaxPods     int64
	MaxServices int64
	MaxPVCs     int64
	
	// 管理员用户
	Admins []string
	// 开发者组
	Developers []string
	// 只读用户组
	Viewers []string
}

// TenantManager 租户管理器
type TenantManager struct {
	clientset *kubernetes.Clientset
}

// NewTenantManager 创建租户管理器
func NewTenantManager(kubeconfig string) (*TenantManager, error) {
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create clientset: %v", err)
	}
	
	return &TenantManager{clientset: clientset}, nil
}

// CreateTenant 创建租户
func (tm *TenantManager) CreateTenant(ctx context.Context, config *TenantConfig) error {
	// 1. 创建Namespace
	if err := tm.createNamespace(ctx, config); err != nil {
		return fmt.Errorf("failed to create namespace: %v", err)
	}
	log.Printf("Created namespace: %s", config.Name)
	
	// 2. 创建ResourceQuota
	if err := tm.createResourceQuota(ctx, config); err != nil {
		return fmt.Errorf("failed to create resource quota: %v", err)
	}
	log.Printf("Created resource quota for: %s", config.Name)
	
	// 3. 创建LimitRange
	if err := tm.createLimitRange(ctx, config); err != nil {
		return fmt.Errorf("failed to create limit range: %v", err)
	}
	log.Printf("Created limit range for: %s", config.Name)
	
	// 4. 创建NetworkPolicy
	if err := tm.createNetworkPolicy(ctx, config); err != nil {
		return fmt.Errorf("failed to create network policy: %v", err)
	}
	log.Printf("Created network policy for: %s", config.Name)
	
	// 5. 创建RBAC
	if err := tm.createRBAC(ctx, config); err != nil {
		return fmt.Errorf("failed to create RBAC: %v", err)
	}
	log.Printf("Created RBAC for: %s", config.Name)
	
	log.Printf("Successfully created tenant: %s", config.Name)
	return nil
}

// createNamespace 创建Namespace
func (tm *TenantManager) createNamespace(ctx context.Context, config *TenantConfig) error {
	ns := &corev1.Namespace{
		ObjectMeta: metav1.ObjectMeta{
			Name: config.Name,
			Labels: map[string]string{
				"tenant":      config.Name,
				"environment": config.Environment,
				"cost-center": config.CostCenter,
			},
			Annotations: map[string]string{
				"contact":    config.Contact,
				"created-by": "tenant-manager",
			},
		},
	}
	
	_, err := tm.clientset.CoreV1().Namespaces().Create(ctx, ns, metav1.CreateOptions{})
	return err
}

// createResourceQuota 创建ResourceQuota
func (tm *TenantManager) createResourceQuota(ctx context.Context, config *TenantConfig) error {
	quota := &corev1.ResourceQuota{
		ObjectMeta: metav1.ObjectMeta{
			Name:      fmt.Sprintf("%s-quota", config.Name),
			Namespace: config.Name,
		},
		Spec: corev1.ResourceQuotaSpec{
			Hard: corev1.ResourceList{
				"requests.cpu":           resource.MustParse(config.CPURequest),
				"requests.memory":        resource.MustParse(config.MemoryRequest),
				"limits.cpu":             resource.MustParse(config.CPULimit),
				"limits.memory":          resource.MustParse(config.MemoryLimit),
				"requests.storage":       resource.MustParse(config.StorageQuota),
				"pods":                   *resource.NewQuantity(config.MaxPods, resource.DecimalSI),
				"services":               *resource.NewQuantity(config.MaxServices, resource.DecimalSI),
				"persistentvolumeclaims": *resource.NewQuantity(config.MaxPVCs, resource.DecimalSI),
			},
		},
	}
	
	_, err := tm.clientset.CoreV1().ResourceQuotas(config.Name).Create(ctx, quota, metav1.CreateOptions{})
	return err
}

// createLimitRange 创建LimitRange
func (tm *TenantManager) createLimitRange(ctx context.Context, config *TenantConfig) error {
	limitRange := &corev1.LimitRange{
		ObjectMeta: metav1.ObjectMeta{
			Name:      fmt.Sprintf("%s-limits", config.Name),
			Namespace: config.Name,
		},
		Spec: corev1.LimitRangeSpec{
			Limits: []corev1.LimitRangeItem{
				{
					Type: corev1.LimitTypeContainer,
					Default: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("1"),
						corev1.ResourceMemory: resource.MustParse("1Gi"),
					},
					DefaultRequest: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("500m"),
						corev1.ResourceMemory: resource.MustParse("512Mi"),
					},
					Max: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("8"),
						corev1.ResourceMemory: resource.MustParse("16Gi"),
					},
					Min: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("100m"),
						corev1.ResourceMemory: resource.MustParse("128Mi"),
					},
				},
			},
		},
	}
	
	_, err := tm.clientset.CoreV1().LimitRanges(config.Name).Create(ctx, limitRange, metav1.CreateOptions{})
	return err
}

// createNetworkPolicy 创建NetworkPolicy（简化版）
func (tm *TenantManager) createNetworkPolicy(ctx context.Context, config *TenantConfig) error {
	// 这里简化实现，实际应该使用networking.k8s.io/v1的NetworkPolicy
	// 由于篇幅限制，省略详细实现
	log.Printf("NetworkPolicy creation skipped in this example")
	return nil
}

// createRBAC 创建RBAC
func (tm *TenantManager) createRBAC(ctx context.Context, config *TenantConfig) error {
	// 创建管理员Role
	adminRole := &rbacv1.Role{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "tenant-admin",
			Namespace: config.Name,
		},
		Rules: []rbacv1.PolicyRule{
			{
				APIGroups: []string{"", "apps", "batch"},
				Resources: []string{"*"},
				Verbs:     []string{"*"},
			},
		},
	}
	
	if _, err := tm.clientset.RbacV1().Roles(config.Name).Create(ctx, adminRole, metav1.CreateOptions{}); err != nil {
		return err
	}
	
	// 创建管理员RoleBinding
	for _, admin := range config.Admins {
		binding := &rbacv1.RoleBinding{
			ObjectMeta: metav1.ObjectMeta{
				Name:      fmt.Sprintf("%s-admin-binding", admin),
				Namespace: config.Name,
			},
			Subjects: []rbacv1.Subject{
				{
					Kind:     "User",
					Name:     admin,
					APIGroup: "rbac.authorization.k8s.io",
				},
			},
			RoleRef: rbacv1.RoleRef{
				Kind:     "Role",
				Name:     "tenant-admin",
				APIGroup: "rbac.authorization.k8s.io",
			},
		}
		
		if _, err := tm.clientset.RbacV1().RoleBindings(config.Name).Create(ctx, binding, metav1.CreateOptions{}); err != nil {
			return err
		}
	}
	
	return nil
}

// DeleteTenant 删除租户
func (tm *TenantManager) DeleteTenant(ctx context.Context, tenantName string) error {
	// 删除Namespace会级联删除所有资源
	err := tm.clientset.CoreV1().Namespaces().Delete(ctx, tenantName, metav1.DeleteOptions{})
	if err != nil {
		return fmt.Errorf("failed to delete namespace: %v", err)
	}
	
	log.Printf("Successfully deleted tenant: %s", tenantName)
	return nil
}

// GetTenantResourceUsage 获取租户资源使用情况
func (tm *TenantManager) GetTenantResourceUsage(ctx context.Context, tenantName string) (*ResourceUsage, error) {
	// 获取ResourceQuota
	quota, err := tm.clientset.CoreV1().ResourceQuotas(tenantName).Get(ctx, fmt.Sprintf("%s-quota", tenantName), metav1.GetOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get resource quota: %v", err)
	}
	
	usage := &ResourceUsage{
		TenantName: tenantName,
		CPU: ResourceMetric{
			Used:  quota.Status.Used[corev1.ResourceRequestsCPU],
			Limit: quota.Status.Hard[corev1.ResourceRequestsCPU],
		},
		Memory: ResourceMetric{
			Used:  quota.Status.Used[corev1.ResourceRequestsMemory],
			Limit: quota.Status.Hard[corev1.ResourceRequestsMemory],
		},
		Storage: ResourceMetric{
			Used:  quota.Status.Used[corev1.ResourceRequestsStorage],
			Limit: quota.Status.Hard[corev1.ResourceRequestsStorage],
		},
		Pods: ObjectMetric{
			Used:  int(quota.Status.Used.Pods().Value()),
			Limit: int(quota.Status.Hard.Pods().Value()),
		},
	}
	
	return usage, nil
}

// ResourceUsage 资源使用情况
type ResourceUsage struct {
	TenantName string
	CPU        ResourceMetric
	Memory     ResourceMetric
	Storage    ResourceMetric
	Pods       ObjectMetric
}

// ResourceMetric 资源指标
type ResourceMetric struct {
	Used  resource.Quantity
	Limit resource.Quantity
}

// ObjectMetric 对象指标
type ObjectMetric struct {
	Used  int
	Limit int
}

// 使用示例
func main() {
	ctx := context.Background()
	
	// 创建租户管理器
	manager, err := NewTenantManager("/path/to/kubeconfig")
	if err != nil {
		log.Fatalf("Failed to create tenant manager: %v", err)
	}
	
	// 配置租户
	config := &TenantConfig{
		Name:          "tenant-alpha",
		Environment:   "production",
		CostCenter:    "engineering",
		Contact:       "alpha-team@example.com",
		CPURequest:    "100",
		MemoryRequest: "200Gi",
		CPULimit:      "200",
		MemoryLimit:   "400Gi",
		StorageQuota:  "1Ti",
		MaxPods:       500,
		MaxServices:   100,
		MaxPVCs:       50,
		Admins:        []string{"alice@example.com"},
		Developers:    []string{"alpha-developers"},
		Viewers:       []string{"alpha-viewers"},
	}
	
	// 创建租户
	if err := manager.CreateTenant(ctx, config); err != nil {
		log.Fatalf("Failed to create tenant: %v", err)
	}
	
	// 获取资源使用情况
	usage, err := manager.GetTenantResourceUsage(ctx, "tenant-alpha")
	if err != nil {
		log.Fatalf("Failed to get resource usage: %v", err)
	}
	
	fmt.Printf("Tenant: %s\n", usage.TenantName)
	fmt.Printf("CPU: %s / %s\n", usage.CPU.Used.String(), usage.CPU.Limit.String())
	fmt.Printf("Memory: %s / %s\n", usage.Memory.Used.String(), usage.Memory.Limit.String())
	fmt.Printf("Pods: %d / %d\n", usage.Pods.Used, usage.Pods.Limit)
}
```

**租户隔离验证**

```bash
#!/bin/bash
# tenant-isolation-test.sh
# 验证租户隔离的有效性

set -e

TENANT_NAME="tenant-alpha"
TEST_USER="alice@example.com"

echo "=== 租户隔离验证测试 ==="

# 1. 验证Namespace隔离
echo "1. 验证Namespace隔离..."
kubectl get ns $TENANT_NAME
kubectl describe ns $TENANT_NAME

# 2. 验证ResourceQuota
echo "2. 验证ResourceQuota..."
kubectl get resourcequota -n $TENANT_NAME
kubectl describe resourcequota -n $TENANT_NAME

# 3. 验证LimitRange
echo "3. 验证LimitRange..."
kubectl get limitrange -n $TENANT_NAME
kubectl describe limitrange -n $TENANT_NAME

# 4. 验证NetworkPolicy
echo "4. 验证NetworkPolicy..."
kubectl get networkpolicy -n $TENANT_NAME
kubectl describe networkpolicy -n $TENANT_NAME

# 5. 验证RBAC权限
echo "5. 验证RBAC权限..."
kubectl get role,rolebinding -n $TENANT_NAME

# 6. 测试跨Namespace访问（应该被拒绝）
echo "6. 测试跨Namespace访问..."
kubectl auth can-i get pods -n default --as=$TEST_USER
kubectl auth can-i get pods -n $TENANT_NAME --as=$TEST_USER

# 7. 测试资源配额限制
echo "7. 测试资源配额限制..."
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: quota-test
  namespace: $TENANT_NAME
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    resources:
      requests:
        cpu: "10000"  # 超过配额
        memory: "10000Gi"
EOF

# 8. 测试网络隔离
echo "8. 测试网络隔离..."
kubectl run test-pod -n $TENANT_NAME --image=busybox --rm -it -- sh -c "
  # 测试同Namespace内访问
  wget -O- http://service-in-same-ns:8080
  
  # 测试跨Namespace访问（应该被拒绝）
  wget -O- http://service-in-other-ns.other-namespace:8080
"

echo "=== 验证完成 ==="
```

**最佳实践总结**

```yaml
# Namespace级别资源隔离的最佳实践

1. Namespace命名规范:
   格式: <team>-<environment>-<purpose>
   示例:
     - alpha-prod-web
     - beta-dev-api
     - platform-staging-db

2. ResourceQuota设置原则:
   - 根据团队规模和应用需求设置合理配额
   - 预留20-30%的缓冲空间
   - 定期审查和调整配额
   - 使用PriorityClass区分关键和非关键工作负载

3. LimitRange配置建议:
   - 设置合理的默认值，避免资源浪费
   - 限制单个容器的最大资源，防止资源垄断
   - 设置合理的limit/request比率（建议2-4倍）

4. NetworkPolicy策略:
   - 默认拒绝所有流量
   - 显式允许必要的通信
   - 使用标签选择器而非IP地址
   - 定期审查和更新策略

5. RBAC权限管理:
   - 遵循最小权限原则
   - 使用Role而非ClusterRole
   - 定期审计权限使用情况
   - 使用ServiceAccount而非用户凭证

6. 监控和告警:
   - 监控资源配额使用率
   - 设置配额使用率告警（如80%、90%）
   - 监控NetworkPolicy拒绝的连接
   - 审计RBAC权限变更

7. 自动化管理:
   - 使用GitOps管理租户配置
   - 自动化租户创建和删除流程
   - 定期清理未使用的资源
   - 使用Admission Webhook强制策略
```



### 10.1.3 虚拟集群技术

**虚拟集群概述**

虚拟集群（Virtual Cluster）是一种在物理Kubernetes集群之上创建逻辑隔离集群的技术。每个虚拟集群拥有独立的控制平面（API Server、Controller Manager、Scheduler），但共享底层的计算、存储和网络资源。

**虚拟集群的优势**：

```yaml
# 虚拟集群 vs 传统Namespace隔离

传统Namespace隔离:
  优点:
    - 实现简单
    - 资源开销小
    - 原生Kubernetes支持
  
  缺点:
    - 隔离强度有限
    - 共享控制平面
    - 无法隔离CRD
    - 集群级资源冲突
    - 版本升级影响所有租户

虚拟集群:
  优点:
    - 强隔离（独立控制平面）
    - 完全的API兼容性
    - 独立的CRD和集群资源
    - 独立升级和配置
    - 更好的安全性
  
  缺点:
    - 资源开销较大
    - 实现复杂
    - 需要额外工具支持
```

**虚拟集群架构**

```yaml
# 虚拟集群架构设计

物理集群（Host Cluster）:
  角色: 提供底层基础设施
  组件:
    - 物理节点
    - 存储系统
    - 网络系统
    - 基础控制平面

虚拟集群（Virtual Cluster）:
  角色: 为租户提供独立的Kubernetes环境
  组件:
    - 虚拟API Server
    - 虚拟Controller Manager
    - 虚拟Scheduler
    - 虚拟etcd（可选）
  
  资源映射:
    虚拟资源 -> 物理资源:
      - vPod -> Pod（在host namespace中）
      - vService -> Service（带前缀）
      - vPVC -> PVC（带前缀）
      - vNode -> 虚拟节点（映射到物理节点）

同步机制:
  下行同步（Virtual -> Host）:
    - Pod规格
    - Service定义
    - PVC请求
    - ConfigMap/Secret
  
  上行同步（Host -> Virtual）:
    - Pod状态
    - Service端点
    - PVC绑定状态
    - 事件信息
```

**vcluster实战**

vcluster是最流行的开源虚拟集群解决方案，由Loft Labs开发。

```yaml
# vcluster-values.yaml
# vcluster配置文件

# 虚拟集群基本配置
vcluster:
  # 镜像配置
  image: rancher/k3s:v1.28.2-k3s1
  
  # 资源限制
  resources:
    limits:
      cpu: "2"
      memory: 4Gi
    requests:
      cpu: "1"
      memory: 2Gi
  
  # 持久化存储
  storage:
    size: 10Gi
    className: fast-ssd
  
  # 高可用配置
  replicas: 3
  
  # 额外参数
  extraArgs:
    - --service-cidr=10.96.0.0/16
    - --cluster-cidr=10.244.0.0/16

# 同步配置
sync:
  # 同步的资源类型
  services:
    enabled: true
  configmaps:
    enabled: true
    all: false  # 只同步被Pod使用的ConfigMap
  secrets:
    enabled: true
    all: false  # 只同步被Pod使用的Secret
  endpoints:
    enabled: true
  pods:
    enabled: true
    ephemeralContainers: true
    status: true
  events:
    enabled: true
  persistentvolumeclaims:
    enabled: true
  ingresses:
    enabled: true
  storageclasses:
    enabled: true
  priorityclasses:
    enabled: false
  networkpolicies:
    enabled: true
  volumesnapshots:
    enabled: false
  poddisruptionbudgets:
    enabled: false
  serviceaccounts:
    enabled: true

# 网络配置
networking:
  # 使用host网络
  replicateServices:
    toHost:
      - from: default/*
        to: vcluster-{{ .Release.Name }}-*
  
  # DNS配置
  advanced:
    clusterDomain: "cluster.local"
    fallbackHostCluster: true

# RBAC配置
rbac:
  clusterRole:
    create: true
  role:
    create: true

# 隔离配置
isolation:
  enabled: true
  namespace: vcluster-{{ .Release.Name }}
  
  # Pod安全策略
  podSecurityStandard: baseline
  
  # 资源配额
  resourceQuota:
    enabled: true
    quota:
      requests.cpu: "10"
      requests.memory: 20Gi
      requests.storage: 100Gi
      persistentvolumeclaims: "20"
      services.loadbalancers: "5"
  
  # 限制范围
  limitRange:
    enabled: true
    default:
      cpu: "1"
      memory: 1Gi
    defaultRequest:
      cpu: "100m"
      memory: 128Mi

# 监控配置
monitoring:
  serviceMonitor:
    enabled: true
    labels:
      prometheus: kube-prometheus

# 备份配置
backup:
  enabled: true
  schedule: "0 2 * * *"
  retention: 7
```

**部署vcluster**

```bash
#!/bin/bash
# deploy-vcluster.sh
# 部署虚拟集群

set -e

VCLUSTER_NAME="tenant-alpha-vcluster"
NAMESPACE="vcluster-alpha"
KUBECONFIG_OUTPUT="./kubeconfig-${VCLUSTER_NAME}.yaml"

echo "=== 部署虚拟集群 ==="

# 1. 安装vcluster CLI
echo "1. 安装vcluster CLI..."
curl -L -o vcluster "https://github.com/loft-sh/vcluster/releases/latest/download/vcluster-linux-amd64"
chmod +x vcluster
sudo mv vcluster /usr/local/bin/

# 2. 创建Namespace
echo "2. 创建Namespace..."
kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

# 3. 部署vcluster
echo "3. 部署vcluster..."
vcluster create $VCLUSTER_NAME   --namespace $NAMESPACE   --values vcluster-values.yaml   --connect=false

# 4. 等待vcluster就绪
echo "4. 等待vcluster就绪..."
kubectl wait --for=condition=ready pod   -l app=$VCLUSTER_NAME   -n $NAMESPACE   --timeout=300s

# 5. 获取kubeconfig
echo "5. 获取kubeconfig..."
vcluster connect $VCLUSTER_NAME   --namespace $NAMESPACE   --update-current=false   --print > $KUBECONFIG_OUTPUT

echo "虚拟集群已部署！"
echo "Kubeconfig文件: $KUBECONFIG_OUTPUT"
echo ""
echo "使用以下命令访问虚拟集群:"
echo "export KUBECONFIG=$KUBECONFIG_OUTPUT"
echo "kubectl get nodes"

# 6. 验证虚拟集群
echo ""
echo "6. 验证虚拟集群..."
export KUBECONFIG=$KUBECONFIG_OUTPUT
kubectl cluster-info
kubectl get nodes
kubectl get namespaces

echo "=== 部署完成 ==="
```

**虚拟集群管理**

```go
// vcluster-manager.go
// 虚拟集群管理工具

package main

import (
	"context"
	"fmt"
	"log"
	"os/exec"
	"time"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
)

// VClusterConfig 虚拟集群配置
type VClusterConfig struct {
	Name           string
	Namespace      string
	K8sVersion     string
	Replicas       int32
	CPULimit       string
	MemoryLimit    string
	StorageSize    string
	StorageClass   string
	EnableBackup   bool
	EnableMonitoring bool
}

// VClusterManager 虚拟集群管理器
type VClusterManager struct {
	clientset *kubernetes.Clientset
}

// NewVClusterManager 创建虚拟集群管理器
func NewVClusterManager(kubeconfig string) (*VClusterManager, error) {
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create clientset: %v", err)
	}
	
	return &VClusterManager{clientset: clientset}, nil
}

// CreateVCluster 创建虚拟集群
func (vm *VClusterManager) CreateVCluster(ctx context.Context, config *VClusterConfig) error {
	log.Printf("Creating virtual cluster: %s", config.Name)
	
	// 1. 创建Namespace
	if err := vm.createNamespace(ctx, config); err != nil {
		return fmt.Errorf("failed to create namespace: %v", err)
	}
	
	// 2. 生成Helm values
	valuesFile, err := vm.generateHelmValues(config)
	if err != nil {
		return fmt.Errorf("failed to generate helm values: %v", err)
	}
	
	// 3. 使用vcluster CLI创建
	cmd := exec.Command("vcluster", "create", config.Name,
		"--namespace", config.Namespace,
		"--values", valuesFile,
		"--connect=false",
	)
	
	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("failed to create vcluster: %v, output: %s", err, output)
	}
	
	log.Printf("Virtual cluster created: %s", config.Name)
	
	// 4. 等待就绪
	if err := vm.waitForReady(ctx, config); err != nil {
		return fmt.Errorf("vcluster not ready: %v", err)
	}
	
	log.Printf("Virtual cluster is ready: %s", config.Name)
	return nil
}

// createNamespace 创建Namespace
func (vm *VClusterManager) createNamespace(ctx context.Context, config *VClusterConfig) error {
	ns := &corev1.Namespace{
		ObjectMeta: metav1.ObjectMeta{
			Name: config.Namespace,
			Labels: map[string]string{
				"vcluster": config.Name,
				"type":     "virtual-cluster",
			},
		},
	}
	
	_, err := vm.clientset.CoreV1().Namespaces().Create(ctx, ns, metav1.CreateOptions{})
	if err != nil {
		// 忽略已存在错误
		return nil
	}
	
	return nil
}

// generateHelmValues 生成Helm values文件
func (vm *VClusterManager) generateHelmValues(config *VClusterConfig) (string, error) {
	// 这里简化实现，实际应该生成完整的values.yaml
	valuesContent := fmt.Sprintf(`
vcluster:
  image: rancher/k3s:%s
  replicas: %d
  resources:
    limits:
      cpu: %s
      memory: %s
  storage:
    size: %s
    className: %s

isolation:
  enabled: true
  namespace: %s

monitoring:
  serviceMonitor:
    enabled: %t

backup:
  enabled: %t
`, config.K8sVersion, config.Replicas, config.CPULimit, config.MemoryLimit,
		config.StorageSize, config.StorageClass, config.Namespace,
		config.EnableMonitoring, config.EnableBackup)
	
	// 写入临时文件
	tmpFile := fmt.Sprintf("/tmp/vcluster-values-%s.yaml", config.Name)
	// 实际实现应该写入文件
	log.Printf("Generated values file: %s", tmpFile)
	
	return tmpFile, nil
}

// waitForReady 等待虚拟集群就绪
func (vm *VClusterManager) waitForReady(ctx context.Context, config *VClusterConfig) error {
	timeout := time.After(5 * time.Minute)
	ticker := time.NewTicker(10 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-timeout:
			return fmt.Errorf("timeout waiting for vcluster to be ready")
		case <-ticker.C:
			// 检查Pod状态
			pods, err := vm.clientset.CoreV1().Pods(config.Namespace).List(ctx, metav1.ListOptions{
				LabelSelector: fmt.Sprintf("app=%s", config.Name),
			})
			if err != nil {
				log.Printf("Error checking pod status: %v", err)
				continue
			}
			
			if len(pods.Items) == 0 {
				log.Printf("Waiting for pods to be created...")
				continue
			}
			
			allReady := true
			for _, pod := range pods.Items {
				if pod.Status.Phase != corev1.PodRunning {
					allReady = false
					break
				}
			}
			
			if allReady {
				return nil
			}
			
			log.Printf("Waiting for all pods to be ready...")
		}
	}
}

// DeleteVCluster 删除虚拟集群
func (vm *VClusterManager) DeleteVCluster(ctx context.Context, name, namespace string) error {
	log.Printf("Deleting virtual cluster: %s", name)
	
	cmd := exec.Command("vcluster", "delete", name,
		"--namespace", namespace,
	)
	
	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("failed to delete vcluster: %v, output: %s", err, output)
	}
	
	log.Printf("Virtual cluster deleted: %s", name)
	return nil
}

// GetVClusterKubeconfig 获取虚拟集群的kubeconfig
func (vm *VClusterManager) GetVClusterKubeconfig(name, namespace string) (string, error) {
	cmd := exec.Command("vcluster", "connect", name,
		"--namespace", namespace,
		"--update-current=false",
		"--print",
	)
	
	output, err := cmd.Output()
	if err != nil {
		return "", fmt.Errorf("failed to get kubeconfig: %v", err)
	}
	
	return string(output), nil
}

// ListVClusters 列出所有虚拟集群
func (vm *VClusterManager) ListVClusters(ctx context.Context) ([]VClusterInfo, error) {
	// 查找所有vcluster相关的Namespace
	namespaces, err := vm.clientset.CoreV1().Namespaces().List(ctx, metav1.ListOptions{
		LabelSelector: "type=virtual-cluster",
	})
	if err != nil {
		return nil, fmt.Errorf("failed to list namespaces: %v", err)
	}
	
	var vclusters []VClusterInfo
	for _, ns := range namespaces.Items {
		vclusterName := ns.Labels["vcluster"]
		if vclusterName == "" {
			continue
		}
		
		// 获取Pod状态
		pods, err := vm.clientset.CoreV1().Pods(ns.Name).List(ctx, metav1.ListOptions{
			LabelSelector: fmt.Sprintf("app=%s", vclusterName),
		})
		if err != nil {
			log.Printf("Error getting pods for vcluster %s: %v", vclusterName, err)
			continue
		}
		
		status := "Unknown"
		if len(pods.Items) > 0 {
			allRunning := true
			for _, pod := range pods.Items {
				if pod.Status.Phase != corev1.PodRunning {
					allRunning = false
					break
				}
			}
			if allRunning {
				status = "Running"
			} else {
				status = "Pending"
			}
		}
		
		vclusters = append(vclusters, VClusterInfo{
			Name:      vclusterName,
			Namespace: ns.Name,
			Status:    status,
			Age:       time.Since(ns.CreationTimestamp.Time).String(),
		})
	}
	
	return vclusters, nil
}

// VClusterInfo 虚拟集群信息
type VClusterInfo struct {
	Name      string
	Namespace string
	Status    string
	Age       string
}

// 使用示例
func main() {
	ctx := context.Background()
	
	// 创建管理器
	manager, err := NewVClusterManager("/path/to/kubeconfig")
	if err != nil {
		log.Fatalf("Failed to create manager: %v", err)
	}
	
	// 配置虚拟集群
	config := &VClusterConfig{
		Name:             "tenant-alpha-vcluster",
		Namespace:        "vcluster-alpha",
		K8sVersion:       "v1.28.2-k3s1",
		Replicas:         3,
		CPULimit:         "2",
		MemoryLimit:      "4Gi",
		StorageSize:      "10Gi",
		StorageClass:     "fast-ssd",
		EnableBackup:     true,
		EnableMonitoring: true,
	}
	
	// 创建虚拟集群
	if err := manager.CreateVCluster(ctx, config); err != nil {
		log.Fatalf("Failed to create vcluster: %v", err)
	}
	
	// 获取kubeconfig
	kubeconfig, err := manager.GetVClusterKubeconfig(config.Name, config.Namespace)
	if err != nil {
		log.Fatalf("Failed to get kubeconfig: %v", err)
	}
	fmt.Printf("Kubeconfig:\n%s\n", kubeconfig)
	
	// 列出所有虚拟集群
	vclusters, err := manager.ListVClusters(ctx)
	if err != nil {
		log.Fatalf("Failed to list vclusters: %v", err)
	}
	
	fmt.Println("\nVirtual Clusters:")
	for _, vc := range vclusters {
		fmt.Printf("- %s (namespace: %s, status: %s, age: %s)\n",
			vc.Name, vc.Namespace, vc.Status, vc.Age)
	}
}
```

**其他虚拟集群方案**

```yaml
# 虚拟集群技术对比

1. vcluster (Loft Labs):
   架构: 在Pod中运行完整的K3s/K8s
   优势:
     - 开源免费
     - 轻量级（基于K3s）
     - 易于部署和管理
     - 良好的资源隔离
   劣势:
     - 资源开销相对较大
     - 需要额外的同步机制
   适用场景:
     - 开发测试环境
     - 多租户SaaS平台
     - CI/CD流水线

2. Kamaji:
   架构: 托管控制平面，共享数据平面
   优势:
     - 控制平面完全隔离
     - 支持多种etcd后端
     - 更好的性能
     - 企业级特性
   劣势:
     - 实现复杂
     - 需要额外的基础设施
   适用场景:
     - 大规模多租户
     - 云服务提供商
     - 企业私有云

3. Cluster API vCluster Provider:
   架构: 基于Cluster API的虚拟集群
   优势:
     - 标准化的集群管理
     - 与Cluster API生态集成
     - 声明式管理
   劣势:
     - 学习曲线陡峭
     - 依赖Cluster API
   适用场景:
     - 已使用Cluster API的环境
     - 需要统一管理物理和虚拟集群

4. kcp (Kubernetes Control Plane):
   架构: 轻量级控制平面，无节点概念
   优势:
     - 极致轻量
     - 快速启动
     - 适合大规模场景
   劣势:
     - 仍在早期开发阶段
     - 生态不完善
   适用场景:
     - 实验性项目
     - 超大规模多租户
```

**虚拟集群网络配置**

```yaml
# vcluster-network-config.yaml
# 虚拟集群网络配置示例

apiVersion: v1
kind: ConfigMap
metadata:
  name: vcluster-network-config
  namespace: vcluster-alpha
data:
  # 网络模式配置
  network-mode: |
    # 模式1: 默认模式（推荐）
    # Pod使用host集群的网络，Service通过同步实现
    mode: default
    
    # 模式2: 独立网络
    # 使用CNI插件创建独立网络
    mode: isolated
    cni:
      plugin: calico
      podCIDR: 10.100.0.0/16
      serviceCIDR: 10.101.0.0/16
    
    # 模式3: 混合模式
    # 部分资源使用独立网络，部分共享
    mode: hybrid
    isolated:
      - namespace: production
      - namespace: staging
    shared:
      - namespace: development

  # Service同步配置
  service-sync: |
    # 从虚拟集群同步到host集群
    toHost:
      - from: "*/nginx-*"
        to: "vcluster-alpha-nginx-*"
      - from: "production/*"
        to: "vcluster-alpha-prod-*"
    
    # 从host集群同步到虚拟集群
    fromHost:
      - from: "shared-services/*"
        to: "external/*"

  # DNS配置
  dns-config: |
    # 虚拟集群DNS设置
    clusterDomain: vcluster.local
    
    # DNS转发规则
    forwarding:
      # 转发到host集群DNS
      - domain: "*.svc.cluster.local"
        nameserver: 10.96.0.10
      
      # 转发到外部DNS
      - domain: "*.example.com"
        nameserver: 8.8.8.8

  # Ingress配置
  ingress-config: |
    # Ingress同步策略
    sync:
      enabled: true
      # 自动添加前缀避免冲突
      hostnamePrefix: "vcluster-alpha-"
      
      # TLS证书同步
      tlsSecrets:
        enabled: true
        namespace: cert-manager
    
    # Ingress Controller配置
    controller:
      # 使用host集群的Ingress Controller
      useHost: true
      # 或部署独立的Ingress Controller
      deploy: false

---
# NetworkPolicy for vcluster
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: vcluster-network-policy
  namespace: vcluster-alpha
spec:
  podSelector:
    matchLabels:
      app: tenant-alpha-vcluster
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  # 允许来自同namespace的流量
  - from:
    - podSelector: {}
  
  # 允许来自host集群API Server的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 8443
  
  egress:
  # 允许访问host集群API Server
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 6443
  
  # 允许DNS查询
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  
  # 允许访问同步的Pod
  - to:
    - podSelector: {}
```

**虚拟集群监控和日志**

```yaml
# vcluster-monitoring.yaml
# 虚拟集群监控配置

---
# ServiceMonitor for Prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vcluster-metrics
  namespace: vcluster-alpha
  labels:
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: tenant-alpha-vcluster
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---
# PrometheusRule for alerting
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: vcluster-alerts
  namespace: vcluster-alpha
spec:
  groups:
  - name: vcluster
    interval: 30s
    rules:
    # vcluster Pod不健康告警
    - alert: VClusterPodNotReady
      expr: |
        kube_pod_status_phase{namespace="vcluster-alpha",phase!="Running"} > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "vcluster Pod不健康"
        description: "虚拟集群 {{ $labels.pod }} 状态异常"
    
    # vcluster API Server延迟告警
    - alert: VClusterAPIServerHighLatency
      expr: |
        histogram_quantile(0.99, 
          rate(apiserver_request_duration_seconds_bucket{namespace="vcluster-alpha"}[5m])
        ) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "vcluster API Server延迟过高"
        description: "P99延迟: {{ $value }}s"
    
    # vcluster etcd存储告警
    - alert: VClusterEtcdStorageHigh
      expr: |
        (etcd_mvcc_db_total_size_in_bytes{namespace="vcluster-alpha"} 
         / etcd_server_quota_backend_bytes{namespace="vcluster-alpha"}) > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "vcluster etcd存储使用率过高"
        description: "存储使用率: {{ $value | humanizePercentage }}"

---
# Grafana Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: vcluster-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  vcluster-dashboard.json: |
    {
      "dashboard": {
        "title": "Virtual Cluster Monitoring",
        "panels": [
          {
            "title": "vcluster Pod状态",
            "targets": [
              {
                "expr": "kube_pod_status_phase{namespace=~"vcluster-.*"}"
              }
            ]
          },
          {
            "title": "API Server请求率",
            "targets": [
              {
                "expr": "rate(apiserver_request_total{namespace=~"vcluster-.*"}[5m])"
              }
            ]
          },
          {
            "title": "资源使用情况",
            "targets": [
              {
                "expr": "container_memory_usage_bytes{namespace=~"vcluster-.*"}"
              },
              {
                "expr": "rate(container_cpu_usage_seconds_total{namespace=~"vcluster-.*"}[5m])"
              }
            ]
          }
        ]
      }
    }
```

**虚拟集群最佳实践**

```yaml
# 虚拟集群最佳实践总结

1. 资源规划:
   控制平面资源:
     - 小型vcluster（<50 nodes）: 1 CPU, 2Gi内存
     - 中型vcluster（50-200 nodes）: 2 CPU, 4Gi内存
     - 大型vcluster（>200 nodes）: 4 CPU, 8Gi内存
   
   存储规划:
     - etcd存储: 至少10Gi，建议使用SSD
     - 日志存储: 根据日志量规划
     - 备份存储: etcd大小的3-5倍
   
   网络规划:
     - 预留足够的IP地址空间
     - 避免CIDR冲突
     - 考虑跨集群通信需求

2. 高可用配置:
   控制平面:
     - 至少3个副本
     - 使用Pod反亲和性分散到不同节点
     - 配置PodDisruptionBudget
   
   etcd:
     - 使用持久化存储
     - 定期备份
     - 监控存储使用率
   
   网络:
     - 配置健康检查
     - 使用Service Mesh提高可靠性
     - 实施重试和超时策略

3. 安全加固:
   网络隔离:
     - 使用NetworkPolicy限制流量
     - 启用mTLS加密
     - 限制对host集群的访问
   
   访问控制:
     - 为每个vcluster配置独立的RBAC
     - 使用ServiceAccount而非用户凭证
     - 定期轮换证书和密钥
   
   审计:
     - 启用API Server审计日志
     - 监控异常访问模式
     - 定期安全扫描

4. 性能优化:
   API Server:
     - 调整--max-requests-inflight参数
     - 启用API优先级和公平性
     - 使用缓存减少etcd负载
   
   etcd:
     - 使用SSD存储
     - 调整--quota-backend-bytes
     - 定期压缩和碎片整理
   
   同步优化:
     - 只同步必要的资源
     - 使用标签选择器过滤
     - 调整同步间隔

5. 运维管理:
   监控:
     - 监控控制平面健康状态
     - 跟踪资源使用趋势
     - 设置合理的告警阈值
   
   备份恢复:
     - 定期备份etcd数据
     - 测试恢复流程
     - 保留多个备份版本
   
   升级策略:
     - 先在测试环境验证
     - 使用滚动升级
     - 准备回滚方案
   
   成本优化:
     - 合理设置资源限制
     - 使用节点亲和性优化调度
     - 定期清理未使用的vcluster

6. 故障排查:
   常见问题:
     - Pod无法启动: 检查资源配额和镜像
     - 网络不通: 验证NetworkPolicy和DNS
     - 性能下降: 检查etcd和API Server负载
   
   调试工具:
     - kubectl logs: 查看控制平面日志
     - kubectl exec: 进入容器调试
     - vcluster connect: 连接到虚拟集群
     - kubectl describe: 查看资源详情
   
   日志收集:
     - 收集控制平面日志
     - 收集同步器日志
     - 收集host集群相关日志
```

**虚拟集群使用场景**

```yaml
# 虚拟集群典型使用场景

场景1: 多租户SaaS平台
  需求:
    - 为每个客户提供独立的Kubernetes环境
    - 完全的资源和API隔离
    - 独立的版本和配置
  
  方案:
    - 每个客户一个vcluster
    - 使用ResourceQuota限制资源
    - 配置独立的Ingress和域名
    - 实施严格的网络隔离
  
  优势:
    - 强隔离保证安全性
    - 客户可以自由管理资源
    - 降低运维复杂度

场景2: 开发测试环境
  需求:
    - 快速创建和销毁环境
    - 与生产环境隔离
    - 支持多个并行测试
  
  方案:
    - 为每个分支/PR创建临时vcluster
    - 测试完成后自动清理
    - 使用较小的资源配额
    - 共享镜像仓库和存储
  
  优势:
    - 快速环境准备
    - 完全隔离避免干扰
    - 节省成本

场景3: CI/CD流水线
  需求:
    - 并行执行多个构建任务
    - 隔离不同的构建环境
    - 自动化管理
  
  方案:
    - 为每个构建任务创建临时vcluster
    - 使用GitOps管理配置
    - 集成到CI/CD工具链
    - 构建完成后自动清理
  
  优势:
    - 提高并行度
    - 避免环境污染
    - 可重复的构建环境

场景4: 培训和演示
  需求:
    - 为学员提供独立环境
    - 快速重置环境
    - 限制资源使用
  
  方案:
    - 为每个学员创建vcluster
    - 预配置常用工具和应用
    - 设置资源配额防止滥用
    - 定期清理和重置
  
  优势:
    - 学员有完整的管理权限
    - 互不干扰
    - 易于管理和重置
```



### 10.1.4 多租户最佳实践

**多租户架构选型决策树**

```yaml
# 多租户方案选择指南

决策因素:
  1. 租户信任级别
  2. 隔离强度要求
  3. 成本预算
  4. 运维复杂度
  5. 性能要求
  6. 合规性要求

决策流程:
  问题1: 租户是否来自不同组织？
    是 -> 考虑硬多租户或虚拟集群
    否 -> 继续问题2
  
  问题2: 是否需要独立的Kubernetes版本？
    是 -> 使用虚拟集群
    否 -> 继续问题3
  
  问题3: 是否需要隔离CRD和集群资源？
    是 -> 使用虚拟集群
    否 -> 继续问题4
  
  问题4: 预算是否充足？
    是 -> 使用虚拟集群或独立集群
    否 -> 使用Namespace隔离
  
  问题5: 是否有严格的合规要求？
    是 -> 使用独立集群
    否 -> 根据前面的答案选择方案

推荐方案:
  场景A - 企业内部团队:
    方案: Namespace + RBAC + ResourceQuota
    理由: 成本低，管理简单，隔离足够
  
  场景B - 多客户SaaS:
    方案: 虚拟集群（vcluster）
    理由: 强隔离，独立管理，成本可控
  
  场景C - 金融/医疗等高合规行业:
    方案: 独立物理集群
    理由: 最强隔离，满足合规要求
  
  场景D - 开发测试环境:
    方案: 虚拟集群（临时）
    理由: 快速创建销毁，完全隔离
```

**多租户安全加固**

```yaml
# multi-tenant-security.yaml
# 多租户安全加固配置

---
# PodSecurityPolicy（已废弃，使用Pod Security Standards）
# Pod Security Standards配置
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-alpha
  labels:
    # 设置Pod安全标准
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
spec: {}

---
# 使用Gatekeeper/OPA实施策略
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        openAPIV3Schema:
          type: object
          properties:
            labels:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels
        
        violation[{"msg": msg, "details": {"missing_labels": missing}}] {
          provided := {label | input.review.object.metadata.labels[label]}
          required := {label | label := input.parameters.labels[_]}
          missing := required - provided
          count(missing) > 0
          msg := sprintf("必须包含标签: %v", [missing])
        }

---
# 应用策略：要求所有Pod必须有租户标签
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: require-tenant-label
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - tenant-alpha
      - tenant-beta
  parameters:
    labels:
      - "tenant"
      - "environment"
      - "cost-center"

---
# 限制容器镜像来源
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8sallowedrepos
spec:
  crd:
    spec:
      names:
        kind: K8sAllowedRepos
      validation:
        openAPIV3Schema:
          type: object
          properties:
            repos:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sallowedrepos
        
        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          satisfied := [good | repo = input.parameters.repos[_]
                              good = startswith(container.image, repo)]
          not any(satisfied)
          msg := sprintf("容器镜像 %v 不在允许的仓库列表中", [container.image])
        }

---
# 应用策略：限制镜像仓库
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sAllowedRepos
metadata:
  name: allowed-repos
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - tenant-alpha
  parameters:
    repos:
      - "registry.example.com/"
      - "docker.io/library/"

---
# 禁止特权容器
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8spsprivilegedcontainer
spec:
  crd:
    spec:
      names:
        kind: K8sPSPrivilegedContainer
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8spsprivileged
        
        violation[{"msg": msg}] {
          c := input_containers[_]
          c.securityContext.privileged
          msg := sprintf("禁止使用特权容器: %v", [c.name])
        }
        
        input_containers[c] {
          c := input.review.object.spec.containers[_]
        }
        
        input_containers[c] {
          c := input.review.object.spec.initContainers[_]
        }

---
# 应用策略：禁止特权容器
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sPSPrivilegedContainer
metadata:
  name: deny-privileged-containers
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - tenant-alpha
      - tenant-beta
```

**多租户成本管理**

```go
// cost-manager.go
// 多租户成本管理和计费系统

package main

import (
	"context"
	"fmt"
	"log"
	"time"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
	metricsv "k8s.io/metrics/pkg/client/clientset/versioned"
)

// CostManager 成本管理器
type CostManager struct {
	clientset        *kubernetes.Clientset
	metricsClientset *metricsv.Clientset
	
	// 价格配置（每小时）
	cpuPrice     float64 // 每核CPU价格
	memoryPrice  float64 // 每GB内存价格
	storagePrice float64 // 每GB存储价格
}

// TenantCost 租户成本
type TenantCost struct {
	TenantName    string
	Namespace     string
	Period        string
	
	// 资源使用量
	CPUCoreHours    float64
	MemoryGBHours   float64
	StorageGBHours  float64
	
	// 成本
	CPUCost         float64
	MemoryCost      float64
	StorageCost     float64
	TotalCost       float64
	
	// 资源详情
	PodCount        int
	ServiceCount    int
	PVCCount        int
}

// NewCostManager 创建成本管理器
func NewCostManager(kubeconfig string) (*CostManager, error) {
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create clientset: %v", err)
	}
	
	metricsClientset, err := metricsv.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create metrics clientset: %v", err)
	}
	
	return &CostManager{
		clientset:        clientset,
		metricsClientset: metricsClientset,
		cpuPrice:         0.05,  // $0.05/核/小时
		memoryPrice:      0.01,  // $0.01/GB/小时
		storagePrice:     0.001, // $0.001/GB/小时
	}, nil
}

// CalculateTenantCost 计算租户成本
func (cm *CostManager) CalculateTenantCost(ctx context.Context, namespace string, hours float64) (*TenantCost, error) {
	cost := &TenantCost{
		Namespace: namespace,
		Period:    fmt.Sprintf("%.1f hours", hours),
	}
	
	// 1. 获取Pod指标
	podMetrics, err := cm.metricsClientset.MetricsV1beta1().PodMetricses(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get pod metrics: %v", err)
	}
	
	// 计算CPU和内存使用
	var totalCPU, totalMemory float64
	for _, pm := range podMetrics.Items {
		for _, container := range pm.Containers {
			// CPU（毫核转换为核）
			cpuMillis := float64(container.Usage.Cpu().MilliValue())
			totalCPU += cpuMillis / 1000.0
			
			// 内存（字节转换为GB）
			memoryBytes := float64(container.Usage.Memory().Value())
			totalMemory += memoryBytes / (1024 * 1024 * 1024)
		}
	}
	
	cost.CPUCoreHours = totalCPU * hours
	cost.MemoryGBHours = totalMemory * hours
	cost.CPUCost = cost.CPUCoreHours * cm.cpuPrice
	cost.MemoryCost = cost.MemoryGBHours * cm.memoryPrice
	
	// 2. 获取存储使用
	pvcs, err := cm.clientset.CoreV1().PersistentVolumeClaims(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get PVCs: %v", err)
	}
	
	var totalStorage float64
	for _, pvc := range pvcs.Items {
		if pvc.Status.Phase == corev1.ClaimBound {
			storageBytes := float64(pvc.Spec.Resources.Requests.Storage().Value())
			totalStorage += storageBytes / (1024 * 1024 * 1024)
		}
	}
	
	cost.StorageGBHours = totalStorage * hours
	cost.StorageCost = cost.StorageGBHours * cm.storagePrice
	cost.PVCCount = len(pvcs.Items)
	
	// 3. 获取资源数量
	pods, err := cm.clientset.CoreV1().Pods(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get pods: %v", err)
	}
	cost.PodCount = len(pods.Items)
	
	services, err := cm.clientset.CoreV1().Services(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get services: %v", err)
	}
	cost.ServiceCount = len(services.Items)
	
	// 4. 计算总成本
	cost.TotalCost = cost.CPUCost + cost.MemoryCost + cost.StorageCost
	
	// 从Namespace标签获取租户名称
	ns, err := cm.clientset.CoreV1().Namespaces().Get(ctx, namespace, metav1.GetOptions{})
	if err == nil {
		cost.TenantName = ns.Labels["tenant"]
	}
	
	return cost, nil
}

// GenerateCostReport 生成成本报告
func (cm *CostManager) GenerateCostReport(ctx context.Context, tenantLabel string) ([]*TenantCost, error) {
	// 获取所有租户Namespace
	namespaces, err := cm.clientset.CoreV1().Namespaces().List(ctx, metav1.ListOptions{
		LabelSelector: labels.SelectorFromSet(map[string]string{
			"tenant": tenantLabel,
		}).String(),
	})
	if err != nil {
		return nil, fmt.Errorf("failed to list namespaces: %v", err)
	}
	
	var costs []*TenantCost
	for _, ns := range namespaces.Items {
		// 计算过去24小时的成本
		cost, err := cm.CalculateTenantCost(ctx, ns.Name, 24.0)
		if err != nil {
			log.Printf("Failed to calculate cost for %s: %v", ns.Name, err)
			continue
		}
		costs = append(costs, cost)
	}
	
	return costs, nil
}

// ExportCostReport 导出成本报告
func (cm *CostManager) ExportCostReport(costs []*TenantCost) string {
	report := "租户成本报告\n"
	report += "=" + "\n\n"
	
	var totalCost float64
	for _, cost := range costs {
		report += fmt.Sprintf("租户: %s (Namespace: %s)\n", cost.TenantName, cost.Namespace)
		report += fmt.Sprintf("  周期: %s\n", cost.Period)
		report += fmt.Sprintf("  资源:\n")
		report += fmt.Sprintf("    - Pod数量: %d\n", cost.PodCount)
		report += fmt.Sprintf("    - Service数量: %d\n", cost.ServiceCount)
		report += fmt.Sprintf("    - PVC数量: %d\n", cost.PVCCount)
		report += fmt.Sprintf("  使用量:\n")
		report += fmt.Sprintf("    - CPU: %.2f 核·小时\n", cost.CPUCoreHours)
		report += fmt.Sprintf("    - 内存: %.2f GB·小时\n", cost.MemoryGBHours)
		report += fmt.Sprintf("    - 存储: %.2f GB·小时\n", cost.StorageGBHours)
		report += fmt.Sprintf("  成本:\n")
		report += fmt.Sprintf("    - CPU成本: $%.2f\n", cost.CPUCost)
		report += fmt.Sprintf("    - 内存成本: $%.2f\n", cost.MemoryCost)
		report += fmt.Sprintf("    - 存储成本: $%.2f\n", cost.StorageCost)
		report += fmt.Sprintf("    - 总成本: $%.2f\n", cost.TotalCost)
		report += "\n"
		
		totalCost += cost.TotalCost
	}
	
	report += fmt.Sprintf("总计成本: $%.2f\n", totalCost)
	
	return report
}

// SetupCostAlerts 设置成本告警
func (cm *CostManager) SetupCostAlerts(ctx context.Context, namespace string, threshold float64) error {
	// 这里简化实现，实际应该集成到告警系统
	cost, err := cm.CalculateTenantCost(ctx, namespace, 24.0)
	if err != nil {
		return err
	}
	
	if cost.TotalCost > threshold {
		log.Printf("⚠️  成本告警: 租户 %s 的日成本 $%.2f 超过阈值 $%.2f",
			cost.TenantName, cost.TotalCost, threshold)
		// 发送告警通知
		// sendAlert(cost)
	}
	
	return nil
}

// 使用示例
func main() {
	ctx := context.Background()
	
	// 创建成本管理器
	manager, err := NewCostManager("/path/to/kubeconfig")
	if err != nil {
		log.Fatalf("Failed to create cost manager: %v", err)
	}
	
	// 计算单个租户成本
	cost, err := manager.CalculateTenantCost(ctx, "tenant-alpha", 24.0)
	if err != nil {
		log.Fatalf("Failed to calculate cost: %v", err)
	}
	
	fmt.Printf("租户: %s\n", cost.TenantName)
	fmt.Printf("总成本: $%.2f\n", cost.TotalCost)
	
	// 生成所有租户的成本报告
	costs, err := manager.GenerateCostReport(ctx, "")
	if err != nil {
		log.Fatalf("Failed to generate report: %v", err)
	}
	
	report := manager.ExportCostReport(costs)
	fmt.Println(report)
	
	// 设置成本告警
	if err := manager.SetupCostAlerts(ctx, "tenant-alpha", 100.0); err != nil {
		log.Printf("Failed to setup alerts: %v", err)
	}
}
```

**多租户运维自动化**

```yaml
# tenant-operator.yaml
# 使用Operator模式自动化租户管理

apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: tenants.multitenancy.example.com
spec:
  group: multitenancy.example.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                # 租户基本信息
                displayName:
                  type: string
                contact:
                  type: string
                costCenter:
                  type: string
                environment:
                  type: string
                  enum: [development, staging, production]
                
                # 资源配额
                resourceQuota:
                  type: object
                  properties:
                    cpu:
                      type: string
                    memory:
                      type: string
                    storage:
                      type: string
                    pods:
                      type: integer
                
                # 网络配置
                networking:
                  type: object
                  properties:
                    isolation:
                      type: boolean
                    allowedNamespaces:
                      type: array
                      items:
                        type: string
                    externalAccess:
                      type: boolean
                
                # 安全配置
                security:
                  type: object
                  properties:
                    podSecurityStandard:
                      type: string
                      enum: [privileged, baseline, restricted]
                    allowPrivileged:
                      type: boolean
                    allowedRegistries:
                      type: array
                      items:
                        type: string
                
                # 用户和权限
                users:
                  type: array
                  items:
                    type: object
                    properties:
                      email:
                        type: string
                      role:
                        type: string
                        enum: [admin, developer, viewer]
            
            status:
              type: object
              properties:
                phase:
                  type: string
                  enum: [Pending, Active, Terminating, Failed]
                conditions:
                  type: array
                  items:
                    type: object
                    properties:
                      type:
                        type: string
                      status:
                        type: string
                      reason:
                        type: string
                      message:
                        type: string
                      lastTransitionTime:
                        type: string
                        format: date-time
                resourceUsage:
                  type: object
                  properties:
                    cpu:
                      type: string
                    memory:
                      type: string
                    storage:
                      type: string
                    pods:
                      type: integer
  scope: Cluster
  names:
    plural: tenants
    singular: tenant
    kind: Tenant
    shortNames:
    - tn

---
# 租户CR示例
apiVersion: multitenancy.example.com/v1
kind: Tenant
metadata:
  name: alpha-team
spec:
  displayName: "Alpha Team"
  contact: "alpha-team@example.com"
  costCenter: "engineering"
  environment: production
  
  resourceQuota:
    cpu: "100"
    memory: "200Gi"
    storage: "1Ti"
    pods: 500
  
  networking:
    isolation: true
    allowedNamespaces:
      - shared-services
      - monitoring
    externalAccess: true
  
  security:
    podSecurityStandard: baseline
    allowPrivileged: false
    allowedRegistries:
      - "registry.example.com"
      - "docker.io/library"
  
  users:
    - email: "alice@example.com"
      role: admin
    - email: "bob@example.com"
      role: developer
    - email: "charlie@example.com"
      role: viewer
```

**多租户监控和可观测性**

```yaml
# tenant-monitoring.yaml
# 租户级别的监控配置

---
# Prometheus规则：租户资源使用监控
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: tenant-resource-monitoring
  namespace: monitoring
spec:
  groups:
  - name: tenant-resources
    interval: 30s
    rules:
    # CPU配额使用率
    - record: tenant:cpu_quota_usage:ratio
      expr: |
        sum by (namespace) (
          rate(container_cpu_usage_seconds_total{container!=""}[5m])
        ) / on(namespace) group_left()
        kube_resourcequota{resource="requests.cpu", type="hard"}
    
    # 内存配额使用率
    - record: tenant:memory_quota_usage:ratio
      expr: |
        sum by (namespace) (
          container_memory_working_set_bytes{container!=""}
        ) / on(namespace) group_left()
        kube_resourcequota{resource="requests.memory", type="hard"}
    
    # Pod数量配额使用率
    - record: tenant:pod_quota_usage:ratio
      expr: |
        count by (namespace) (
          kube_pod_info
        ) / on(namespace) group_left()
        kube_resourcequota{resource="pods", type="hard"}
    
    # CPU配额使用率告警
    - alert: TenantCPUQuotaHigh
      expr: tenant:cpu_quota_usage:ratio > 0.8
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "租户CPU配额使用率过高"
        description: "租户 {{ $labels.namespace }} 的CPU使用率为 {{ $value | humanizePercentage }}"
    
    # 内存配额使用率告警
    - alert: TenantMemoryQuotaHigh
      expr: tenant:memory_quota_usage:ratio > 0.8
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "租户内存配额使用率过高"
        description: "租户 {{ $labels.namespace }} 的内存使用率为 {{ $value | humanizePercentage }}"
    
    # Pod配额即将耗尽
    - alert: TenantPodQuotaNearLimit
      expr: tenant:pod_quota_usage:ratio > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "租户Pod配额即将耗尽"
        description: "租户 {{ $labels.namespace }} 的Pod使用率为 {{ $value | humanizePercentage }}"

---
# Grafana Dashboard for Tenants
apiVersion: v1
kind: ConfigMap
metadata:
  name: tenant-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  tenant-overview.json: |
    {
      "dashboard": {
        "title": "Multi-Tenant Overview",
        "panels": [
          {
            "title": "租户资源配额使用率",
            "targets": [
              {
                "expr": "tenant:cpu_quota_usage:ratio",
                "legendFormat": "{{ namespace }} - CPU"
              },
              {
                "expr": "tenant:memory_quota_usage:ratio",
                "legendFormat": "{{ namespace }} - Memory"
              }
            ],
            "type": "graph"
          },
          {
            "title": "租户Pod数量",
            "targets": [
              {
                "expr": "count by (namespace) (kube_pod_info)"
              }
            ],
            "type": "graph"
          },
          {
            "title": "租户成本分布",
            "targets": [
              {
                "expr": "sum by (namespace) (tenant_cost_total)"
              }
            ],
            "type": "piechart"
          },
          {
            "title": "租户网络流量",
            "targets": [
              {
                "expr": "sum by (namespace) (rate(container_network_receive_bytes_total[5m]))"
              }
            ],
            "type": "graph"
          }
        ]
      }
    }
```

**多租户故障排查指南**

```bash
#!/bin/bash
# tenant-troubleshooting.sh
# 多租户故障排查工具

set -e

TENANT_NS="$1"

if [ -z "$TENANT_NS" ]; then
    echo "用法: $0 <tenant-namespace>"
    exit 1
fi

echo "=== 租户故障排查: $TENANT_NS ==="
echo ""

# 1. 检查Namespace状态
echo "1. Namespace状态:"
kubectl get ns $TENANT_NS -o yaml | grep -A 5 "status:"
echo ""

# 2. 检查ResourceQuota使用情况
echo "2. ResourceQuota使用情况:"
kubectl describe resourcequota -n $TENANT_NS
echo ""

# 3. 检查LimitRange配置
echo "3. LimitRange配置:"
kubectl describe limitrange -n $TENANT_NS
echo ""

# 4. 检查Pod状态
echo "4. Pod状态:"
kubectl get pods -n $TENANT_NS -o wide
echo ""

# 5. 检查失败的Pod
echo "5. 失败的Pod详情:"
kubectl get pods -n $TENANT_NS --field-selector=status.phase!=Running,status.phase!=Succeeded
for pod in $(kubectl get pods -n $TENANT_NS --field-selector=status.phase!=Running,status.phase!=Succeeded -o jsonpath='{.items[*].metadata.name}'); do
    echo "Pod: $pod"
    kubectl describe pod $pod -n $TENANT_NS | grep -A 10 "Events:"
    echo ""
done

# 6. 检查NetworkPolicy
echo "6. NetworkPolicy配置:"
kubectl get networkpolicy -n $TENANT_NS
echo ""

# 7. 检查RBAC权限
echo "7. RBAC权限:"
kubectl get role,rolebinding -n $TENANT_NS
echo ""

# 8. 检查事件
echo "8. 最近的事件:"
kubectl get events -n $TENANT_NS --sort-by='.lastTimestamp' | tail -20
echo ""

# 9. 检查资源使用情况
echo "9. 实际资源使用:"
kubectl top pods -n $TENANT_NS 2>/dev/null || echo "Metrics Server未安装"
echo ""

# 10. 检查PVC状态
echo "10. PVC状态:"
kubectl get pvc -n $TENANT_NS
echo ""

# 11. 检查Service和Endpoints
echo "11. Service和Endpoints:"
kubectl get svc,endpoints -n $TENANT_NS
echo ""

# 12. 生成诊断报告
echo "12. 生成诊断报告..."
REPORT_FILE="tenant-${TENANT_NS}-diagnostic-$(date +%Y%m%d-%H%M%S).txt"
{
    echo "租户诊断报告: $TENANT_NS"
    echo "生成时间: $(date)"
    echo "="
    echo ""
    
    echo "Namespace信息:"
    kubectl get ns $TENANT_NS -o yaml
    echo ""
    
    echo "所有资源:"
    kubectl get all -n $TENANT_NS
    echo ""
    
    echo "ResourceQuota:"
    kubectl get resourcequota -n $TENANT_NS -o yaml
    echo ""
    
    echo "NetworkPolicy:"
    kubectl get networkpolicy -n $TENANT_NS -o yaml
    echo ""
    
    echo "最近事件:"
    kubectl get events -n $TENANT_NS --sort-by='.lastTimestamp'
} > $REPORT_FILE

echo "诊断报告已保存到: $REPORT_FILE"
echo ""

echo "=== 故障排查完成 ==="
```

**多租户最佳实践总结**

```yaml
# 多租户实施路线图

阶段1: 规划设计（1-2周）
  任务:
    - 评估多租户需求和场景
    - 选择合适的多租户模型
    - 设计资源配额和隔离策略
    - 规划网络和安全架构
  
  交付物:
    - 多租户架构设计文档
    - 资源配额规划表
    - 安全策略文档
    - 实施计划

阶段2: 基础设施准备（1-2周）
  任务:
    - 部署和配置Kubernetes集群
    - 安装必要的插件（CNI、CSI、Ingress）
    - 配置监控和日志系统
    - 准备镜像仓库和存储
  
  交付物:
    - 就绪的Kubernetes集群
    - 监控和日志系统
    - 基础设施文档

阶段3: 多租户实施（2-3周）
  任务:
    - 创建租户Namespace和配额
    - 配置RBAC和网络策略
    - 部署策略引擎（OPA/Gatekeeper）
    - 实施虚拟集群（如需要）
    - 配置租户监控和告警
  
  交付物:
    - 租户环境
    - 策略配置
    - 监控仪表盘
    - 运维手册

阶段4: 测试验证（1-2周）
  任务:
    - 功能测试（资源隔离、网络隔离）
    - 性能测试（负载测试、压力测试）
    - 安全测试（渗透测试、合规检查）
    - 故障演练（混沌工程）
  
  交付物:
    - 测试报告
    - 性能基准
    - 安全评估报告
    - 改进建议

阶段5: 上线和优化（持续）
  任务:
    - 租户迁移和上线
    - 监控和调优
    - 成本优化
    - 持续改进
  
  交付物:
    - 上线报告
    - 优化建议
    - 运维文档更新
    - 最佳实践总结

# 关键成功因素

技术层面:
  1. 选择合适的多租户模型
  2. 实施多层次的隔离机制
  3. 建立完善的监控和告警
  4. 自动化租户管理流程
  5. 定期进行安全审计

管理层面:
  1. 明确的租户SLA
  2. 透明的成本计费
  3. 完善的文档和培训
  4. 快速的问题响应机制
  5. 持续的优化和改进

# 常见陷阱和避免方法

陷阱1: 过度隔离
  问题: 隔离过于严格导致资源浪费和管理复杂
  避免: 根据实际需求选择合适的隔离级别

陷阱2: 配额设置不合理
  问题: 配额过小影响业务，过大浪费资源
  避免: 基于历史数据和业务预测设置配额，定期调整

陷阱3: 忽视网络隔离
  问题: 租户之间可以互相访问，存在安全风险
  避免: 实施默认拒绝的NetworkPolicy

陷阱4: 缺乏监控和告警
  问题: 无法及时发现和解决问题
  避免: 建立完善的监控体系和告警机制

陷阱5: 手动管理租户
  问题: 效率低下，容易出错
  避免: 使用Operator或自动化工具管理租户

# 检查清单

部署前检查:
  ☐ 多租户架构设计已评审
  ☐ 资源配额已规划
  ☐ 网络策略已配置
  ☐ RBAC权限已设置
  ☐ 监控和日志已就绪
  ☐ 备份恢复已测试
  ☐ 文档已完善

运行时检查:
  ☐ 资源配额使用率正常
  ☐ 网络隔离有效
  ☐ 无权限泄露
  ☐ 监控告警正常
  ☐ 成本在预算内
  ☐ 租户满意度高

定期审查:
  ☐ 每月审查资源使用情况
  ☐ 每季度审查安全策略
  ☐ 每半年审查架构设计
  ☐ 每年进行全面评估
```

**本节总结**

在本节中，我们深入探讨了Kubernetes多租户与资源隔离的各个方面：

1. **多租户架构概述**：介绍了多租户的概念、价值和挑战，以及三种多租户模型（软多租户、硬多租户、混合多租户）的特点和适用场景。

2. **Namespace级别的资源隔离**：详细讲解了如何使用Namespace、ResourceQuota、LimitRange、NetworkPolicy和RBAC实现基础的多租户隔离，并提供了完整的配置示例和自动化管理工具。

3. **虚拟集群技术**：深入介绍了vcluster等虚拟集群解决方案，展示了如何在物理集群之上创建逻辑隔离的虚拟集群，提供更强的隔离和独立性。

4. **多租户最佳实践**：总结了多租户架构选型、安全加固、成本管理、运维自动化等方面的最佳实践，并提供了实施路线图和检查清单。

**关键要点**：

- 根据信任级别和隔离需求选择合适的多租户模型
- 实施多层次的隔离机制（资源、网络、安全）
- 使用自动化工具简化租户管理
- 建立完善的监控、告警和成本管理体系
- 定期审查和优化多租户配置

**下一节预告**：

在下一节中，我们将学习集群联邦与多集群管理，探讨如何管理跨地域、跨云的多个Kubernetes集群，实现统一的资源调度和故障转移。



## 10.2 集群联邦与多集群管理

### 10.2.1 多集群架构设计

**为什么需要多集群**

在企业级Kubernetes实践中，单一集群往往无法满足所有需求。多集群架构成为必然选择。

**多集群的驱动因素**：

```yaml
# 多集群需求场景

1. 高可用和灾难恢复:
   需求:
     - 避免单点故障
     - 跨地域容灾
     - 业务连续性保障
   
   方案:
     - 主备集群架构
     - 多活集群架构
     - 跨区域部署

2. 地理分布和就近访问:
   需求:
     - 降低网络延迟
     - 满足数据主权要求
     - 提升用户体验
   
   方案:
     - 按地域部署集群
     - 边缘计算集群
     - CDN集成

3. 环境隔离:
   需求:
     - 开发、测试、生产环境分离
     - 不同安全级别隔离
     - 合规性要求
   
   方案:
     - 按环境划分集群
     - 按安全级别划分
     - 按合规要求划分

4. 资源和成本优化:
   需求:
     - 利用不同云厂商优势
     - 避免厂商锁定
     - 成本优化
   
   方案:
     - 混合云架构
     - 多云架构
     - 按需弹性扩展

5. 组织和团队隔离:
   需求:
     - 不同业务线独立管理
     - 团队自主权
     - 爆炸半径控制
   
   方案:
     - 按业务线划分集群
     - 按团队划分集群
     - 层级化管理

6. 技术和版本多样性:
   需求:
     - 支持不同Kubernetes版本
     - 渐进式升级
     - 技术栈多样性
   
   方案:
     - 版本分离集群
     - 金丝雀集群
     - 实验性集群
```

**多集群架构模式**

```yaml
# 多集群架构的主要模式

模式1: 独立集群（Independent Clusters）
  描述: 每个集群完全独立运行，无统一管理
  
  特点:
    - 集群间无依赖
    - 独立的控制平面和数据平面
    - 手动管理和协调
  
  优势:
    - 实现简单
    - 故障隔离好
    - 灵活性高
  
  劣势:
    - 管理复杂度高
    - 资源利用率低
    - 缺乏统一视图
  
  适用场景:
    - 小规模部署（<5个集群）
    - 完全独立的业务
    - 临时或实验性集群

---

模式2: 主从集群（Hub-Spoke）
  描述: 一个主集群管理多个从集群
  
  架构:
    主集群（Hub）:
      - 统一的管理平面
      - 策略和配置中心
      - 监控和日志聚合
    
    从集群（Spoke）:
      - 执行工作负载
      - 接收主集群指令
      - 上报状态和指标
  
  优势:
    - 集中管理
    - 统一视图
    - 策略一致性
  
  劣势:
    - 主集群单点故障
    - 扩展性受限
    - 网络依赖强
  
  适用场景:
    - 中等规模部署（5-20个集群）
    - 需要集中管理
    - 地理分布的边缘集群

---

模式3: 网格集群（Mesh）
  描述: 集群间对等连接，无中心节点
  
  特点:
    - 去中心化
    - 集群间直接通信
    - 分布式决策
  
  优势:
    - 无单点故障
    - 扩展性好
    - 灵活性高
  
  劣势:
    - 实现复杂
    - 管理难度大
    - 网络开销大
  
  适用场景:
    - 大规模部署（>20个集群）
    - 需要高可用
    - 复杂的跨集群通信

---

模式4: 联邦集群（Federation）
  描述: 使用联邦控制平面统一管理多个集群
  
  架构:
    联邦控制平面:
      - 统一的API入口
      - 资源分发和调度
      - 跨集群协调
    
    成员集群:
      - 独立的Kubernetes集群
      - 接收联邦指令
      - 保持自主性
  
  优势:
    - 统一管理接口
    - 跨集群资源调度
    - 高可用和容灾
  
  劣势:
    - 实现复杂
    - 学习曲线陡峭
    - 额外的管理开销
  
  适用场景:
    - 需要跨集群资源调度
    - 多地域部署
    - 高可用要求高
```

**多集群网络架构**

```yaml
# 多集群网络连接方案

方案1: VPN隧道
  描述: 通过VPN建立集群间的安全连接
  
  实现:
    - IPSec VPN
    - WireGuard
    - OpenVPN
  
  特点:
    - 加密传输
    - 跨云支持
    - 配置相对简单
  
  性能:
    - 延迟: 中等（+5-20ms）
    - 吞吐: 受VPN网关限制
    - 开销: 加密解密开销
  
  适用场景:
    - 跨云连接
    - 安全要求高
    - 流量不大

---

方案2: 专线连接
  描述: 使用云厂商提供的专线服务
  
  实现:
    - AWS Direct Connect
    - Azure ExpressRoute
    - GCP Cloud Interconnect
    - 阿里云高速通道
  
  特点:
    - 低延迟
    - 高带宽
    - 稳定可靠
  
  性能:
    - 延迟: 低（<5ms）
    - 吞吐: 高（1-100Gbps）
    - 开销: 成本高
  
  适用场景:
    - 大流量场景
    - 延迟敏感应用
    - 混合云架构

---

方案3: Service Mesh
  描述: 使用服务网格实现跨集群通信
  
  实现:
    - Istio Multi-Cluster
    - Linkerd Multi-Cluster
    - Consul Connect
  
  特点:
    - 应用层连接
    - 流量管理
    - 可观测性
  
  性能:
    - 延迟: 中等（+2-10ms）
    - 吞吐: 受Sidecar限制
    - 开销: 额外的Proxy
  
  适用场景:
    - 微服务架构
    - 需要高级流量管理
    - 已使用Service Mesh

---

方案4: Submariner
  描述: 专为Kubernetes设计的跨集群网络方案
  
  实现:
    - Gateway节点
    - IPSec隧道
    - Service Discovery
  
  特点:
    - Kubernetes原生
    - Pod到Pod直连
    - Service跨集群访问
  
  性能:
    - 延迟: 低（<5ms）
    - 吞吐: 高
    - 开销: 较小
  
  适用场景:
    - Kubernetes专用
    - 需要Pod直连
    - 开源方案优先
```

**多集群存储架构**

```yaml
# 多集群存储策略

策略1: 独立存储
  描述: 每个集群使用独立的存储系统
  
  实现:
    - 本地存储（Local PV）
    - 云厂商存储（EBS、Azure Disk）
    - 分布式存储（Ceph、GlusterFS）
  
  优势:
    - 简单直接
    - 性能好
    - 故障隔离
  
  劣势:
    - 数据不共享
    - 迁移困难
    - 成本高
  
  适用场景:
    - 无状态应用为主
    - 数据本地化要求
    - 集群间无数据共享需求

---

策略2: 跨集群存储
  描述: 使用支持跨集群的存储系统
  
  实现:
    - Portworx
    - StorageOS
    - Rook/Ceph（跨集群配置）
  
  优势:
    - 数据共享
    - 统一管理
    - 支持迁移
  
  劣势:
    - 实现复杂
    - 性能开销
    - 网络依赖
  
  适用场景:
    - 有状态应用
    - 需要数据共享
    - 跨集群迁移

---

策略3: 对象存储
  描述: 使用对象存储作为共享存储
  
  实现:
    - S3（AWS、MinIO）
    - Azure Blob Storage
    - Google Cloud Storage
  
  优势:
    - 天然跨集群
    - 高可用
    - 成本低
  
  劣势:
    - 非POSIX接口
    - 延迟较高
    - 不适合所有场景
  
  适用场景:
    - 文件存储
    - 备份归档
    - 大数据处理

---

策略4: 数据库复制
  描述: 使用数据库的复制功能实现跨集群数据同步
  
  实现:
    - MySQL主从复制
    - PostgreSQL流复制
    - MongoDB副本集
    - Redis Cluster
  
  优势:
    - 数据库原生支持
    - 性能好
    - 成熟稳定
  
  劣势:
    - 应用层实现
    - 复杂度高
    - 一致性挑战
  
  适用场景:
    - 数据库应用
    - 读写分离
    - 多活架构
```

**多集群架构设计原则**

```yaml
# 多集群设计的核心原则

1. 松耦合原则:
   描述: 集群间应保持松耦合，减少依赖
   
   实践:
     - 避免跨集群的强依赖
     - 使用异步通信
     - 设计容错机制
     - 支持独立运行
   
   反模式:
     - 跨集群的同步调用
     - 共享状态
     - 紧密耦合的服务

2. 自治原则:
   描述: 每个集群应能独立运行和决策
   
   实践:
     - 本地化数据和配置
     - 独立的控制平面
     - 本地故障处理
     - 降级策略
   
   反模式:
     - 依赖中心化决策
     - 单点故障
     - 无降级方案

3. 一致性原则:
   描述: 保持配置和策略的一致性
   
   实践:
     - 统一的配置管理
     - 版本控制
     - 自动化部署
     - 配置验证
   
   工具:
     - GitOps（ArgoCD、Flux）
     - Helm
     - Kustomize

4. 可观测性原则:
   描述: 建立统一的监控和日志体系
   
   实践:
     - 集中式监控
     - 分布式追踪
     - 统一日志收集
     - 跨集群告警
   
   工具:
     - Prometheus Federation
     - Thanos
     - Grafana
     - Jaeger

5. 安全性原则:
   描述: 确保跨集群通信的安全性
   
   实践:
     - mTLS加密
     - 网络隔离
     - 身份认证
     - 访问控制
   
   工具:
     - Service Mesh
     - VPN
     - Certificate Manager

6. 成本优化原则:
   描述: 平衡功能和成本
   
   实践:
     - 合理规划集群数量
     - 资源共享
     - 按需扩展
     - 成本监控
   
   策略:
     - 混合云
     - Spot实例
     - 资源预留
```

**多集群架构决策树**

```yaml
# 多集群架构选择指南

决策流程:
  
  问题1: 是否需要跨集群资源调度？
    是 -> 考虑联邦集群（KubeFed）
    否 -> 继续问题2
  
  问题2: 集群数量是多少？
    <5个 -> 考虑独立集群或主从模式
    5-20个 -> 考虑主从模式
    >20个 -> 考虑网格模式或联邦
  
  问题3: 是否需要跨集群服务通信？
    是 -> 考虑Service Mesh或Submariner
    否 -> 继续问题4
  
  问题4: 是否需要统一管理？
    是 -> 考虑主从模式或联邦
    否 -> 考虑独立集群
  
  问题5: 预算是否充足？
    是 -> 选择功能完善的方案
    否 -> 选择开源轻量方案

推荐方案:
  
  场景A - 小型企业（<5个集群）:
    方案: 独立集群 + GitOps
    理由: 简单直接，成本低
    工具: ArgoCD、Helm
  
  场景B - 中型企业（5-20个集群）:
    方案: 主从模式 + Rancher/OCM
    理由: 集中管理，易于运维
    工具: Rancher、Open Cluster Management
  
  场景C - 大型企业（>20个集群）:
    方案: 联邦集群 + Service Mesh
    理由: 扩展性好，功能完善
    工具: KubeFed、Istio Multi-Cluster
  
  场景D - 多云架构:
    方案: 混合方案（主从+联邦）
    理由: 灵活性高，避免锁定
    工具: Rancher + KubeFed
  
  场景E - 边缘计算:
    方案: 主从模式 + KubeEdge
    理由: 适合边缘场景
    工具: KubeEdge、K3s
```



### 10.2.2 KubeFed集群联邦

**KubeFed概述**

KubeFed（Kubernetes Cluster Federation）是Kubernetes官方的多集群管理解决方案，它提供了跨集群的资源分发、调度和管理能力。

**KubeFed核心概念**：

```yaml
# KubeFed架构组件

1. Host Cluster（宿主集群）:
   角色: 运行KubeFed控制平面
   组件:
     - KubeFed Controller Manager
     - KubeFed API Server
     - KubeFed Webhook
   职责:
     - 管理联邦配置
     - 协调成员集群
     - 资源分发和调度

2. Member Cluster（成员集群）:
   角色: 接收和执行联邦指令
   要求:
     - 可被Host Cluster访问
     - 注册到联邦
     - 运行工作负载
   职责:
     - 执行联邦资源
     - 上报状态
     - 本地资源管理

3. Federated Resources（联邦资源）:
   类型:
     - FederatedDeployment
     - FederatedService
     - FederatedConfigMap
     - FederatedSecret
     - FederatedNamespace
   特点:
     - 跨集群分发
     - 统一管理
     - 差异化配置

4. Placement Policy（放置策略）:
   功能: 决定资源部署到哪些集群
   策略:
     - ClusterSelector: 基于标签选择
     - Weight: 权重分配
     - Affinity: 亲和性规则
     - Spread: 分散策略

5. Override Policy（覆盖策略）:
   功能: 为不同集群定制配置
   用途:
     - 资源规格调整
     - 环境变量覆盖
     - 镜像地址替换
     - 副本数调整
```

**KubeFed安装部署**

```bash
#!/bin/bash
# install-kubefed.sh
# 安装KubeFed v2

set -e

KUBEFED_VERSION="v0.10.0"
HOST_CLUSTER_CONTEXT="host-cluster"
KUBEFED_NAMESPACE="kube-federation-system"

echo "=== 安装KubeFed ==="

# 1. 安装kubefedctl CLI
echo "1. 安装kubefedctl CLI..."
curl -LO "https://github.com/kubernetes-sigs/kubefed/releases/download/${KUBEFED_VERSION}/kubefedctl-${KUBEFED_VERSION}-linux-amd64.tgz"
tar -xzf "kubefedctl-${KUBEFED_VERSION}-linux-amd64.tgz"
sudo mv kubefedctl /usr/local/bin/
rm "kubefedctl-${KUBEFED_VERSION}-linux-amd64.tgz"

# 2. 验证安装
echo "2. 验证kubefedctl安装..."
kubefedctl version

# 3. 在Host Cluster上安装KubeFed
echo "3. 在Host Cluster上安装KubeFed..."
kubectl config use-context $HOST_CLUSTER_CONTEXT

# 创建namespace
kubectl create namespace $KUBEFED_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

# 使用Helm安装KubeFed
helm repo add kubefed-charts https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts
helm repo update

helm install kubefed kubefed-charts/kubefed   --namespace $KUBEFED_NAMESPACE   --version $KUBEFED_VERSION   --set controllermanager.replicaCount=3   --set controllermanager.resources.limits.cpu=500m   --set controllermanager.resources.limits.memory=512Mi

# 4. 等待KubeFed就绪
echo "4. 等待KubeFed就绪..."
kubectl wait --for=condition=available --timeout=300s   deployment/kubefed-controller-manager   -n $KUBEFED_NAMESPACE

# 5. 验证安装
echo "5. 验证KubeFed安装..."
kubectl get pods -n $KUBEFED_NAMESPACE
kubectl get crd | grep kubefed

echo "=== KubeFed安装完成 ==="
```

**注册成员集群**

```bash
#!/bin/bash
# join-clusters.sh
# 将集群加入联邦

set -e

HOST_CLUSTER="host-cluster"
MEMBER_CLUSTERS=("cluster-1" "cluster-2" "cluster-3")
KUBEFED_NAMESPACE="kube-federation-system"

echo "=== 注册成员集群到联邦 ==="

# 切换到Host Cluster
kubectl config use-context $HOST_CLUSTER

for cluster in "${MEMBER_CLUSTERS[@]}"; do
    echo "注册集群: $cluster"
    
    # 使用kubefedctl join命令
    kubefedctl join $cluster         --cluster-context $cluster         --host-cluster-context $HOST_CLUSTER         --kubefed-namespace $KUBEFED_NAMESPACE         --v=2
    
    # 验证注册
    kubectl get kubefedcluster $cluster -n $KUBEFED_NAMESPACE
    
    echo "集群 $cluster 注册成功"
    echo ""
done

# 查看所有注册的集群
echo "所有联邦集群:"
kubectl get kubefedclusters -n $KUBEFED_NAMESPACE

echo "=== 集群注册完成 ==="
```

**联邦资源配置**

```yaml
# federated-deployment.yaml
# 联邦Deployment示例

---
# 1. 创建联邦Namespace
apiVersion: types.kubefed.io/v1beta1
kind: FederatedNamespace
metadata:
  name: demo-app
  namespace: demo-app
spec:
  # 放置策略：部署到所有集群
  placement:
    clusters:
    - name: cluster-1
    - name: cluster-2
    - name: cluster-3

---
# 2. 联邦Deployment
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: nginx
  namespace: demo-app
spec:
  # 模板：基础配置
  template:
    metadata:
      labels:
        app: nginx
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx:1.21
            ports:
            - containerPort: 80
            resources:
              requests:
                cpu: 100m
                memory: 128Mi
              limits:
                cpu: 500m
                memory: 512Mi
  
  # 放置策略：选择性部署
  placement:
    clusterSelector:
      matchLabels:
        environment: production
  
  # 覆盖策略：差异化配置
  overrides:
  # cluster-1: 增加副本数
  - clusterName: cluster-1
    clusterOverrides:
    - path: "/spec/replicas"
      value: 5
    - path: "/spec/template/spec/containers/0/resources/limits/cpu"
      value: "1"
  
  # cluster-2: 使用不同镜像
  - clusterName: cluster-2
    clusterOverrides:
    - path: "/spec/template/spec/containers/0/image"
      value: "nginx:1.22"
  
  # cluster-3: 添加环境变量
  - clusterName: cluster-3
    clusterOverrides:
    - path: "/spec/template/spec/containers/0/env"
      value:
      - name: CLUSTER_NAME
        value: cluster-3
      - name: REGION
        value: us-west

---
# 3. 联邦Service
apiVersion: types.kubefed.io/v1beta1
kind: FederatedService
metadata:
  name: nginx
  namespace: demo-app
spec:
  template:
    metadata:
      labels:
        app: nginx
    spec:
      type: LoadBalancer
      selector:
        app: nginx
      ports:
      - port: 80
        targetPort: 80
        protocol: TCP
  
  placement:
    clusters:
    - name: cluster-1
    - name: cluster-2
    - name: cluster-3
  
  # Service类型覆盖
  overrides:
  # cluster-1使用LoadBalancer
  - clusterName: cluster-1
    clusterOverrides:
    - path: "/spec/type"
      value: LoadBalancer
  
  # cluster-2和cluster-3使用NodePort
  - clusterName: cluster-2
    clusterOverrides:
    - path: "/spec/type"
      value: NodePort
  - clusterName: cluster-3
    clusterOverrides:
    - path: "/spec/type"
      value: NodePort

---
# 4. 联邦ConfigMap
apiVersion: types.kubefed.io/v1beta1
kind: FederatedConfigMap
metadata:
  name: app-config
  namespace: demo-app
spec:
  template:
    data:
      app.conf: |
        server {
          listen 80;
          server_name _;
          location / {
            root /usr/share/nginx/html;
            index index.html;
          }
        }
      log.level: "info"
  
  placement:
    clusters:
    - name: cluster-1
    - name: cluster-2
    - name: cluster-3
  
  # 不同集群使用不同配置
  overrides:
  - clusterName: cluster-1
    clusterOverrides:
    - path: "/data/log.level"
      value: "debug"
  - clusterName: cluster-3
    clusterOverrides:
    - path: "/data/log.level"
      value: "warn"

---
# 5. 联邦Secret
apiVersion: types.kubefed.io/v1beta1
kind: FederatedSecret
metadata:
  name: app-secret
  namespace: demo-app
spec:
  template:
    type: Opaque
    stringData:
      username: admin
      password: changeme
  
  placement:
    clusters:
    - name: cluster-1
    - name: cluster-2
    - name: cluster-3
```

**高级放置策略**

```yaml
# advanced-placement.yaml
# 高级放置和调度策略

---
# 1. 基于标签的集群选择
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: web-app
  namespace: demo-app
spec:
  template:
    # ... deployment spec ...
  
  placement:
    # 选择带有特定标签的集群
    clusterSelector:
      matchLabels:
        environment: production
        region: us-west
      matchExpressions:
      - key: tier
        operator: In
        values:
        - frontend
        - backend

---
# 2. 权重分配策略
apiVersion: types.kubefed.io/v1beta1
kind: ReplicaSchedulingPreference
metadata:
  name: web-app
  namespace: demo-app
spec:
  targetKind: FederatedDeployment
  totalReplicas: 100
  
  # 按权重分配副本
  clusters:
    cluster-1:
      weight: 50  # 50个副本
    cluster-2:
      weight: 30  # 30个副本
    cluster-3:
      weight: 20  # 20个副本
  
  # 最小副本数限制
  rebalance: true
  minReplicas:
    cluster-1: 10
    cluster-2: 5
    cluster-3: 5

---
# 3. 容量感知调度
apiVersion: types.kubefed.io/v1beta1
kind: ReplicaSchedulingPreference
metadata:
  name: capacity-aware
  namespace: demo-app
spec:
  targetKind: FederatedDeployment
  totalReplicas: 50
  
  # 基于集群容量自动分配
  rebalance: true
  
  # 集群容量限制
  clusters:
    cluster-1:
      maxReplicas: 30
      minReplicas: 5
    cluster-2:
      maxReplicas: 20
      minReplicas: 5
    cluster-3:
      maxReplicas: 15
      minReplicas: 5

---
# 4. 亲和性和反亲和性
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: affinity-app
  namespace: demo-app
spec:
  template:
    # ... deployment spec ...
  
  placement:
    clusterSelector:
      matchLabels:
        environment: production
    
    # 集群亲和性
    clusterAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        clusterSelectorTerms:
        - matchExpressions:
          - key: region
            operator: In
            values:
            - us-west
            - us-east
      
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: tier
            operator: In
            values:
            - premium

---
# 5. 分散策略
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: spread-app
  namespace: demo-app
spec:
  template:
    # ... deployment spec ...
  
  placement:
    # 尽可能分散到多个集群
    clusterSelector:
      matchLabels:
        environment: production
    
    # 分散约束
    spreadConstraints:
    - maxSkew: 2  # 集群间副本数差异不超过2
      topologyKey: region
      whenUnsatisfiable: DoNotSchedule
```


**KubeFed管理工具**

```go
// kubefed-manager.go
// KubeFed管理工具

package main

import (
	"context"
	"fmt"
	"log"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic"
	"k8s.io/client-go/tools/clientcmd"
)

// KubeFedManager KubeFed管理器
type KubeFedManager struct {
	dynamicClient dynamic.Interface
	namespace     string
}

// NewKubeFedManager 创建KubeFed管理器
func NewKubeFedManager(kubeconfig, namespace string) (*KubeFedManager, error) {
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	dynamicClient, err := dynamic.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create dynamic client: %v", err)
	}
	
	return &KubeFedManager{
		dynamicClient: dynamicClient,
		namespace:     namespace,
	}, nil
}

// ListClusters 列出所有联邦集群
func (km *KubeFedManager) ListClusters(ctx context.Context) ([]ClusterInfo, error) {
	gvr := schema.GroupVersionResource{
		Group:    "core.kubefed.io",
		Version:  "v1beta1",
		Resource: "kubefedclusters",
	}
	
	list, err := km.dynamicClient.Resource(gvr).Namespace(km.namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list clusters: %v", err)
	}
	
	var clusters []ClusterInfo
	for _, item := range list.Items {
		cluster := ClusterInfo{
			Name: item.GetName(),
		}
		
		// 获取状态
		status, found, err := unstructured.NestedMap(item.Object, "status")
		if err == nil && found {
			if conditions, ok := status["conditions"].([]interface{}); ok {
				for _, cond := range conditions {
					if condMap, ok := cond.(map[string]interface{}); ok {
						if condMap["type"] == "Ready" {
							cluster.Ready = condMap["status"] == "True"
						}
					}
				}
			}
		}
		
		// 获取标签
		cluster.Labels = item.GetLabels()
		
		clusters = append(clusters, cluster)
	}
	
	return clusters, nil
}

// ClusterInfo 集群信息
type ClusterInfo struct {
	Name   string
	Ready  bool
	Labels map[string]string
}

// CreateFederatedDeployment 创建联邦Deployment
func (km *KubeFedManager) CreateFederatedDeployment(ctx context.Context, config *FederatedDeploymentConfig) error {
	gvr := schema.GroupVersionResource{
		Group:    "types.kubefed.io",
		Version:  "v1beta1",
		Resource: "federateddeployments",
	}
	
	// 构建联邦Deployment对象
	fedDeploy := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"apiVersion": "types.kubefed.io/v1beta1",
			"kind":       "FederatedDeployment",
			"metadata": map[string]interface{}{
				"name":      config.Name,
				"namespace": config.Namespace,
			},
			"spec": map[string]interface{}{
				"template": config.Template,
				"placement": map[string]interface{}{
					"clusters": config.Clusters,
				},
				"overrides": config.Overrides,
			},
		},
	}
	
	_, err := km.dynamicClient.Resource(gvr).Namespace(config.Namespace).Create(ctx, fedDeploy, metav1.CreateOptions{})
	if err != nil {
		return fmt.Errorf("failed to create federated deployment: %v", err)
	}
	
	log.Printf("Created federated deployment: %s/%s", config.Namespace, config.Name)
	return nil
}

// FederatedDeploymentConfig 联邦Deployment配置
type FederatedDeploymentConfig struct {
	Name      string
	Namespace string
	Template  map[string]interface{}
	Clusters  []map[string]interface{}
	Overrides []map[string]interface{}
}

// GetFederatedDeploymentStatus 获取联邦Deployment状态
func (km *KubeFedManager) GetFederatedDeploymentStatus(ctx context.Context, namespace, name string) (*FederatedDeploymentStatus, error) {
	gvr := schema.GroupVersionResource{
		Group:    "types.kubefed.io",
		Version:  "v1beta1",
		Resource: "federateddeployments",
	}
	
	fedDeploy, err := km.dynamicClient.Resource(gvr).Namespace(namespace).Get(ctx, name, metav1.GetOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get federated deployment: %v", err)
	}
	
	status := &FederatedDeploymentStatus{
		Name:      name,
		Namespace: namespace,
		Clusters:  make(map[string]ClusterStatus),
	}
	
	// 解析状态
	statusMap, found, err := unstructured.NestedMap(fedDeploy.Object, "status")
	if err != nil || !found {
		return status, nil
	}
	
	// 获取集群状态
	if clusters, ok := statusMap["clusters"].([]interface{}); ok {
		for _, cluster := range clusters {
			if clusterMap, ok := cluster.(map[string]interface{}); ok {
				clusterName := clusterMap["name"].(string)
				status.Clusters[clusterName] = ClusterStatus{
					Name:  clusterName,
					Ready: clusterMap["status"] == "Ready",
				}
			}
		}
	}
	
	return status, nil
}

// FederatedDeploymentStatus 联邦Deployment状态
type FederatedDeploymentStatus struct {
	Name      string
	Namespace string
	Clusters  map[string]ClusterStatus
}

// ClusterStatus 集群状态
type ClusterStatus struct {
	Name  string
	Ready bool
}

// UpdateReplicaScheduling 更新副本调度策略
func (km *KubeFedManager) UpdateReplicaScheduling(ctx context.Context, config *ReplicaSchedulingConfig) error {
	gvr := schema.GroupVersionResource{
		Group:    "scheduling.kubefed.io",
		Version:  "v1alpha1",
		Resource: "replicaschedulingpreferences",
	}
	
	rsp := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"apiVersion": "scheduling.kubefed.io/v1alpha1",
			"kind":       "ReplicaSchedulingPreference",
			"metadata": map[string]interface{}{
				"name":      config.Name,
				"namespace": config.Namespace,
			},
			"spec": map[string]interface{}{
				"targetKind":   "FederatedDeployment",
				"totalReplicas": config.TotalReplicas,
				"rebalance":    config.Rebalance,
				"clusters":     config.Clusters,
			},
		},
	}
	
	_, err := km.dynamicClient.Resource(gvr).Namespace(config.Namespace).Update(ctx, rsp, metav1.UpdateOptions{})
	if err != nil {
		return fmt.Errorf("failed to update replica scheduling: %v", err)
	}
	
	log.Printf("Updated replica scheduling: %s/%s", config.Namespace, config.Name)
	return nil
}

// ReplicaSchedulingConfig 副本调度配置
type ReplicaSchedulingConfig struct {
	Name          string
	Namespace     string
	TotalReplicas int
	Rebalance     bool
	Clusters      map[string]interface{}
}

// PropagateResource 传播资源到所有集群
func (km *KubeFedManager) PropagateResource(ctx context.Context, resourceType, namespace, name string) error {
	log.Printf("Propagating %s %s/%s to all clusters", resourceType, namespace, name)
	
	// 这里简化实现，实际需要根据资源类型创建对应的联邦资源
	// 例如：Deployment -> FederatedDeployment
	
	return nil
}

// MigrateWorkload 迁移工作负载到其他集群
func (km *KubeFedManager) MigrateWorkload(ctx context.Context, namespace, name, fromCluster, toCluster string) error {
	log.Printf("Migrating workload %s/%s from %s to %s", namespace, name, fromCluster, toCluster)
	
	// 1. 获取当前联邦Deployment
	gvr := schema.GroupVersionResource{
		Group:    "types.kubefed.io",
		Version:  "v1beta1",
		Resource: "federateddeployments",
	}
	
	fedDeploy, err := km.dynamicClient.Resource(gvr).Namespace(namespace).Get(ctx, name, metav1.GetOptions{})
	if err != nil {
		return fmt.Errorf("failed to get federated deployment: %v", err)
	}
	
	// 2. 更新placement
	placement, found, err := unstructured.NestedSlice(fedDeploy.Object, "spec", "placement", "clusters")
	if err != nil || !found {
		return fmt.Errorf("failed to get placement: %v", err)
	}
	
	// 移除fromCluster，添加toCluster
	var newPlacement []interface{}
	for _, cluster := range placement {
		if clusterMap, ok := cluster.(map[string]interface{}); ok {
			if clusterMap["name"] != fromCluster {
				newPlacement = append(newPlacement, cluster)
			}
		}
	}
	newPlacement = append(newPlacement, map[string]interface{}{
		"name": toCluster,
	})
	
	// 3. 更新资源
	if err := unstructured.SetNestedSlice(fedDeploy.Object, newPlacement, "spec", "placement", "clusters"); err != nil {
		return fmt.Errorf("failed to set placement: %v", err)
	}
	
	_, err = km.dynamicClient.Resource(gvr).Namespace(namespace).Update(ctx, fedDeploy, metav1.UpdateOptions{})
	if err != nil {
		return fmt.Errorf("failed to update federated deployment: %v", err)
	}
	
	log.Printf("Workload migrated successfully")
	return nil
}

// 使用示例
func main() {
	ctx := context.Background()
	
	// 创建管理器
	manager, err := NewKubeFedManager("/path/to/kubeconfig", "kube-federation-system")
	if err != nil {
		log.Fatalf("Failed to create manager: %v", err)
	}
	
	// 列出所有集群
	clusters, err := manager.ListClusters(ctx)
	if err != nil {
		log.Fatalf("Failed to list clusters: %v", err)
	}
	
	fmt.Println("Federated Clusters:")
	for _, cluster := range clusters {
		status := "Not Ready"
		if cluster.Ready {
			status = "Ready"
		}
		fmt.Printf("- %s: %s\n", cluster.Name, status)
	}
	
	// 创建联邦Deployment
	config := &FederatedDeploymentConfig{
		Name:      "nginx",
		Namespace: "demo-app",
		Template: map[string]interface{}{
			"spec": map[string]interface{}{
				"replicas": 3,
				// ... 其他配置
			},
		},
		Clusters: []map[string]interface{}{
			{"name": "cluster-1"},
			{"name": "cluster-2"},
		},
	}
	
	if err := manager.CreateFederatedDeployment(ctx, config); err != nil {
		log.Fatalf("Failed to create federated deployment: %v", err)
	}
	
	// 获取状态
	status, err := manager.GetFederatedDeploymentStatus(ctx, "demo-app", "nginx")
	if err != nil {
		log.Fatalf("Failed to get status: %v", err)
	}
	
	fmt.Printf("\nDeployment Status:\n")
	for clusterName, clusterStatus := range status.Clusters {
		fmt.Printf("- %s: Ready=%v\n", clusterName, clusterStatus.Ready)
	}
	
	// 迁移工作负载
	if err := manager.MigrateWorkload(ctx, "demo-app", "nginx", "cluster-1", "cluster-3"); err != nil {
		log.Printf("Failed to migrate workload: %v", err)
	}
}
```

**KubeFed实战案例**

```yaml
# use-case-multi-region.yaml
# 案例1：多地域高可用部署

---
# 场景：在3个地域部署应用，实现高可用和就近访问

# 1. 标记集群地域
# 在每个集群上执行：
# kubectl label kubefedcluster cluster-us-west region=us-west -n kube-federation-system
# kubectl label kubefedcluster cluster-us-east region=us-east -n kube-federation-system
# kubectl label kubefedcluster cluster-eu-west region=eu-west -n kube-federation-system

---
# 2. 创建联邦应用
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: global-api
  namespace: production
spec:
  template:
    metadata:
      labels:
        app: global-api
    spec:
      replicas: 10
      selector:
        matchLabels:
          app: global-api
      template:
        metadata:
          labels:
            app: global-api
        spec:
          containers:
          - name: api
            image: myregistry/api:v1.0
            ports:
            - containerPort: 8080
            resources:
              requests:
                cpu: 500m
                memory: 512Mi
              limits:
                cpu: 1000m
                memory: 1Gi
            livenessProbe:
              httpGet:
                path: /health
                port: 8080
              initialDelaySeconds: 30
              periodSeconds: 10
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              initialDelaySeconds: 5
              periodSeconds: 5
  
  # 部署到所有地域
  placement:
    clusterSelector:
      matchLabels:
        environment: production
  
  # 每个地域使用不同配置
  overrides:
  # 美西：最多副本
  - clusterName: cluster-us-west
    clusterOverrides:
    - path: "/spec/replicas"
      value: 15
    - path: "/spec/template/spec/containers/0/env"
      value:
      - name: REGION
        value: us-west
      - name: DB_ENDPOINT
        value: db.us-west.example.com
  
  # 美东：中等副本
  - clusterName: cluster-us-east
    clusterOverrides:
    - path: "/spec/replicas"
      value: 10
    - path: "/spec/template/spec/containers/0/env"
      value:
      - name: REGION
        value: us-east
      - name: DB_ENDPOINT
        value: db.us-east.example.com
  
  # 欧洲：较少副本
  - clusterName: cluster-eu-west
    clusterOverrides:
    - path: "/spec/replicas"
      value: 5
    - path: "/spec/template/spec/containers/0/env"
      value:
      - name: REGION
        value: eu-west
      - name: DB_ENDPOINT
        value: db.eu-west.example.com

---
# 3. 副本调度策略
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: global-api
  namespace: production
spec:
  targetKind: FederatedDeployment
  totalReplicas: 30
  rebalance: true
  
  # 按地域流量分配副本
  clusters:
    cluster-us-west:
      weight: 50    # 15个副本
      minReplicas: 10
      maxReplicas: 20
    cluster-us-east:
      weight: 33    # 10个副本
      minReplicas: 5
      maxReplicas: 15
    cluster-eu-west:
      weight: 17    # 5个副本
      minReplicas: 3
      maxReplicas: 10

---
# 4. 全局负载均衡Service
apiVersion: types.kubefed.io/v1beta1
kind: FederatedService
metadata:
  name: global-api
  namespace: production
spec:
  template:
    metadata:
      labels:
        app: global-api
      annotations:
        # 使用外部DNS实现全局负载均衡
        external-dns.alpha.kubernetes.io/hostname: api.example.com
    spec:
      type: LoadBalancer
      selector:
        app: global-api
      ports:
      - port: 80
        targetPort: 8080
        protocol: TCP
  
  placement:
    clusters:
    - name: cluster-us-west
    - name: cluster-us-east
    - name: cluster-eu-west
```


**KubeFed故障转移**

```yaml
# failover-strategy.yaml
# 自动故障转移配置

---
# 案例2：自动故障转移

# 1. 健康检查配置
apiVersion: core.kubefed.io/v1beta1
kind: KubeFedCluster
metadata:
  name: cluster-1
  namespace: kube-federation-system
spec:
  apiEndpoint: https://cluster-1.example.com:6443
  secretRef:
    name: cluster-1-secret
  
  # 健康检查配置
  healthCheck:
    enabled: true
    period: 10s
    timeout: 5s
    failureThreshold: 3
    successThreshold: 1

---
# 2. 故障转移Deployment
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: critical-app
  namespace: production
spec:
  template:
    # ... deployment spec ...
  
  # 初始放置
  placement:
    clusters:
    - name: cluster-1  # 主集群
    - name: cluster-2  # 备集群
  
  # 副本分配
  overrides:
  - clusterName: cluster-1
    clusterOverrides:
    - path: "/spec/replicas"
      value: 10  # 主集群运行10个副本
  - clusterName: cluster-2
    clusterOverrides:
    - path: "/spec/replicas"
      value: 0   # 备集群初始为0

---
# 3. 副本调度策略（支持故障转移）
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: critical-app
  namespace: production
spec:
  targetKind: FederatedDeployment
  totalReplicas: 10
  rebalance: true
  
  clusters:
    cluster-1:
      weight: 100
      minReplicas: 0
      maxReplicas: 10
    cluster-2:
      weight: 100
      minReplicas: 0
      maxReplicas: 10
  
  # 当cluster-1不可用时，自动将副本调度到cluster-2
  intersectWithClusterSelector: true
```

**KubeFed监控和告警**

```yaml
# kubefed-monitoring.yaml
# KubeFed监控配置

---
# Prometheus规则
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubefed-alerts
  namespace: kube-federation-system
spec:
  groups:
  - name: kubefed
    interval: 30s
    rules:
    # 集群不健康告警
    - alert: KubeFedClusterUnhealthy
      expr: |
        kubefed_cluster_status{condition="Ready",status="False"} == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "联邦集群不健康"
        description: "集群 {{ $labels.cluster }} 已不健康超过5分钟"
    
    # 资源同步失败告警
    - alert: KubeFedResourceSyncFailed
      expr: |
        rate(kubefed_sync_errors_total[5m]) > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "联邦资源同步失败"
        description: "资源 {{ $labels.resource }} 同步失败率: {{ $value }}"
    
    # 集群连接延迟告警
    - alert: KubeFedClusterHighLatency
      expr: |
        kubefed_cluster_api_latency_seconds > 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "集群API延迟过高"
        description: "集群 {{ $labels.cluster }} API延迟: {{ $value }}s"
    
    # 副本分配不均告警
    - alert: KubeFedReplicaImbalance
      expr: |
        abs(
          kubefed_deployment_replicas{cluster="cluster-1"} - 
          kubefed_deployment_replicas{cluster="cluster-2"}
        ) > 5
      for: 15m
      labels:
        severity: info
      annotations:
        summary: "副本分配不均衡"
        description: "Deployment {{ $labels.deployment }} 副本分配差异过大"

---
# Grafana Dashboard
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubefed-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  kubefed-overview.json: |
    {
      "dashboard": {
        "title": "KubeFed Overview",
        "panels": [
          {
            "title": "集群健康状态",
            "targets": [
              {
                "expr": "kubefed_cluster_status{condition=\"Ready\"}"
              }
            ],
            "type": "stat"
          },
          {
            "title": "联邦资源数量",
            "targets": [
              {
                "expr": "count by (kind) (kubefed_resource_total)"
              }
            ],
            "type": "graph"
          },
          {
            "title": "同步错误率",
            "targets": [
              {
                "expr": "rate(kubefed_sync_errors_total[5m])"
              }
            ],
            "type": "graph"
          },
          {
            "title": "集群API延迟",
            "targets": [
              {
                "expr": "kubefed_cluster_api_latency_seconds"
              }
            ],
            "type": "graph"
          },
          {
            "title": "副本分布",
            "targets": [
              {
                "expr": "sum by (cluster) (kubefed_deployment_replicas)"
              }
            ],
            "type": "piechart"
          }
        ]
      }
    }
```

**KubeFed最佳实践**

```yaml
# KubeFed使用最佳实践

1. 集群管理:
   准备工作:
     - 确保所有集群版本兼容
     - 配置集群间网络连通性
     - 统一时钟同步
     - 准备足够的资源配额
   
   集群注册:
     - 使用有意义的集群名称
     - 添加描述性标签（region、environment、tier）
     - 配置合理的健康检查参数
     - 定期验证集群连接
   
   集群维护:
     - 定期更新集群证书
     - 监控集群健康状态
     - 及时处理不健康集群
     - 保持集群配置一致性

2. 资源管理:
   联邦资源设计:
     - 使用模板定义通用配置
     - 通过覆盖实现差异化
     - 避免过度使用覆盖
     - 保持配置简洁明了
   
   放置策略:
     - 使用标签选择器而非硬编码集群名
     - 设置合理的副本分配权重
     - 配置最小/最大副本限制
     - 启用自动重平衡
   
   版本管理:
     - 使用GitOps管理联邦资源
     - 版本控制所有配置
     - 实施变更审批流程
     - 支持快速回滚

3. 网络和服务:
   服务发现:
     - 使用MultiClusterService实现跨集群服务发现
     - 配置合理的DNS TTL
     - 实施健康检查
     - 考虑使用Service Mesh
   
   负载均衡:
     - 使用全局负载均衡器（GSLB）
     - 配置地理位置路由
     - 实施故障转移策略
     - 监控流量分布
   
   网络策略:
     - 配置跨集群NetworkPolicy
     - 限制不必要的跨集群通信
     - 使用加密传输
     - 定期审计网络规则

4. 安全性:
   认证授权:
     - 使用RBAC控制联邦资源访问
     - 最小权限原则
     - 定期审计权限
     - 使用ServiceAccount而非用户凭证
   
   密钥管理:
     - 使用FederatedSecret分发密钥
     - 定期轮换密钥
     - 避免在配置中硬编码密钥
     - 考虑使用外部密钥管理系统
   
   网络安全:
     - 使用mTLS加密集群间通信
     - 配置防火墙规则
     - 限制API Server访问
     - 启用审计日志

5. 监控和运维:
   监控指标:
     - 集群健康状态
     - 资源同步状态
     - API延迟和错误率
     - 副本分布情况
   
   告警配置:
     - 集群不可用告警
     - 同步失败告警
     - 性能下降告警
     - 配置漂移告警
   
   日志管理:
     - 收集KubeFed控制器日志
     - 收集集群事件
     - 集中存储和分析
     - 保留足够的历史记录
   
   故障处理:
     - 建立故障响应流程
     - 准备回滚方案
     - 定期演练故障场景
     - 记录故障和解决方案

6. 性能优化:
   控制器优化:
     - 调整并发数
     - 配置合理的同步间隔
     - 使用缓存减少API调用
     - 监控控制器资源使用
   
   网络优化:
     - 使用专线连接
     - 配置合理的超时时间
     - 启用连接池
     - 压缩大型资源
   
   资源优化:
     - 避免创建过多联邦资源
     - 合并相关资源
     - 定期清理未使用资源
     - 使用命名空间隔离

7. 成本管理:
   资源规划:
     - 合理分配集群资源
     - 避免过度冗余
     - 使用Spot实例
     - 实施自动扩缩容
   
   成本监控:
     - 跟踪每个集群的成本
     - 分析资源利用率
     - 识别浪费
     - 优化资源分配
   
   成本优化:
     - 使用混合云策略
     - 利用不同云厂商优势
     - 按需扩展集群
     - 定期审查和优化

8. 故障排查:
   常见问题:
     - 集群无法注册: 检查网络和证书
     - 资源同步失败: 检查RBAC和配额
     - 副本分配不均: 检查调度策略
     - 性能下降: 检查网络延迟和资源
   
   调试工具:
     - kubefedctl: 命令行工具
     - kubectl: 查看联邦资源
     - 日志分析: 控制器日志
     - 事件查看: kubectl get events
   
   诊断步骤:
     1. 检查集群健康状态
     2. 查看联邦资源状态
     3. 检查控制器日志
     4. 验证网络连通性
     5. 检查RBAC权限
     6. 查看资源配额
```

**KubeFed vs 其他方案对比**

```yaml
# 多集群管理方案对比

KubeFed:
  优势:
    - Kubernetes官方项目
    - 统一的API和资源模型
    - 支持跨集群资源调度
    - 灵活的放置和覆盖策略
  
  劣势:
    - 学习曲线陡峭
    - 实现复杂
    - 社区活跃度下降
    - 缺少企业级支持
  
  适用场景:
    - 需要跨集群资源调度
    - 多地域部署
    - 高可用要求

Rancher:
  优势:
    - 完整的管理界面
    - 易于使用
    - 企业级支持
    - 丰富的功能
  
  劣势:
    - 商业产品
    - 额外的管理开销
    - 可能的厂商锁定
  
  适用场景:
    - 需要图形界面
    - 企业级部署
    - 多云管理

Open Cluster Management (OCM):
  优势:
    - 红帽支持
    - 模块化设计
    - 扩展性好
    - 活跃的社区
  
  劣势:
    - 相对较新
    - 文档不够完善
    - 学习成本
  
  适用场景:
    - 大规模集群管理
    - 需要扩展性
    - 红帽生态

ArgoCD + ApplicationSet:
  优势:
    - GitOps原生
    - 声明式管理
    - 易于集成CI/CD
    - 活跃的社区
  
  劣势:
    - 不支持跨集群调度
    - 需要额外工具
    - 功能相对简单
  
  适用场景:
    - GitOps工作流
    - 应用部署管理
    - 中小规模集群
```



### 10.2.3 多集群服务网格

**多集群服务网格概述**

服务网格（Service Mesh）为微服务提供了流量管理、安全、可观测性等能力。在多集群环境中，服务网格可以实现跨集群的服务通信和统一管理。

**多集群服务网格的价值**：

```yaml
# 多集群服务网格的核心能力

1. 跨集群服务发现:
   功能:
     - 自动发现所有集群的服务
     - 统一的服务注册表
     - 动态更新服务端点
   
   实现:
     - DNS解析
     - 服务注册中心
     - 控制平面同步

2. 智能流量路由:
   功能:
     - 跨集群负载均衡
     - 地理位置路由
     - 故障转移
     - 金丝雀发布
   
   策略:
     - 基于延迟的路由
     - 基于地域的路由
     - 权重分配
     - 流量镜像

3. 安全通信:
   功能:
     - mTLS加密
     - 身份认证
     - 授权策略
     - 证书管理
   
   实现:
     - 自动证书轮换
     - 统一的身份体系
     - 细粒度访问控制

4. 可观测性:
   功能:
     - 分布式追踪
     - 指标收集
     - 日志聚合
     - 服务拓扑
   
   工具:
     - Jaeger/Zipkin
     - Prometheus
     - Grafana
     - Kiali

5. 弹性和容错:
   功能:
     - 超时控制
     - 重试策略
     - 熔断器
     - 限流
   
   策略:
     - 自动重试
     - 故障注入
     - 降级处理
```

**Istio多集群部署**

```yaml
# istio-multi-cluster.yaml
# Istio多集群配置

---
# 模式1: 单一控制平面（Primary-Remote）

# 架构：
#   - Primary集群：运行Istio控制平面
#   - Remote集群：只运行数据平面，连接到Primary

# Primary集群配置
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-primary
  namespace: istio-system
spec:
  profile: default
  
  # 多集群配置
  values:
    global:
      # 网络配置
      meshID: mesh1
      multiCluster:
        clusterName: cluster-1
      network: network1
      
      # 启用多集群
      meshNetworks:
        network1:
          endpoints:
          - fromRegistry: cluster-1
          gateways:
          - address: 0.0.0.0
            port: 15443
        network2:
          endpoints:
          - fromRegistry: cluster-2
          gateways:
          - registryServiceName: istio-eastwestgateway.istio-system.svc.cluster.local
            port: 15443
  
  components:
    # 东西向网关（用于跨集群通信）
    ingressGateways:
    - name: istio-eastwestgateway
      label:
        istio: eastwestgateway
        app: istio-eastwestgateway
        topology.istio.io/network: network1
      enabled: true
      k8s:
        env:
        - name: ISTIO_META_ROUTER_MODE
          value: "sni-dnat"
        - name: ISTIO_META_REQUESTED_NETWORK_VIEW
          value: network1
        service:
          type: LoadBalancer
          ports:
          - name: status-port
            port: 15021
            targetPort: 15021
          - name: tls
            port: 15443
            targetPort: 15443
          - name: tls-istiod
            port: 15012
            targetPort: 15012
          - name: tls-webhook
            port: 15017
            targetPort: 15017

---
# Remote集群配置
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-remote
  namespace: istio-system
spec:
  profile: remote
  
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster-2
      network: network2
      
      # 连接到Primary集群的控制平面
      remotePilotAddress: istiod.istio-system.svc.cluster-1.global
  
  components:
    # 东西向网关
    ingressGateways:
    - name: istio-eastwestgateway
      label:
        istio: eastwestgateway
        app: istio-eastwestgateway
        topology.istio.io/network: network2
      enabled: true
      k8s:
        env:
        - name: ISTIO_META_ROUTER_MODE
          value: "sni-dnat"
        - name: ISTIO_META_REQUESTED_NETWORK_VIEW
          value: network2
        service:
          type: LoadBalancer
          ports:
          - name: status-port
            port: 15021
          - name: tls
            port: 15443
          - name: tls-istiod
            port: 15012
          - name: tls-webhook
            port: 15017

---
# 模式2: 多控制平面（Multi-Primary）

# 架构：
#   - 每个集群都运行完整的Istio控制平面
#   - 控制平面之间同步服务发现信息

# Cluster-1配置
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-cluster1
  namespace: istio-system
spec:
  profile: default
  
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster-1
      network: network1
  
  components:
    ingressGateways:
    - name: istio-eastwestgateway
      enabled: true
      # ... 配置同上 ...

---
# Cluster-2配置
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-cluster2
  namespace: istio-system
spec:
  profile: default
  
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster-2
      network: network2
  
  components:
    ingressGateways:
    - name: istio-eastwestgateway
      enabled: true
      # ... 配置同上 ...
```

**Istio多集群部署脚本**

```bash
#!/bin/bash
# deploy-istio-multicluster.sh
# 部署Istio多集群

set -e

ISTIO_VERSION="1.20.0"
PRIMARY_CLUSTER="cluster-1"
REMOTE_CLUSTER="cluster-2"
MESH_ID="mesh1"
NETWORK1="network1"
NETWORK2="network2"

echo "=== 部署Istio多集群 ==="

# 1. 下载Istio
echo "1. 下载Istio ${ISTIO_VERSION}..."
curl -L https://istio.io/downloadIstio | ISTIO_VERSION=${ISTIO_VERSION} sh -
cd istio-${ISTIO_VERSION}
export PATH=$PWD/bin:$PATH

# 2. 在Primary集群安装Istio
echo "2. 在Primary集群安装Istio..."
kubectl config use-context $PRIMARY_CLUSTER

# 创建istio-system namespace
kubectl create namespace istio-system --dry-run=client -o yaml | kubectl apply -f -

# 创建CA证书（用于多集群mTLS）
kubectl create secret generic cacerts -n istio-system     --from-file=samples/certs/ca-cert.pem     --from-file=samples/certs/ca-key.pem     --from-file=samples/certs/root-cert.pem     --from-file=samples/certs/cert-chain.pem     --dry-run=client -o yaml | kubectl apply -f -

# 安装Istio（Primary配置）
istioctl install -y -f - <<EOF
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: default
  values:
    global:
      meshID: ${MESH_ID}
      multiCluster:
        clusterName: ${PRIMARY_CLUSTER}
      network: ${NETWORK1}
  components:
    ingressGateways:
    - name: istio-eastwestgateway
      label:
        istio: eastwestgateway
      enabled: true
      k8s:
        env:
        - name: ISTIO_META_ROUTER_MODE
          value: "sni-dnat"
        service:
          type: LoadBalancer
          ports:
          - port: 15021
            name: status-port
          - port: 15443
            name: tls
          - port: 15012
            name: tls-istiod
          - port: 15017
            name: tls-webhook
EOF

# 3. 暴露Primary集群的控制平面
echo "3. 暴露Primary集群的控制平面..."
kubectl apply -n istio-system -f samples/multicluster/expose-istiod.yaml

# 4. 暴露Primary集群的服务
echo "4. 暴露Primary集群的服务..."
kubectl apply -n istio-system -f samples/multicluster/expose-services.yaml

# 5. 获取东西向网关地址
echo "5. 获取东西向网关地址..."
export DISCOVERY_ADDRESS=$(kubectl -n istio-system get svc istio-eastwestgateway     -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo "Discovery Address: $DISCOVERY_ADDRESS"

# 6. 在Remote集群安装Istio
echo "6. 在Remote集群安装Istio..."
kubectl config use-context $REMOTE_CLUSTER

# 创建istio-system namespace
kubectl create namespace istio-system --dry-run=client -o yaml | kubectl apply -f -

# 复制CA证书到Remote集群
kubectl get secret cacerts -n istio-system --context=$PRIMARY_CLUSTER -o yaml |     kubectl apply --context=$REMOTE_CLUSTER -f -

# 安装Istio（Remote配置）
istioctl install -y --context=$REMOTE_CLUSTER -f - <<EOF
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: remote
  values:
    global:
      meshID: ${MESH_ID}
      multiCluster:
        clusterName: ${REMOTE_CLUSTER}
      network: ${NETWORK2}
      remotePilotAddress: ${DISCOVERY_ADDRESS}
  components:
    ingressGateways:
    - name: istio-eastwestgateway
      label:
        istio: eastwestgateway
      enabled: true
      k8s:
        env:
        - name: ISTIO_META_ROUTER_MODE
          value: "sni-dnat"
        service:
          type: LoadBalancer
          ports:
          - port: 15021
            name: status-port
          - port: 15443
            name: tls
EOF

# 7. 配置Remote集群访问
echo "7. 配置Remote集群访问..."
istioctl create-remote-secret     --context=$REMOTE_CLUSTER     --name=$REMOTE_CLUSTER |     kubectl apply -f - --context=$PRIMARY_CLUSTER

# 8. 验证安装
echo "8. 验证安装..."
kubectl config use-context $PRIMARY_CLUSTER
kubectl get pods -n istio-system

kubectl config use-context $REMOTE_CLUSTER
kubectl get pods -n istio-system

echo "=== Istio多集群部署完成 ==="
```

**跨集群服务通信**

```yaml
# cross-cluster-service.yaml
# 跨集群服务配置

---
# 1. 在Cluster-1部署服务A
apiVersion: v1
kind: Namespace
metadata:
  name: demo
  labels:
    istio-injection: enabled

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-a
  namespace: demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: service-a
  template:
    metadata:
      labels:
        app: service-a
        version: v1
    spec:
      containers:
      - name: service-a
        image: myregistry/service-a:v1
        ports:
        - containerPort: 8080
        env:
        - name: CLUSTER_NAME
          value: cluster-1

---
apiVersion: v1
kind: Service
metadata:
  name: service-a
  namespace: demo
spec:
  selector:
    app: service-a
  ports:
  - port: 8080
    targetPort: 8080

---
# 2. 在Cluster-2部署服务B
# （在cluster-2上执行）
apiVersion: v1
kind: Namespace
metadata:
  name: demo
  labels:
    istio-injection: enabled

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-b
  namespace: demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: service-b
  template:
    metadata:
      labels:
        app: service-b
        version: v1
    spec:
      containers:
      - name: service-b
        image: myregistry/service-b:v1
        ports:
        - containerPort: 8080
        env:
        - name: CLUSTER_NAME
          value: cluster-2

---
apiVersion: v1
kind: Service
metadata:
  name: service-b
  namespace: demo
spec:
  selector:
    app: service-b
  ports:
  - port: 8080
    targetPort: 8080

---
# 3. ServiceEntry（使服务在两个集群都可见）
# 在Primary集群创建
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: service-b-cluster2
  namespace: demo
spec:
  hosts:
  - service-b.demo.svc.cluster.local
  location: MESH_INTERNAL
  ports:
  - number: 8080
    name: http
    protocol: HTTP
  resolution: DNS
  endpoints:
  - address: service-b.demo.svc.cluster-2.global
    ports:
      http: 8080
    labels:
      cluster: cluster-2

---
# 4. DestinationRule（流量策略）
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: service-b
  namespace: demo
spec:
  host: service-b.demo.svc.cluster.local
  trafficPolicy:
    # 连接池配置
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100
    
    # 负载均衡
    loadBalancer:
      simple: LEAST_REQUEST
    
    # 异常检测
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
  
  # 子集定义
  subsets:
  - name: cluster-1
    labels:
      cluster: cluster-1
  - name: cluster-2
    labels:
      cluster: cluster-2

---
# 5. VirtualService（路由规则）
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: service-b
  namespace: demo
spec:
  hosts:
  - service-b.demo.svc.cluster.local
  http:
  - match:
    - headers:
        x-cluster:
          exact: cluster-2
    route:
    - destination:
        host: service-b.demo.svc.cluster.local
        subset: cluster-2
  
  # 默认路由：按权重分配
  - route:
    - destination:
        host: service-b.demo.svc.cluster.local
        subset: cluster-1
      weight: 70
    - destination:
        host: service-b.demo.svc.cluster.local
        subset: cluster-2
      weight: 30
    
    # 超时和重试
    timeout: 10s
    retries:
      attempts: 3
      perTryTimeout: 2s
      retryOn: 5xx,reset,connect-failure,refused-stream
```


**多集群流量管理**

```yaml
# advanced-traffic-management.yaml
# 高级流量管理策略

---
# 1. 地理位置路由
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: geo-routing
  namespace: demo
spec:
  hosts:
  - api.example.com
  gateways:
  - istio-system/global-gateway
  http:
  # 欧洲用户路由到EU集群
  - match:
    - headers:
        x-user-region:
          exact: eu
    route:
    - destination:
        host: api-service.demo.svc.cluster.local
        subset: eu-cluster
  
  # 亚洲用户路由到APAC集群
  - match:
    - headers:
        x-user-region:
          exact: apac
    route:
    - destination:
        host: api-service.demo.svc.cluster.local
        subset: apac-cluster
  
  # 默认路由到最近的集群
  - route:
    - destination:
        host: api-service.demo.svc.cluster.local
      weight: 100
    mirror:
      host: api-service.demo.svc.cluster.local
      subset: backup-cluster
    mirrorPercentage:
      value: 10.0

---
# 2. 故障转移
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: failover
  namespace: demo
spec:
  host: critical-service.demo.svc.cluster.local
  trafficPolicy:
    # 连接池
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 2
    
    # 负载均衡（优先本地）
    loadBalancer:
      localityLbSetting:
        enabled: true
        distribute:
        - from: us-west/*
          to:
            "us-west/*": 80
            "us-east/*": 20
        - from: us-east/*
          to:
            "us-east/*": 80
            "us-west/*": 20
        failover:
        - from: us-west/zone1
          to: us-west/zone2
        - from: us-west/zone2
          to: us-east/zone1
    
    # 异常检测和熔断
    outlierDetection:
      consecutiveGatewayErrors: 5
      consecutive5xxErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
      minHealthPercent: 40

---
# 3. 金丝雀发布（跨集群）
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: canary-release
  namespace: demo
spec:
  hosts:
  - app.example.com
  http:
  # 10%流量到新版本（cluster-2）
  - match:
    - headers:
        x-canary:
          exact: "true"
    route:
    - destination:
        host: app-service.demo.svc.cluster.local
        subset: v2-cluster2
      weight: 100
  
  # 普通流量：90% v1，10% v2
  - route:
    - destination:
        host: app-service.demo.svc.cluster.local
        subset: v1-cluster1
      weight: 90
    - destination:
        host: app-service.demo.svc.cluster.local
        subset: v2-cluster2
      weight: 10

---
# 4. 流量镜像（跨集群测试）
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: traffic-mirroring
  namespace: demo
spec:
  hosts:
  - test-service.demo.svc.cluster.local
  http:
  - route:
    - destination:
        host: test-service.demo.svc.cluster.local
        subset: production-cluster1
      weight: 100
    
    # 镜像20%流量到测试集群
    mirror:
      host: test-service.demo.svc.cluster.local
      subset: test-cluster2
    mirrorPercentage:
      value: 20.0

---
# 5. 超时和重试策略
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: resilience
  namespace: demo
spec:
  hosts:
  - resilient-service.demo.svc.cluster.local
  http:
  - route:
    - destination:
        host: resilient-service.demo.svc.cluster.local
    
    # 超时配置
    timeout: 10s
    
    # 重试策略
    retries:
      attempts: 3
      perTryTimeout: 2s
      retryOn: 5xx,reset,connect-failure,refused-stream
    
    # 故障注入（测试）
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
      abort:
        percentage:
          value: 0.01
        httpStatus: 503
```

**多集群安全配置**

```yaml
# multi-cluster-security.yaml
# 多集群安全配置

---
# 1. PeerAuthentication（mTLS）
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: istio-system
spec:
  # 全局启用严格mTLS
  mtls:
    mode: STRICT

---
# 2. AuthorizationPolicy（跨集群访问控制）
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: cross-cluster-authz
  namespace: demo
spec:
  selector:
    matchLabels:
      app: sensitive-service
  
  action: ALLOW
  
  rules:
  # 允许来自cluster-1的service-a访问
  - from:
    - source:
        principals:
        - "cluster.local/ns/demo/sa/service-a"
        namespaces:
        - demo
    to:
    - operation:
        methods: ["GET", "POST"]
        paths: ["/api/*"]
  
  # 允许来自cluster-2的service-b访问
  - from:
    - source:
        principals:
        - "cluster-2.local/ns/demo/sa/service-b"
        namespaces:
        - demo
    to:
    - operation:
        methods: ["GET"]
        paths: ["/api/read/*"]

---
# 3. RequestAuthentication（JWT验证）
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-auth
  namespace: demo
spec:
  selector:
    matchLabels:
      app: api-gateway
  
  jwtRules:
  - issuer: "https://auth.example.com"
    jwksUri: "https://auth.example.com/.well-known/jwks.json"
    audiences:
    - "api.example.com"
    forwardOriginalToken: true

---
# 4. 证书管理
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: istio-ca
  namespace: istio-system
spec:
  secretName: cacerts
  duration: 87600h # 10 years
  renewBefore: 8760h # 1 year
  commonName: istio-ca
  isCA: true
  issuerRef:
    name: selfsigned-issuer
    kind: ClusterIssuer
```

**Linkerd多集群**

```yaml
# linkerd-multi-cluster.yaml
# Linkerd多集群配置

---
# Linkerd多集群架构更简单，使用Service Mirror

# 1. 在源集群创建Link
apiVersion: multicluster.linkerd.io/v1alpha1
kind: Link
metadata:
  name: cluster-2
  namespace: linkerd-multicluster
spec:
  targetClusterName: cluster-2
  targetClusterDomain: cluster.local
  gatewayAddress: 10.0.2.100
  gatewayPort: 4143
  gatewayIdentity: linkerd-gateway.linkerd-multicluster.serviceaccount.identity.linkerd.cluster.local
  probeSpec:
    path: /ready
    port: 4191
    period: 3s

---
# 2. 导出服务（在目标集群）
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: demo
  annotations:
    # 标记为可导出
    mirror.linkerd.io/exported: "true"
spec:
  selector:
    app: backend
  ports:
  - port: 8080
    targetPort: 8080

---
# 3. 在源集群自动创建镜像服务
# Linkerd会自动创建：backend-cluster-2.demo.svc.cluster.local

---
# 4. 流量分割
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  name: backend-split
  namespace: demo
spec:
  service: backend
  backends:
  - service: backend
    weight: 70
  - service: backend-cluster-2
    weight: 30
```

**Consul Connect多集群**

```yaml
# consul-multi-cluster.yaml
# Consul Connect多集群配置

---
# 1. Consul配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: consul-config
  namespace: consul
data:
  config.json: |
    {
      "datacenter": "dc1",
      "primary_datacenter": "dc1",
      "connect": {
        "enabled": true
      },
      "ports": {
        "grpc": 8502
      },
      "enable_central_service_config": true,
      "config_entries": {
        "bootstrap": [
          {
            "kind": "proxy-defaults",
            "name": "global",
            "config": {
              "protocol": "http"
            },
            "mesh_gateway": {
              "mode": "local"
            }
          }
        ]
      }
    }

---
# 2. Mesh Gateway（用于跨集群通信）
apiVersion: v1
kind: Service
metadata:
  name: mesh-gateway
  namespace: consul
spec:
  type: LoadBalancer
  selector:
    app: mesh-gateway
  ports:
  - port: 443
    targetPort: 8443
    name: https

---
# 3. 服务导出
apiVersion: consul.hashicorp.com/v1alpha1
kind: ExportedServices
metadata:
  name: default
  namespace: consul
spec:
  services:
  - name: backend
    consumers:
    - peer: cluster-2

---
# 4. 服务意图（访问控制）
apiVersion: consul.hashicorp.com/v1alpha1
kind: ServiceIntentions
metadata:
  name: backend
  namespace: demo
spec:
  destination:
    name: backend
  sources:
  - name: frontend
    action: allow
    permissions:
    - http:
        pathPrefix: "/api"
        methods: ["GET", "POST"]
  - name: frontend
    peer: cluster-2
    action: allow
```

**多集群服务网格对比**

```yaml
# 服务网格方案对比

Istio:
  优势:
    - 功能最完善
    - 社区最活跃
    - 企业级支持
    - 丰富的流量管理
  
  劣势:
    - 复杂度高
    - 资源开销大
    - 学习曲线陡峭
  
  多集群支持:
    - Primary-Remote模式
    - Multi-Primary模式
    - 跨网络支持
    - 东西向网关
  
  适用场景:
    - 大型企业
    - 复杂微服务
    - 需要高级功能

Linkerd:
  优势:
    - 轻量级
    - 性能好
    - 易于使用
    - Rust实现
  
  劣势:
    - 功能相对简单
    - 社区较小
    - 扩展性有限
  
  多集群支持:
    - Service Mirror
    - Gateway模式
    - 简单配置
  
  适用场景:
    - 中小型企业
    - 性能敏感
    - 简单场景

Consul Connect:
  优势:
    - 与Consul集成
    - 多数据中心原生支持
    - 服务发现强大
    - 跨平台
  
  劣势:
    - Kubernetes集成不如Istio
    - 流量管理功能较少
    - 学习成本
  
  多集群支持:
    - Mesh Gateway
    - WAN Federation
    - Cluster Peering
  
  适用场景:
    - 已使用Consul
    - 多数据中心
    - 混合环境

Cilium Service Mesh:
  优势:
    - eBPF性能
    - 网络和服务网格一体
    - 无Sidecar
    - 低延迟
  
  劣势:
    - 相对较新
    - 功能还在完善
    - 需要较新内核
  
  多集群支持:
    - Cluster Mesh
    - 全局服务
    - 跨集群网络
  
  适用场景:
    - 性能要求极高
    - 已使用Cilium CNI
    - 新项目
```



### 10.2.4 多集群管理最佳实践

**多集群运维自动化**

```bash
#!/bin/bash
# multi-cluster-ops.sh
# 多集群运维自动化脚本

set -e

# 集群列表
CLUSTERS=("cluster-1" "cluster-2" "cluster-3")

# 颜色输出
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# 日志函数
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# 1. 健康检查所有集群
check_cluster_health() {
    log_info "检查所有集群健康状态..."
    
    for cluster in "${CLUSTERS[@]}"; do
        echo ""
        log_info "检查集群: $cluster"
        
        # 切换context
        kubectl config use-context $cluster > /dev/null 2>&1
        
        # 检查节点状态
        echo "节点状态:"
        kubectl get nodes --no-headers | while read line; do
            node=$(echo $line | awk '{print $1}')
            status=$(echo $line | awk '{print $2}')
            if [ "$status" != "Ready" ]; then
                log_error "节点 $node 状态异常: $status"
            else
                log_info "节点 $node: $status"
            fi
        done
        
        # 检查系统Pod
        echo "系统Pod状态:"
        kubectl get pods -n kube-system --no-headers | while read line; do
            pod=$(echo $line | awk '{print $1}')
            status=$(echo $line | awk '{print $3}')
            if [ "$status" != "Running" ] && [ "$status" != "Completed" ]; then
                log_error "Pod $pod 状态异常: $status"
            fi
        done
        
        # 检查API Server延迟
        start_time=$(date +%s%N)
        kubectl get --raw /healthz > /dev/null 2>&1
        end_time=$(date +%s%N)
        latency=$(( (end_time - start_time) / 1000000 ))
        
        if [ $latency -gt 1000 ]; then
            log_warn "API Server延迟过高: ${latency}ms"
        else
            log_info "API Server延迟: ${latency}ms"
        fi
    done
}

# 2. 同步配置到所有集群
sync_config() {
    local config_file=$1
    log_info "同步配置到所有集群: $config_file"
    
    for cluster in "${CLUSTERS[@]}"; do
        log_info "应用到集群: $cluster"
        kubectl --context=$cluster apply -f $config_file
    done
}

# 3. 批量执行命令
exec_on_all_clusters() {
    local command=$1
    log_info "在所有集群执行命令: $command"
    
    for cluster in "${CLUSTERS[@]}"; do
        echo ""
        log_info "集群: $cluster"
        kubectl --context=$cluster $command
    done
}

# 4. 收集所有集群的资源使用情况
collect_resource_usage() {
    log_info "收集资源使用情况..."
    
    for cluster in "${CLUSTERS[@]}"; do
        echo ""
        log_info "集群: $cluster"
        
        # CPU和内存使用
        kubectl --context=$cluster top nodes 2>/dev/null || log_warn "Metrics Server未安装"
        
        # Pod数量
        total_pods=$(kubectl --context=$cluster get pods --all-namespaces --no-headers | wc -l)
        running_pods=$(kubectl --context=$cluster get pods --all-namespaces --field-selector=status.phase=Running --no-headers | wc -l)
        log_info "Pod数量: $running_pods/$total_pods (Running/Total)"
        
        # PVC使用
        pvc_count=$(kubectl --context=$cluster get pvc --all-namespaces --no-headers | wc -l)
        log_info "PVC数量: $pvc_count"
    done
}

# 5. 备份所有集群的关键资源
backup_clusters() {
    local backup_dir="./backups/$(date +%Y%m%d-%H%M%S)"
    mkdir -p $backup_dir
    
    log_info "备份所有集群到: $backup_dir"
    
    for cluster in "${CLUSTERS[@]}"; do
        log_info "备份集群: $cluster"
        cluster_backup_dir="$backup_dir/$cluster"
        mkdir -p $cluster_backup_dir
        
        # 备份所有namespace的资源
        for ns in $(kubectl --context=$cluster get ns -o jsonpath='{.items[*].metadata.name}'); do
            log_info "备份namespace: $ns"
            kubectl --context=$cluster get all,cm,secret,pvc -n $ns -o yaml > "$cluster_backup_dir/$ns.yaml" 2>/dev/null || true
        done
        
        # 备份集群级资源
        kubectl --context=$cluster get clusterrole,clusterrolebinding,pv,sc -o yaml > "$cluster_backup_dir/cluster-resources.yaml" 2>/dev/null || true
    done
    
    log_info "备份完成: $backup_dir"
}

# 6. 验证跨集群连通性
test_connectivity() {
    log_info "测试跨集群连通性..."
    
    # 在每个集群部署测试Pod
    for cluster in "${CLUSTERS[@]}"; do
        kubectl --context=$cluster run test-pod-$cluster             --image=busybox             --restart=Never             --command -- sleep 3600 2>/dev/null || true
    done
    
    sleep 10
    
    # 测试连通性
    for src_cluster in "${CLUSTERS[@]}"; do
        for dst_cluster in "${CLUSTERS[@]}"; do
            if [ "$src_cluster" != "$dst_cluster" ]; then
                log_info "测试 $src_cluster -> $dst_cluster"
                # 这里简化，实际需要获取目标集群的Service IP并测试
                # kubectl --context=$src_cluster exec test-pod-$src_cluster -- ping -c 1 <target-ip>
            fi
        done
    done
    
    # 清理测试Pod
    for cluster in "${CLUSTERS[@]}"; do
        kubectl --context=$cluster delete pod test-pod-$cluster --ignore-not-found=true
    done
}

# 7. 生成多集群报告
generate_report() {
    local report_file="multi-cluster-report-$(date +%Y%m%d-%H%M%S).txt"
    
    log_info "生成多集群报告: $report_file"
    
    {
        echo "多集群状态报告"
        echo "生成时间: $(date)"
        echo "="
        echo ""
        
        for cluster in "${CLUSTERS[@]}"; do
            echo "集群: $cluster"
            echo "-"
            
            kubectl --context=$cluster cluster-info
            echo ""
            
            echo "节点数量:"
            kubectl --context=$cluster get nodes --no-headers | wc -l
            echo ""
            
            echo "Namespace数量:"
            kubectl --context=$cluster get ns --no-headers | wc -l
            echo ""
            
            echo "Pod数量:"
            kubectl --context=$cluster get pods --all-namespaces --no-headers | wc -l
            echo ""
            
            echo "Service数量:"
            kubectl --context=$cluster get svc --all-namespaces --no-headers | wc -l
            echo ""
            
            echo ""
        done
    } > $report_file
    
    log_info "报告已生成: $report_file"
}

# 主菜单
show_menu() {
    echo ""
    echo "多集群运维工具"
    echo "="
    echo "1. 健康检查"
    echo "2. 同步配置"
    echo "3. 批量执行命令"
    echo "4. 收集资源使用"
    echo "5. 备份集群"
    echo "6. 测试连通性"
    echo "7. 生成报告"
    echo "0. 退出"
    echo ""
    read -p "请选择操作: " choice
    
    case $choice in
        1) check_cluster_health ;;
        2) read -p "配置文件路径: " config_file; sync_config $config_file ;;
        3) read -p "kubectl命令: " command; exec_on_all_clusters "$command" ;;
        4) collect_resource_usage ;;
        5) backup_clusters ;;
        6) test_connectivity ;;
        7) generate_report ;;
        0) exit 0 ;;
        *) log_error "无效选择" ;;
    esac
    
    show_menu
}

# 如果有参数，直接执行对应功能
if [ $# -gt 0 ]; then
    case $1 in
        health) check_cluster_health ;;
        sync) sync_config $2 ;;
        exec) exec_on_all_clusters "$2" ;;
        usage) collect_resource_usage ;;
        backup) backup_clusters ;;
        test) test_connectivity ;;
        report) generate_report ;;
        *) log_error "未知命令: $1" ;;
    esac
else
    show_menu
fi
```

**多集群监控和可观测性**

```yaml
# multi-cluster-monitoring.yaml
# 多集群监控配置

---
# 1. Prometheus Federation（联邦）
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    # 联邦配置：从其他集群抓取指标
    scrape_configs:
    # 本地集群
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
    
    # 联邦：从cluster-1抓取
    - job_name: 'federate-cluster-1'
      honor_labels: true
      metrics_path: '/federate'
      params:
        'match[]':
        - '{job=~".+"}'
      static_configs:
      - targets:
        - 'prometheus.monitoring.svc.cluster-1.global:9090'
        labels:
          cluster: 'cluster-1'
    
    # 联邦：从cluster-2抓取
    - job_name: 'federate-cluster-2'
      honor_labels: true
      metrics_path: '/federate'
      params:
        'match[]':
        - '{job=~".+"}'
      static_configs:
      - targets:
        - 'prometheus.monitoring.svc.cluster-2.global:9090'
        labels:
          cluster: 'cluster-2'

---
# 2. Thanos（长期存储和全局查询）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: thanos-query
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: thanos-query
  template:
    metadata:
      labels:
        app: thanos-query
    spec:
      containers:
      - name: thanos-query
        image: thanosio/thanos:v0.32.0
        args:
        - query
        - --http-address=0.0.0.0:9090
        - --grpc-address=0.0.0.0:10901
        - --query.replica-label=replica
        # 连接到所有集群的Thanos Sidecar
        - --store=thanos-sidecar.monitoring.svc.cluster-1.global:10901
        - --store=thanos-sidecar.monitoring.svc.cluster-2.global:10901
        - --store=thanos-sidecar.monitoring.svc.cluster-3.global:10901
        # 连接到Thanos Store（长期存储）
        - --store=thanos-store.monitoring.svc.cluster.local:10901
        ports:
        - containerPort: 9090
          name: http
        - containerPort: 10901
          name: grpc

---
# 3. Grafana多集群Dashboard
apiVersion: v1
kind: ConfigMap
metadata:
  name: multi-cluster-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  multi-cluster-overview.json: |
    {
      "dashboard": {
        "title": "Multi-Cluster Overview",
        "panels": [
          {
            "title": "集群健康状态",
            "targets": [
              {
                "expr": "up{job=\"kubernetes-nodes\"}"
              }
            ],
            "type": "stat",
            "fieldConfig": {
              "defaults": {
                "mappings": [
                  {"value": 1, "text": "健康", "color": "green"},
                  {"value": 0, "text": "异常", "color": "red"}
                ]
              }
            }
          },
          {
            "title": "各集群Pod数量",
            "targets": [
              {
                "expr": "count by (cluster) (kube_pod_info)"
              }
            ],
            "type": "graph"
          },
          {
            "title": "各集群CPU使用率",
            "targets": [
              {
                "expr": "sum by (cluster) (rate(container_cpu_usage_seconds_total[5m]))"
              }
            ],
            "type": "graph"
          },
          {
            "title": "各集群内存使用率",
            "targets": [
              {
                "expr": "sum by (cluster) (container_memory_working_set_bytes) / sum by (cluster) (machine_memory_bytes)"
              }
            ],
            "type": "graph"
          },
          {
            "title": "跨集群流量",
            "targets": [
              {
                "expr": "sum by (source_cluster, destination_cluster) (rate(istio_requests_total[5m]))"
              }
            ],
            "type": "heatmap"
          },
          {
            "title": "集群API延迟",
            "targets": [
              {
                "expr": "histogram_quantile(0.99, sum by (cluster, le) (rate(apiserver_request_duration_seconds_bucket[5m])))"
              }
            ],
            "type": "graph"
          }
        ]
      }
    }

---
# 4. 告警规则
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: multi-cluster-alerts
  namespace: monitoring
spec:
  groups:
  - name: multi-cluster
    interval: 30s
    rules:
    # 集群不可达告警
    - alert: ClusterUnreachable
      expr: |
        up{job="federate-cluster-*"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "集群不可达"
        description: "集群 {{ $labels.cluster }} 已不可达超过5分钟"
    
    # 跨集群延迟过高
    - alert: CrossClusterHighLatency
      expr: |
        histogram_quantile(0.99, 
          sum by (source_cluster, destination_cluster, le) (
            rate(istio_request_duration_milliseconds_bucket{
              source_cluster!="",
              destination_cluster!="",
              source_cluster!=destination_cluster
            }[5m])
          )
        ) > 1000
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "跨集群延迟过高"
        description: "从 {{ $labels.source_cluster }} 到 {{ $labels.destination_cluster }} 的P99延迟: {{ $value }}ms"
    
    # 集群资源不均衡
    - alert: ClusterResourceImbalance
      expr: |
        abs(
          max by (cluster) (sum by (cluster) (kube_pod_info)) - 
          min by (cluster) (sum by (cluster) (kube_pod_info))
        ) > 100
      for: 30m
      labels:
        severity: info
      annotations:
        summary: "集群资源分配不均衡"
        description: "集群间Pod数量差异过大"
```


**多集群灾难恢复**

```yaml
# disaster-recovery.yaml
# 多集群灾难恢复策略

---
# 1. 备份策略

备份范围:
  应用数据:
    - 数据库备份
    - 对象存储备份
    - PV快照
  
  配置数据:
    - Kubernetes资源定义
    - ConfigMap和Secret
    - RBAC配置
  
  集群状态:
    - etcd备份
    - 证书和密钥
    - 自定义资源

备份频率:
  关键数据: 每小时
  应用配置: 每天
  集群状态: 每周

备份存储:
  - 跨地域对象存储
  - 多副本保存
  - 加密存储
  - 定期验证

---
# 2. Velero多集群备份配置

apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  name: default
  namespace: velero
spec:
  provider: aws
  objectStorage:
    bucket: multi-cluster-backups
    prefix: cluster-1
  config:
    region: us-west-2
    s3ForcePathStyle: "true"

---
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup
  namespace: velero
spec:
  schedule: "0 2 * * *"  # 每天凌晨2点
  template:
    includedNamespaces:
    - "*"
    excludedNamespaces:
    - kube-system
    - kube-public
    includedResources:
    - "*"
    snapshotVolumes: true
    ttl: 720h  # 保留30天

---
# 3. 故障转移流程

故障检测:
  自动检测:
    - 健康检查失败
    - API Server不可达
    - 节点大量失败
    - 网络分区
  
  手动触发:
    - 计划维护
    - 灾难演练
    - 紧急情况

故障转移步骤:
  1. 确认故障:
     - 验证集群状态
     - 评估影响范围
     - 确定恢复策略
  
  2. 流量切换:
     - 更新DNS记录
     - 调整负载均衡器
     - 修改路由规则
  
  3. 数据恢复:
     - 恢复最新备份
     - 验证数据完整性
     - 同步增量数据
  
  4. 服务验证:
     - 健康检查
     - 功能测试
     - 性能验证
  
  5. 监控和调优:
     - 监控关键指标
     - 调整资源配置
     - 优化性能

故障恢复:
  1. 修复原集群
  2. 数据同步
  3. 流量回切
  4. 验证和监控

---
# 4. 自动故障转移配置

apiVersion: v1
kind: ConfigMap
metadata:
  name: failover-config
  namespace: kube-system
data:
  config.yaml: |
    # 故障检测配置
    healthCheck:
      interval: 30s
      timeout: 10s
      failureThreshold: 3
      successThreshold: 1
    
    # 故障转移策略
    failover:
      # 自动故障转移
      automatic: true
      
      # 故障转移目标
      targets:
      - cluster: cluster-2
        priority: 1
        weight: 100
      - cluster: cluster-3
        priority: 2
        weight: 50
      
      # 回切策略
      failback:
        automatic: false
        cooldown: 1h
    
    # 通知配置
    notifications:
      slack:
        webhook: https://hooks.slack.com/services/xxx
      email:
        to: ops@example.com
```

**多集群成本优化**

```yaml
# cost-optimization.yaml
# 多集群成本优化策略

---
# 1. 成本分析维度

成本构成:
  计算成本:
    - 节点费用（EC2、GCE等）
    - CPU和内存使用
    - Spot/Preemptible实例
  
  存储成本:
    - 持久化存储（EBS、PD等）
    - 对象存储（S3、GCS等）
    - 备份存储
  
  网络成本:
    - 跨区域流量
    - 跨云流量
    - 负载均衡器
  
  管理成本:
    - 控制平面费用
    - 监控和日志
    - 备份和恢复

---
# 2. 成本优化策略

计算优化:
  策略:
    - 使用Spot/Preemptible实例
    - 自动扩缩容
    - 合理设置资源请求和限制
    - 使用节点亲和性优化调度
  
  实施:
    # Cluster Autoscaler配置
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: cluster-autoscaler-config
      namespace: kube-system
    data:
      config.yaml: |
        scaleDown:
          enabled: true
          delayAfterAdd: 10m
          delayAfterDelete: 10s
          delayAfterFailure: 3m
          unneededTime: 10m
          utilizationThreshold: 0.5
        
        nodeGroups:
        - name: spot-nodes
          minSize: 0
          maxSize: 100
          priority: 100
        - name: on-demand-nodes
          minSize: 3
          maxSize: 20
          priority: 50

存储优化:
  策略:
    - 使用合适的存储类型
    - 定期清理未使用的PV
    - 压缩和去重
    - 生命周期管理
  
  实施:
    # StorageClass配置
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: cost-optimized
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: gp3
      iops: "3000"
      throughput: "125"
    reclaimPolicy: Delete
    volumeBindingMode: WaitForFirstConsumer

网络优化:
  策略:
    - 减少跨区域流量
    - 使用CDN
    - 优化服务网格配置
    - 压缩数据传输
  
  实施:
    # 本地优先路由
    apiVersion: networking.istio.io/v1beta1
    kind: DestinationRule
    metadata:
      name: locality-lb
    spec:
      host: "*.svc.cluster.local"
      trafficPolicy:
        loadBalancer:
          localityLbSetting:
            enabled: true
            distribute:
            - from: "us-west/*"
              to:
                "us-west/*": 90
                "us-east/*": 10

---
# 3. 成本监控和告警

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cost-alerts
  namespace: monitoring
spec:
  groups:
  - name: cost
    interval: 1h
    rules:
    # 成本超预算告警
    - alert: CostOverBudget
      expr: |
        sum by (cluster) (cluster_cost_hourly) * 24 * 30 > cluster_budget_monthly
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "集群成本超预算"
        description: "集群 {{ $labels.cluster }} 月度成本预计: ${{ $value }}"
    
    # 资源浪费告警
    - alert: ResourceWaste
      expr: |
        (
          sum by (cluster) (kube_pod_container_resource_requests_cpu_cores) - 
          sum by (cluster) (rate(container_cpu_usage_seconds_total[5m]))
        ) / sum by (cluster) (kube_pod_container_resource_requests_cpu_cores) > 0.5
      for: 6h
      labels:
        severity: info
      annotations:
        summary: "资源利用率低"
        description: "集群 {{ $labels.cluster }} CPU利用率: {{ $value | humanizePercentage }}"
```

**多集群最佳实践总结**

```yaml
# 多集群管理最佳实践

1. 架构设计:
   原则:
     - 松耦合设计
     - 故障隔离
     - 自治能力
     - 可扩展性
   
   建议:
     - 根据业务需求选择合适的多集群模式
     - 设计清晰的集群边界和职责
     - 预留足够的扩展空间
     - 考虑未来的演进路径

2. 网络规划:
   原则:
     - 避免IP冲突
     - 低延迟连接
     - 安全隔离
     - 高可用性
   
   建议:
     - 提前规划IP地址空间
     - 使用专线或VPN连接
     - 实施网络分段和隔离
     - 配置冗余网络路径

3. 服务发现:
   原则:
     - 统一命名
     - 自动发现
     - 健康检查
     - 负载均衡
   
   建议:
     - 使用服务网格实现跨集群服务发现
     - 配置合理的DNS TTL
     - 实施健康检查和故障转移
     - 监控服务可用性

4. 数据管理:
   原则:
     - 数据一致性
     - 备份恢复
     - 跨集群同步
     - 合规性
   
   建议:
     - 选择合适的数据复制策略
     - 定期备份关键数据
     - 测试恢复流程
     - 遵守数据主权要求

5. 安全性:
   原则:
     - 零信任
     - 最小权限
     - 加密传输
     - 审计日志
   
   建议:
     - 使用mTLS加密集群间通信
     - 实施细粒度的访问控制
     - 定期轮换证书和密钥
     - 启用审计日志并定期审查

6. 监控和可观测性:
   原则:
     - 统一视图
     - 实时监控
     - 分布式追踪
     - 告警及时
   
   建议:
     - 使用Prometheus Federation或Thanos
     - 配置跨集群的分布式追踪
     - 建立统一的日志收集系统
     - 设置合理的告警阈值

7. 运维自动化:
   原则:
     - 基础设施即代码
     - GitOps工作流
     - 自动化部署
     - 自愈能力
   
   建议:
     - 使用Terraform管理基础设施
     - 使用ArgoCD或Flux实现GitOps
     - 自动化常见运维任务
     - 实施自动故障恢复

8. 成本优化:
   原则:
     - 资源利用率
     - 按需扩展
     - 成本可见性
     - 持续优化
   
   建议:
     - 使用Spot/Preemptible实例
     - 实施自动扩缩容
     - 监控和分析成本
     - 定期审查和优化

9. 灾难恢复:
   原则:
     - 定期备份
     - 快速恢复
     - 演练验证
     - 文档完善
   
   建议:
     - 制定详细的DR计划
     - 定期备份关键数据和配置
     - 定期进行DR演练
     - 保持DR文档更新

10. 团队协作:
    原则:
      - 清晰职责
      - 有效沟通
      - 知识共享
      - 持续改进
    
    建议:
      - 建立清晰的运维流程
      - 使用协作工具（Slack、Jira等）
      - 定期分享经验和教训
      - 建立知识库和文档

# 实施路线图

阶段1: 规划和设计（2-4周）
  任务:
    - 评估多集群需求
    - 设计架构方案
    - 规划网络和存储
    - 制定实施计划
  
  交付物:
    - 架构设计文档
    - 网络规划文档
    - 实施计划
    - 风险评估

阶段2: 基础设施准备（2-4周）
  任务:
    - 部署Kubernetes集群
    - 配置网络连接
    - 安装基础组件
    - 配置监控和日志
  
  交付物:
    - 就绪的集群环境
    - 网络连通性验证
    - 监控系统
    - 运维文档

阶段3: 多集群管理实施（3-6周）
  任务:
    - 部署联邦或服务网格
    - 配置跨集群服务发现
    - 实施安全策略
    - 配置备份和恢复
  
  交付物:
    - 多集群管理平台
    - 服务网格配置
    - 安全策略
    - 备份系统

阶段4: 应用迁移（4-8周）
  任务:
    - 迁移应用到多集群
    - 配置跨集群路由
    - 优化性能
    - 验证功能
  
  交付物:
    - 迁移后的应用
    - 性能测试报告
    - 功能验证报告
    - 用户文档

阶段5: 优化和完善（持续）
  任务:
    - 监控和调优
    - 成本优化
    - 安全加固
    - 持续改进
  
  交付物:
    - 优化报告
    - 成本分析
    - 安全审计报告
    - 改进建议

# 常见陷阱和避免方法

陷阱1: 过度复杂化
  问题: 引入不必要的复杂性
  避免: 从简单开始，按需扩展

陷阱2: 忽视网络延迟
  问题: 跨集群通信延迟高
  避免: 优化网络连接，使用本地优先路由

陷阱3: 数据一致性问题
  问题: 跨集群数据不一致
  避免: 选择合适的一致性模型，实施数据验证

陷阱4: 安全配置不当
  问题: 集群间通信不安全
  避免: 使用mTLS，实施严格的访问控制

陷阱5: 缺乏监控
  问题: 无法及时发现问题
  避免: 建立完善的监控和告警体系

陷阱6: 成本失控
  问题: 多集群成本过高
  避免: 监控成本，实施优化策略

陷阱7: 缺乏灾难恢复计划
  问题: 灾难发生时无法快速恢复
  避免: 制定DR计划，定期演练

陷阱8: 文档不完善
  问题: 运维困难，知识流失
  避免: 建立完善的文档体系
```

**本节总结**

在本节中，我们全面探讨了集群联邦与多集群管理的各个方面：

1. **多集群架构设计**：介绍了多集群的驱动因素、架构模式（独立集群、主从集群、网格集群、联邦集群）、网络架构、存储架构和设计原则，并提供了详细的决策树帮助选择合适的方案。

2. **KubeFed集群联邦**：深入讲解了KubeFed的核心概念、安装部署、联邦资源配置、高级放置策略、管理工具、实战案例、故障转移和最佳实践，提供了完整的Go管理工具和多地域高可用部署案例。

3. **多集群服务网格**：详细介绍了Istio、Linkerd、Consul Connect等服务网格的多集群部署方案，包括跨集群服务通信、高级流量管理（地理位置路由、故障转移、金丝雀发布）、安全配置和方案对比。

4. **多集群管理最佳实践**：总结了运维自动化、监控和可观测性、灾难恢复、成本优化等方面的最佳实践，提供了完整的运维脚本、监控配置和实施路线图。

**关键要点**：

- 根据业务需求选择合适的多集群架构模式
- 重视网络规划和连通性
- 使用服务网格实现跨集群服务通信
- 建立完善的监控和可观测性体系
- 制定灾难恢复计划并定期演练
- 持续优化成本和性能
- 自动化运维流程

**下一节预告**：

在下一节中，我们将学习灾难恢复与业务连续性，探讨如何设计和实施完善的备份恢复策略，确保业务在各种故障场景下的连续性。



## 10.3 灾难恢复与业务连续性

### 10.3.1 灾难恢复基础

**灾难恢复概述**

灾难恢复（Disaster Recovery, DR）是指在发生灾难性事件后，恢复IT系统和业务运营的能力。在Kubernetes环境中，灾难恢复涉及集群、应用、数据等多个层面。

**灾难类型分类**：

```yaml
# 灾难类型和影响范围

1. 基础设施故障:
   类型:
     - 数据中心断电
     - 网络中断
     - 硬件故障
     - 云服务中断
   
   影响:
     - 集群不可用
     - 服务中断
     - 数据访问受限
   
   恢复时间: 分钟到小时级别
   
   应对策略:
     - 多可用区部署
     - 冗余网络
     - 备用电源
     - 多云架构

2. 人为错误:
   类型:
     - 误删除资源
     - 错误配置
     - 权限误操作
     - 部署失败
   
   影响:
     - 应用不可用
     - 数据丢失
     - 配置错误
   
   恢复时间: 分钟到小时级别
   
   应对策略:
     - 版本控制
     - 变更审批
     - 自动备份
     - 快速回滚

3. 安全事件:
   类型:
     - 恶意攻击
     - 数据泄露
     - 勒索软件
     - 供应链攻击
   
   影响:
     - 系统被入侵
     - 数据被加密
     - 服务被劫持
   
   恢复时间: 小时到天级别
   
   应对策略:
     - 安全加固
     - 入侵检测
     - 隔离备份
     - 应急响应

4. 自然灾害:
   类型:
     - 地震
     - 洪水
     - 火灾
     - 极端天气
   
   影响:
     - 整个数据中心不可用
     - 长时间服务中断
     - 物理设备损坏
   
   恢复时间: 天到周级别
   
   应对策略:
     - 异地备份
     - 多地域部署
     - 灾备中心
     - 应急预案

5. 软件故障:
   类型:
     - 应用Bug
     - 依赖故障
     - 资源耗尽
     - 配置冲突
   
   影响:
     - 应用崩溃
     - 性能下降
     - 功能异常
   
   恢复时间: 分钟到小时级别
   
   应对策略:
     - 灰度发布
     - 自动回滚
     - 健康检查
     - 限流熔断
```

**核心概念和指标**

```yaml
# 灾难恢复的关键指标

1. RTO (Recovery Time Objective):
   定义: 恢复时间目标，系统可接受的最大停机时间
   
   分级:
     - Tier 1 (关键业务): RTO < 1小时
     - Tier 2 (重要业务): RTO < 4小时
     - Tier 3 (一般业务): RTO < 24小时
     - Tier 4 (非关键业务): RTO < 72小时
   
   影响因素:
     - 业务重要性
     - 恢复复杂度
     - 资源可用性
     - 自动化程度
   
   优化方法:
     - 自动化恢复流程
     - 热备份系统
     - 快速故障切换
     - 预演练

2. RPO (Recovery Point Objective):
   定义: 恢复点目标，可接受的最大数据丢失量
   
   分级:
     - Tier 1 (零丢失): RPO = 0
     - Tier 2 (近实时): RPO < 5分钟
     - Tier 3 (小时级): RPO < 1小时
     - Tier 4 (天级): RPO < 24小时
   
   影响因素:
     - 数据重要性
     - 备份频率
     - 复制延迟
     - 存储成本
   
   优化方法:
     - 实时复制
     - 增量备份
     - 快照技术
     - 事务日志

3. RTA (Recovery Time Actual):
   定义: 实际恢复时间
   
   计算:
     RTA = 检测时间 + 决策时间 + 恢复时间 + 验证时间
   
   优化:
     - 自动检测
     - 预定义流程
     - 自动化恢复
     - 并行验证

4. MTTR (Mean Time To Repair):
   定义: 平均修复时间
   
   计算:
     MTTR = 总修复时间 / 故障次数
   
   目标:
     - 关键系统: MTTR < 30分钟
     - 重要系统: MTTR < 2小时
     - 一般系统: MTTR < 8小时

5. MTBF (Mean Time Between Failures):
   定义: 平均故障间隔时间
   
   计算:
     MTBF = 总运行时间 / 故障次数
   
   目标:
     - 高可用系统: MTBF > 720小时（30天）
     - 标准系统: MTBF > 168小时（7天）

6. 可用性 (Availability):
   定义: 系统正常运行时间的百分比
   
   计算:
     可用性 = (总时间 - 停机时间) / 总时间 × 100%
   
   等级:
     - 99.999% (5个9): 年停机时间 < 5.26分钟
     - 99.99% (4个9): 年停机时间 < 52.6分钟
     - 99.9% (3个9): 年停机时间 < 8.76小时
     - 99% (2个9): 年停机时间 < 3.65天
```

**灾难恢复策略**

```yaml
# 灾难恢复的四种主要策略

策略1: 备份和恢复 (Backup and Restore)
  描述: 定期备份数据，灾难时从备份恢复
  
  特点:
    - RTO: 小时到天级别
    - RPO: 小时到天级别
    - 成本: 低
    - 复杂度: 低
  
  适用场景:
    - 非关键业务
    - 可接受较长恢复时间
    - 预算有限
  
  实现:
    - 定期全量备份
    - 增量备份
    - 异地存储
    - 定期验证

---

策略2: 冷备份 (Cold Standby)
  描述: 保持备用环境，但不运行，灾难时启动
  
  特点:
    - RTO: 小时级别
    - RPO: 分钟到小时级别
    - 成本: 中等
    - 复杂度: 中等
  
  适用场景:
    - 重要业务
    - 可接受小时级恢复
    - 成本敏感
  
  实现:
    - 预配置环境
    - 定期数据同步
    - 自动化启动脚本
    - 定期演练

---

策略3: 温备份 (Warm Standby)
  描述: 备用环境运行但不接收流量，灾难时切换
  
  特点:
    - RTO: 分钟级别
    - RPO: 分钟级别
    - 成本: 中高
    - 复杂度: 中高
  
  适用场景:
    - 重要业务
    - 需要快速恢复
    - 可接受一定成本
  
  实现:
    - 备用集群运行
    - 实时数据复制
    - 自动故障切换
    - 健康检查

---

策略4: 热备份 (Hot Standby)
  描述: 多个环境同时运行并接收流量
  
  特点:
    - RTO: 秒到分钟级别
    - RPO: 接近零
    - 成本: 高
    - 复杂度: 高
  
  适用场景:
    - 关键业务
    - 零停机要求
    - 充足预算
  
  实现:
    - 多活架构
    - 实时同步
    - 自动负载均衡
    - 智能路由
```

**灾难恢复计划框架**

```yaml
# 灾难恢复计划 (Disaster Recovery Plan, DRP)

1. 业务影响分析 (BIA):
   目的: 识别关键业务和优先级
   
   内容:
     - 业务流程清单
     - 依赖关系分析
     - 影响评估
     - 优先级排序
   
   输出:
     - 关键业务列表
     - RTO/RPO要求
     - 依赖关系图
     - 恢复优先级

2. 风险评估:
   目的: 识别潜在威胁和脆弱性
   
   内容:
     - 威胁识别
     - 脆弱性分析
     - 风险评级
     - 缓解措施
   
   输出:
     - 风险清单
     - 风险矩阵
     - 缓解计划
     - 应急预案

3. 恢复策略:
   目的: 定义恢复方法和资源
   
   内容:
     - 恢复策略选择
     - 资源需求
     - 技术方案
     - 成本分析
   
   输出:
     - 恢复策略文档
     - 资源清单
     - 技术架构图
     - 预算计划

4. 恢复流程:
   目的: 详细的恢复步骤
   
   内容:
     - 检测和通知
     - 评估和决策
     - 恢复执行
     - 验证和切换
   
   输出:
     - 操作手册
     - 检查清单
     - 联系人列表
     - 升级路径

5. 测试和演练:
   目的: 验证计划有效性
   
   内容:
     - 测试计划
     - 演练场景
     - 执行记录
     - 改进措施
   
   输出:
     - 测试报告
     - 问题清单
     - 改进计划
     - 更新文档

6. 维护和更新:
   目的: 保持计划的有效性
   
   内容:
     - 定期审查
     - 变更管理
     - 文档更新
     - 培训计划
   
   输出:
     - 审查报告
     - 更新日志
     - 培训记录
     - 版本控制
```

**Kubernetes灾难恢复架构**

```yaml
# Kubernetes环境的灾难恢复架构

层次1: 数据层
  备份对象:
    - etcd数据
    - 持久化卷数据
    - 数据库数据
    - 对象存储数据
  
  备份方法:
    - etcd快照
    - 卷快照
    - 数据库备份
    - 对象存储复制
  
  恢复方法:
    - etcd恢复
    - 卷恢复
    - 数据库恢复
    - 对象恢复

层次2: 配置层
  备份对象:
    - Kubernetes资源定义
    - ConfigMap和Secret
    - RBAC配置
    - 网络策略
  
  备份方法:
    - GitOps (推荐)
    - Velero备份
    - kubectl导出
    - Helm charts
  
  恢复方法:
    - Git恢复
    - Velero恢复
    - kubectl应用
    - Helm安装

层次3: 应用层
  备份对象:
    - 应用状态
    - 会话数据
    - 缓存数据
    - 消息队列
  
  备份方法:
    - 应用级备份
    - 状态快照
    - 数据导出
    - 队列持久化
  
  恢复方法:
    - 应用恢复
    - 状态恢复
    - 数据导入
    - 队列恢复

层次4: 集群层
  备份对象:
    - 集群配置
    - 节点信息
    - 网络配置
    - 存储配置
  
  备份方法:
    - 基础设施即代码
    - 配置管理
    - 文档记录
    - 自动化脚本
  
  恢复方法:
    - 自动化部署
    - 配置应用
    - 手动重建
    - 脚本执行
```



### 10.3.2 备份和恢复策略

**etcd备份和恢复**

etcd是Kubernetes的核心数据存储，备份etcd是灾难恢复的关键。

```bash
#!/bin/bash
# etcd-backup.sh
# etcd备份脚本

set -e

# 配置
ETCD_ENDPOINTS="https://127.0.0.1:2379"
ETCD_CACERT="/etc/kubernetes/pki/etcd/ca.crt"
ETCD_CERT="/etc/kubernetes/pki/etcd/server.crt"
ETCD_KEY="/etc/kubernetes/pki/etcd/server.key"
BACKUP_DIR="/backup/etcd"
RETENTION_DAYS=30

# 创建备份目录
mkdir -p $BACKUP_DIR

# 备份文件名
BACKUP_FILE="$BACKUP_DIR/etcd-snapshot-$(date +%Y%m%d-%H%M%S).db"

echo "=== etcd备份开始 ==="
echo "备份文件: $BACKUP_FILE"

# 执行备份
ETCDCTL_API=3 etcdctl     --endpoints=$ETCD_ENDPOINTS     --cacert=$ETCD_CACERT     --cert=$ETCD_CERT     --key=$ETCD_KEY     snapshot save $BACKUP_FILE

# 验证备份
echo "验证备份..."
ETCDCTL_API=3 etcdctl     --write-out=table     snapshot status $BACKUP_FILE

# 压缩备份
echo "压缩备份..."
gzip $BACKUP_FILE

# 上传到对象存储（示例：AWS S3）
echo "上传到S3..."
aws s3 cp ${BACKUP_FILE}.gz s3://my-etcd-backups/$(basename ${BACKUP_FILE}.gz)

# 清理旧备份
echo "清理旧备份..."
find $BACKUP_DIR -name "etcd-snapshot-*.db.gz" -mtime +$RETENTION_DAYS -delete

# 清理S3旧备份
aws s3 ls s3://my-etcd-backups/ |     awk '{print $4}' |     while read file; do
        file_date=$(echo $file | grep -oP '\d{8}')
        if [ ! -z "$file_date" ]; then
            days_old=$(( ($(date +%s) - $(date -d $file_date +%s)) / 86400 ))
            if [ $days_old -gt $RETENTION_DAYS ]; then
                echo "删除旧备份: $file"
                aws s3 rm s3://my-etcd-backups/$file
            fi
        fi
    done

echo "=== etcd备份完成 ==="
```

**etcd恢复脚本**

```bash
#!/bin/bash
# etcd-restore.sh
# etcd恢复脚本

set -e

BACKUP_FILE=$1
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_NAME="etcd-0"
ETCD_INITIAL_CLUSTER="etcd-0=https://10.0.1.10:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://10.0.1.10:2380"

if [ -z "$BACKUP_FILE" ]; then
    echo "用法: $0 <backup-file>"
    exit 1
fi

echo "=== etcd恢复开始 ==="
echo "备份文件: $BACKUP_FILE"

# 停止etcd
echo "停止etcd..."
systemctl stop etcd

# 备份当前数据
echo "备份当前数据..."
if [ -d "$ETCD_DATA_DIR" ]; then
    mv $ETCD_DATA_DIR ${ETCD_DATA_DIR}.backup.$(date +%Y%m%d-%H%M%S)
fi

# 如果备份文件是压缩的，先解压
if [[ $BACKUP_FILE == *.gz ]]; then
    echo "解压备份文件..."
    gunzip -c $BACKUP_FILE > /tmp/etcd-snapshot.db
    BACKUP_FILE="/tmp/etcd-snapshot.db"
fi

# 恢复etcd
echo "恢复etcd数据..."
ETCDCTL_API=3 etcdctl snapshot restore $BACKUP_FILE     --name $ETCD_NAME     --initial-cluster $ETCD_INITIAL_CLUSTER     --initial-cluster-token etcd-cluster-1     --initial-advertise-peer-urls $ETCD_INITIAL_ADVERTISE_PEER_URLS     --data-dir $ETCD_DATA_DIR

# 设置权限
chown -R etcd:etcd $ETCD_DATA_DIR

# 启动etcd
echo "启动etcd..."
systemctl start etcd

# 等待etcd就绪
echo "等待etcd就绪..."
sleep 10

# 验证恢复
echo "验证恢复..."
ETCDCTL_API=3 etcdctl     --endpoints=https://127.0.0.1:2379     --cacert=/etc/kubernetes/pki/etcd/ca.crt     --cert=/etc/kubernetes/pki/etcd/server.crt     --key=/etc/kubernetes/pki/etcd/server.key     endpoint health

echo "=== etcd恢复完成 ==="
```

**Velero备份和恢复**

Velero是Kubernetes集群备份和恢复的标准工具。

```yaml
# velero-install.yaml
# Velero安装配置

---
# 1. 创建Velero namespace
apiVersion: v1
kind: Namespace
metadata:
  name: velero

---
# 2. 创建S3凭证Secret
apiVersion: v1
kind: Secret
metadata:
  name: cloud-credentials
  namespace: velero
type: Opaque
stringData:
  cloud: |
    [default]
    aws_access_key_id=YOUR_ACCESS_KEY
    aws_secret_access_key=YOUR_SECRET_KEY

---
# 3. BackupStorageLocation配置
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  name: default
  namespace: velero
spec:
  provider: aws
  objectStorage:
    bucket: kubernetes-backups
    prefix: cluster-1
  config:
    region: us-west-2
    s3ForcePathStyle: "false"
    s3Url: https://s3.us-west-2.amazonaws.com

---
# 4. VolumeSnapshotLocation配置
apiVersion: velero.io/v1
kind: VolumeSnapshotLocation
metadata:
  name: default
  namespace: velero
spec:
  provider: aws
  config:
    region: us-west-2

---
# 5. 定时备份Schedule
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup
  namespace: velero
spec:
  # 每天凌晨2点备份
  schedule: "0 2 * * *"
  template:
    # 备份所有namespace
    includedNamespaces:
    - "*"
    
    # 排除系统namespace
    excludedNamespaces:
    - kube-system
    - kube-public
    - kube-node-lease
    
    # 包含所有资源
    includedResources:
    - "*"
    
    # 包含集群资源
    includeClusterResources: true
    
    # 备份卷
    snapshotVolumes: true
    
    # 保留30天
    ttl: 720h
    
    # 备份标签
    labelSelector:
      matchLabels:
        backup: "true"

---
# 6. 每周全量备份
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: weekly-full-backup
  namespace: velero
spec:
  # 每周日凌晨1点
  schedule: "0 1 * * 0"
  template:
    includedNamespaces:
    - "*"
    includeClusterResources: true
    snapshotVolumes: true
    ttl: 2160h  # 保留90天
    
    # 备份钩子：备份前后执行命令
    hooks:
      resources:
      - name: mysql-backup-hook
        includedNamespaces:
        - production
        labelSelector:
          matchLabels:
            app: mysql
        pre:
        - exec:
            container: mysql
            command:
            - /bin/bash
            - -c
            - "mysqldump -u root -p$MYSQL_ROOT_PASSWORD --all-databases > /backup/dump.sql"
            onError: Fail
            timeout: 10m
        post:
        - exec:
            container: mysql
            command:
            - /bin/bash
            - -c
            - "rm -f /backup/dump.sql"
```

**Velero安装和使用脚本**

```bash
#!/bin/bash
# velero-setup.sh
# Velero安装和配置

set -e

VELERO_VERSION="v1.12.0"
BUCKET_NAME="kubernetes-backups"
REGION="us-west-2"

echo "=== 安装Velero ==="

# 1. 下载Velero CLI
echo "1. 下载Velero CLI..."
wget https://github.com/vmware-tanzu/velero/releases/download/${VELERO_VERSION}/velero-${VELERO_VERSION}-linux-amd64.tar.gz
tar -xzf velero-${VELERO_VERSION}-linux-amd64.tar.gz
sudo mv velero-${VELERO_VERSION}-linux-amd64/velero /usr/local/bin/
rm -rf velero-${VELERO_VERSION}-linux-amd64*

# 2. 创建S3 bucket
echo "2. 创建S3 bucket..."
aws s3 mb s3://$BUCKET_NAME --region $REGION || echo "Bucket已存在"

# 3. 创建IAM用户和策略
echo "3. 创建IAM策略..."
cat > velero-policy.json <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeVolumes",
                "ec2:DescribeSnapshots",
                "ec2:CreateTags",
                "ec2:CreateVolume",
                "ec2:CreateSnapshot",
                "ec2:DeleteSnapshot"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:DeleteObject",
                "s3:PutObject",
                "s3:AbortMultipartUpload",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": [
                "arn:aws:s3:::${BUCKET_NAME}/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::${BUCKET_NAME}"
            ]
        }
    ]
}
EOF

aws iam create-policy     --policy-name VeleroPolicy     --policy-document file://velero-policy.json || echo "策略已存在"

# 4. 创建凭证文件
echo "4. 创建凭证文件..."
cat > credentials-velero <<EOF
[default]
aws_access_key_id = YOUR_ACCESS_KEY_ID
aws_secret_access_key = YOUR_SECRET_ACCESS_KEY
EOF

# 5. 安装Velero到集群
echo "5. 安装Velero到集群..."
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.8.0     --bucket $BUCKET_NAME     --backup-location-config region=$REGION     --snapshot-location-config region=$REGION     --secret-file ./credentials-velero     --use-volume-snapshots=true     --use-node-agent

# 6. 等待Velero就绪
echo "6. 等待Velero就绪..."
kubectl wait --for=condition=available --timeout=300s     deployment/velero -n velero

# 7. 验证安装
echo "7. 验证Velero安装..."
velero version

echo "=== Velero安装完成 ==="

# 清理临时文件
rm -f velero-policy.json credentials-velero
```

**Velero备份和恢复操作**

```bash
#!/bin/bash
# velero-operations.sh
# Velero常用操作

set -e

# 1. 创建即时备份
create_backup() {
    local backup_name=$1
    echo "创建备份: $backup_name"
    
    velero backup create $backup_name         --include-namespaces production,staging         --snapshot-volumes         --ttl 720h
    
    # 等待备份完成
    velero backup wait $backup_name
    
    # 查看备份详情
    velero backup describe $backup_name
}

# 2. 恢复备份
restore_backup() {
    local backup_name=$1
    local restore_name="${backup_name}-restore-$(date +%Y%m%d-%H%M%S)"
    
    echo "恢复备份: $backup_name"
    echo "恢复名称: $restore_name"
    
    velero restore create $restore_name         --from-backup $backup_name         --include-namespaces production         --namespace-mappings production:production-restored
    
    # 等待恢复完成
    velero restore wait $restore_name
    
    # 查看恢复详情
    velero restore describe $restore_name
}

# 3. 列出所有备份
list_backups() {
    echo "所有备份:"
    velero backup get
    
    echo ""
    echo "备份详细信息:"
    velero backup get -o json | jq -r '.items[] | "\(.metadata.name) \(.status.phase) \(.status.startTimestamp)"'
}

# 4. 删除旧备份
cleanup_old_backups() {
    local days=$1
    echo "删除${days}天前的备份..."
    
    velero backup get -o json |         jq -r --arg days "$days" '.items[] | 
            select(
                (.status.completionTimestamp | fromdateiso8601) < 
                (now - ($days | tonumber * 86400))
            ) | .metadata.name' |         while read backup; do
            echo "删除备份: $backup"
            velero backup delete $backup --confirm
        done
}

# 5. 备份特定应用
backup_application() {
    local app_name=$1
    local namespace=$2
    local backup_name="${app_name}-$(date +%Y%m%d-%H%M%S)"
    
    echo "备份应用: $app_name (namespace: $namespace)"
    
    velero backup create $backup_name         --include-namespaces $namespace         --selector app=$app_name         --snapshot-volumes
    
    velero backup wait $backup_name
}

# 6. 迁移到新集群
migrate_to_new_cluster() {
    local backup_name=$1
    local new_cluster_context=$2
    
    echo "迁移到新集群: $new_cluster_context"
    
    # 切换到新集群
    kubectl config use-context $new_cluster_context
    
    # 在新集群安装Velero（使用相同的S3 bucket）
    # ... (省略安装步骤)
    
    # 恢复备份
    velero restore create migration-restore         --from-backup $backup_name
    
    velero restore wait migration-restore
}

# 7. 验证备份完整性
verify_backup() {
    local backup_name=$1
    
    echo "验证备份: $backup_name"
    
    # 下载备份内容
    velero backup download $backup_name
    
    # 检查备份文件
    tar -tzf ${backup_name}.tar.gz | head -20
    
    # 清理
    rm -f ${backup_name}.tar.gz
}

# 主菜单
case "${1:-}" in
    create)
        create_backup "${2:-manual-backup-$(date +%Y%m%d-%H%M%S)}"
        ;;
    restore)
        restore_backup "$2"
        ;;
    list)
        list_backups
        ;;
    cleanup)
        cleanup_old_backups "${2:-30}"
        ;;
    app)
        backup_application "$2" "$3"
        ;;
    migrate)
        migrate_to_new_cluster "$2" "$3"
        ;;
    verify)
        verify_backup "$2"
        ;;
    *)
        echo "用法: $0 {create|restore|list|cleanup|app|migrate|verify} [参数]"
        echo ""
        echo "命令:"
        echo "  create [name]           - 创建备份"
        echo "  restore <backup-name>   - 恢复备份"
        echo "  list                    - 列出所有备份"
        echo "  cleanup [days]          - 清理旧备份（默认30天）"
        echo "  app <name> <namespace>  - 备份特定应用"
        echo "  migrate <backup> <ctx>  - 迁移到新集群"
        echo "  verify <backup-name>    - 验证备份"
        exit 1
        ;;
esac
```


**应用数据备份**

```yaml
# application-backup.yaml
# 应用级备份配置

---
# 1. MySQL备份CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mysql-backup
  namespace: production
spec:
  # 每天凌晨3点执行
  schedule: "0 3 * * *"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: mysql-backup
            image: mysql:8.0
            env:
            - name: MYSQL_HOST
              value: mysql-service
            - name: MYSQL_USER
              value: root
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-secret
                  key: password
            - name: S3_BUCKET
              value: mysql-backups
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-access-key
            command:
            - /bin/bash
            - -c
            - |
              set -e
              
              BACKUP_FILE="mysql-backup-$(date +%Y%m%d-%H%M%S).sql.gz"
              
              echo "开始备份MySQL..."
              mysqldump -h $MYSQL_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD                 --all-databases                 --single-transaction                 --quick                 --lock-tables=false                 | gzip > /tmp/$BACKUP_FILE
              
              echo "上传到S3..."
              aws s3 cp /tmp/$BACKUP_FILE s3://$S3_BUCKET/$BACKUP_FILE
              
              echo "清理本地文件..."
              rm -f /tmp/$BACKUP_FILE
              
              echo "备份完成: $BACKUP_FILE"
          restartPolicy: OnFailure

---
# 2. PostgreSQL备份CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: production
spec:
  schedule: "0 3 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: postgres-backup
            image: postgres:15
            env:
            - name: PGHOST
              value: postgres-service
            - name: PGUSER
              value: postgres
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
            command:
            - /bin/bash
            - -c
            - |
              set -e
              
              BACKUP_FILE="postgres-backup-$(date +%Y%m%d-%H%M%S).dump.gz"
              
              echo "开始备份PostgreSQL..."
              pg_dumpall -c | gzip > /tmp/$BACKUP_FILE
              
              echo "上传到S3..."
              aws s3 cp /tmp/$BACKUP_FILE s3://postgres-backups/$BACKUP_FILE
              
              echo "清理本地文件..."
              rm -f /tmp/$BACKUP_FILE
              
              echo "备份完成: $BACKUP_FILE"
          restartPolicy: OnFailure

---
# 3. MongoDB备份CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mongodb-backup
  namespace: production
spec:
  schedule: "0 3 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: mongodb-backup
            image: mongo:6.0
            env:
            - name: MONGO_HOST
              value: mongodb-service
            - name: MONGO_USER
              value: admin
            - name: MONGO_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-secret
                  key: password
            command:
            - /bin/bash
            - -c
            - |
              set -e
              
              BACKUP_DIR="/tmp/mongodb-backup-$(date +%Y%m%d-%H%M%S)"
              BACKUP_FILE="${BACKUP_DIR}.tar.gz"
              
              echo "开始备份MongoDB..."
              mongodump                 --host $MONGO_HOST                 --username $MONGO_USER                 --password $MONGO_PASSWORD                 --authenticationDatabase admin                 --out $BACKUP_DIR
              
              echo "压缩备份..."
              tar -czf $BACKUP_FILE -C /tmp $(basename $BACKUP_DIR)
              
              echo "上传到S3..."
              aws s3 cp $BACKUP_FILE s3://mongodb-backups/$(basename $BACKUP_FILE)
              
              echo "清理本地文件..."
              rm -rf $BACKUP_DIR $BACKUP_FILE
              
              echo "备份完成"
          restartPolicy: OnFailure

---
# 4. Redis备份CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: redis-backup
  namespace: production
spec:
  schedule: "0 */6 * * *"  # 每6小时
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: redis-backup
            image: redis:7.0
            env:
            - name: REDIS_HOST
              value: redis-service
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: redis-secret
                  key: password
            command:
            - /bin/bash
            - -c
            - |
              set -e
              
              BACKUP_FILE="redis-backup-$(date +%Y%m%d-%H%M%S).rdb"
              
              echo "触发Redis保存..."
              redis-cli -h $REDIS_HOST -a $REDIS_PASSWORD BGSAVE
              
              # 等待保存完成
              while [ $(redis-cli -h $REDIS_HOST -a $REDIS_PASSWORD LASTSAVE) -eq $(redis-cli -h $REDIS_HOST -a $REDIS_PASSWORD LASTSAVE) ]; do
                sleep 1
              done
              
              echo "复制RDB文件..."
              redis-cli -h $REDIS_HOST -a $REDIS_PASSWORD --rdb /tmp/$BACKUP_FILE
              
              echo "上传到S3..."
              aws s3 cp /tmp/$BACKUP_FILE s3://redis-backups/$BACKUP_FILE
              
              echo "清理本地文件..."
              rm -f /tmp/$BACKUP_FILE
              
              echo "备份完成"
          restartPolicy: OnFailure
```

**持久化卷备份**

```yaml
# volume-backup.yaml
# 持久化卷备份配置

---
# 1. VolumeSnapshot配置
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: mysql-data-snapshot
  namespace: production
spec:
  volumeSnapshotClassName: csi-snapclass
  source:
    persistentVolumeClaimName: mysql-data-pvc

---
# 2. VolumeSnapshotClass
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: csi-snapclass
driver: ebs.csi.aws.com
deletionPolicy: Retain
parameters:
  # 快照标签
  tagSpecification_1: "Name=kubernetes-snapshot"
  tagSpecification_2: "Environment=production"

---
# 3. 定时快照CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: volume-snapshot
  namespace: production
spec:
  schedule: "0 4 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: snapshot-creator
          containers:
          - name: snapshot-creator
            image: bitnami/kubectl:latest
            command:
            - /bin/bash
            - -c
            - |
              set -e
              
              SNAPSHOT_NAME="mysql-data-snapshot-$(date +%Y%m%d-%H%M%S)"
              
              echo "创建卷快照: $SNAPSHOT_NAME"
              
              cat <<EOF | kubectl apply -f -
              apiVersion: snapshot.storage.k8s.io/v1
              kind: VolumeSnapshot
              metadata:
                name: $SNAPSHOT_NAME
                namespace: production
              spec:
                volumeSnapshotClassName: csi-snapclass
                source:
                  persistentVolumeClaimName: mysql-data-pvc
              EOF
              
              echo "等待快照就绪..."
              kubectl wait --for=jsonpath='{.status.readyToUse}'=true                 volumesnapshot/$SNAPSHOT_NAME -n production --timeout=300s
              
              echo "快照创建完成: $SNAPSHOT_NAME"
              
              # 清理旧快照（保留最近7个）
              echo "清理旧快照..."
              kubectl get volumesnapshot -n production                 --sort-by=.metadata.creationTimestamp                 -o name | head -n -7 | xargs -r kubectl delete -n production
          restartPolicy: OnFailure

---
# 4. RBAC配置
apiVersion: v1
kind: ServiceAccount
metadata:
  name: snapshot-creator
  namespace: production

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: snapshot-creator
  namespace: production
rules:
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshots"]
  verbs: ["create", "get", "list", "delete"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: snapshot-creator
  namespace: production
subjects:
- kind: ServiceAccount
  name: snapshot-creator
roleRef:
  kind: Role
  name: snapshot-creator
  apiGroup: rbac.authorization.k8s.io
```

**备份验证和测试**

```go
// backup-validator.go
// 备份验证工具

package main

import (
	"context"
	"fmt"
	"log"
	"time"

	"github.com/aws/aws-sdk-go/aws"
	"github.com/aws/aws-sdk-go/aws/session"
	"github.com/aws/aws-sdk-go/service/s3"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
)

// BackupValidator 备份验证器
type BackupValidator struct {
	k8sClient *kubernetes.Clientset
	s3Client  *s3.S3
}

// NewBackupValidator 创建备份验证器
func NewBackupValidator(kubeconfig string) (*BackupValidator, error) {
	// 创建Kubernetes客户端
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	k8sClient, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create k8s client: %v", err)
	}
	
	// 创建S3客户端
	sess, err := session.NewSession(&aws.Config{
		Region: aws.String("us-west-2"),
	})
	if err != nil {
		return nil, fmt.Errorf("failed to create AWS session: %v", err)
	}
	
	s3Client := s3.New(sess)
	
	return &BackupValidator{
		k8sClient: k8sClient,
		s3Client:  s3Client,
	}, nil
}

// ValidateEtcdBackup 验证etcd备份
func (bv *BackupValidator) ValidateEtcdBackup(bucket, prefix string) error {
	log.Println("验证etcd备份...")
	
	// 列出备份文件
	input := &s3.ListObjectsV2Input{
		Bucket: aws.String(bucket),
		Prefix: aws.String(prefix),
	}
	
	result, err := bv.s3Client.ListObjectsV2(input)
	if err != nil {
		return fmt.Errorf("failed to list objects: %v", err)
	}
	
	if len(result.Contents) == 0 {
		return fmt.Errorf("no backup files found")
	}
	
	// 检查最新备份
	var latestBackup *s3.Object
	for _, obj := range result.Contents {
		if latestBackup == nil || obj.LastModified.After(*latestBackup.LastModified) {
			latestBackup = obj
		}
	}
	
	// 检查备份时间
	age := time.Since(*latestBackup.LastModified)
	if age > 24*time.Hour {
		log.Printf("警告: 最新备份已超过24小时 (%.1f小时)", age.Hours())
	}
	
	// 检查备份大小
	if *latestBackup.Size < 1024*1024 { // 小于1MB
		log.Printf("警告: 备份文件过小 (%d bytes)", *latestBackup.Size)
	}
	
	log.Printf("最新etcd备份: %s (%.2f MB, %s)",
		*latestBackup.Key,
		float64(*latestBackup.Size)/(1024*1024),
		latestBackup.LastModified.Format(time.RFC3339))
	
	return nil
}

// ValidateVeleroBackup 验证Velero备份
func (bv *BackupValidator) ValidateVeleroBackup(ctx context.Context, namespace string) error {
	log.Println("验证Velero备份...")
	
	// 获取Velero备份列表
	// 注意：这里简化实现，实际需要使用Velero的API
	pods, err := bv.k8sClient.CoreV1().Pods(namespace).List(ctx, metav1.ListOptions{
		LabelSelector: "component=velero",
	})
	if err != nil {
		return fmt.Errorf("failed to list velero pods: %v", err)
	}
	
	if len(pods.Items) == 0 {
		return fmt.Errorf("velero not installed")
	}
	
	// 检查Velero Pod状态
	for _, pod := range pods.Items {
		if pod.Status.Phase != "Running" {
			log.Printf("警告: Velero Pod %s 状态异常: %s", pod.Name, pod.Status.Phase)
		}
	}
	
	log.Println("Velero运行正常")
	return nil
}

// ValidateApplicationBackup 验证应用备份
func (bv *BackupValidator) ValidateApplicationBackup(bucket, app string) error {
	log.Printf("验证应用备份: %s", app)
	
	prefix := fmt.Sprintf("%s-backup-", app)
	
	input := &s3.ListObjectsV2Input{
		Bucket: aws.String(bucket),
		Prefix: aws.String(prefix),
	}
	
	result, err := bv.s3Client.ListObjectsV2(input)
	if err != nil {
		return fmt.Errorf("failed to list objects: %v", err)
	}
	
	if len(result.Contents) == 0 {
		return fmt.Errorf("no backup files found for %s", app)
	}
	
	// 统计备份
	var totalSize int64
	for _, obj := range result.Contents {
		totalSize += *obj.Size
	}
	
	log.Printf("应用 %s: %d个备份文件, 总大小 %.2f GB",
		app, len(result.Contents), float64(totalSize)/(1024*1024*1024))
	
	return nil
}

// ValidateBackupRetention 验证备份保留策略
func (bv *BackupValidator) ValidateBackupRetention(bucket, prefix string, retentionDays int) error {
	log.Printf("验证备份保留策略 (保留%d天)", retentionDays)
	
	input := &s3.ListObjectsV2Input{
		Bucket: aws.String(bucket),
		Prefix: aws.String(prefix),
	}
	
	result, err := bv.s3Client.ListObjectsV2(input)
	if err != nil {
		return fmt.Errorf("failed to list objects: %v", err)
	}
	
	cutoffTime := time.Now().AddDate(0, 0, -retentionDays)
	var oldBackups []*s3.Object
	
	for _, obj := range result.Contents {
		if obj.LastModified.Before(cutoffTime) {
			oldBackups = append(oldBackups, obj)
		}
	}
	
	if len(oldBackups) > 0 {
		log.Printf("警告: 发现%d个超过保留期的备份", len(oldBackups))
		for _, obj := range oldBackups {
			log.Printf("  - %s (%.1f天前)",
				*obj.Key,
				time.Since(*obj.LastModified).Hours()/24)
		}
	} else {
		log.Println("备份保留策略正常")
	}
	
	return nil
}

// GenerateBackupReport 生成备份报告
func (bv *BackupValidator) GenerateBackupReport(bucket string) (*BackupReport, error) {
	log.Println("生成备份报告...")
	
	report := &BackupReport{
		GeneratedAt: time.Now(),
		Buckets:     make(map[string]*BucketInfo),
	}
	
	// 获取bucket信息
	input := &s3.ListObjectsV2Input{
		Bucket: aws.String(bucket),
	}
	
	result, err := bv.s3Client.ListObjectsV2(input)
	if err != nil {
		return nil, fmt.Errorf("failed to list objects: %v", err)
	}
	
	bucketInfo := &BucketInfo{
		Name:        bucket,
		ObjectCount: len(result.Contents),
	}
	
	for _, obj := range result.Contents {
		bucketInfo.TotalSize += *obj.Size
		
		if bucketInfo.OldestBackup == nil || obj.LastModified.Before(*bucketInfo.OldestBackup) {
			bucketInfo.OldestBackup = obj.LastModified
		}
		
		if bucketInfo.NewestBackup == nil || obj.LastModified.After(*bucketInfo.NewestBackup) {
			bucketInfo.NewestBackup = obj.LastModified
		}
	}
	
	report.Buckets[bucket] = bucketInfo
	
	return report, nil
}

// BackupReport 备份报告
type BackupReport struct {
	GeneratedAt time.Time
	Buckets     map[string]*BucketInfo
}

// BucketInfo Bucket信息
type BucketInfo struct {
	Name         string
	ObjectCount  int
	TotalSize    int64
	OldestBackup *time.Time
	NewestBackup *time.Time
}

// 使用示例
func main() {
	ctx := context.Background()
	
	validator, err := NewBackupValidator("/path/to/kubeconfig")
	if err != nil {
		log.Fatalf("Failed to create validator: %v", err)
	}
	
	// 验证etcd备份
	if err := validator.ValidateEtcdBackup("etcd-backups", "cluster-1/"); err != nil {
		log.Printf("etcd备份验证失败: %v", err)
	}
	
	// 验证Velero备份
	if err := validator.ValidateVeleroBackup(ctx, "velero"); err != nil {
		log.Printf("Velero备份验证失败: %v", err)
	}
	
	// 验证应用备份
	apps := []string{"mysql", "postgres", "mongodb", "redis"}
	for _, app := range apps {
		if err := validator.ValidateApplicationBackup("app-backups", app); err != nil {
			log.Printf("应用备份验证失败 (%s): %v", app, err)
		}
	}
	
	// 验证保留策略
	if err := validator.ValidateBackupRetention("etcd-backups", "cluster-1/", 30); err != nil {
		log.Printf("保留策略验证失败: %v", err)
	}
	
	// 生成报告
	report, err := validator.GenerateBackupReport("etcd-backups")
	if err != nil {
		log.Fatalf("Failed to generate report: %v", err)
	}
	
	fmt.Printf("\n备份报告 (生成时间: %s)\n", report.GeneratedAt.Format(time.RFC3339))
	for name, info := range report.Buckets {
		fmt.Printf("\nBucket: %s\n", name)
		fmt.Printf("  对象数量: %d\n", info.ObjectCount)
		fmt.Printf("  总大小: %.2f GB\n", float64(info.TotalSize)/(1024*1024*1024))
		if info.OldestBackup != nil {
			fmt.Printf("  最旧备份: %s\n", info.OldestBackup.Format(time.RFC3339))
		}
		if info.NewestBackup != nil {
			fmt.Printf("  最新备份: %s\n", info.NewestBackup.Format(time.RFC3339))
		}
	}
}
```



### 10.3.3 高可用架构设计

**高可用架构原则**

高可用（High Availability, HA）架构旨在最大限度地减少系统停机时间，确保业务连续性。

```yaml
# 高可用架构的核心原则

1. 消除单点故障 (Eliminate Single Points of Failure):
   定义: 系统中任何单个组件的故障都不应导致整体服务中断
   
   实现:
     - 组件冗余
     - 多副本部署
     - 跨可用区分布
     - 故障自动切换
   
   示例:
     - 控制平面多副本
     - 工作节点多个
     - 数据库主从复制
     - 负载均衡器冗余

2. 故障检测和自动恢复 (Fault Detection and Auto-Recovery):
   定义: 快速检测故障并自动恢复服务
   
   实现:
     - 健康检查
     - 自动重启
     - 故障转移
     - 自愈机制
   
   示例:
     - Liveness Probe
     - Readiness Probe
     - ReplicaSet自动恢复
     - HPA自动扩展

3. 数据持久化和复制 (Data Persistence and Replication):
   定义: 确保数据不丢失且可快速恢复
   
   实现:
     - 持久化存储
     - 数据复制
     - 定期备份
     - 异地容灾
   
   示例:
     - PersistentVolume
     - 数据库复制
     - etcd集群
     - 跨区域备份

4. 负载均衡 (Load Balancing):
   定义: 均匀分配流量，避免单点过载
   
   实现:
     - Service负载均衡
     - Ingress Controller
     - 全局负载均衡
     - 智能路由
   
   示例:
     - ClusterIP Service
     - LoadBalancer Service
     - Ingress
     - Service Mesh

5. 优雅降级 (Graceful Degradation):
   定义: 部分功能故障时，核心功能仍可用
   
   实现:
     - 功能分级
     - 熔断机制
     - 限流保护
     - 降级策略
   
   示例:
     - Circuit Breaker
     - Rate Limiting
     - 缓存降级
     - 只读模式
```

**Kubernetes控制平面高可用**

```yaml
# 控制平面高可用架构

架构模式1: 堆叠etcd (Stacked etcd)
  描述: etcd和控制平面组件运行在同一节点
  
  拓扑:
    Master Node 1:
      - kube-apiserver
      - kube-controller-manager
      - kube-scheduler
      - etcd
    
    Master Node 2:
      - kube-apiserver
      - kube-controller-manager
      - kube-scheduler
      - etcd
    
    Master Node 3:
      - kube-apiserver
      - kube-controller-manager
      - kube-scheduler
      - etcd
    
    Load Balancer:
      - 负载均衡到所有API Server
  
  优势:
    - 部署简单
    - 资源利用率高
    - 管理方便
  
  劣势:
    - 耦合度高
    - 故障影响范围大
    - 扩展性受限
  
  适用场景:
    - 中小规模集群
    - 资源有限
    - 简单部署

---

架构模式2: 外部etcd (External etcd)
  描述: etcd独立部署，与控制平面分离
  
  拓扑:
    etcd Cluster:
      - etcd Node 1
      - etcd Node 2
      - etcd Node 3
    
    Control Plane:
      Master Node 1:
        - kube-apiserver
        - kube-controller-manager
        - kube-scheduler
      
      Master Node 2:
        - kube-apiserver
        - kube-controller-manager
        - kube-scheduler
      
      Master Node 3:
        - kube-apiserver
        - kube-controller-manager
        - kube-scheduler
    
    Load Balancer:
      - 负载均衡到所有API Server
  
  优势:
    - 解耦合
    - 独立扩展
    - 故障隔离
    - 更高可用性
  
  劣势:
    - 部署复杂
    - 资源开销大
    - 管理成本高
  
  适用场景:
    - 大规模集群
    - 高可用要求
    - 生产环境
```

**高可用控制平面部署**

```yaml
# ha-control-plane.yaml
# 高可用控制平面配置

---
# 1. kubeadm配置文件
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: "lb.example.com:6443"  # 负载均衡器地址
networking:
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"
etcd:
  external:
    endpoints:
    - https://etcd1.example.com:2379
    - https://etcd2.example.com:2379
    - https://etcd3.example.com:2379
    caFile: /etc/kubernetes/pki/etcd/ca.crt
    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
apiServer:
  certSANs:
  - "lb.example.com"
  - "10.0.1.10"
  - "10.0.1.11"
  - "10.0.1.12"
  extraArgs:
    # API Server高可用配置
    apiserver-count: "3"
    endpoint-reconciler-type: "lease"
    # 审计日志
    audit-log-path: /var/log/kubernetes/audit.log
    audit-log-maxage: "30"
    audit-log-maxbackup: "10"
    audit-log-maxsize: "100"
controllerManager:
  extraArgs:
    # Leader选举
    leader-elect: "true"
    leader-elect-lease-duration: "15s"
    leader-elect-renew-deadline: "10s"
    leader-elect-retry-period: "2s"
scheduler:
  extraArgs:
    # Leader选举
    leader-elect: "true"
    leader-elect-lease-duration: "15s"
    leader-elect-renew-deadline: "10s"
    leader-elect-retry-period: "2s"

---
# 2. HAProxy负载均衡器配置
# /etc/haproxy/haproxy.cfg

global:
  log /dev/log local0
  log /dev/log local1 notice
  chroot /var/lib/haproxy
  stats socket /run/haproxy/admin.sock mode 660 level admin
  stats timeout 30s
  user haproxy
  group haproxy
  daemon

defaults:
  log global
  mode tcp
  option tcplog
  option dontlognull
  timeout connect 5000
  timeout client 50000
  timeout server 50000

# API Server前端
frontend kubernetes-apiserver:
  bind *:6443
  mode tcp
  option tcplog
  default_backend kubernetes-apiserver

# API Server后端
backend kubernetes-apiserver:
  mode tcp
  option tcp-check
  balance roundrobin
  server master1 10.0.1.10:6443 check fall 3 rise 2
  server master2 10.0.1.11:6443 check fall 3 rise 2
  server master3 10.0.1.12:6443 check fall 3 rise 2

# 统计页面
listen stats:
  bind *:8404
  mode http
  stats enable
  stats uri /stats
  stats refresh 30s
  stats auth admin:password

---
# 3. Keepalived配置（VIP）
# /etc/keepalived/keepalived.conf

# Master节点配置
vrrp_script check_haproxy:
  script "/usr/bin/killall -0 haproxy"
  interval 2
  weight 2

vrrp_instance VI_1:
  state MASTER
  interface eth0
  virtual_router_id 51
  priority 100
  advert_int 1
  authentication:
    auth_type PASS
    auth_pass secret123
  virtual_ipaddress:
    10.0.1.100/24
  track_script:
    - check_haproxy

# Backup节点配置（priority设为90）
```

**应用层高可用**

```yaml
# application-ha.yaml
# 应用层高可用配置

---
# 1. 多副本Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: production
spec:
  # 至少3个副本
  replicas: 3
  
  # 滚动更新策略
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # 确保始终有副本可用
  
  selector:
    matchLabels:
      app: web-app
  
  template:
    metadata:
      labels:
        app: web-app
    spec:
      # Pod反亲和性：分散到不同节点
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-app
            topologyKey: kubernetes.io/hostname
        
        # 节点亲和性：优先调度到特定节点
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node-role
                operator: In
                values:
                - worker
      
      # 拓扑分布约束：跨可用区分布
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: web-app
      
      containers:
      - name: web-app
        image: myregistry/web-app:v1.0
        ports:
        - containerPort: 8080
        
        # 资源请求和限制
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        
        # 健康检查
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        # 启动探针
        startupProbe:
          httpGet:
            path: /startup
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 30
        
        # 优雅终止
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - sleep 15  # 等待连接排空
      
      # 优雅终止期限
      terminationGracePeriodSeconds: 30

---
# 2. PodDisruptionBudget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-app-pdb
  namespace: production
spec:
  minAvailable: 2  # 至少保持2个Pod可用
  selector:
    matchLabels:
      app: web-app

---
# 3. HorizontalPodAutoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max

---
# 4. Service配置
apiVersion: v1
kind: Service
metadata:
  name: web-app
  namespace: production
spec:
  type: LoadBalancer
  selector:
    app: web-app
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
  # 会话亲和性
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800

---
# 5. Ingress配置
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-app
  namespace: production
  annotations:
    # 健康检查
    nginx.ingress.kubernetes.io/health-check-path: "/health"
    nginx.ingress.kubernetes.io/health-check-interval: "10"
    
    # 超时配置
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "60"
    
    # 重试配置
    nginx.ingress.kubernetes.io/proxy-next-upstream: "error timeout http_502 http_503 http_504"
    nginx.ingress.kubernetes.io/proxy-next-upstream-tries: "3"
    
    # 限流
    nginx.ingress.kubernetes.io/limit-rps: "100"
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-app
            port:
              number: 80
  tls:
  - hosts:
    - app.example.com
    secretName: app-tls
```



#### 3. 数据库高可用配置

**MySQL主从复制高可用**

```yaml
# mysql-ha-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql-ha
  labels:
    app: mysql-ha
spec:
  ports:
  - port: 3306
    name: mysql
  clusterIP: None
  selector:
    app: mysql-ha
---
apiVersion: v1
kind: Service
metadata:
  name: mysql-ha-read
  labels:
    app: mysql-ha
spec:
  ports:
  - port: 3306
    name: mysql
  selector:
    app: mysql-ha
    role: replica
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-ha
spec:
  serviceName: mysql-ha
  replicas: 3
  selector:
    matchLabels:
      app: mysql-ha
  template:
    metadata:
      labels:
        app: mysql-ha
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:8.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # 从Pod序号生成server-id
          [[ $(hostname) =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          # server-id不能为0
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          # 将适当的配置文件从config-map复制到emptyDir
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # 如果已有数据则跳过克隆
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          # 跳过主节点(序号0)的克隆
          [[ $(hostname) =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          # 从前一个节点克隆数据
          ncat --recv-only mysql-ha-$(($ordinal-1)).mysql-ha 3307 | xbstream -x -C /var/lib/mysql
          # 准备备份
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        livenessProbe:
          exec:
            command:
            - mysqladmin
            - ping
            - -h
            - localhost
            - -u
            - root
            - -p${MYSQL_ROOT_PASSWORD}
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command:
            - mysql
            - -h
            - localhost
            - -u
            - root
            - -p${MYSQL_ROOT_PASSWORD}
            - -e
            - SELECT 1
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql
          
          # 确定binlog位置(如果有)
          if [[ -f xtrabackup_slave_info && "x$(<xtrabackup_slave_info)" != "x" ]]; then
            # XtraBackup已经生成了部分的"CHANGE MASTER TO"查询
            cat xtrabackup_slave_info | sed -E 's/;$//g' > change_master_to.sql.in
            rm -f xtrabackup_slave_info xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            # 我们直接从主节点克隆。解析binlog位置
            [[ $(cat xtrabackup_binlog_info) =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm -f xtrabackup_binlog_info xtrabackup_slave_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi
          
          # 检查是否需要完成克隆
          if [[ -f change_master_to.sql.in ]]; then
            echo "等待mysqld准备就绪(接受连接)"
            until mysql -h 127.0.0.1 -u root -p${MYSQL_ROOT_PASSWORD} -e "SELECT 1"; do sleep 1; done
            
            echo "初始化复制从change_master_to.sql.in"
            mysql -h 127.0.0.1 -u root -p${MYSQL_ROOT_PASSWORD}                   -e "$(cat change_master_to.sql.in),                       MASTER_HOST='mysql-ha-0.mysql-ha',                       MASTER_USER='root',                       MASTER_PASSWORD='${MYSQL_ROOT_PASSWORD}',                       MASTER_CONNECT_RETRY=10;                       START SLAVE;" || exit 1
            mv change_master_to.sql.in change_master_to.sql.orig
          fi
          
          # 启动服务器发送备份
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c             "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root --password=${MYSQL_ROOT_PASSWORD}"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql-ha-config
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-ha-config
data:
  master.cnf: |
    [mysqld]
    log-bin=mysql-bin
    binlog_format=ROW
    gtid_mode=ON
    enforce_gtid_consistency=ON
    log_slave_updates=ON
    binlog_expire_logs_seconds=604800
    max_binlog_size=100M
    sync_binlog=1
    innodb_flush_log_at_trx_commit=1
  slave.cnf: |
    [mysqld]
    super-read-only=ON
    log-bin=mysql-bin
    binlog_format=ROW
    gtid_mode=ON
    enforce_gtid_consistency=ON
    log_slave_updates=ON
    relay_log_recovery=ON
    skip_slave_start=ON
```

**PostgreSQL流复制高可用**

```yaml
# postgresql-ha-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgresql-ha
spec:
  ports:
  - port: 5432
    name: postgresql
  clusterIP: None
  selector:
    app: postgresql-ha
---
apiVersion: v1
kind: Service
metadata:
  name: postgresql-ha-read
spec:
  ports:
  - port: 5432
    name: postgresql
  selector:
    app: postgresql-ha
    role: replica
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgresql-ha
spec:
  serviceName: postgresql-ha
  replicas: 3
  selector:
    matchLabels:
      app: postgresql-ha
  template:
    metadata:
      labels:
        app: postgresql-ha
    spec:
      initContainers:
      - name: init-postgresql
        image: postgres:15
        command:
        - bash
        - "-c"
        - |
          set -ex
          [[ $(hostname) =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          
          # 如果不是主节点且数据目录为空,从主节点复制
          if [[ $ordinal -ne 0 ]] && [[ ! -f /var/lib/postgresql/data/PG_VERSION ]]; then
            until pg_isready -h postgresql-ha-0.postgresql-ha -U postgres; do
              echo "等待主节点准备就绪..."
              sleep 2
            done
            
            PGPASSWORD=${POSTGRES_PASSWORD} pg_basebackup               -h postgresql-ha-0.postgresql-ha               -U replicator               -D /var/lib/postgresql/data               -Fp -Xs -P -R
            
            # 配置standby
            cat >> /var/lib/postgresql/data/postgresql.auto.conf <<EOF
          primary_conninfo = 'host=postgresql-ha-0.postgresql-ha port=5432 user=replicator password=${POSTGRES_REPLICATOR_PASSWORD}'
          hot_standby = on
          EOF
          fi
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql-secret
              key: password
        - name: POSTGRES_REPLICATOR_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql-secret
              key: replicator-password
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
      containers:
      - name: postgresql
        image: postgres:15
        ports:
        - containerPort: 5432
          name: postgresql
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql-secret
              key: password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
        - name: config
          mountPath: /etc/postgresql
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres -h localhost
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres -h localhost
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      - name: metrics
        image: prometheuscommunity/postgres-exporter:latest
        ports:
        - containerPort: 9187
          name: metrics
        env:
        - name: DATA_SOURCE_NAME
          value: postgresql://postgres:$(POSTGRES_PASSWORD)@localhost:5432/postgres?sslmode=disable
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql-secret
              key: password
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
      volumes:
      - name: config
        configMap:
          name: postgresql-ha-config
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgresql-ha-config
data:
  postgresql.conf: |
    # 连接设置
    listen_addresses = '*'
    max_connections = 200
    
    # 内存设置
    shared_buffers = 1GB
    effective_cache_size = 3GB
    work_mem = 16MB
    maintenance_work_mem = 256MB
    
    # WAL设置
    wal_level = replica
    max_wal_senders = 10
    max_replication_slots = 10
    wal_keep_size = 1GB
    hot_standby = on
    
    # 检查点设置
    checkpoint_timeout = 15min
    checkpoint_completion_target = 0.9
    
    # 日志设置
    logging_collector = on
    log_directory = 'log'
    log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
    log_rotation_age = 1d
    log_rotation_size = 100MB
    log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
    log_checkpoints = on
    log_connections = on
    log_disconnections = on
    log_lock_waits = on
    log_temp_files = 0
    
  pg_hba.conf: |
    # TYPE  DATABASE        USER            ADDRESS                 METHOD
    local   all             all                                     trust
    host    all             all             127.0.0.1/32            trust
    host    all             all             ::1/128                 trust
    host    all             all             0.0.0.0/0               md5
    host    replication     replicator      0.0.0.0/0               md5
```

**MongoDB副本集高可用**

```yaml
# mongodb-ha-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: mongodb-ha
spec:
  ports:
  - port: 27017
    name: mongodb
  clusterIP: None
  selector:
    app: mongodb-ha
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb-ha
spec:
  serviceName: mongodb-ha
  replicas: 3
  selector:
    matchLabels:
      app: mongodb-ha
  template:
    metadata:
      labels:
        app: mongodb-ha
    spec:
      terminationGracePeriodSeconds: 30
      containers:
      - name: mongodb
        image: mongo:6.0
        command:
        - mongod
        - --replSet
        - rs0
        - --bind_ip_all
        - --auth
        - --keyFile
        - /etc/mongodb/keyfile/mongodb-keyfile
        ports:
        - containerPort: 27017
          name: mongodb
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: password
        volumeMounts:
        - name: data
          mountPath: /data/db
        - name: keyfile
          mountPath: /etc/mongodb/keyfile
          readOnly: true
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        livenessProbe:
          exec:
            command:
            - mongo
            - --eval
            - "db.adminCommand('ping')"
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command:
            - mongo
            - --eval
            - "db.adminCommand('ping')"
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      - name: mongodb-sidecar
        image: cvallance/mongo-k8s-sidecar:latest
        env:
        - name: MONGO_SIDECAR_POD_LABELS
          value: "app=mongodb-ha"
        - name: KUBERNETES_MONGO_SERVICE_NAME
          value: mongodb-ha
        - name: MONGODB_USERNAME
          value: admin
        - name: MONGODB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: password
        - name: MONGODB_DATABASE
          value: admin
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
      volumes:
      - name: keyfile
        secret:
          secretName: mongodb-keyfile
          defaultMode: 0400
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
```

#### 4. 有状态应用HA模式

**StatefulSet最佳实践**

```yaml
# stateful-app-ha.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: stateful-app
spec:
  serviceName: stateful-app
  replicas: 3
  # 有序部署和终止
  podManagementPolicy: OrderedReady
  # 更新策略
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0  # 从最高序号开始更新
  selector:
    matchLabels:
      app: stateful-app
  template:
    metadata:
      labels:
        app: stateful-app
    spec:
      # 反亲和性确保Pod分散
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - stateful-app
            topologyKey: kubernetes.io/hostname
      # 拓扑分布约束
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: stateful-app
      # 初始化容器
      initContainers:
      - name: init-data
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          # 检查数据目录
          if [ ! -f /data/initialized ]; then
            echo "初始化数据目录..."
            mkdir -p /data/app
            touch /data/initialized
          fi
          
          # 设置正确的权限
          chown -R 1000:1000 /data
          chmod -R 755 /data
        volumeMounts:
        - name: data
          mountPath: /data
      containers:
      - name: app
        image: stateful-app:v1.0
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        # 集群成员发现
        - name: CLUSTER_MEMBERS
          value: "stateful-app-0.stateful-app.$(POD_NAMESPACE).svc.cluster.local,stateful-app-1.stateful-app.$(POD_NAMESPACE).svc.cluster.local,stateful-app-2.stateful-app.$(POD_NAMESPACE).svc.cluster.local"
        volumeMounts:
        - name: data
          mountPath: /data
        - name: config
          mountPath: /etc/app
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        # 启动探针 - 给应用足够的启动时间
        startupProbe:
          httpGet:
            path: /health/startup
            port: 8080
          failureThreshold: 30
          periodSeconds: 10
        # 存活探针
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        # 就绪探针
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 3
        # 优雅终止
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - |
                # 从集群中移除自己
                curl -X POST http://localhost:8080/cluster/leave
                # 等待连接排空
                sleep 15
      # 优雅终止期限
      terminationGracePeriodSeconds: 30
      volumes:
      - name: config
        configMap:
          name: stateful-app-config
  # 持久化存储
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 50Gi
---
# Headless Service用于稳定的网络标识
apiVersion: v1
kind: Service
metadata:
  name: stateful-app
spec:
  clusterIP: None
  ports:
  - port: 8080
    name: http
  - port: 9090
    name: metrics
  selector:
    app: stateful-app
---
# 普通Service用于负载均衡
apiVersion: v1
kind: Service
metadata:
  name: stateful-app-lb
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
    name: http
  selector:
    app: stateful-app
---
# PodDisruptionBudget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: stateful-app-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: stateful-app
```

**Operator模式实现自动化HA**

```go
// stateful-operator.go
package main

import (
    "context"
    "fmt"
    "time"
    
    appsv1 "k8s.io/api/apps/v1"
    corev1 "k8s.io/api/core/v1"
    "k8s.io/apimachinery/pkg/api/errors"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/apimachinery/pkg/runtime"
    "k8s.io/apimachinery/pkg/watch"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/tools/cache"
    "k8s.io/client-go/util/workqueue"
    "sigs.k8s.io/controller-runtime/pkg/client"
    "sigs.k8s.io/controller-runtime/pkg/controller"
    "sigs.k8s.io/controller-runtime/pkg/handler"
    "sigs.k8s.io/controller-runtime/pkg/manager"
    "sigs.k8s.io/controller-runtime/pkg/reconcile"
    "sigs.k8s.io/controller-runtime/pkg/source"
)

// StatefulAppReconciler 有状态应用协调器
type StatefulAppReconciler struct {
    client.Client
    Scheme    *runtime.Scheme
    K8sClient *kubernetes.Clientset
}

// Reconcile 协调循环
func (r *StatefulAppReconciler) Reconcile(ctx context.Context, req reconcile.Request) (reconcile.Result, error) {
    // 获取StatefulSet
    sts := &appsv1.StatefulSet{}
    err := r.Get(ctx, req.NamespacedName, sts)
    if err != nil {
        if errors.IsNotFound(err) {
            return reconcile.Result{}, nil
        }
        return reconcile.Result{}, err
    }
    
    // 检查StatefulSet健康状态
    if err := r.checkStatefulSetHealth(ctx, sts); err != nil {
        return reconcile.Result{RequeueAfter: 30 * time.Second}, err
    }
    
    // 检查Pod健康状态
    if err := r.checkPodsHealth(ctx, sts); err != nil {
        return reconcile.Result{RequeueAfter: 30 * time.Second}, err
    }
    
    // 检查PVC状态
    if err := r.checkPVCStatus(ctx, sts); err != nil {
        return reconcile.Result{RequeueAfter: 30 * time.Second}, err
    }
    
    // 执行自动修复
    if err := r.autoHeal(ctx, sts); err != nil {
        return reconcile.Result{RequeueAfter: 1 * time.Minute}, err
    }
    
    return reconcile.Result{RequeueAfter: 5 * time.Minute}, nil
}

// checkStatefulSetHealth 检查StatefulSet健康状态
func (r *StatefulAppReconciler) checkStatefulSetHealth(ctx context.Context, sts *appsv1.StatefulSet) error {
    // 检查副本数
    if sts.Status.ReadyReplicas < *sts.Spec.Replicas {
        return fmt.Errorf("StatefulSet %s/%s 就绪副本数不足: %d/%d",
            sts.Namespace, sts.Name, sts.Status.ReadyReplicas, *sts.Spec.Replicas)
    }
    
    // 检查更新状态
    if sts.Status.UpdatedReplicas < *sts.Spec.Replicas {
        return fmt.Errorf("StatefulSet %s/%s 更新未完成: %d/%d",
            sts.Namespace, sts.Name, sts.Status.UpdatedReplicas, *sts.Spec.Replicas)
    }
    
    return nil
}

// checkPodsHealth 检查Pod健康状态
func (r *StatefulAppReconciler) checkPodsHealth(ctx context.Context, sts *appsv1.StatefulSet) error {
    // 获取StatefulSet的所有Pod
    podList := &corev1.PodList{}
    listOpts := []client.ListOption{
        client.InNamespace(sts.Namespace),
        client.MatchingLabels(sts.Spec.Selector.MatchLabels),
    }
    
    if err := r.List(ctx, podList, listOpts...); err != nil {
        return err
    }
    
    for _, pod := range podList.Items {
        // 检查Pod状态
        if pod.Status.Phase != corev1.PodRunning {
            return fmt.Errorf("Pod %s/%s 状态异常: %s", pod.Namespace, pod.Name, pod.Status.Phase)
        }
        
        // 检查容器状态
        for _, containerStatus := range pod.Status.ContainerStatuses {
            if !containerStatus.Ready {
                return fmt.Errorf("Pod %s/%s 容器 %s 未就绪",
                    pod.Namespace, pod.Name, containerStatus.Name)
            }
            
            // 检查重启次数
            if containerStatus.RestartCount > 5 {
                return fmt.Errorf("Pod %s/%s 容器 %s 重启次数过多: %d",
                    pod.Namespace, pod.Name, containerStatus.Name, containerStatus.RestartCount)
            }
        }
    }
    
    return nil
}

// checkPVCStatus 检查PVC状态
func (r *StatefulAppReconciler) checkPVCStatus(ctx context.Context, sts *appsv1.StatefulSet) error {
    // 遍历VolumeClaimTemplates
    for _, vct := range sts.Spec.VolumeClaimTemplates {
        // 检查每个副本的PVC
        for i := 0; i < int(*sts.Spec.Replicas); i++ {
            pvcName := fmt.Sprintf("%s-%s-%d", vct.Name, sts.Name, i)
            pvc := &corev1.PersistentVolumeClaim{}
            
            err := r.Get(ctx, client.ObjectKey{
                Namespace: sts.Namespace,
                Name:      pvcName,
            }, pvc)
            
            if err != nil {
                return fmt.Errorf("PVC %s/%s 不存在: %v", sts.Namespace, pvcName, err)
            }
            
            // 检查PVC状态
            if pvc.Status.Phase != corev1.ClaimBound {
                return fmt.Errorf("PVC %s/%s 状态异常: %s", sts.Namespace, pvcName, pvc.Status.Phase)
            }
        }
    }
    
    return nil
}

// autoHeal 自动修复
func (r *StatefulAppReconciler) autoHeal(ctx context.Context, sts *appsv1.StatefulSet) error {
    // 获取所有Pod
    podList := &corev1.PodList{}
    listOpts := []client.ListOption{
        client.InNamespace(sts.Namespace),
        client.MatchingLabels(sts.Spec.Selector.MatchLabels),
    }
    
    if err := r.List(ctx, podList, listOpts...); err != nil {
        return err
    }
    
    for _, pod := range podList.Items {
        // 检查Pod是否需要重启
        if r.needsRestart(&pod) {
            fmt.Printf("重启Pod %s/%s
", pod.Namespace, pod.Name)
            if err := r.Delete(ctx, &pod); err != nil {
                return err
            }
        }
        
        // 检查Pod是否卡在Pending状态
        if pod.Status.Phase == corev1.PodPending {
            // 检查是否是调度问题
            for _, condition := range pod.Status.Conditions {
                if condition.Type == corev1.PodScheduled && condition.Status == corev1.ConditionFalse {
                    fmt.Printf("Pod %s/%s 调度失败: %s
", pod.Namespace, pod.Name, condition.Message)
                    // 可以在这里实现自动扩容节点等逻辑
                }
            }
        }
    }
    
    return nil
}

// needsRestart 判断Pod是否需要重启
func (r *StatefulAppReconciler) needsRestart(pod *corev1.Pod) bool {
    // 检查容器重启次数
    for _, containerStatus := range pod.Status.ContainerStatuses {
        if containerStatus.RestartCount > 10 {
            return true
        }
        
        // 检查容器是否处于CrashLoopBackOff状态
        if containerStatus.State.Waiting != nil &&
            containerStatus.State.Waiting.Reason == "CrashLoopBackOff" {
            return true
        }
    }
    
    // 检查Pod年龄
    age := time.Since(pod.CreationTimestamp.Time)
    if age > 7*24*time.Hour {
        // Pod运行超过7天,考虑重启
        return true
    }
    
    return false
}

// SetupWithManager 设置管理器
func (r *StatefulAppReconciler) SetupWithManager(mgr manager.Manager) error {
    return controller.NewControllerManagedBy(mgr).
        For(&appsv1.StatefulSet{}).
        Owns(&corev1.Pod{}).
        Complete(r)
}

func main() {
    // 创建管理器
    mgr, err := manager.New(config, manager.Options{
        Scheme: scheme,
    })
    if err != nil {
        panic(err)
    }
    
    // 创建协调器
    reconciler := &StatefulAppReconciler{
        Client:    mgr.GetClient(),
        Scheme:    mgr.GetScheme(),
        K8sClient: kubernetes.NewForConfigOrDie(config),
    }
    
    // 设置协调器
    if err := reconciler.SetupWithManager(mgr); err != nil {
        panic(err)
    }
    
    // 启动管理器
    if err := mgr.Start(context.Background()); err != nil {
        panic(err)
    }
}
```



#### 5. 存储层高可用

**Ceph分布式存储高可用**

```yaml
# ceph-rook-cluster.yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  # Ceph版本
  cephVersion:
    image: quay.io/ceph/ceph:v17.2.5
    allowUnsupported: false
  
  # 数据目录
  dataDirHostPath: /var/lib/rook
  
  # 跳过升级检查
  skipUpgradeChecks: false
  
  # 持续健康检查
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  
  # 等待健康检查超时
  waitTimeoutForHealthyOSDInMinutes: 10
  
  # Mon配置
  mon:
    count: 3  # 奇数个Mon,推荐3或5
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: local-storage
        resources:
          requests:
            storage: 10Gi
  
  # MGR配置
  mgr:
    count: 2  # 至少2个MGR实现HA
    allowMultiplePerNode: false
    modules:
    - name: pg_autoscaler
      enabled: true
    - name: rook
      enabled: true
  
  # 崩溃收集器
  crashCollector:
    disable: false
  
  # 日志收集器
  logCollector:
    enabled: true
    periodicity: 24h
  
  # 清理策略
  cleanupPolicy:
    confirmation: ""
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
    allowUninstallWithVolumes: false
  
  # 资源配置
  resources:
    mon:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
    mgr:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
    osd:
      limits:
        cpu: "4000m"
        memory: "8Gi"
      requests:
        cpu: "1000m"
        memory: "4Gi"
  
  # 存储配置
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
    - name: "node1"
      devices:
      - name: "sdb"
      - name: "sdc"
    - name: "node2"
      devices:
      - name: "sdb"
      - name: "sdc"
    - name: "node3"
      devices:
      - name: "sdb"
      - name: "sdc"
  
  # 网络配置
  network:
    provider: host
    connections:
      encryption:
        enabled: false
      compression:
        enabled: false
  
  # 中断预算
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0
  
  # 健康检查
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
---
# CephBlockPool - 块存储池
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  # 故障域
  failureDomain: host
  # 副本数
  replicated:
    size: 3  # 3副本
    requireSafeReplicaSize: true
    replicasPerFailureDomain: 1
  # 纠删码(可选,用于冷数据)
  # erasureCoded:
  #   dataChunks: 2
  #   codingChunks: 1
  # 压缩
  compressionMode: none
  # 配额
  quotas:
    maxSize: "1Ti"
    maxObjects: 1000000
---
# StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: replicapool
  imageFormat: "2"
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/fstype: ext4
allowVolumeExpansion: true
reclaimPolicy: Delete
```

**Longhorn分布式块存储**

```yaml
# longhorn-system.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: longhorn-system
---
# Longhorn配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: longhorn-default-setting
  namespace: longhorn-system
data:
  default-replica-count: "3"
  guaranteed-engine-manager-cpu: "12"
  guaranteed-replica-manager-cpu: "12"
  create-default-disk-labeled-nodes: "true"
  default-data-path: "/var/lib/longhorn"
  replica-soft-anti-affinity: "true"
  replica-auto-balance: "best-effort"
  storage-over-provisioning-percentage: "200"
  storage-minimal-available-percentage: "25"
  upgrade-checker: "true"
  default-longhorn-static-storage-class: "longhorn"
  backupstore-poll-interval: "300"
  taint-toleration: ""
  system-managed-components-node-selector: ""
  priority-class: "longhorn-critical"
  auto-salvage: "true"
  auto-delete-pod-when-volume-detached-unexpectedly: "true"
  disable-scheduling-on-cordoned-node: "true"
  replica-zone-soft-anti-affinity: "true"
  node-down-pod-deletion-policy: "delete-both-statefulset-and-deployment-pod"
  allow-node-drain-with-last-healthy-replica: "false"
  mkfs-ext4-parameters: "-O ^64bit,^metadata_csum"
  disable-replica-rebuild: "false"
  replica-replenishment-wait-interval: "600"
  concurrent-replica-rebuild-per-node-limit: "5"
  disable-revision-counter: "true"
  system-managed-pods-image-pull-policy: "if-not-present"
  allow-volume-creation-with-degraded-availability: "true"
  auto-cleanup-system-generated-snapshot: "true"
  concurrent-automatic-engine-upgrade-per-node-limit: "0"
  backing-image-cleanup-wait-interval: "60"
  backing-image-recovery-wait-interval: "300"
  guaranteed-engine-cpu: "0.25"
  guaranteed-replica-cpu: "0.25"
---
# StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"
  dataLocality: "disabled"
  replicaAutoBalance: "best-effort"
---
# RecurringJob - 定期快照
apiVersion: longhorn.io/v1beta2
kind: RecurringJob
metadata:
  name: snapshot-daily
  namespace: longhorn-system
spec:
  cron: "0 2 * * *"  # 每天凌晨2点
  task: "snapshot"
  groups:
  - default
  retain: 7  # 保留7天
  concurrency: 10
  labels:
    type: snapshot
---
apiVersion: longhorn.io/v1beta2
kind: RecurringJob
metadata:
  name: backup-weekly
  namespace: longhorn-system
spec:
  cron: "0 3 * * 0"  # 每周日凌晨3点
  task: "backup"
  groups:
  - default
  retain: 4  # 保留4周
  concurrency: 5
  labels:
    type: backup
```

**MinIO对象存储高可用**

```yaml
# minio-ha.yaml
apiVersion: v1
kind: Service
metadata:
  name: minio
spec:
  clusterIP: None
  ports:
  - port: 9000
    name: api
  - port: 9001
    name: console
  selector:
    app: minio
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
spec:
  serviceName: minio
  replicas: 4  # 至少4个节点
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      labels:
        app: minio
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - minio
            topologyKey: kubernetes.io/hostname
      containers:
      - name: minio
        image: minio/minio:latest
        args:
        - server
        - http://minio-{0...3}.minio.default.svc.cluster.local/data
        - --console-address
        - ":9001"
        env:
        - name: MINIO_ROOT_USER
          valueFrom:
            secretKeyRef:
              name: minio-secret
              key: root-user
        - name: MINIO_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: minio-secret
              key: root-password
        - name: MINIO_PROMETHEUS_AUTH_TYPE
          value: "public"
        ports:
        - containerPort: 9000
          name: api
        - containerPort: 9001
          name: console
        volumeMounts:
        - name: data
          mountPath: /data
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        livenessProbe:
          httpGet:
            path: /minio/health/live
            port: 9000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /minio/health/ready
            port: 9000
          initialDelaySeconds: 10
          periodSeconds: 5
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi
```

#### 6. 网络层高可用

**Calico网络高可用配置**

```yaml
# calico-ha-config.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: calico-config
  namespace: kube-system
data:
  # Typha配置 - 用于大规模集群
  typha_service_name: "calico-typha"
  
  # BGP配置
  calico_backend: "bird"
  
  # IP池配置
  cni_network_config: |-
    {
      "name": "k8s-pod-network",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "calico",
          "log_level": "info",
          "datastore_type": "kubernetes",
          "nodename": "__KUBERNETES_NODE_NAME__",
          "mtu": __CNI_MTU__,
          "ipam": {
            "type": "calico-ipam",
            "assign_ipv4": "true",
            "assign_ipv6": "false"
          },
          "policy": {
            "type": "k8s"
          },
          "kubernetes": {
            "kubeconfig": "__KUBECONFIG_FILEPATH__"
          }
        },
        {
          "type": "portmap",
          "snat": true,
          "capabilities": {"portMappings": true}
        },
        {
          "type": "bandwidth",
          "capabilities": {"bandwidth": true}
        }
      ]
    }
---
# Typha Deployment - 提高可扩展性
apiVersion: apps/v1
kind: Deployment
metadata:
  name: calico-typha
  namespace: kube-system
spec:
  replicas: 3  # 根据集群规模调整
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      k8s-app: calico-typha
  template:
    metadata:
      labels:
        k8s-app: calico-typha
    spec:
      nodeSelector:
        kubernetes.io/os: linux
      hostNetwork: true
      tolerations:
      - operator: Exists
      serviceAccountName: calico-node
      priorityClassName: system-cluster-critical
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - calico-typha
            topologyKey: kubernetes.io/hostname
      containers:
      - name: calico-typha
        image: calico/typha:v3.25.0
        ports:
        - containerPort: 5473
          name: calico-typha
          protocol: TCP
        env:
        - name: TYPHA_LOGFILEPATH
          value: "none"
        - name: TYPHA_LOGSEVERITYSYS
          value: "none"
        - name: TYPHA_LOGSEVERITYSCREEN
          value: "info"
        - name: TYPHA_PROMETHEUSMETRICSENABLED
          value: "true"
        - name: TYPHA_CONNECTIONREBALANCINGMODE
          value: "kubernetes"
        - name: TYPHA_PROMETHEUSMETRICSPORT
          value: "9093"
        - name: TYPHA_DATASTORETYPE
          value: "kubernetes"
        - name: TYPHA_MAXCONNECTIONSLOWERLIMIT
          value: "1"
        - name: TYPHA_HEALTHENABLED
          value: "true"
        livenessProbe:
          httpGet:
            path: /liveness
            port: 9098
            host: localhost
          periodSeconds: 30
          initialDelaySeconds: 30
        readinessProbe:
          httpGet:
            path: /readiness
            port: 9098
            host: localhost
          periodSeconds: 10
        resources:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 2Gi
---
# Typha Service
apiVersion: v1
kind: Service
metadata:
  name: calico-typha
  namespace: kube-system
spec:
  ports:
  - port: 5473
    protocol: TCP
    targetPort: calico-typha
    name: calico-typha
  selector:
    k8s-app: calico-typha
---
# BGP配置 - 多路径ECMP
apiVersion: projectcalico.org/v3
kind: BGPConfiguration
metadata:
  name: default
spec:
  logSeverityScreen: Info
  nodeToNodeMeshEnabled: true
  asNumber: 64512
  serviceClusterIPs:
  - cidr: 10.96.0.0/12
  serviceExternalIPs:
  - cidr: 192.168.0.0/16
  listenPort: 179
  bindMode: NodeIP
---
# IP池配置
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: default-ipv4-ippool
spec:
  cidr: 10.244.0.0/16
  ipipMode: Never
  natOutgoing: true
  disabled: false
  nodeSelector: all()
  vxlanMode: CrossSubnet
  blockSize: 26
```

**Cilium网络高可用配置**

```yaml
# cilium-ha-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: kube-system
data:
  # 身份分配模式
  identity-allocation-mode: crd
  
  # Cilium operator副本数
  cilium-operator-replicas: "3"
  
  # 启用Hubble
  enable-hubble: "true"
  hubble-listen-address: ":4244"
  hubble-metrics-server: ":9091"
  hubble-metrics:
    - dns
    - drop
    - tcp
    - flow
    - icmp
    - http
  
  # 启用Prometheus指标
  prometheus-serve-addr: ":9090"
  operator-prometheus-serve-addr: ":6942"
  
  # 启用健康检查
  enable-health-checking: "true"
  enable-endpoint-health-checking: "true"
  
  # IPAM模式
  ipam: "kubernetes"
  
  # 启用策略
  enable-policy: "default"
  
  # 启用L7代理
  enable-l7-proxy: "true"
  
  # 启用带宽管理
  enable-bandwidth-manager: "true"
  
  # 启用本地重定向策略
  enable-local-redirect-policy: "true"
  
  # 启用BBR拥塞控制
  enable-bbr: "true"
  
  # 启用IPv4
  enable-ipv4: "true"
  enable-ipv6: "false"
  
  # 隧道模式
  tunnel: "vxlan"
  
  # 启用主机路由
  enable-host-reachable-services: "true"
  
  # 启用节点端口
  enable-node-port: "true"
  node-port-mode: "hybrid"
  
  # 启用外部IP
  enable-external-ips: "true"
  
  # 启用会话亲和性
  enable-session-affinity: "true"
  
  # 启用健康检查节点端口
  enable-health-check-nodeport: "true"
  
  # 启用自动直接路由
  auto-direct-node-routes: "true"
  
  # 启用端点路由
  enable-endpoint-routes: "true"
  
  # 启用本地重定向
  enable-local-node-route: "false"
---
# Cilium DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: cilium
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cilium
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 2
  template:
    metadata:
      labels:
        k8s-app: cilium
    spec:
      containers:
      - name: cilium-agent
        image: quay.io/cilium/cilium:v1.13.0
        command:
        - cilium-agent
        args:
        - --config-dir=/tmp/cilium/config-map
        env:
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: CILIUM_K8S_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9876
            scheme: HTTP
            httpHeaders:
            - name: "brief"
              value: "true"
          failureThreshold: 10
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9876
            scheme: HTTP
            httpHeaders:
            - name: "brief"
              value: "true"
          failureThreshold: 3
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            cpu: 2000m
            memory: 4Gi
        volumeMounts:
        - name: bpf-maps
          mountPath: /sys/fs/bpf
          mountPropagation: Bidirectional
        - name: cilium-run
          mountPath: /var/run/cilium
        - name: cni-path
          mountPath: /host/opt/cni/bin
        - name: etc-cni-netd
          mountPath: /host/etc/cni/net.d
        - name: cilium-config-path
          mountPath: /tmp/cilium/config-map
          readOnly: true
      hostNetwork: true
      priorityClassName: system-node-critical
      restartPolicy: Always
      serviceAccount: cilium
      serviceAccountName: cilium
      terminationGracePeriodSeconds: 1
      tolerations:
      - operator: Exists
      volumes:
      - name: bpf-maps
        hostPath:
          path: /sys/fs/bpf
          type: DirectoryOrCreate
      - name: cilium-run
        hostPath:
          path: /var/run/cilium
          type: DirectoryOrCreate
      - name: cni-path
        hostPath:
          path: /opt/cni/bin
          type: DirectoryOrCreate
      - name: etc-cni-netd
        hostPath:
          path: /etc/cni/net.d
          type: DirectoryOrCreate
      - name: cilium-config-path
        configMap:
          name: cilium-config
---
# Cilium Operator Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cilium-operator
  namespace: kube-system
spec:
  replicas: 3
  selector:
    matchLabels:
      name: cilium-operator
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        name: cilium-operator
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: name
                operator: In
                values:
                - cilium-operator
            topologyKey: kubernetes.io/hostname
      containers:
      - name: cilium-operator
        image: quay.io/cilium/operator-generic:v1.13.0
        command:
        - cilium-operator-generic
        args:
        - --config-dir=/tmp/cilium/config-map
        - --debug=$(CILIUM_DEBUG)
        env:
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: CILIUM_K8S_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CILIUM_DEBUG
          valueFrom:
            configMapKeyRef:
              key: debug
              name: cilium-config
              optional: true
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9234
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 3
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9234
            scheme: HTTP
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
        resources:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 2Gi
        volumeMounts:
        - name: cilium-config-path
          mountPath: /tmp/cilium/config-map
          readOnly: true
      hostNetwork: true
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      serviceAccount: cilium-operator
      serviceAccountName: cilium-operator
      volumes:
      - name: cilium-config-path
        configMap:
          name: cilium-config
```



#### 7. 高可用测试与验证

**混沌工程测试工具**

```yaml
# chaos-mesh-experiment.yaml
# 安装Chaos Mesh
apiVersion: v1
kind: Namespace
metadata:
  name: chaos-testing
---
# Pod故障注入
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: pod-failure-test
  namespace: chaos-testing
spec:
  action: pod-failure
  mode: one
  duration: "30s"
  selector:
    namespaces:
    - default
    labelSelectors:
      app: web-app
  scheduler:
    cron: "@every 1h"
---
# Pod杀死测试
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: pod-kill-test
  namespace: chaos-testing
spec:
  action: pod-kill
  mode: fixed
  value: "1"
  duration: "10s"
  selector:
    namespaces:
    - default
    labelSelectors:
      app: web-app
  scheduler:
    cron: "@every 2h"
---
# 容器杀死测试
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: container-kill-test
  namespace: chaos-testing
spec:
  action: container-kill
  mode: one
  containerNames:
  - app
  duration: "20s"
  selector:
    namespaces:
    - default
    labelSelectors:
      app: web-app
---
# 网络延迟注入
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-delay-test
  namespace: chaos-testing
spec:
  action: delay
  mode: one
  selector:
    namespaces:
    - default
    labelSelectors:
      app: web-app
  delay:
    latency: "100ms"
    correlation: "100"
    jitter: "10ms"
  duration: "1m"
  direction: to
  target:
    mode: all
    selector:
      namespaces:
      - default
      labelSelectors:
        app: database
---
# 网络丢包注入
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-loss-test
  namespace: chaos-testing
spec:
  action: loss
  mode: one
  selector:
    namespaces:
    - default
    labelSelectors:
      app: web-app
  loss:
    loss: "25"
    correlation: "100"
  duration: "30s"
  direction: to
  target:
    mode: all
    selector:
      namespaces:
      - default
      labelSelectors:
        app: database
---
# 网络分区测试
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-partition-test
  namespace: chaos-testing
spec:
  action: partition
  mode: all
  selector:
    namespaces:
    - default
    labelSelectors:
      app: web-app
  direction: both
  target:
    mode: all
    selector:
      namespaces:
      - default
      labelSelectors:
        app: database
  duration: "1m"
---
# 网络带宽限制
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-bandwidth-test
  namespace: chaos-testing
spec:
  action: bandwidth
  mode: one
  selector:
    namespaces:
    - default
    labelSelectors:
      app: web-app
  bandwidth:
    rate: "1mbps"
    limit: 20000
    buffer: 10000
  duration: "2m"
  direction: to
  target:
    mode: all
    selector:
      namespaces:
      - default
---
# IO延迟注入
apiVersion: chaos-mesh.org/v1alpha1
kind: IOChaos
metadata:
  name: io-delay-test
  namespace: chaos-testing
spec:
  action: latency
  mode: one
  selector:
    namespaces:
    - default
    labelSelectors:
      app: database
  volumePath: /var/lib/mysql
  path: /var/lib/mysql/**/*
  delay: "100ms"
  percent: 50
  duration: "1m"
---
# IO错误注入
apiVersion: chaos-mesh.org/v1alpha1
kind: IOChaos
metadata:
  name: io-errno-test
  namespace: chaos-testing
spec:
  action: fault
  mode: one
  selector:
    namespaces:
    - default
    labelSelectors:
      app: database
  volumePath: /var/lib/mysql
  path: /var/lib/mysql/**/*
  errno: 5  # EIO
  percent: 10
  duration: "30s"
---
# 压力测试
apiVersion: chaos-mesh.org/v1alpha1
kind: StressChaos
metadata:
  name: stress-test
  namespace: chaos-testing
spec:
  mode: one
  selector:
    namespaces:
    - default
    labelSelectors:
      app: web-app
  stressors:
    cpu:
      workers: 2
      load: 80
    memory:
      workers: 2
      size: "512MB"
  duration: "5m"
---
# 时间偏移测试
apiVersion: chaos-mesh.org/v1alpha1
kind: TimeChaos
metadata:
  name: time-shift-test
  namespace: chaos-testing
spec:
  mode: one
  selector:
    namespaces:
    - default
    labelSelectors:
      app: web-app
  timeOffset: "-1h"
  duration: "10m"
---
# DNS故障注入
apiVersion: chaos-mesh.org/v1alpha1
kind: DNSChaos
metadata:
  name: dns-error-test
  namespace: chaos-testing
spec:
  action: error
  mode: all
  selector:
    namespaces:
    - default
    labelSelectors:
      app: web-app
  patterns:
  - database.default.svc.cluster.local
  - "*.example.com"
  duration: "1m"
---
# HTTP故障注入
apiVersion: chaos-mesh.org/v1alpha1
kind: HTTPChaos
metadata:
  name: http-abort-test
  namespace: chaos-testing
spec:
  mode: one
  selector:
    namespaces:
    - default
    labelSelectors:
      app: web-app
  target: Request
  port: 8080
  path: /api/*
  abort: true
  duration: "30s"
```

**自动化HA测试脚本**

```bash
#!/bin/bash
# ha-test-suite.sh - 高可用测试套件

set -e

# 颜色输出
RED='[0;31m'
GREEN='[0;32m'
YELLOW='[1;33m'
NC='[0m'

# 日志函数
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# 测试结果
TESTS_PASSED=0
TESTS_FAILED=0
TEST_RESULTS=()

# 记录测试结果
record_test() {
    local test_name=$1
    local result=$2
    local message=$3
    
    if [ "$result" = "PASS" ]; then
        ((TESTS_PASSED++))
        log_info "✓ $test_name: PASSED"
    else
        ((TESTS_FAILED++))
        log_error "✗ $test_name: FAILED - $message"
    fi
    
    TEST_RESULTS+=("$test_name|$result|$message")
}

# 1. 测试控制平面高可用
test_control_plane_ha() {
    log_info "测试控制平面高可用..."
    
    # 获取API Server端点
    local api_servers=$(kubectl get endpoints kubernetes -n default -o jsonpath='{.subsets[*].addresses[*].ip}')
    local api_count=$(echo $api_servers | wc -w)
    
    if [ $api_count -ge 3 ]; then
        record_test "控制平面副本数" "PASS" "API Server数量: $api_count"
    else
        record_test "控制平面副本数" "FAIL" "API Server数量不足: $api_count < 3"
        return 1
    fi
    
    # 测试API Server可用性
    for ip in $api_servers; do
        if curl -k -s https://$ip:6443/healthz > /dev/null; then
            log_info "API Server $ip 健康检查通过"
        else
            record_test "API Server健康检查" "FAIL" "API Server $ip 不可用"
            return 1
        fi
    done
    
    record_test "API Server健康检查" "PASS" "所有API Server健康"
    
    # 测试etcd集群
    local etcd_pods=$(kubectl get pods -n kube-system -l component=etcd -o name | wc -l)
    if [ $etcd_pods -ge 3 ]; then
        record_test "etcd集群规模" "PASS" "etcd节点数: $etcd_pods"
    else
        record_test "etcd集群规模" "FAIL" "etcd节点数不足: $etcd_pods < 3"
        return 1
    fi
}

# 2. 测试节点故障恢复
test_node_failure_recovery() {
    log_info "测试节点故障恢复..."
    
    # 选择一个工作节点
    local test_node=$(kubectl get nodes -l node-role.kubernetes.io/worker=true -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$test_node" ]; then
        record_test "节点故障恢复" "SKIP" "没有可用的工作节点"
        return 0
    fi
    
    log_info "选择测试节点: $test_node"
    
    # 获取节点上的Pod数量
    local pod_count_before=$(kubectl get pods --all-namespaces --field-selector spec.nodeName=$test_node -o name | wc -l)
    log_info "节点上Pod数量: $pod_count_before"
    
    # 标记节点为不可调度
    kubectl cordon $test_node
    
    # 驱逐节点上的Pod
    kubectl drain $test_node --ignore-daemonsets --delete-emptydir-data --force --grace-period=30 &
    local drain_pid=$!
    
    # 等待Pod重新调度
    sleep 60
    
    # 检查Pod是否已重新调度
    local pod_count_after=$(kubectl get pods --all-namespaces --field-selector spec.nodeName=$test_node -o name | wc -l)
    
    # 恢复节点
    kubectl uncordon $test_node
    
    if [ $pod_count_after -lt $pod_count_before ]; then
        record_test "节点故障恢复" "PASS" "Pod已重新调度: $pod_count_before -> $pod_count_after"
    else
        record_test "节点故障恢复" "FAIL" "Pod未能重新调度"
        return 1
    fi
}

# 3. 测试Pod故障恢复
test_pod_failure_recovery() {
    log_info "测试Pod故障恢复..."
    
    # 部署测试应用
    kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ha-test-app
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ha-test
  template:
    metadata:
      labels:
        app: ha-test
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
EOF
    
    # 等待部署就绪
    kubectl wait --for=condition=available --timeout=60s deployment/ha-test-app
    
    # 获取一个Pod
    local test_pod=$(kubectl get pods -l app=ha-test -o jsonpath='{.items[0].metadata.name}')
    log_info "删除测试Pod: $test_pod"
    
    # 删除Pod
    kubectl delete pod $test_pod
    
    # 等待新Pod创建
    sleep 10
    
    # 检查Deployment是否恢复
    local ready_replicas=$(kubectl get deployment ha-test-app -o jsonpath='{.status.readyReplicas}')
    
    # 清理
    kubectl delete deployment ha-test-app
    
    if [ "$ready_replicas" = "3" ]; then
        record_test "Pod故障恢复" "PASS" "Pod已自动恢复"
    else
        record_test "Pod故障恢复" "FAIL" "Pod未能恢复: $ready_replicas/3"
        return 1
    fi
}

# 4. 测试服务可用性
test_service_availability() {
    log_info "测试服务可用性..."
    
    # 部署测试服务
    kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: ha-test-service
  namespace: default
spec:
  selector:
    app: ha-test
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ha-test-service-app
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ha-test
  template:
    metadata:
      labels:
        app: ha-test
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
EOF
    
    # 等待部署就绪
    kubectl wait --for=condition=available --timeout=60s deployment/ha-test-service-app
    
    # 获取Service ClusterIP
    local service_ip=$(kubectl get service ha-test-service -o jsonpath='{.spec.clusterIP}')
    
    # 测试服务可用性
    local success_count=0
    for i in {1..10}; do
        if kubectl run test-curl-$i --image=curlimages/curl:latest --rm -i --restart=Never -- curl -s -o /dev/null -w "%{http_code}" http://$service_ip | grep -q 200; then
            ((success_count++))
        fi
        sleep 1
    done
    
    # 清理
    kubectl delete deployment ha-test-service-app
    kubectl delete service ha-test-service
    
    if [ $success_count -ge 9 ]; then
        record_test "服务可用性" "PASS" "成功率: $success_count/10"
    else
        record_test "服务可用性" "FAIL" "成功率过低: $success_count/10"
        return 1
    fi
}

# 5. 测试存储高可用
test_storage_ha() {
    log_info "测试存储高可用..."
    
    # 检查StorageClass
    local storage_classes=$(kubectl get storageclass -o name | wc -l)
    if [ $storage_classes -eq 0 ]; then
        record_test "存储高可用" "SKIP" "没有可用的StorageClass"
        return 0
    fi
    
    # 创建测试PVC
    kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ha-test-pvc
  namespace: default
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
EOF
    
    # 等待PVC绑定
    sleep 10
    
    local pvc_status=$(kubectl get pvc ha-test-pvc -o jsonpath='{.status.phase}')
    
    # 清理
    kubectl delete pvc ha-test-pvc
    
    if [ "$pvc_status" = "Bound" ]; then
        record_test "存储高可用" "PASS" "PVC成功绑定"
    else
        record_test "存储高可用" "FAIL" "PVC绑定失败: $pvc_status"
        return 1
    fi
}

# 6. 测试网络高可用
test_network_ha() {
    log_info "测试网络高可用..."
    
    # 检查CNI插件
    local cni_pods=$(kubectl get pods -n kube-system -l k8s-app=calico-node -o name 2>/dev/null | wc -l)
    if [ $cni_pods -eq 0 ]; then
        cni_pods=$(kubectl get pods -n kube-system -l k8s-app=cilium -o name 2>/dev/null | wc -l)
    fi
    
    if [ $cni_pods -eq 0 ]; then
        record_test "网络高可用" "FAIL" "未找到CNI插件"
        return 1
    fi
    
    # 测试Pod间通信
    kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: network-test-server
  namespace: default
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: network-test-client
  namespace: default
spec:
  containers:
  - name: curl
    image: curlimages/curl:latest
    command: ["sleep", "3600"]
EOF
    
    # 等待Pod就绪
    kubectl wait --for=condition=ready --timeout=60s pod/network-test-server
    kubectl wait --for=condition=ready --timeout=60s pod/network-test-client
    
    # 获取服务器Pod IP
    local server_ip=$(kubectl get pod network-test-server -o jsonpath='{.status.podIP}')
    
    # 测试连接
    local result=$(kubectl exec network-test-client -- curl -s -o /dev/null -w "%{http_code}" http://$server_ip)
    
    # 清理
    kubectl delete pod network-test-server network-test-client
    
    if [ "$result" = "200" ]; then
        record_test "网络高可用" "PASS" "Pod间通信正常"
    else
        record_test "网络高可用" "FAIL" "Pod间通信失败: HTTP $result"
        return 1
    fi
}

# 7. 测试DNS高可用
test_dns_ha() {
    log_info "测试DNS高可用..."
    
    # 检查CoreDNS副本数
    local coredns_replicas=$(kubectl get deployment coredns -n kube-system -o jsonpath='{.spec.replicas}')
    
    if [ $coredns_replicas -ge 2 ]; then
        record_test "DNS副本数" "PASS" "CoreDNS副本数: $coredns_replicas"
    else
        record_test "DNS副本数" "WARN" "CoreDNS副本数不足: $coredns_replicas < 2"
    fi
    
    # 测试DNS解析
    kubectl run dns-test --image=busybox:1.35 --rm -i --restart=Never -- nslookup kubernetes.default > /dev/null 2>&1
    
    if [ $? -eq 0 ]; then
        record_test "DNS解析" "PASS" "DNS解析正常"
    else
        record_test "DNS解析" "FAIL" "DNS解析失败"
        return 1
    fi
}

# 8. 生成测试报告
generate_report() {
    log_info "生成测试报告..."
    
    local total_tests=$((TESTS_PASSED + TESTS_FAILED))
    local pass_rate=0
    
    if [ $total_tests -gt 0 ]; then
        pass_rate=$((TESTS_PASSED * 100 / total_tests))
    fi
    
    cat > ha-test-report.txt <<EOF
========================================
Kubernetes 高可用测试报告
========================================
测试时间: $(date)
集群信息: $(kubectl cluster-info | head -1)

测试统计:
- 总测试数: $total_tests
- 通过: $TESTS_PASSED
- 失败: $TESTS_FAILED
- 通过率: $pass_rate%

详细结果:
EOF
    
    for result in "${TEST_RESULTS[@]}"; do
        IFS='|' read -r name status message <<< "$result"
        echo "- [$status] $name: $message" >> ha-test-report.txt
    done
    
    cat >> ha-test-report.txt <<EOF

========================================
建议:
EOF
    
    if [ $TESTS_FAILED -gt 0 ]; then
        echo "- 存在 $TESTS_FAILED 项测试失败,请检查上述失败项并进行修复" >> ha-test-report.txt
    fi
    
    if [ $pass_rate -lt 80 ]; then
        echo "- 通过率低于80%,集群高可用性存在严重问题" >> ha-test-report.txt
    elif [ $pass_rate -lt 95 ]; then
        echo "- 通过率在80-95%之间,建议优化高可用配置" >> ha-test-report.txt
    else
        echo "- 通过率达到95%以上,集群高可用性良好" >> ha-test-report.txt
    fi
    
    cat ha-test-report.txt
    
    log_info "测试报告已保存到: ha-test-report.txt"
}

# 主函数
main() {
    log_info "开始Kubernetes高可用测试..."
    
    # 检查kubectl
    if ! command -v kubectl &> /dev/null; then
        log_error "kubectl未安装"
        exit 1
    fi
    
    # 检查集群连接
    if ! kubectl cluster-info &> /dev/null; then
        log_error "无法连接到Kubernetes集群"
        exit 1
    fi
    
    # 执行测试
    test_control_plane_ha || true
    test_node_failure_recovery || true
    test_pod_failure_recovery || true
    test_service_availability || true
    test_storage_ha || true
    test_network_ha || true
    test_dns_ha || true
    
    # 生成报告
    generate_report
    
    # 返回结果
    if [ $TESTS_FAILED -eq 0 ]; then
        log_info "所有测试通过!"
        exit 0
    else
        log_error "$TESTS_FAILED 项测试失败"
        exit 1
    fi
}

# 运行主函数
main "$@"
```

#### 8. 高可用最佳实践总结

**架构设计原则**

1. **消除单点故障**
   - 控制平面至少3个节点
   - etcd集群奇数节点(3/5/7)
   - 应用多副本部署
   - 数据库主从/集群模式
   - 存储多副本或纠删码

2. **故障检测与自动恢复**
   - 配置健康检查探针
   - 设置合理的超时和重试
   - 使用PodDisruptionBudget
   - 实现自动扩缩容
   - 部署Operator自动化运维

3. **数据持久化与复制**
   - 使用持久化存储
   - 配置数据备份策略
   - 实现跨区域复制
   - 定期验证备份有效性
   - 制定数据恢复流程

4. **负载均衡**
   - API Server前置负载均衡器
   - Service实现Pod负载均衡
   - Ingress实现应用负载均衡
   - 使用会话亲和性(如需要)
   - 配置健康检查和故障转移

5. **优雅降级**
   - 实现熔断机制
   - 配置限流策略
   - 提供降级方案
   - 缓存关键数据
   - 异步处理非关键任务

**运维最佳实践**

1. **监控告警**
   - 监控所有关键组件
   - 设置合理的告警阈值
   - 建立告警升级机制
   - 定期审查告警规则
   - 避免告警疲劳

2. **容量规划**
   - 预留足够的资源余量
   - 定期评估资源使用
   - 规划扩容策略
   - 考虑突发流量
   - 实施成本优化

3. **变更管理**
   - 使用滚动更新策略
   - 实施蓝绿部署或金丝雀发布
   - 保留回滚能力
   - 在非高峰期变更
   - 充分测试后上线

4. **灾难演练**
   - 定期进行故障演练
   - 测试备份恢复流程
   - 验证故障转移机制
   - 更新应急预案
   - 总结演练经验

5. **文档与培训**
   - 维护架构文档
   - 编写运维手册
   - 制定应急预案
   - 培训运维团队
   - 建立知识库

**性能优化**

1. **资源优化**
   - 合理设置资源请求和限制
   - 使用HPA自动扩缩容
   - 启用VPA优化资源配置
   - 使用节点亲和性优化调度
   - 配置优先级和抢占

2. **网络优化**
   - 使用本地DNS缓存
   - 启用服务拓扑感知
   - 配置网络策略
   - 优化MTU设置
   - 使用eBPF加速

3. **存储优化**
   - 选择合适的存储类型
   - 使用本地存储提升性能
   - 配置存储QoS
   - 启用存储快照
   - 实施存储分层

4. **应用优化**
   - 优化容器镜像大小
   - 使用多阶段构建
   - 配置资源限制
   - 实施连接池
   - 启用应用缓存

**安全加固**

1. **访问控制**
   - 启用RBAC
   - 最小权限原则
   - 使用ServiceAccount
   - 配置NetworkPolicy
   - 启用PodSecurityPolicy

2. **数据加密**
   - 启用etcd加密
   - 使用TLS通信
   - 加密敏感数据
   - 使用Secret管理
   - 定期轮换密钥

3. **审计日志**
   - 启用审计日志
   - 集中日志收集
   - 定期审查日志
   - 设置日志告警
   - 保留足够时长

4. **镜像安全**
   - 使用可信镜像源
   - 扫描镜像漏洞
   - 签名镜像
   - 定期更新镜像
   - 使用最小化镜像

**成本优化**

1. **资源利用率**
   - 监控资源使用情况
   - 识别闲置资源
   - 使用Spot实例
   - 配置自动扩缩容
   - 实施资源配额

2. **存储成本**
   - 使用合适的存储类型
   - 清理未使用的卷
   - 配置存储生命周期
   - 使用对象存储
   - 启用数据压缩

3. **网络成本**
   - 优化跨区域流量
   - 使用CDN加速
   - 配置流量策略
   - 监控网络费用
   - 使用私有网络

通过以上高可用架构设计和最佳实践,可以构建一个稳定、可靠、高性能的Kubernetes生产环境,确保业务连续性和服务质量。



### 10.3.4 业务连续性最佳实践

业务连续性(Business Continuity)是指组织在面对各种中断事件时,能够持续提供关键业务服务的能力。本节将介绍Kubernetes环境下的业务连续性规划、实施和管理最佳实践。

#### 1. 业务连续性规划(BCP)

**业务影响分析(BIA)**

```yaml
# bia-assessment.yaml - 业务影响分析配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: bia-assessment
  namespace: business-continuity
data:
  assessment.yaml: |
    # 业务影响分析
    business_services:
      # 核心业务服务
      - name: "用户认证服务"
        criticality: "critical"
        rto: "5m"
        rpo: "0"
        dependencies:
          - "数据库集群"
          - "Redis缓存"
          - "LDAP服务"
        impact:
          financial: "high"
          reputation: "high"
          regulatory: "high"
        recovery_priority: 1
        
      - name: "订单处理服务"
        criticality: "critical"
        rto: "15m"
        rpo: "5m"
        dependencies:
          - "数据库集群"
          - "消息队列"
          - "支付网关"
        impact:
          financial: "high"
          reputation: "high"
          regulatory: "medium"
        recovery_priority: 1
        
      - name: "库存管理服务"
        criticality: "high"
        rto: "30m"
        rpo: "15m"
        dependencies:
          - "数据库集群"
          - "缓存服务"
        impact:
          financial: "medium"
          reputation: "medium"
          regulatory: "low"
        recovery_priority: 2
        
      - name: "报表分析服务"
        criticality: "medium"
        rto: "4h"
        rpo: "1h"
        dependencies:
          - "数据仓库"
          - "分析引擎"
        impact:
          financial: "low"
          reputation: "low"
          regulatory: "low"
        recovery_priority: 3
        
      - name: "日志收集服务"
        criticality: "low"
        rto: "24h"
        rpo: "24h"
        dependencies:
          - "对象存储"
        impact:
          financial: "low"
          reputation: "low"
          regulatory: "low"
        recovery_priority: 4
    
    # 关键资源
    critical_resources:
      - name: "数据库集群"
        type: "infrastructure"
        ha_config:
          replicas: 3
          backup_frequency: "15m"
          backup_retention: "30d"
        
      - name: "消息队列"
        type: "infrastructure"
        ha_config:
          replicas: 3
          persistence: true
          replication_factor: 3
        
      - name: "对象存储"
        type: "infrastructure"
        ha_config:
          replication: "cross-region"
          versioning: true
          lifecycle_policy: true
    
    # 恢复策略
    recovery_strategies:
      - scenario: "单节点故障"
        strategy: "自动故障转移"
        estimated_rto: "5m"
        automation_level: "full"
        
      - scenario: "多节点故障"
        strategy: "自动扩容+手动干预"
        estimated_rto: "15m"
        automation_level: "partial"
        
      - scenario: "区域故障"
        strategy: "跨区域故障转移"
        estimated_rto: "30m"
        automation_level: "semi-automatic"
        
      - scenario: "数据中心故障"
        strategy: "灾备中心切换"
        estimated_rto: "2h"
        automation_level: "manual"
```

**业务连续性计划文档生成工具**

```go
// bcp-generator.go
package main

import (
    "fmt"
    "os"
    "text/template"
    "time"
    
    "gopkg.in/yaml.v3"
)

// BusinessService 业务服务
type BusinessService struct {
    Name             string            `yaml:"name"`
    Criticality      string            `yaml:"criticality"`
    RTO              string            `yaml:"rto"`
    RPO              string            `yaml:"rpo"`
    Dependencies     []string          `yaml:"dependencies"`
    Impact           Impact            `yaml:"impact"`
    RecoveryPriority int               `yaml:"recovery_priority"`
}

// Impact 影响评估
type Impact struct {
    Financial  string `yaml:"financial"`
    Reputation string `yaml:"reputation"`
    Regulatory string `yaml:"regulatory"`
}

// RecoveryStrategy 恢复策略
type RecoveryStrategy struct {
    Scenario        string `yaml:"scenario"`
    Strategy        string `yaml:"strategy"`
    EstimatedRTO    string `yaml:"estimated_rto"`
    AutomationLevel string `yaml:"automation_level"`
}

// BIAAssessment 业务影响分析
type BIAAssessment struct {
    BusinessServices   []BusinessService   `yaml:"business_services"`
    CriticalResources  []CriticalResource  `yaml:"critical_resources"`
    RecoveryStrategies []RecoveryStrategy  `yaml:"recovery_strategies"`
}

// CriticalResource 关键资源
type CriticalResource struct {
    Name     string    `yaml:"name"`
    Type     string    `yaml:"type"`
    HAConfig HAConfig  `yaml:"ha_config"`
}

// HAConfig 高可用配置
type HAConfig struct {
    Replicas         int    `yaml:"replicas,omitempty"`
    BackupFrequency  string `yaml:"backup_frequency,omitempty"`
    BackupRetention  string `yaml:"backup_retention,omitempty"`
    Persistence      bool   `yaml:"persistence,omitempty"`
    ReplicationFactor int   `yaml:"replication_factor,omitempty"`
    Replication      string `yaml:"replication,omitempty"`
    Versioning       bool   `yaml:"versioning,omitempty"`
    LifecyclePolicy  bool   `yaml:"lifecycle_policy,omitempty"`
}

// BCPGenerator BCP生成器
type BCPGenerator struct {
    Assessment BIAAssessment
}

// NewBCPGenerator 创建BCP生成器
func NewBCPGenerator(assessmentFile string) (*BCPGenerator, error) {
    data, err := os.ReadFile(assessmentFile)
    if err != nil {
        return nil, fmt.Errorf("读取评估文件失败: %v", err)
    }
    
    var assessment BIAAssessment
    if err := yaml.Unmarshal(data, &assessment); err != nil {
        return nil, fmt.Errorf("解析评估文件失败: %v", err)
    }
    
    return &BCPGenerator{
        Assessment: assessment,
    }, nil
}

// GenerateBCPDocument 生成BCP文档
func (g *BCPGenerator) GenerateBCPDocument(outputFile string) error {
    tmpl := `
# 业务连续性计划 (Business Continuity Plan)

**文档版本:** 1.0
**生成时间:** {{.GeneratedAt}}
**有效期:** {{.ValidUntil}}

## 1. 执行摘要

本业务连续性计划旨在确保组织在面对各种中断事件时,能够持续提供关键业务服务。
本计划涵盖了业务影响分析、恢复策略、应急响应流程和持续改进机制。

## 2. 业务影响分析

### 2.1 关键业务服务

{{range .Assessment.BusinessServices}}
#### {{.Name}}

- **关键程度:** {{.Criticality}}
- **恢复时间目标(RTO):** {{.RTO}}
- **恢复点目标(RPO):** {{.RPO}}
- **恢复优先级:** P{{.RecoveryPriority}}

**依赖关系:**
{{range .Dependencies}}
- {{.}}
{{end}}

**影响评估:**
- 财务影响: {{.Impact.Financial}}
- 声誉影响: {{.Impact.Reputation}}
- 合规影响: {{.Impact.Regulatory}}

---
{{end}}

### 2.2 关键资源

{{range .Assessment.CriticalResources}}
#### {{.Name}}

- **资源类型:** {{.Type}}
- **高可用配置:**
{{if .HAConfig.Replicas}}  - 副本数: {{.HAConfig.Replicas}}{{end}}
{{if .HAConfig.BackupFrequency}}  - 备份频率: {{.HAConfig.BackupFrequency}}{{end}}
{{if .HAConfig.BackupRetention}}  - 备份保留: {{.HAConfig.BackupRetention}}{{end}}
{{if .HAConfig.Persistence}}  - 持久化: 启用{{end}}
{{if .HAConfig.ReplicationFactor}}  - 复制因子: {{.HAConfig.ReplicationFactor}}{{end}}
{{if .HAConfig.Replication}}  - 复制策略: {{.HAConfig.Replication}}{{end}}
{{if .HAConfig.Versioning}}  - 版本控制: 启用{{end}}
{{if .HAConfig.LifecyclePolicy}}  - 生命周期策略: 启用{{end}}

---
{{end}}

## 3. 恢复策略

{{range .Assessment.RecoveryStrategies}}
### {{.Scenario}}

- **恢复策略:** {{.Strategy}}
- **预计RTO:** {{.EstimatedRTO}}
- **自动化程度:** {{.AutomationLevel}}

---
{{end}}

## 4. 应急响应流程

### 4.1 事件分类

- **P1 - 严重:** 核心业务服务完全中断
- **P2 - 高:** 核心业务服务部分中断
- **P3 - 中:** 非核心业务服务中断
- **P4 - 低:** 性能下降或非关键功能异常

### 4.2 响应流程

1. **事件检测与报告**
   - 监控系统自动检测
   - 用户报告
   - 定期巡检发现

2. **事件评估与分类**
   - 确定影响范围
   - 评估业务影响
   - 分配优先级

3. **应急响应**
   - 启动应急预案
   - 通知相关人员
   - 执行恢复操作

4. **恢复验证**
   - 验证服务恢复
   - 检查数据完整性
   - 确认业务正常

5. **事后分析**
   - 根因分析
   - 改进措施
   - 更新文档

### 4.3 联系人列表

| 角色 | 姓名 | 电话 | 邮箱 | 备注 |
|------|------|------|------|------|
| 应急总指挥 | [待填写] | [待填写] | [待填写] | 负责整体协调 |
| 技术负责人 | [待填写] | [待填写] | [待填写] | 负责技术决策 |
| 运维负责人 | [待填写] | [待填写] | [待填写] | 负责执行恢复 |
| 业务负责人 | [待填写] | [待填写] | [待填写] | 负责业务协调 |
| 安全负责人 | [待填写] | [待填写] | [待填写] | 负责安全评估 |

## 5. 测试与演练

### 5.1 测试计划

- **桌面演练:** 每季度一次
- **功能测试:** 每月一次
- **全面演练:** 每半年一次

### 5.2 演练场景

1. 单节点故障恢复
2. 多节点故障恢复
3. 区域故障切换
4. 数据恢复演练
5. 完整灾难恢复

## 6. 持续改进

### 6.1 审查周期

- **月度审查:** 检查监控数据和事件记录
- **季度审查:** 评估恢复能力和测试结果
- **年度审查:** 全面评估BCP有效性

### 6.2 更新触发条件

- 业务流程变更
- 技术架构调整
- 组织结构变化
- 演练发现问题
- 实际事件经验

## 7. 附录

### 7.1 术语表

- **RTO (Recovery Time Objective):** 恢复时间目标
- **RPO (Recovery Point Objective):** 恢复点目标
- **BCP (Business Continuity Plan):** 业务连续性计划
- **DR (Disaster Recovery):** 灾难恢复
- **HA (High Availability):** 高可用

### 7.2 参考文档

- Kubernetes高可用架构设计
- 备份和恢复操作手册
- 监控告警配置文档
- 应急响应操作手册

---

**文档所有者:** [待填写]
**审批人:** [待填写]
**下次审查日期:** {{.NextReviewDate}}
`
    
    t, err := template.New("bcp").Parse(tmpl)
    if err != nil {
        return fmt.Errorf("解析模板失败: %v", err)
    }
    
    f, err := os.Create(outputFile)
    if err != nil {
        return fmt.Errorf("创建输出文件失败: %v", err)
    }
    defer f.Close()
    
    data := struct {
        Assessment     BIAAssessment
        GeneratedAt    string
        ValidUntil     string
        NextReviewDate string
    }{
        Assessment:     g.Assessment,
        GeneratedAt:    time.Now().Format("2006-01-02 15:04:05"),
        ValidUntil:     time.Now().AddDate(1, 0, 0).Format("2006-01-02"),
        NextReviewDate: time.Now().AddDate(0, 3, 0).Format("2006-01-02"),
    }
    
    if err := t.Execute(f, data); err != nil {
        return fmt.Errorf("生成文档失败: %v", err)
    }
    
    return nil
}

// GenerateRunbook 生成运维手册
func (g *BCPGenerator) GenerateRunbook(outputFile string) error {
    tmpl := `
# 业务连续性运维手册

## 快速参考

### 服务恢复优先级

{{range .Assessment.BusinessServices}}
**P{{.RecoveryPriority}} - {{.Name}}**
- RTO: {{.RTO}}
- RPO: {{.RPO}}
- 关键程度: {{.Criticality}}

{{end}}

## 恢复操作手册

{{range .Assessment.RecoveryStrategies}}
### {{.Scenario}}

**恢复策略:** {{.Strategy}}
**预计RTO:** {{.EstimatedRTO}}
**自动化程度:** {{.AutomationLevel}}

#### 操作步骤

1. 评估故障范围
   \`\`\`bash
   # 检查集群状态
   kubectl get nodes
   kubectl get pods --all-namespaces
   kubectl top nodes
   \`\`\`

2. 确认影响的服务
   \`\`\`bash
   # 检查服务状态
   kubectl get svc --all-namespaces
   kubectl get ingress --all-namespaces
   \`\`\`

3. 执行恢复操作
   \`\`\`bash
   # 根据具体场景执行相应的恢复命令
   # [待补充具体命令]
   \`\`\`

4. 验证恢复结果
   \`\`\`bash
   # 验证服务可用性
   # [待补充验证命令]
   \`\`\`

---
{{end}}

## 常见问题处理

### Pod无法启动

**症状:** Pod处于Pending或CrashLoopBackOff状态

**排查步骤:**
\`\`\`bash
# 查看Pod详情
kubectl describe pod <pod-name> -n <namespace>

# 查看Pod日志
kubectl logs <pod-name> -n <namespace>

# 查看事件
kubectl get events -n <namespace> --sort-by='.lastTimestamp'
\`\`\`

**常见原因:**
- 资源不足
- 镜像拉取失败
- 配置错误
- 存储挂载失败

### 服务不可访问

**症状:** 无法访问服务端点

**排查步骤:**
\`\`\`bash
# 检查Service
kubectl get svc <service-name> -n <namespace>

# 检查Endpoints
kubectl get endpoints <service-name> -n <namespace>

# 检查Ingress
kubectl get ingress -n <namespace>

# 测试Pod连接
kubectl run test-pod --image=curlimages/curl:latest --rm -i --restart=Never -- curl <service-url>
\`\`\`

### 存储问题

**症状:** PVC无法绑定或Pod无法挂载卷

**排查步骤:**
\`\`\`bash
# 检查PVC状态
kubectl get pvc -n <namespace>

# 检查PV状态
kubectl get pv

# 查看StorageClass
kubectl get storageclass

# 查看PVC详情
kubectl describe pvc <pvc-name> -n <namespace>
\`\`\`

## 联系方式

- **应急热线:** [待填写]
- **技术支持:** [待填写]
- **供应商支持:** [待填写]

---

**最后更新:** {{.GeneratedAt}}
`
    
    t, err := template.New("runbook").Parse(tmpl)
    if err != nil {
        return fmt.Errorf("解析模板失败: %v", err)
    }
    
    f, err := os.Create(outputFile)
    if err != nil {
        return fmt.Errorf("创建输出文件失败: %v", err)
    }
    defer f.Close()
    
    data := struct {
        Assessment  BIAAssessment
        GeneratedAt string
    }{
        Assessment:  g.Assessment,
        GeneratedAt: time.Now().Format("2006-01-02 15:04:05"),
    }
    
    if err := t.Execute(f, data); err != nil {
        return fmt.Errorf("生成文档失败: %v", err)
    }
    
    return nil
}

func main() {
    if len(os.Args) < 2 {
        fmt.Println("用法: bcp-generator <assessment-file>")
        os.Exit(1)
    }
    
    generator, err := NewBCPGenerator(os.Args[1])
    if err != nil {
        fmt.Printf("创建生成器失败: %v
", err)
        os.Exit(1)
    }
    
    // 生成BCP文档
    if err := generator.GenerateBCPDocument("business-continuity-plan.md"); err != nil {
        fmt.Printf("生成BCP文档失败: %v
", err)
        os.Exit(1)
    }
    fmt.Println("BCP文档已生成: business-continuity-plan.md")
    
    // 生成运维手册
    if err := generator.GenerateRunbook("operations-runbook.md"); err != nil {
        fmt.Printf("生成运维手册失败: %v
", err)
        os.Exit(1)
    }
    fmt.Println("运维手册已生成: operations-runbook.md")
}
```



#### 2. 灾难演练与测试

**演练计划管理**

```yaml
# drill-schedule.yaml - 演练计划配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: drill-schedule
  namespace: business-continuity
data:
  schedule.yaml: |
    # 年度演练计划
    annual_drills:
      # Q1演练
      - quarter: "Q1"
        drills:
          - name: "单节点故障恢复演练"
            type: "functional"
            date: "2024-01-15"
            duration: "2h"
            participants:
              - "运维团队"
              - "开发团队"
            objectives:
              - "验证节点故障自动恢复能力"
              - "测试Pod重新调度机制"
              - "评估服务中断时间"
            
          - name: "数据库备份恢复演练"
            type: "functional"
            date: "2024-02-20"
            duration: "4h"
            participants:
              - "DBA团队"
              - "运维团队"
            objectives:
              - "验证数据库备份完整性"
              - "测试数据恢复流程"
              - "评估恢复时间"
            
          - name: "桌面推演"
            type: "tabletop"
            date: "2024-03-25"
            duration: "3h"
            participants:
              - "管理层"
              - "技术团队"
              - "业务团队"
            objectives:
              - "评估应急响应流程"
              - "识别潜在问题"
              - "更新应急预案"
      
      # Q2演练
      - quarter: "Q2"
        drills:
          - name: "区域故障切换演练"
            type: "comprehensive"
            date: "2024-04-15"
            duration: "6h"
            participants:
              - "全体技术团队"
              - "业务团队"
            objectives:
              - "验证跨区域故障转移"
              - "测试DNS切换机制"
              - "评估业务连续性"
            
          - name: "网络分区演练"
            type: "functional"
            date: "2024-05-20"
            duration: "3h"
            participants:
              - "网络团队"
              - "运维团队"
            objectives:
              - "测试网络分区场景"
              - "验证服务降级机制"
              - "评估数据一致性"
            
          - name: "桌面推演"
            type: "tabletop"
            date: "2024-06-25"
            duration: "3h"
            participants:
              - "管理层"
              - "技术团队"
            objectives:
              - "回顾Q1-Q2演练结果"
              - "更新恢复策略"
      
      # Q3演练
      - quarter: "Q3"
        drills:
          - name: "完整灾难恢复演练"
            type: "full-scale"
            date: "2024-07-15"
            duration: "8h"
            participants:
              - "全体员工"
            objectives:
              - "模拟数据中心完全故障"
              - "验证灾备中心切换"
              - "测试完整恢复流程"
              - "评估业务影响"
            
          - name: "应用故障注入演练"
            type: "functional"
            date: "2024-08-20"
            duration: "4h"
            participants:
              - "开发团队"
              - "运维团队"
            objectives:
              - "使用Chaos Mesh注入故障"
              - "验证应用容错能力"
              - "测试监控告警"
            
          - name: "桌面推演"
            type: "tabletop"
            date: "2024-09-25"
            duration: "3h"
            participants:
              - "管理层"
              - "技术团队"
            objectives:
              - "评估年度演练效果"
              - "规划改进措施"
      
      # Q4演练
      - quarter: "Q4"
        drills:
          - name: "存储故障恢复演练"
            type: "functional"
            date: "2024-10-15"
            duration: "4h"
            participants:
              - "存储团队"
              - "运维团队"
            objectives:
              - "测试存储故障场景"
              - "验证数据恢复能力"
              - "评估性能影响"
            
          - name: "安全事件响应演练"
            type: "functional"
            date: "2024-11-20"
            duration: "4h"
            participants:
              - "安全团队"
              - "运维团队"
            objectives:
              - "模拟安全攻击场景"
              - "测试应急响应流程"
              - "验证隔离和恢复能力"
            
          - name: "年度总结与规划"
            type: "tabletop"
            date: "2024-12-20"
            duration: "4h"
            participants:
              - "全体管理层"
              - "技术负责人"
            objectives:
              - "总结年度演练成果"
              - "制定下年度计划"
              - "更新BCP文档"
```

**自动化演练执行工具**

```go
// drill-executor.go
package main

import (
    "context"
    "fmt"
    "log"
    "time"
    
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/rest"
)

// DrillExecutor 演练执行器
type DrillExecutor struct {
    clientset *kubernetes.Clientset
    namespace string
    drillName string
    startTime time.Time
    results   []DrillResult
}

// DrillResult 演练结果
type DrillResult struct {
    Step      string
    Status    string
    Duration  time.Duration
    Message   string
    Timestamp time.Time
}

// NewDrillExecutor 创建演练执行器
func NewDrillExecutor(namespace, drillName string) (*DrillExecutor, error) {
    config, err := rest.InClusterConfig()
    if err != nil {
        return nil, err
    }
    
    clientset, err := kubernetes.NewForConfig(config)
    if err != nil {
        return nil, err
    }
    
    return &DrillExecutor{
        clientset: clientset,
        namespace: namespace,
        drillName: drillName,
        results:   make([]DrillResult, 0),
    }, nil
}

// ExecuteNodeFailureDrill 执行节点故障演练
func (de *DrillExecutor) ExecuteNodeFailureDrill(ctx context.Context, nodeName string) error {
    de.startTime = time.Now()
    log.Printf("开始节点故障演练: %s", nodeName)
    
    // 步骤1: 记录初始状态
    if err := de.recordInitialState(ctx, nodeName); err != nil {
        return err
    }
    
    // 步骤2: 标记节点为不可调度
    if err := de.cordonNode(ctx, nodeName); err != nil {
        return err
    }
    
    // 步骤3: 驱逐节点上的Pod
    if err := de.drainNode(ctx, nodeName); err != nil {
        return err
    }
    
    // 步骤4: 等待Pod重新调度
    if err := de.waitForPodRescheduling(ctx, nodeName); err != nil {
        return err
    }
    
    // 步骤5: 验证服务可用性
    if err := de.verifyServiceAvailability(ctx); err != nil {
        return err
    }
    
    // 步骤6: 恢复节点
    if err := de.uncordonNode(ctx, nodeName); err != nil {
        return err
    }
    
    // 步骤7: 生成报告
    if err := de.generateReport(); err != nil {
        return err
    }
    
    log.Printf("节点故障演练完成,总耗时: %v", time.Since(de.startTime))
    return nil
}

// recordInitialState 记录初始状态
func (de *DrillExecutor) recordInitialState(ctx context.Context, nodeName string) error {
    stepStart := time.Now()
    
    // 获取节点信息
    node, err := de.clientset.CoreV1().Nodes().Get(ctx, nodeName, metav1.GetOptions{})
    if err != nil {
        de.recordResult("记录初始状态", "失败", time.Since(stepStart), err.Error())
        return err
    }
    
    // 获取节点上的Pod列表
    pods, err := de.clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{
        FieldSelector: fmt.Sprintf("spec.nodeName=%s", nodeName),
    })
    if err != nil {
        de.recordResult("记录初始状态", "失败", time.Since(stepStart), err.Error())
        return err
    }
    
    message := fmt.Sprintf("节点: %s, Pod数量: %d, 状态: %v",
        node.Name, len(pods.Items), node.Status.Conditions)
    
    de.recordResult("记录初始状态", "成功", time.Since(stepStart), message)
    return nil
}

// cordonNode 标记节点为不可调度
func (de *DrillExecutor) cordonNode(ctx context.Context, nodeName string) error {
    stepStart := time.Now()
    
    node, err := de.clientset.CoreV1().Nodes().Get(ctx, nodeName, metav1.GetOptions{})
    if err != nil {
        de.recordResult("标记节点不可调度", "失败", time.Since(stepStart), err.Error())
        return err
    }
    
    node.Spec.Unschedulable = true
    _, err = de.clientset.CoreV1().Nodes().Update(ctx, node, metav1.UpdateOptions{})
    if err != nil {
        de.recordResult("标记节点不可调度", "失败", time.Since(stepStart), err.Error())
        return err
    }
    
    de.recordResult("标记节点不可调度", "成功", time.Since(stepStart),
        fmt.Sprintf("节点 %s 已标记为不可调度", nodeName))
    return nil
}

// drainNode 驱逐节点上的Pod
func (de *DrillExecutor) drainNode(ctx context.Context, nodeName string) error {
    stepStart := time.Now()
    
    pods, err := de.clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{
        FieldSelector: fmt.Sprintf("spec.nodeName=%s", nodeName),
    })
    if err != nil {
        de.recordResult("驱逐Pod", "失败", time.Since(stepStart), err.Error())
        return err
    }
    
    deletedCount := 0
    for _, pod := range pods.Items {
        // 跳过DaemonSet管理的Pod
        if pod.OwnerReferences != nil {
            for _, owner := range pod.OwnerReferences {
                if owner.Kind == "DaemonSet" {
                    continue
                }
            }
        }
        
        err := de.clientset.CoreV1().Pods(pod.Namespace).Delete(ctx, pod.Name, metav1.DeleteOptions{
            GracePeriodSeconds: new(int64),
        })
        if err != nil {
            log.Printf("删除Pod失败: %s/%s: %v", pod.Namespace, pod.Name, err)
            continue
        }
        deletedCount++
    }
    
    de.recordResult("驱逐Pod", "成功", time.Since(stepStart),
        fmt.Sprintf("已驱逐 %d 个Pod", deletedCount))
    return nil
}

// waitForPodRescheduling 等待Pod重新调度
func (de *DrillExecutor) waitForPodRescheduling(ctx context.Context, nodeName string) error {
    stepStart := time.Now()
    timeout := 5 * time.Minute
    
    ticker := time.NewTicker(10 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            de.recordResult("等待Pod重新调度", "失败", time.Since(stepStart), "上下文取消")
            return ctx.Err()
        case <-ticker.C:
            pods, err := de.clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{
                FieldSelector: fmt.Sprintf("spec.nodeName=%s", nodeName),
            })
            if err != nil {
                continue
            }
            
            // 检查是否还有非DaemonSet的Pod
            nonDaemonSetPods := 0
            for _, pod := range pods.Items {
                isDaemonSet := false
                if pod.OwnerReferences != nil {
                    for _, owner := range pod.OwnerReferences {
                        if owner.Kind == "DaemonSet" {
                            isDaemonSet = true
                            break
                        }
                    }
                }
                if !isDaemonSet {
                    nonDaemonSetPods++
                }
            }
            
            if nonDaemonSetPods == 0 {
                de.recordResult("等待Pod重新调度", "成功", time.Since(stepStart),
                    "所有Pod已重新调度")
                return nil
            }
            
            if time.Since(stepStart) > timeout {
                de.recordResult("等待Pod重新调度", "超时", time.Since(stepStart),
                    fmt.Sprintf("仍有 %d 个Pod未重新调度", nonDaemonSetPods))
                return fmt.Errorf("等待超时")
            }
        }
    }
}

// verifyServiceAvailability 验证服务可用性
func (de *DrillExecutor) verifyServiceAvailability(ctx context.Context) error {
    stepStart := time.Now()
    
    // 获取所有Service
    services, err := de.clientset.CoreV1().Services("").List(ctx, metav1.ListOptions{})
    if err != nil {
        de.recordResult("验证服务可用性", "失败", time.Since(stepStart), err.Error())
        return err
    }
    
    unavailableServices := 0
    for _, svc := range services.Items {
        // 获取Service的Endpoints
        endpoints, err := de.clientset.CoreV1().Endpoints(svc.Namespace).Get(ctx, svc.Name, metav1.GetOptions{})
        if err != nil {
            continue
        }
        
        // 检查是否有可用的Endpoint
        hasEndpoints := false
        for _, subset := range endpoints.Subsets {
            if len(subset.Addresses) > 0 {
                hasEndpoints = true
                break
            }
        }
        
        if !hasEndpoints {
            unavailableServices++
            log.Printf("服务不可用: %s/%s", svc.Namespace, svc.Name)
        }
    }
    
    if unavailableServices > 0 {
        de.recordResult("验证服务可用性", "警告", time.Since(stepStart),
            fmt.Sprintf("%d 个服务不可用", unavailableServices))
    } else {
        de.recordResult("验证服务可用性", "成功", time.Since(stepStart), "所有服务可用")
    }
    
    return nil
}

// uncordonNode 恢复节点可调度状态
func (de *DrillExecutor) uncordonNode(ctx context.Context, nodeName string) error {
    stepStart := time.Now()
    
    node, err := de.clientset.CoreV1().Nodes().Get(ctx, nodeName, metav1.GetOptions{})
    if err != nil {
        de.recordResult("恢复节点可调度", "失败", time.Since(stepStart), err.Error())
        return err
    }
    
    node.Spec.Unschedulable = false
    _, err = de.clientset.CoreV1().Nodes().Update(ctx, node, metav1.UpdateOptions{})
    if err != nil {
        de.recordResult("恢复节点可调度", "失败", time.Since(stepStart), err.Error())
        return err
    }
    
    de.recordResult("恢复节点可调度", "成功", time.Since(stepStart),
        fmt.Sprintf("节点 %s 已恢复可调度", nodeName))
    return nil
}

// recordResult 记录结果
func (de *DrillExecutor) recordResult(step, status string, duration time.Duration, message string) {
    result := DrillResult{
        Step:      step,
        Status:    status,
        Duration:  duration,
        Message:   message,
        Timestamp: time.Now(),
    }
    de.results = append(de.results, result)
    log.Printf("[%s] %s - %s (耗时: %v)", status, step, message, duration)
}

// generateReport 生成报告
func (de *DrillExecutor) generateReport() error {
    fmt.Println("\n========================================")
    fmt.Printf("演练报告: %s\n", de.drillName)
    fmt.Println("========================================")
    fmt.Printf("开始时间: %s\n", de.startTime.Format("2006-01-02 15:04:05"))
    fmt.Printf("结束时间: %s\n", time.Now().Format("2006-01-02 15:04:05"))
    fmt.Printf("总耗时: %v\n", time.Since(de.startTime))
    fmt.Println("\n详细步骤:")
    fmt.Println("----------------------------------------")
    
    for i, result := range de.results {
        fmt.Printf("%d. %s\n", i+1, result.Step)
        fmt.Printf("   状态: %s\n", result.Status)
        fmt.Printf("   耗时: %v\n", result.Duration)
        fmt.Printf("   说明: %s\n", result.Message)
        fmt.Printf("   时间: %s\n", result.Timestamp.Format("2006-01-02 15:04:05"))
        fmt.Println()
    }
    
    // 统计
    successCount := 0
    failureCount := 0
    warningCount := 0
    
    for _, result := range de.results {
        switch result.Status {
        case "成功":
            successCount++
        case "失败":
            failureCount++
        case "警告":
            warningCount++
        }
    }
    
    fmt.Println("========================================")
    fmt.Println("统计信息:")
    fmt.Printf("- 总步骤数: %d\n", len(de.results))
    fmt.Printf("- 成功: %d\n", successCount)
    fmt.Printf("- 失败: %d\n", failureCount)
    fmt.Printf("- 警告: %d\n", warningCount)
    fmt.Println("========================================")
    
    return nil
}

func main() {
    executor, err := NewDrillExecutor("default", "节点故障演练")
    if err != nil {
        log.Fatalf("创建执行器失败: %v", err)
    }
    
    ctx := context.Background()
    
    // 执行节点故障演练
    if err := executor.ExecuteNodeFailureDrill(ctx, "worker-node-1"); err != nil {
        log.Fatalf("演练执行失败: %v", err)
    }
}
```



#### 3. 事件响应与管理

**事件响应流程自动化**

```yaml
# incident-response-workflow.yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: incident-response
  namespace: business-continuity
spec:
  entrypoint: incident-response-flow
  arguments:
    parameters:
    - name: incident-id
      value: "INC-2024-001"
    - name: severity
      value: "P1"
    - name: affected-service
      value: "web-app"
  
  templates:
  # 主流程
  - name: incident-response-flow
    steps:
    # 阶段1: 检测与报告
    - - name: detect-incident
        template: detect
    
    # 阶段2: 评估与分类
    - - name: assess-impact
        template: assess
      - name: classify-severity
        template: classify
    
    # 阶段3: 通知相关人员
    - - name: notify-oncall
        template: notify
        arguments:
          parameters:
          - name: severity
            value: "{{workflow.parameters.severity}}"
    
    # 阶段4: 应急响应
    - - name: execute-response
        template: response
        arguments:
          parameters:
          - name: incident-id
            value: "{{workflow.parameters.incident-id}}"
          - name: affected-service
            value: "{{workflow.parameters.affected-service}}"
    
    # 阶段5: 恢复验证
    - - name: verify-recovery
        template: verify
    
    # 阶段6: 事后分析
    - - name: post-incident-review
        template: review
        arguments:
          parameters:
          - name: incident-id
            value: "{{workflow.parameters.incident-id}}"
  
  # 检测模板
  - name: detect
    container:
      image: curlimages/curl:latest
      command: [sh, -c]
      args:
      - |
        echo "检测事件..."
        # 检查服务健康状态
        # 收集初始诊断信息
        echo "事件已检测"
  
  # 评估模板
  - name: assess
    container:
      image: bitnami/kubectl:latest
      command: [sh, -c]
      args:
      - |
        echo "评估影响范围..."
        kubectl get pods -l app={{workflow.parameters.affected-service}}
        kubectl get svc {{workflow.parameters.affected-service}}
        kubectl top pods -l app={{workflow.parameters.affected-service}}
  
  # 分类模板
  - name: classify
    container:
      image: alpine:latest
      command: [sh, -c]
      args:
      - |
        echo "分类事件严重程度: {{workflow.parameters.severity}}"
        case "{{workflow.parameters.severity}}" in
          P1) echo "严重事件 - 核心业务完全中断" ;;
          P2) echo "高优先级 - 核心业务部分中断" ;;
          P3) echo "中优先级 - 非核心业务中断" ;;
          P4) echo "低优先级 - 性能下降" ;;
        esac
  
  # 通知模板
  - name: notify
    inputs:
      parameters:
      - name: severity
    container:
      image: curlimages/curl:latest
      command: [sh, -c]
      args:
      - |
        echo "通知值班人员..."
        # 发送告警到Slack/钉钉/企业微信
        # curl -X POST webhook-url -d "..."
        echo "通知已发送"
  
  # 响应模板
  - name: response
    inputs:
      parameters:
      - name: incident-id
      - name: affected-service
    container:
      image: bitnami/kubectl:latest
      command: [sh, -c]
      args:
      - |
        echo "执行应急响应..."
        
        # 检查Pod状态
        kubectl get pods -l app={{inputs.parameters.affected-service}}
        
        # 尝试重启失败的Pod
        kubectl delete pods -l app={{inputs.parameters.affected-service}} \
          --field-selector=status.phase=Failed
        
        # 检查是否需要扩容
        CURRENT_REPLICAS=$(kubectl get deployment {{inputs.parameters.affected-service}} \
          -o jsonpath='{.spec.replicas}')
        echo "当前副本数: $CURRENT_REPLICAS"
        
        # 如果副本数不足,进行扩容
        if [ "$CURRENT_REPLICAS" -lt 3 ]; then
          kubectl scale deployment {{inputs.parameters.affected-service}} --replicas=3
        fi
        
        echo "应急响应完成"
  
  # 验证模板
  - name: verify
    container:
      image: bitnami/kubectl:latest
      command: [sh, -c]
      args:
      - |
        echo "验证恢复状态..."
        
        # 等待Pod就绪
        kubectl wait --for=condition=ready \
          --timeout=300s \
          pods -l app={{workflow.parameters.affected-service}}
        
        # 检查服务可用性
        kubectl get svc {{workflow.parameters.affected-service}}
        
        # 验证Endpoints
        kubectl get endpoints {{workflow.parameters.affected-service}}
        
        echo "恢复验证完成"
  
  # 事后分析模板
  - name: review
    inputs:
      parameters:
      - name: incident-id
    container:
      image: alpine:latest
      command: [sh, -c]
      args:
      - |
        echo "开始事后分析..."
        echo "事件ID: {{inputs.parameters.incident-id}}"
        
        # 收集事件数据
        # 生成事后分析报告
        # 识别改进措施
        
        cat > /tmp/post-incident-report.md <<EOF
        # 事后分析报告
        
        **事件ID:** {{inputs.parameters.incident-id}}
        **发生时间:** $(date)
        **影响服务:** {{workflow.parameters.affected-service}}
        **严重程度:** {{workflow.parameters.severity}}
        
        ## 事件时间线
        
        - 检测时间: [自动填充]
        - 响应时间: [自动填充]
        - 恢复时间: [自动填充]
        
        ## 根因分析
        
        [待补充]
        
        ## 改进措施
        
        [待补充]
        
        ## 经验教训
        
        [待补充]
        EOF
        
        cat /tmp/post-incident-report.md
        echo "事后分析完成"
```

**事件管理系统集成**

```go
// incident-manager.go
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"
    "time"
    
    "github.com/slack-go/slack"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
)

// Incident 事件
type Incident struct {
    ID              string
    Title           string
    Description     string
    Severity        string
    Status          string
    AffectedService string
    DetectedAt      time.Time
    ResolvedAt      *time.Time
    Assignee        string
    Timeline        []TimelineEvent
    Metrics         IncidentMetrics
}

// TimelineEvent 时间线事件
type TimelineEvent struct {
    Timestamp   time.Time
    EventType   string
    Description string
    Actor       string
}

// IncidentMetrics 事件指标
type IncidentMetrics struct {
    TimeToDetect  time.Duration
    TimeToRespond time.Duration
    TimeToResolve time.Duration
    MTTR          time.Duration
}

// IncidentManager 事件管理器
type IncidentManager struct {
    k8sClient   *kubernetes.Clientset
    slackClient *slack.Client
    incidents   map[string]*Incident
}

// NewIncidentManager 创建事件管理器
func NewIncidentManager(k8sClient *kubernetes.Clientset, slackToken string) *IncidentManager {
    return &IncidentManager{
        k8sClient:   k8sClient,
        slackClient: slack.New(slackToken),
        incidents:   make(map[string]*Incident),
    }
}

// CreateIncident 创建事件
func (im *IncidentManager) CreateIncident(ctx context.Context, incident *Incident) error {
    incident.ID = fmt.Sprintf("INC-%s-%d", time.Now().Format("20060102"), len(im.incidents)+1)
    incident.DetectedAt = time.Now()
    incident.Status = "detected"
    
    // 添加检测事件到时间线
    incident.Timeline = append(incident.Timeline, TimelineEvent{
        Timestamp:   time.Now(),
        EventType:   "detected",
        Description: "事件已检测",
        Actor:       "system",
    })
    
    im.incidents[incident.ID] = incident
    
    // 发送通知
    if err := im.notifyIncident(incident); err != nil {
        log.Printf("发送通知失败: %v", err)
    }
    
    // 自动响应
    if err := im.autoRespond(ctx, incident); err != nil {
        log.Printf("自动响应失败: %v", err)
    }
    
    log.Printf("事件已创建: %s - %s", incident.ID, incident.Title)
    return nil
}

// UpdateIncident 更新事件
func (im *IncidentManager) UpdateIncident(incidentID string, status string, description string) error {
    incident, exists := im.incidents[incidentID]
    if !exists {
        return fmt.Errorf("事件不存在: %s", incidentID)
    }
    
    incident.Status = status
    
    // 添加更新事件到时间线
    incident.Timeline = append(incident.Timeline, TimelineEvent{
        Timestamp:   time.Now(),
        EventType:   status,
        Description: description,
        Actor:       "operator",
    })
    
    // 如果事件已解决,记录解决时间
    if status == "resolved" {
        now := time.Now()
        incident.ResolvedAt = &now
        
        // 计算指标
        incident.Metrics.TimeToDetect = incident.DetectedAt.Sub(incident.DetectedAt)
        incident.Metrics.TimeToResolve = now.Sub(incident.DetectedAt)
        incident.Metrics.MTTR = incident.Metrics.TimeToResolve
        
        // 发送解决通知
        if err := im.notifyResolution(incident); err != nil {
            log.Printf("发送解决通知失败: %v", err)
        }
    }
    
    log.Printf("事件已更新: %s - %s", incidentID, status)
    return nil
}

// notifyIncident 通知事件
func (im *IncidentManager) notifyIncident(incident *Incident) error {
    // 构建Slack消息
    attachment := slack.Attachment{
        Color:      im.getSeverityColor(incident.Severity),
        Title:      fmt.Sprintf("🚨 新事件: %s", incident.Title),
        Text:       incident.Description,
        Fields: []slack.AttachmentField{
            {
                Title: "事件ID",
                Value: incident.ID,
                Short: true,
            },
            {
                Title: "严重程度",
                Value: incident.Severity,
                Short: true,
            },
            {
                Title: "影响服务",
                Value: incident.AffectedService,
                Short: true,
            },
            {
                Title: "检测时间",
                Value: incident.DetectedAt.Format("2006-01-02 15:04:05"),
                Short: true,
            },
        },
        Footer: "Kubernetes Incident Manager",
        Ts:     json.Number(fmt.Sprintf("%d", incident.DetectedAt.Unix())),
    }
    
    _, _, err := im.slackClient.PostMessage(
        "#incidents",
        slack.MsgOptionAttachments(attachment),
    )
    
    return err
}

// notifyResolution 通知解决
func (im *IncidentManager) notifyResolution(incident *Incident) error {
    attachment := slack.Attachment{
        Color: "good",
        Title: fmt.Sprintf("✅ 事件已解决: %s", incident.Title),
        Fields: []slack.AttachmentField{
            {
                Title: "事件ID",
                Value: incident.ID,
                Short: true,
            },
            {
                Title: "解决时间",
                Value: incident.ResolvedAt.Format("2006-01-02 15:04:05"),
                Short: true,
            },
            {
                Title: "总耗时",
                Value: incident.Metrics.TimeToResolve.String(),
                Short: true,
            },
        },
        Footer: "Kubernetes Incident Manager",
    }
    
    _, _, err := im.slackClient.PostMessage(
        "#incidents",
        slack.MsgOptionAttachments(attachment),
    )
    
    return err
}

// autoRespond 自动响应
func (im *IncidentManager) autoRespond(ctx context.Context, incident *Incident) error {
    log.Printf("执行自动响应: %s", incident.ID)
    
    // 根据严重程度执行不同的响应策略
    switch incident.Severity {
    case "P1":
        // 严重事件 - 立即扩容
        return im.scaleUp(ctx, incident.AffectedService, 5)
    case "P2":
        // 高优先级 - 重启失败的Pod
        return im.restartFailedPods(ctx, incident.AffectedService)
    case "P3":
        // 中优先级 - 记录日志
        log.Printf("中优先级事件,等待人工处理: %s", incident.ID)
    case "P4":
        // 低优先级 - 仅监控
        log.Printf("低优先级事件,持续监控: %s", incident.ID)
    }
    
    return nil
}

// scaleUp 扩容
func (im *IncidentManager) scaleUp(ctx context.Context, serviceName string, replicas int32) error {
    deployment, err := im.k8sClient.AppsV1().Deployments("default").Get(ctx, serviceName, metav1.GetOptions{})
    if err != nil {
        return err
    }
    
    deployment.Spec.Replicas = &replicas
    _, err = im.k8sClient.AppsV1().Deployments("default").Update(ctx, deployment, metav1.UpdateOptions{})
    if err != nil {
        return err
    }
    
    log.Printf("已扩容服务 %s 到 %d 个副本", serviceName, replicas)
    return nil
}

// restartFailedPods 重启失败的Pod
func (im *IncidentManager) restartFailedPods(ctx context.Context, serviceName string) error {
    pods, err := im.k8sClient.CoreV1().Pods("default").List(ctx, metav1.ListOptions{
        LabelSelector: fmt.Sprintf("app=%s", serviceName),
    })
    if err != nil {
        return err
    }
    
    for _, pod := range pods.Items {
        if pod.Status.Phase == "Failed" || pod.Status.Phase == "Unknown" {
            err := im.k8sClient.CoreV1().Pods("default").Delete(ctx, pod.Name, metav1.DeleteOptions{})
            if err != nil {
                log.Printf("删除Pod失败: %s: %v", pod.Name, err)
                continue
            }
            log.Printf("已删除失败的Pod: %s", pod.Name)
        }
    }
    
    return nil
}

// getSeverityColor 获取严重程度颜色
func (im *IncidentManager) getSeverityColor(severity string) string {
    switch severity {
    case "P1":
        return "danger"
    case "P2":
        return "warning"
    case "P3":
        return "#439FE0"
    case "P4":
        return "good"
    default:
        return "#808080"
    }
}

// GenerateReport 生成报告
func (im *IncidentManager) GenerateReport() string {
    report := "# 事件管理报告\n\n"
    report += fmt.Sprintf("**生成时间:** %s\n\n", time.Now().Format("2006-01-02 15:04:05"))
    
    // 统计
    totalIncidents := len(im.incidents)
    resolvedIncidents := 0
    var totalMTTR time.Duration
    
    severityCount := make(map[string]int)
    
    for _, incident := range im.incidents {
        severityCount[incident.Severity]++
        
        if incident.Status == "resolved" {
            resolvedIncidents++
            totalMTTR += incident.Metrics.MTTR
        }
    }
    
    report += "## 统计信息\n\n"
    report += fmt.Sprintf("- 总事件数: %d\n", totalIncidents)
    report += fmt.Sprintf("- 已解决: %d\n", resolvedIncidents)
    report += fmt.Sprintf("- 未解决: %d\n", totalIncidents-resolvedIncidents)
    
    if resolvedIncidents > 0 {
        avgMTTR := totalMTTR / time.Duration(resolvedIncidents)
        report += fmt.Sprintf("- 平均MTTR: %v\n", avgMTTR)
    }
    
    report += "\n## 按严重程度分类\n\n"
    for severity, count := range severityCount {
        report += fmt.Sprintf("- %s: %d\n", severity, count)
    }
    
    report += "\n## 事件列表\n\n"
    for _, incident := range im.incidents {
        report += fmt.Sprintf("### %s - %s\n\n", incident.ID, incident.Title)
        report += fmt.Sprintf("- **严重程度:** %s\n", incident.Severity)
        report += fmt.Sprintf("- **状态:** %s\n", incident.Status)
        report += fmt.Sprintf("- **影响服务:** %s\n", incident.AffectedService)
        report += fmt.Sprintf("- **检测时间:** %s\n", incident.DetectedAt.Format("2006-01-02 15:04:05"))
        
        if incident.ResolvedAt != nil {
            report += fmt.Sprintf("- **解决时间:** %s\n", incident.ResolvedAt.Format("2006-01-02 15:04:05"))
            report += fmt.Sprintf("- **耗时:** %v\n", incident.Metrics.TimeToResolve)
        }
        
        report += "\n"
    }
    
    return report
}

func main() {
    // 示例使用
    manager := NewIncidentManager(nil, "slack-token")
    
    // 创建测试事件
    incident := &Incident{
        Title:           "Web应用响应缓慢",
        Description:     "用户报告Web应用响应时间超过5秒",
        Severity:        "P2",
        AffectedService: "web-app",
    }
    
    ctx := context.Background()
    if err := manager.CreateIncident(ctx, incident); err != nil {
        log.Fatalf("创建事件失败: %v", err)
    }
    
    // 模拟事件处理
    time.Sleep(5 * time.Minute)
    
    // 更新事件状态
    if err := manager.UpdateIncident(incident.ID, "investigating", "正在调查根因"); err != nil {
        log.Fatalf("更新事件失败: %v", err)
    }
    
    time.Sleep(10 * time.Minute)
    
    // 解决事件
    if err := manager.UpdateIncident(incident.ID, "resolved", "已通过扩容解决"); err != nil {
        log.Fatalf("更新事件失败: %v", err)
    }
    
    // 生成报告
    report := manager.GenerateReport()
    fmt.Println(report)
}
```



#### 4. 持续改进机制

**改进措施跟踪系统**

```yaml
# improvement-tracking.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: improvement-tracking
  namespace: business-continuity
data:
  improvements.yaml: |
    # 改进措施跟踪
    improvements:
      # 来自事件INC-2024-001的改进措施
      - id: "IMP-2024-001"
        source: "INC-2024-001"
        title: "增加数据库连接池大小"
        description: "数据库连接池耗尽导致服务不可用"
        category: "configuration"
        priority: "high"
        status: "completed"
        assignee: "DBA团队"
        created_at: "2024-01-15"
        completed_at: "2024-01-20"
        verification:
          method: "load-test"
          result: "通过"
          notes: "连接池增加到500后,负载测试通过"
      
      - id: "IMP-2024-002"
        source: "INC-2024-001"
        title: "实施数据库连接监控"
        description: "缺少数据库连接数监控,无法提前发现问题"
        category: "monitoring"
        priority: "high"
        status: "completed"
        assignee: "运维团队"
        created_at: "2024-01-15"
        completed_at: "2024-01-22"
        verification:
          method: "alert-test"
          result: "通过"
          notes: "已配置告警,连接数超过80%时触发"
      
      - id: "IMP-2024-003"
        source: "INC-2024-002"
        title: "优化Pod反亲和性配置"
        description: "多个Pod调度到同一节点,节点故障导致服务完全中断"
        category: "architecture"
        priority: "critical"
        status: "in-progress"
        assignee: "架构团队"
        created_at: "2024-02-01"
        estimated_completion: "2024-02-15"
        progress: 60
        blockers:
          - "需要评估对现有服务的影响"
          - "需要协调变更窗口"
      
      - id: "IMP-2024-004"
        source: "DRILL-2024-Q1"
        title: "自动化故障转移流程"
        description: "演练发现故障转移需要过多手动操作"
        category: "automation"
        priority: "medium"
        status: "planned"
        assignee: "自动化团队"
        created_at: "2024-03-01"
        estimated_start: "2024-03-15"
        dependencies:
          - "IMP-2024-003"
      
      - id: "IMP-2024-005"
        source: "REVIEW-2024-Q1"
        title: "更新应急联系人列表"
        description: "季度审查发现联系人信息过期"
        category: "documentation"
        priority: "low"
        status: "completed"
        assignee: "行政团队"
        created_at: "2024-03-25"
        completed_at: "2024-03-26"
    
    # 改进指标
    metrics:
      total_improvements: 5
      completed: 3
      in_progress: 1
      planned: 1
      completion_rate: 60
      
      by_category:
        configuration: 1
        monitoring: 1
        architecture: 1
        automation: 1
        documentation: 1
      
      by_priority:
        critical: 1
        high: 2
        medium: 1
        low: 1
      
      average_completion_time: "5.7 days"
```

**持续改进仪表盘**

```go
// improvement-dashboard.go
package main

import (
    "fmt"
    "html/template"
    "net/http"
    "time"
    
    "gopkg.in/yaml.v3"
)

// Improvement 改进措施
type Improvement struct {
    ID                  string    `yaml:"id"`
    Source              string    `yaml:"source"`
    Title               string    `yaml:"title"`
    Description         string    `yaml:"description"`
    Category            string    `yaml:"category"`
    Priority            string    `yaml:"priority"`
    Status              string    `yaml:"status"`
    Assignee            string    `yaml:"assignee"`
    CreatedAt           string    `yaml:"created_at"`
    CompletedAt         string    `yaml:"completed_at,omitempty"`
    EstimatedCompletion string    `yaml:"estimated_completion,omitempty"`
    EstimatedStart      string    `yaml:"estimated_start,omitempty"`
    Progress            int       `yaml:"progress,omitempty"`
    Blockers            []string  `yaml:"blockers,omitempty"`
    Dependencies        []string  `yaml:"dependencies,omitempty"`
    Verification        *Verification `yaml:"verification,omitempty"`
}

// Verification 验证信息
type Verification struct {
    Method string `yaml:"method"`
    Result string `yaml:"result"`
    Notes  string `yaml:"notes"`
}

// ImprovementMetrics 改进指标
type ImprovementMetrics struct {
    TotalImprovements     int               `yaml:"total_improvements"`
    Completed             int               `yaml:"completed"`
    InProgress            int               `yaml:"in_progress"`
    Planned               int               `yaml:"planned"`
    CompletionRate        int               `yaml:"completion_rate"`
    ByCategory            map[string]int    `yaml:"by_category"`
    ByPriority            map[string]int    `yaml:"by_priority"`
    AverageCompletionTime string            `yaml:"average_completion_time"`
}

// ImprovementData 改进数据
type ImprovementData struct {
    Improvements []Improvement      `yaml:"improvements"`
    Metrics      ImprovementMetrics `yaml:"metrics"`
}

// DashboardServer 仪表盘服务器
type DashboardServer struct {
    data *ImprovementData
}

// NewDashboardServer 创建仪表盘服务器
func NewDashboardServer(dataFile string) (*DashboardServer, error) {
    // 加载数据
    data := &ImprovementData{}
    // 实际应用中从文件或数据库加载
    
    return &DashboardServer{
        data: data,
    }, nil
}

// ServeHTTP 处理HTTP请求
func (ds *DashboardServer) ServeHTTP(w http.ResponseWriter, r *http.Request) {
    tmpl := `
<!DOCTYPE html>
<html>
<head>
    <title>持续改进仪表盘</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            border-bottom: 2px solid #4CAF50;
            padding-bottom: 10px;
        }
        .metrics {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        .metric-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }
        .metric-value {
            font-size: 36px;
            font-weight: bold;
            margin: 10px 0;
        }
        .metric-label {
            font-size: 14px;
            opacity: 0.9;
        }
        .improvements {
            margin-top: 30px;
        }
        .improvement-card {
            background: white;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
            transition: box-shadow 0.3s;
        }
        .improvement-card:hover {
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        .improvement-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 10px;
        }
        .improvement-title {
            font-size: 18px;
            font-weight: bold;
            color: #333;
        }
        .status-badge {
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 12px;
            font-weight: bold;
        }
        .status-completed {
            background-color: #4CAF50;
            color: white;
        }
        .status-in-progress {
            background-color: #2196F3;
            color: white;
        }
        .status-planned {
            background-color: #FF9800;
            color: white;
        }
        .priority-critical {
            color: #f44336;
        }
        .priority-high {
            color: #ff9800;
        }
        .priority-medium {
            color: #2196F3;
        }
        .priority-low {
            color: #4CAF50;
        }
        .progress-bar {
            width: 100%;
            height: 20px;
            background-color: #e0e0e0;
            border-radius: 10px;
            overflow: hidden;
            margin: 10px 0;
        }
        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #4CAF50 0%, #8BC34A 100%);
            transition: width 0.3s;
        }
        .blockers {
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 10px;
            margin: 10px 0;
        }
        .verification {
            background-color: #d4edda;
            border-left: 4px solid #28a745;
            padding: 10px;
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>🎯 持续改进仪表盘</h1>
        
        <div class="metrics">
            <div class="metric-card">
                <div class="metric-label">总改进措施</div>
                <div class="metric-value">{{.Metrics.TotalImprovements}}</div>
            </div>
            <div class="metric-card">
                <div class="metric-label">已完成</div>
                <div class="metric-value">{{.Metrics.Completed}}</div>
            </div>
            <div class="metric-card">
                <div class="metric-label">进行中</div>
                <div class="metric-value">{{.Metrics.InProgress}}</div>
            </div>
            <div class="metric-card">
                <div class="metric-label">完成率</div>
                <div class="metric-value">{{.Metrics.CompletionRate}}%</div>
            </div>
        </div>
        
        <div class="improvements">
            <h2>改进措施列表</h2>
            {{range .Improvements}}
            <div class="improvement-card">
                <div class="improvement-header">
                    <div class="improvement-title">
                        {{.ID}} - {{.Title}}
                    </div>
                    <span class="status-badge status-{{.Status}}">{{.Status}}</span>
                </div>
                
                <p>{{.Description}}</p>
                
                <div style="display: flex; gap: 20px; margin: 10px 0;">
                    <div><strong>来源:</strong> {{.Source}}</div>
                    <div><strong>类别:</strong> {{.Category}}</div>
                    <div><strong>优先级:</strong> <span class="priority-{{.Priority}}">{{.Priority}}</span></div>
                    <div><strong>负责人:</strong> {{.Assignee}}</div>
                </div>
                
                {{if .Progress}}
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {{.Progress}}%"></div>
                </div>
                <div style="text-align: center; font-size: 12px;">进度: {{.Progress}}%</div>
                {{end}}
                
                {{if .Blockers}}
                <div class="blockers">
                    <strong>⚠️ 阻塞因素:</strong>
                    <ul>
                    {{range .Blockers}}
                        <li>{{.}}</li>
                    {{end}}
                    </ul>
                </div>
                {{end}}
                
                {{if .Verification}}
                <div class="verification">
                    <strong>✅ 验证信息:</strong>
                    <div>方法: {{.Verification.Method}}</div>
                    <div>结果: {{.Verification.Result}}</div>
                    <div>备注: {{.Verification.Notes}}</div>
                </div>
                {{end}}
            </div>
            {{end}}
        </div>
    </div>
</body>
</html>
`
    
    t, err := template.New("dashboard").Parse(tmpl)
    if err != nil {
        http.Error(w, err.Error(), http.StatusInternalServerError)
        return
    }
    
    if err := t.Execute(w, ds.data); err != nil {
        http.Error(w, err.Error(), http.StatusInternalServerError)
    }
}

func main() {
    server, err := NewDashboardServer("improvements.yaml")
    if err != nil {
        panic(err)
    }
    
    http.Handle("/", server)
    
    fmt.Println("持续改进仪表盘启动在 http://localhost:8080")
    if err := http.ListenAndServe(":8080", nil); err != nil {
        panic(err)
    }
}
```

#### 5. 业务连续性最佳实践总结

**组织与文化**

1. **建立业务连续性文化**
   - 高层管理支持和承诺
   - 全员参与和责任共担
   - 定期培训和意识提升
   - 奖励主动发现和解决问题
   - 鼓励从失败中学习

2. **明确角色和职责**
   - 业务连续性经理
   - 技术负责人
   - 应急响应团队
   - 业务联络人
   - 供应商协调人

3. **建立沟通机制**
   - 定期状态更新
   - 透明的事件通报
   - 有效的升级路径
   - 跨团队协作平台
   - 利益相关者沟通

**技术实践**

1. **架构设计**
   - 无单点故障设计
   - 多层次冗余
   - 故障隔离和限流
   - 优雅降级机制
   - 跨区域部署

2. **自动化**
   - 自动化部署和回滚
   - 自动化监控和告警
   - 自动化故障恢复
   - 自动化测试和验证
   - 自动化文档生成

3. **监控与可观测性**
   - 全面的指标收集
   - 集中式日志管理
   - 分布式追踪
   - 实时告警
   - 可视化仪表盘

4. **数据管理**
   - 定期备份
   - 备份验证
   - 多地域复制
   - 版本控制
   - 数据加密

**流程与管理**

1. **变更管理**
   - 变更审批流程
   - 变更窗口规划
   - 回滚计划
   - 变更影响评估
   - 变更后验证

2. **容量规划**
   - 定期容量评估
   - 增长趋势分析
   - 资源预留
   - 弹性扩展策略
   - 成本优化

3. **供应商管理**
   - SLA协议
   - 定期审查
   - 应急联系方式
   - 备用供应商
   - 依赖关系管理

4. **合规与审计**
   - 合规要求识别
   - 定期审计
   - 证据收集
   - 差距分析
   - 整改跟踪

**测试与演练**

1. **测试策略**
   - 单元测试
   - 集成测试
   - 端到端测试
   - 性能测试
   - 混沌工程

2. **演练类型**
   - 桌面推演
   - 功能测试
   - 全面演练
   - 突击演练
   - 供应商联合演练

3. **演练评估**
   - 目标达成度
   - 发现的问题
   - 改进建议
   - 经验教训
   - 后续行动

**持续改进**

1. **事后分析**
   - 无责文化
   - 根因分析
   - 时间线重建
   - 改进措施
   - 知识分享

2. **指标跟踪**
   - 可用性指标
   - 性能指标
   - 恢复指标
   - 成本指标
   - 改进指标

3. **知识管理**
   - 文档维护
   - 知识库建设
   - 最佳实践分享
   - 案例研究
   - 培训材料

4. **定期审查**
   - 月度运营审查
   - 季度战略审查
   - 年度全面审查
   - BCP文档更新
   - 技术栈评估

**关键成功因素**

1. **管理支持**
   - 充足的资源投入
   - 明确的优先级
   - 跨部门协作
   - 长期承诺

2. **技术能力**
   - 熟练的技术团队
   - 先进的工具和平台
   - 自动化能力
   - 创新意识

3. **流程规范**
   - 清晰的流程文档
   - 标准化操作
   - 有效的变更管理
   - 持续优化

4. **文化建设**
   - 安全意识
   - 质量意识
   - 学习型组织
   - 开放透明

**常见陷阱与避免方法**

1. **过度依赖自动化**
   - 保持人工干预能力
   - 定期演练手动流程
   - 文档化手动步骤
   - 培训应急操作

2. **忽视非技术因素**
   - 重视沟通和协调
   - 考虑业务影响
   - 关注用户体验
   - 管理利益相关者期望

3. **测试不充分**
   - 定期全面测试
   - 覆盖各种场景
   - 包含边界情况
   - 验证恢复流程

4. **文档过时**
   - 建立更新机制
   - 定期审查文档
   - 自动化文档生成
   - 版本控制

**未来趋势**

1. **AI/ML应用**
   - 智能故障预测
   - 自动根因分析
   - 智能容量规划
   - 异常检测

2. **云原生技术**
   - Serverless架构
   - 服务网格
   - GitOps
   - 策略即代码

3. **可观测性增强**
   - OpenTelemetry
   - eBPF技术
   - 实时分析
   - 智能告警

4. **安全集成**
   - DevSecOps
   - 零信任架构
   - 自动化合规
   - 威胁情报

通过遵循这些最佳实践,组织可以建立健全的业务连续性能力,确保在面对各种中断事件时,能够快速恢复并持续提供关键业务服务,最大限度地减少业务影响和损失。

---

**本节小结**

业务连续性是一个持续的过程,而不是一次性的项目。它需要组织在技术、流程、人员和文化等多个维度进行系统性的建设和持续改进。通过建立完善的业务连续性计划、定期演练测试、有效的事件响应机制和持续改进文化,组织可以在Kubernetes环境下实现高水平的业务连续性,为业务发展提供坚实的保障。

