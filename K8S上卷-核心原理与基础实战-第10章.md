# 第10章 Kubernetes高级特性与企业级实践

## 本章概述

在前面的章节中，我们系统学习了Kubernetes的核心概念、资源对象、网络存储、安全机制和可观测性等内容。本章将深入探讨Kubernetes的高级特性和企业级实践，帮助你构建生产级的、可扩展的、安全的Kubernetes平台。

**本章主要内容**：

1. **多租户与资源隔离**：如何在单个集群中安全地支持多个团队或应用
2. **集群联邦与多集群管理**：跨地域、跨云的多集群架构设计
3. **灾难恢复与业务连续性**：备份恢复策略和高可用架构
4. **安全加固与合规性**：企业级安全最佳实践和合规要求
5. **大规模集群运维**：管理数千节点的超大规模集群
6. **Kubernetes扩展开发**：自定义资源、Operator和准入控制器
7. **云原生最佳实践**：从传统应用到云原生的迁移路径
8. **本章总结**：高级特性的综合应用和未来展望

**学习目标**：

- 掌握多租户架构设计和资源隔离技术
- 理解多集群管理的挑战和解决方案
- 建立完善的灾难恢复和业务连续性体系
- 实施企业级安全加固和合规性管理
- 掌握大规模集群的运维技巧和性能优化
- 学会开发Kubernetes扩展和自定义控制器
- 了解云原生最佳实践和迁移策略

**前置知识**：

- 熟悉Kubernetes核心概念和资源对象（第1-4章）
- 理解网络、存储和安全机制（第5-7章）
- 掌握应用部署和可观测性（第8-9章）

让我们开始探索Kubernetes的高级特性和企业级实践！

---

## 10.1 多租户与资源隔离

### 10.1.1 多租户架构概述

**什么是多租户**

多租户（Multi-Tenancy）是指在单个Kubernetes集群中安全地支持多个用户、团队或应用的能力。每个租户拥有独立的资源配额、网络隔离和访问控制，互不干扰。

**多租户的价值**：

1. **成本优化**：共享集群基础设施，降低运维成本
2. **资源利用率**：通过资源共享提高整体利用率
3. **简化管理**：统一的集群管理和升级
4. **快速交付**：租户可以快速获得资源和环境

**多租户的挑战**：

1. **资源隔离**：防止租户之间的资源争抢
2. **网络隔离**：确保租户之间的网络安全
3. **安全隔离**：防止权限泄露和越权访问
4. **性能隔离**：避免"吵闹邻居"问题
5. **可见性隔离**：租户只能看到自己的资源

**多租户模型**

Kubernetes支持三种多租户模型：

```yaml
# 1. 软多租户（Soft Multi-Tenancy）
# 适用场景：同一组织内的不同团队
# 隔离级别：中等
# 信任级别：高
特点:
  - 使用Namespace进行逻辑隔离
  - 通过RBAC控制访问权限
  - ResourceQuota限制资源使用
  - NetworkPolicy实现网络隔离
  
优势:
  - 实现简单，运维成本低
  - 资源利用率高
  - 适合可信环境
  
劣势:
  - 隔离强度有限
  - 存在安全风险
  - 不适合多客户场景

---

# 2. 硬多租户（Hard Multi-Tenancy）
# 适用场景：不同组织或客户
# 隔离级别：高
# 信任级别：低
特点:
  - 每个租户独立集群
  - 完全的资源和网络隔离
  - 独立的控制平面
  - 强安全边界
  
优势:
  - 安全性最高
  - 完全隔离
  - 适合多客户SaaS
  
劣势:
  - 成本高
  - 运维复杂
  - 资源利用率低

---

# 3. 混合多租户（Hybrid Multi-Tenancy）
# 适用场景：大型企业或云服务商
# 隔离级别：可配置
# 信任级别：中等
特点:
  - 结合软硬多租户优势
  - 使用虚拟集群技术
  - 共享控制平面，隔离数据平面
  - 灵活的隔离策略
  
优势:
  - 平衡成本和安全
  - 灵活性高
  - 适合复杂场景
  
劣势:
  - 实现复杂
  - 需要额外工具支持
```

**多租户架构设计原则**

```yaml
# 多租户架构设计的核心原则

1. 最小权限原则（Principle of Least Privilege）:
   描述: 租户只能访问其必需的资源
   实现:
     - 使用RBAC精细化权限控制
     - 默认拒绝，显式授权
     - 定期审计权限使用情况
   
2. 纵深防御（Defense in Depth）:
   描述: 多层次的安全防护
   实现:
     - Namespace隔离
     - NetworkPolicy网络隔离
     - PodSecurityPolicy安全策略
     - ResourceQuota资源限制
     - 审计日志记录
   
3. 故障隔离（Fault Isolation）:
   描述: 一个租户的故障不影响其他租户
   实现:
     - 资源配额限制
     - PriorityClass优先级
     - Pod反亲和性
     - 独立的存储和网络
   
4. 公平性（Fairness）:
   描述: 资源分配公平合理
   实现:
     - ResourceQuota配额管理
     - LimitRange默认限制
     - 资源预留和超卖策略
     - 动态资源调整
   
5. 可观测性（Observability）:
   描述: 租户可以监控自己的资源
   实现:
     - 租户级别的监控指标
     - 独立的日志收集
     - 资源使用报表
     - 告警和通知机制
```

**多租户实现技术栈**

```yaml
# Kubernetes原生能力
核心组件:
  - Namespace: 逻辑隔离
  - RBAC: 访问控制
  - ResourceQuota: 资源配额
  - LimitRange: 资源限制
  - NetworkPolicy: 网络隔离
  - PodSecurityPolicy: 安全策略

# 增强工具
虚拟集群:
  - vcluster: 轻量级虚拟集群
  - Kamaji: 托管控制平面
  - Cluster API: 集群生命周期管理

策略引擎:
  - OPA/Gatekeeper: 策略即代码
  - Kyverno: Kubernetes原生策略引擎
  - Polaris: 最佳实践验证

网络隔离:
  - Calico: 高级网络策略
  - Cilium: eBPF网络和安全
  - Istio: 服务网格隔离

资源管理:
  - Hierarchical Namespace Controller: 层级命名空间
  - Capsule: 多租户Operator
  - Loft: 虚拟集群平台
```


### 10.1.2 Namespace级别的资源隔离

**Namespace隔离基础**

Namespace是Kubernetes中最基本的多租户隔离机制，它提供了逻辑上的资源分组和隔离。

**Namespace隔离的资源类型**：

```yaml
# Namespace级别的资源（受Namespace隔离）
namespace_scoped_resources:
  工作负载:
    - Pod
    - Deployment
    - StatefulSet
    - DaemonSet
    - Job
    - CronJob
  
  配置和存储:
    - ConfigMap
    - Secret
    - PersistentVolumeClaim
  
  服务发现:
    - Service
    - Endpoints
    - Ingress
  
  访问控制:
    - Role
    - RoleBinding
    - ServiceAccount
  
  资源管理:
    - ResourceQuota
    - LimitRange

# 集群级别的资源（不受Namespace隔离）
cluster_scoped_resources:
  - Node
  - PersistentVolume
  - StorageClass
  - ClusterRole
  - ClusterRoleBinding
  - Namespace
  - CustomResourceDefinition
```

**创建租户Namespace**

```yaml
# tenant-namespace.yaml
# 为租户创建完整的Namespace配置

apiVersion: v1
kind: Namespace
metadata:
  name: tenant-alpha
  labels:
    tenant: alpha
    environment: production
    cost-center: "engineering"
  annotations:
    description: "Alpha团队的生产环境"
    contact: "alpha-team@example.com"
    created-by: "platform-team"

---
# ResourceQuota - 限制租户的资源使用
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-alpha-quota
  namespace: tenant-alpha
spec:
  hard:
    # 计算资源限制
    requests.cpu: "100"           # 最多请求100核CPU
    requests.memory: 200Gi        # 最多请求200GB内存
    limits.cpu: "200"             # 最多限制200核CPU
    limits.memory: 400Gi          # 最多限制400GB内存
    
    # 存储资源限制
    requests.storage: 1Ti         # 最多请求1TB存储
    persistentvolumeclaims: "50"  # 最多50个PVC
    
    # 对象数量限制
    pods: "500"                   # 最多500个Pod
    services: "100"               # 最多100个Service
    services.loadbalancers: "10"  # 最多10个LoadBalancer
    services.nodeports: "20"      # 最多20个NodePort
    
    # 其他资源限制
    configmaps: "200"             # 最多200个ConfigMap
    secrets: "200"                # 最多200个Secret
    replicationcontrollers: "50"  # 最多50个RC
    
  # 作用域限制
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["high", "medium"]

---
# LimitRange - 设置默认资源限制
apiVersion: v1
kind: LimitRange
metadata:
  name: tenant-alpha-limits
  namespace: tenant-alpha
spec:
  limits:
  # Pod级别限制
  - type: Pod
    max:
      cpu: "16"
      memory: 32Gi
    min:
      cpu: "100m"
      memory: 128Mi
  
  # Container级别限制
  - type: Container
    default:              # 默认限制
      cpu: "1"
      memory: 1Gi
    defaultRequest:       # 默认请求
      cpu: "500m"
      memory: 512Mi
    max:                  # 最大限制
      cpu: "8"
      memory: 16Gi
    min:                  # 最小请求
      cpu: "100m"
      memory: 128Mi
    maxLimitRequestRatio: # 限制/请求比率
      cpu: "4"
      memory: "4"
  
  # PVC级别限制
  - type: PersistentVolumeClaim
    max:
      storage: 100Gi
    min:
      storage: 1Gi

---
# NetworkPolicy - 网络隔离
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tenant-alpha-network-policy
  namespace: tenant-alpha
spec:
  podSelector: {}  # 应用到所有Pod
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  # 允许同Namespace内的Pod互相访问
  - from:
    - podSelector: {}
  
  # 允许来自Ingress Controller的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
  
  egress:
  # 允许访问同Namespace内的Pod
  - to:
    - podSelector: {}
  
  # 允许DNS查询
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  
  # 允许访问外部服务（示例：数据库）
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432
  
  # 允许访问互联网（可选）
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
```

**RBAC权限配置**

```yaml
# tenant-rbac.yaml
# 为租户配置细粒度的访问控制

---
# ServiceAccount - 租户的服务账号
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tenant-alpha-admin
  namespace: tenant-alpha

---
# Role - 租户管理员角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: tenant-admin
  namespace: tenant-alpha
rules:
# 完全控制工作负载
- apiGroups: ["", "apps", "batch"]
  resources:
    - pods
    - pods/log
    - pods/exec
    - deployments
    - statefulsets
    - daemonsets
    - jobs
    - cronjobs
    - replicasets
  verbs: ["*"]

# 完全控制配置和存储
- apiGroups: [""]
  resources:
    - configmaps
    - secrets
    - persistentvolumeclaims
  verbs: ["*"]

# 完全控制服务发现
- apiGroups: ["", "networking.k8s.io"]
  resources:
    - services
    - endpoints
    - ingresses
  verbs: ["*"]

# 只读访问资源配额
- apiGroups: [""]
  resources:
    - resourcequotas
    - limitranges
  verbs: ["get", "list", "watch"]

# 只读访问事件
- apiGroups: [""]
  resources:
    - events
  verbs: ["get", "list", "watch"]

---
# Role - 租户开发者角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: tenant-developer
  namespace: tenant-alpha
rules:
# 管理工作负载（不包括删除）
- apiGroups: ["", "apps", "batch"]
  resources:
    - pods
    - deployments
    - statefulsets
    - jobs
    - cronjobs
  verbs: ["get", "list", "watch", "create", "update", "patch"]

# 查看日志和执行命令
- apiGroups: [""]
  resources:
    - pods/log
    - pods/exec
  verbs: ["get", "create"]

# 管理配置（不包括Secret）
- apiGroups: [""]
  resources:
    - configmaps
  verbs: ["get", "list", "watch", "create", "update", "patch"]

# 只读访问Secret
- apiGroups: [""]
  resources:
    - secrets
  verbs: ["get", "list"]

# 只读访问服务
- apiGroups: [""]
  resources:
    - services
    - endpoints
  verbs: ["get", "list", "watch"]

---
# Role - 租户只读角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: tenant-viewer
  namespace: tenant-alpha
rules:
# 只读访问所有资源
- apiGroups: ["", "apps", "batch", "networking.k8s.io"]
  resources: ["*"]
  verbs: ["get", "list", "watch"]

# 查看日志
- apiGroups: [""]
  resources:
    - pods/log
  verbs: ["get"]

---
# RoleBinding - 绑定管理员角色
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-alpha-admin-binding
  namespace: tenant-alpha
subjects:
- kind: User
  name: alice@example.com
  apiGroup: rbac.authorization.k8s.io
- kind: ServiceAccount
  name: tenant-alpha-admin
  namespace: tenant-alpha
roleRef:
  kind: Role
  name: tenant-admin
  apiGroup: rbac.authorization.k8s.io

---
# RoleBinding - 绑定开发者角色
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-alpha-developer-binding
  namespace: tenant-alpha
subjects:
- kind: Group
  name: alpha-developers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: tenant-developer
  apiGroup: rbac.authorization.k8s.io

---
# RoleBinding - 绑定只读角色
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-alpha-viewer-binding
  namespace: tenant-alpha
subjects:
- kind: Group
  name: alpha-viewers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: tenant-viewer
  apiGroup: rbac.authorization.k8s.io
```

**租户管理自动化脚本**

```go
// tenant-manager.go
// 自动化租户创建和管理的Go程序

package main

import (
	"context"
	"fmt"
	"log"

	corev1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
)

// TenantConfig 租户配置
type TenantConfig struct {
	Name        string
	Environment string
	CostCenter  string
	Contact     string
	
	// 资源配额
	CPURequest    string
	MemoryRequest string
	CPULimit      string
	MemoryLimit   string
	StorageQuota  string
	
	// 对象数量限制
	MaxPods     int64
	MaxServices int64
	MaxPVCs     int64
	
	// 管理员用户
	Admins []string
	// 开发者组
	Developers []string
	// 只读用户组
	Viewers []string
}

// TenantManager 租户管理器
type TenantManager struct {
	clientset *kubernetes.Clientset
}

// NewTenantManager 创建租户管理器
func NewTenantManager(kubeconfig string) (*TenantManager, error) {
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create clientset: %v", err)
	}
	
	return &TenantManager{clientset: clientset}, nil
}

// CreateTenant 创建租户
func (tm *TenantManager) CreateTenant(ctx context.Context, config *TenantConfig) error {
	// 1. 创建Namespace
	if err := tm.createNamespace(ctx, config); err != nil {
		return fmt.Errorf("failed to create namespace: %v", err)
	}
	log.Printf("Created namespace: %s", config.Name)
	
	// 2. 创建ResourceQuota
	if err := tm.createResourceQuota(ctx, config); err != nil {
		return fmt.Errorf("failed to create resource quota: %v", err)
	}
	log.Printf("Created resource quota for: %s", config.Name)
	
	// 3. 创建LimitRange
	if err := tm.createLimitRange(ctx, config); err != nil {
		return fmt.Errorf("failed to create limit range: %v", err)
	}
	log.Printf("Created limit range for: %s", config.Name)
	
	// 4. 创建NetworkPolicy
	if err := tm.createNetworkPolicy(ctx, config); err != nil {
		return fmt.Errorf("failed to create network policy: %v", err)
	}
	log.Printf("Created network policy for: %s", config.Name)
	
	// 5. 创建RBAC
	if err := tm.createRBAC(ctx, config); err != nil {
		return fmt.Errorf("failed to create RBAC: %v", err)
	}
	log.Printf("Created RBAC for: %s", config.Name)
	
	log.Printf("Successfully created tenant: %s", config.Name)
	return nil
}

// createNamespace 创建Namespace
func (tm *TenantManager) createNamespace(ctx context.Context, config *TenantConfig) error {
	ns := &corev1.Namespace{
		ObjectMeta: metav1.ObjectMeta{
			Name: config.Name,
			Labels: map[string]string{
				"tenant":      config.Name,
				"environment": config.Environment,
				"cost-center": config.CostCenter,
			},
			Annotations: map[string]string{
				"contact":    config.Contact,
				"created-by": "tenant-manager",
			},
		},
	}
	
	_, err := tm.clientset.CoreV1().Namespaces().Create(ctx, ns, metav1.CreateOptions{})
	return err
}

// createResourceQuota 创建ResourceQuota
func (tm *TenantManager) createResourceQuota(ctx context.Context, config *TenantConfig) error {
	quota := &corev1.ResourceQuota{
		ObjectMeta: metav1.ObjectMeta{
			Name:      fmt.Sprintf("%s-quota", config.Name),
			Namespace: config.Name,
		},
		Spec: corev1.ResourceQuotaSpec{
			Hard: corev1.ResourceList{
				"requests.cpu":           resource.MustParse(config.CPURequest),
				"requests.memory":        resource.MustParse(config.MemoryRequest),
				"limits.cpu":             resource.MustParse(config.CPULimit),
				"limits.memory":          resource.MustParse(config.MemoryLimit),
				"requests.storage":       resource.MustParse(config.StorageQuota),
				"pods":                   *resource.NewQuantity(config.MaxPods, resource.DecimalSI),
				"services":               *resource.NewQuantity(config.MaxServices, resource.DecimalSI),
				"persistentvolumeclaims": *resource.NewQuantity(config.MaxPVCs, resource.DecimalSI),
			},
		},
	}
	
	_, err := tm.clientset.CoreV1().ResourceQuotas(config.Name).Create(ctx, quota, metav1.CreateOptions{})
	return err
}

// createLimitRange 创建LimitRange
func (tm *TenantManager) createLimitRange(ctx context.Context, config *TenantConfig) error {
	limitRange := &corev1.LimitRange{
		ObjectMeta: metav1.ObjectMeta{
			Name:      fmt.Sprintf("%s-limits", config.Name),
			Namespace: config.Name,
		},
		Spec: corev1.LimitRangeSpec{
			Limits: []corev1.LimitRangeItem{
				{
					Type: corev1.LimitTypeContainer,
					Default: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("1"),
						corev1.ResourceMemory: resource.MustParse("1Gi"),
					},
					DefaultRequest: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("500m"),
						corev1.ResourceMemory: resource.MustParse("512Mi"),
					},
					Max: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("8"),
						corev1.ResourceMemory: resource.MustParse("16Gi"),
					},
					Min: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("100m"),
						corev1.ResourceMemory: resource.MustParse("128Mi"),
					},
				},
			},
		},
	}
	
	_, err := tm.clientset.CoreV1().LimitRanges(config.Name).Create(ctx, limitRange, metav1.CreateOptions{})
	return err
}

// createNetworkPolicy 创建NetworkPolicy（简化版）
func (tm *TenantManager) createNetworkPolicy(ctx context.Context, config *TenantConfig) error {
	// 这里简化实现，实际应该使用networking.k8s.io/v1的NetworkPolicy
	// 由于篇幅限制，省略详细实现
	log.Printf("NetworkPolicy creation skipped in this example")
	return nil
}

// createRBAC 创建RBAC
func (tm *TenantManager) createRBAC(ctx context.Context, config *TenantConfig) error {
	// 创建管理员Role
	adminRole := &rbacv1.Role{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "tenant-admin",
			Namespace: config.Name,
		},
		Rules: []rbacv1.PolicyRule{
			{
				APIGroups: []string{"", "apps", "batch"},
				Resources: []string{"*"},
				Verbs:     []string{"*"},
			},
		},
	}
	
	if _, err := tm.clientset.RbacV1().Roles(config.Name).Create(ctx, adminRole, metav1.CreateOptions{}); err != nil {
		return err
	}
	
	// 创建管理员RoleBinding
	for _, admin := range config.Admins {
		binding := &rbacv1.RoleBinding{
			ObjectMeta: metav1.ObjectMeta{
				Name:      fmt.Sprintf("%s-admin-binding", admin),
				Namespace: config.Name,
			},
			Subjects: []rbacv1.Subject{
				{
					Kind:     "User",
					Name:     admin,
					APIGroup: "rbac.authorization.k8s.io",
				},
			},
			RoleRef: rbacv1.RoleRef{
				Kind:     "Role",
				Name:     "tenant-admin",
				APIGroup: "rbac.authorization.k8s.io",
			},
		}
		
		if _, err := tm.clientset.RbacV1().RoleBindings(config.Name).Create(ctx, binding, metav1.CreateOptions{}); err != nil {
			return err
		}
	}
	
	return nil
}

// DeleteTenant 删除租户
func (tm *TenantManager) DeleteTenant(ctx context.Context, tenantName string) error {
	// 删除Namespace会级联删除所有资源
	err := tm.clientset.CoreV1().Namespaces().Delete(ctx, tenantName, metav1.DeleteOptions{})
	if err != nil {
		return fmt.Errorf("failed to delete namespace: %v", err)
	}
	
	log.Printf("Successfully deleted tenant: %s", tenantName)
	return nil
}

// GetTenantResourceUsage 获取租户资源使用情况
func (tm *TenantManager) GetTenantResourceUsage(ctx context.Context, tenantName string) (*ResourceUsage, error) {
	// 获取ResourceQuota
	quota, err := tm.clientset.CoreV1().ResourceQuotas(tenantName).Get(ctx, fmt.Sprintf("%s-quota", tenantName), metav1.GetOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get resource quota: %v", err)
	}
	
	usage := &ResourceUsage{
		TenantName: tenantName,
		CPU: ResourceMetric{
			Used:  quota.Status.Used[corev1.ResourceRequestsCPU],
			Limit: quota.Status.Hard[corev1.ResourceRequestsCPU],
		},
		Memory: ResourceMetric{
			Used:  quota.Status.Used[corev1.ResourceRequestsMemory],
			Limit: quota.Status.Hard[corev1.ResourceRequestsMemory],
		},
		Storage: ResourceMetric{
			Used:  quota.Status.Used[corev1.ResourceRequestsStorage],
			Limit: quota.Status.Hard[corev1.ResourceRequestsStorage],
		},
		Pods: ObjectMetric{
			Used:  int(quota.Status.Used.Pods().Value()),
			Limit: int(quota.Status.Hard.Pods().Value()),
		},
	}
	
	return usage, nil
}

// ResourceUsage 资源使用情况
type ResourceUsage struct {
	TenantName string
	CPU        ResourceMetric
	Memory     ResourceMetric
	Storage    ResourceMetric
	Pods       ObjectMetric
}

// ResourceMetric 资源指标
type ResourceMetric struct {
	Used  resource.Quantity
	Limit resource.Quantity
}

// ObjectMetric 对象指标
type ObjectMetric struct {
	Used  int
	Limit int
}

// 使用示例
func main() {
	ctx := context.Background()
	
	// 创建租户管理器
	manager, err := NewTenantManager("/path/to/kubeconfig")
	if err != nil {
		log.Fatalf("Failed to create tenant manager: %v", err)
	}
	
	// 配置租户
	config := &TenantConfig{
		Name:          "tenant-alpha",
		Environment:   "production",
		CostCenter:    "engineering",
		Contact:       "alpha-team@example.com",
		CPURequest:    "100",
		MemoryRequest: "200Gi",
		CPULimit:      "200",
		MemoryLimit:   "400Gi",
		StorageQuota:  "1Ti",
		MaxPods:       500,
		MaxServices:   100,
		MaxPVCs:       50,
		Admins:        []string{"alice@example.com"},
		Developers:    []string{"alpha-developers"},
		Viewers:       []string{"alpha-viewers"},
	}
	
	// 创建租户
	if err := manager.CreateTenant(ctx, config); err != nil {
		log.Fatalf("Failed to create tenant: %v", err)
	}
	
	// 获取资源使用情况
	usage, err := manager.GetTenantResourceUsage(ctx, "tenant-alpha")
	if err != nil {
		log.Fatalf("Failed to get resource usage: %v", err)
	}
	
	fmt.Printf("Tenant: %s\n", usage.TenantName)
	fmt.Printf("CPU: %s / %s\n", usage.CPU.Used.String(), usage.CPU.Limit.String())
	fmt.Printf("Memory: %s / %s\n", usage.Memory.Used.String(), usage.Memory.Limit.String())
	fmt.Printf("Pods: %d / %d\n", usage.Pods.Used, usage.Pods.Limit)
}
```

**租户隔离验证**

```bash
#!/bin/bash
# tenant-isolation-test.sh
# 验证租户隔离的有效性

set -e

TENANT_NAME="tenant-alpha"
TEST_USER="alice@example.com"

echo "=== 租户隔离验证测试 ==="

# 1. 验证Namespace隔离
echo "1. 验证Namespace隔离..."
kubectl get ns $TENANT_NAME
kubectl describe ns $TENANT_NAME

# 2. 验证ResourceQuota
echo "2. 验证ResourceQuota..."
kubectl get resourcequota -n $TENANT_NAME
kubectl describe resourcequota -n $TENANT_NAME

# 3. 验证LimitRange
echo "3. 验证LimitRange..."
kubectl get limitrange -n $TENANT_NAME
kubectl describe limitrange -n $TENANT_NAME

# 4. 验证NetworkPolicy
echo "4. 验证NetworkPolicy..."
kubectl get networkpolicy -n $TENANT_NAME
kubectl describe networkpolicy -n $TENANT_NAME

# 5. 验证RBAC权限
echo "5. 验证RBAC权限..."
kubectl get role,rolebinding -n $TENANT_NAME

# 6. 测试跨Namespace访问（应该被拒绝）
echo "6. 测试跨Namespace访问..."
kubectl auth can-i get pods -n default --as=$TEST_USER
kubectl auth can-i get pods -n $TENANT_NAME --as=$TEST_USER

# 7. 测试资源配额限制
echo "7. 测试资源配额限制..."
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: quota-test
  namespace: $TENANT_NAME
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    resources:
      requests:
        cpu: "10000"  # 超过配额
        memory: "10000Gi"
EOF

# 8. 测试网络隔离
echo "8. 测试网络隔离..."
kubectl run test-pod -n $TENANT_NAME --image=busybox --rm -it -- sh -c "
  # 测试同Namespace内访问
  wget -O- http://service-in-same-ns:8080
  
  # 测试跨Namespace访问（应该被拒绝）
  wget -O- http://service-in-other-ns.other-namespace:8080
"

echo "=== 验证完成 ==="
```

**最佳实践总结**

```yaml
# Namespace级别资源隔离的最佳实践

1. Namespace命名规范:
   格式: <team>-<environment>-<purpose>
   示例:
     - alpha-prod-web
     - beta-dev-api
     - platform-staging-db

2. ResourceQuota设置原则:
   - 根据团队规模和应用需求设置合理配额
   - 预留20-30%的缓冲空间
   - 定期审查和调整配额
   - 使用PriorityClass区分关键和非关键工作负载

3. LimitRange配置建议:
   - 设置合理的默认值，避免资源浪费
   - 限制单个容器的最大资源，防止资源垄断
   - 设置合理的limit/request比率（建议2-4倍）

4. NetworkPolicy策略:
   - 默认拒绝所有流量
   - 显式允许必要的通信
   - 使用标签选择器而非IP地址
   - 定期审查和更新策略

5. RBAC权限管理:
   - 遵循最小权限原则
   - 使用Role而非ClusterRole
   - 定期审计权限使用情况
   - 使用ServiceAccount而非用户凭证

6. 监控和告警:
   - 监控资源配额使用率
   - 设置配额使用率告警（如80%、90%）
   - 监控NetworkPolicy拒绝的连接
   - 审计RBAC权限变更

7. 自动化管理:
   - 使用GitOps管理租户配置
   - 自动化租户创建和删除流程
   - 定期清理未使用的资源
   - 使用Admission Webhook强制策略
```



### 10.1.3 虚拟集群技术

**虚拟集群概述**

虚拟集群（Virtual Cluster）是一种在物理Kubernetes集群之上创建逻辑隔离集群的技术。每个虚拟集群拥有独立的控制平面（API Server、Controller Manager、Scheduler），但共享底层的计算、存储和网络资源。

**虚拟集群的优势**：

```yaml
# 虚拟集群 vs 传统Namespace隔离

传统Namespace隔离:
  优点:
    - 实现简单
    - 资源开销小
    - 原生Kubernetes支持
  
  缺点:
    - 隔离强度有限
    - 共享控制平面
    - 无法隔离CRD
    - 集群级资源冲突
    - 版本升级影响所有租户

虚拟集群:
  优点:
    - 强隔离（独立控制平面）
    - 完全的API兼容性
    - 独立的CRD和集群资源
    - 独立升级和配置
    - 更好的安全性
  
  缺点:
    - 资源开销较大
    - 实现复杂
    - 需要额外工具支持
```

**虚拟集群架构**

```yaml
# 虚拟集群架构设计

物理集群（Host Cluster）:
  角色: 提供底层基础设施
  组件:
    - 物理节点
    - 存储系统
    - 网络系统
    - 基础控制平面

虚拟集群（Virtual Cluster）:
  角色: 为租户提供独立的Kubernetes环境
  组件:
    - 虚拟API Server
    - 虚拟Controller Manager
    - 虚拟Scheduler
    - 虚拟etcd（可选）
  
  资源映射:
    虚拟资源 -> 物理资源:
      - vPod -> Pod（在host namespace中）
      - vService -> Service（带前缀）
      - vPVC -> PVC（带前缀）
      - vNode -> 虚拟节点（映射到物理节点）

同步机制:
  下行同步（Virtual -> Host）:
    - Pod规格
    - Service定义
    - PVC请求
    - ConfigMap/Secret
  
  上行同步（Host -> Virtual）:
    - Pod状态
    - Service端点
    - PVC绑定状态
    - 事件信息
```

**vcluster实战**

vcluster是最流行的开源虚拟集群解决方案，由Loft Labs开发。

```yaml
# vcluster-values.yaml
# vcluster配置文件

# 虚拟集群基本配置
vcluster:
  # 镜像配置
  image: rancher/k3s:v1.28.2-k3s1
  
  # 资源限制
  resources:
    limits:
      cpu: "2"
      memory: 4Gi
    requests:
      cpu: "1"
      memory: 2Gi
  
  # 持久化存储
  storage:
    size: 10Gi
    className: fast-ssd
  
  # 高可用配置
  replicas: 3
  
  # 额外参数
  extraArgs:
    - --service-cidr=10.96.0.0/16
    - --cluster-cidr=10.244.0.0/16

# 同步配置
sync:
  # 同步的资源类型
  services:
    enabled: true
  configmaps:
    enabled: true
    all: false  # 只同步被Pod使用的ConfigMap
  secrets:
    enabled: true
    all: false  # 只同步被Pod使用的Secret
  endpoints:
    enabled: true
  pods:
    enabled: true
    ephemeralContainers: true
    status: true
  events:
    enabled: true
  persistentvolumeclaims:
    enabled: true
  ingresses:
    enabled: true
  storageclasses:
    enabled: true
  priorityclasses:
    enabled: false
  networkpolicies:
    enabled: true
  volumesnapshots:
    enabled: false
  poddisruptionbudgets:
    enabled: false
  serviceaccounts:
    enabled: true

# 网络配置
networking:
  # 使用host网络
  replicateServices:
    toHost:
      - from: default/*
        to: vcluster-{{ .Release.Name }}-*
  
  # DNS配置
  advanced:
    clusterDomain: "cluster.local"
    fallbackHostCluster: true

# RBAC配置
rbac:
  clusterRole:
    create: true
  role:
    create: true

# 隔离配置
isolation:
  enabled: true
  namespace: vcluster-{{ .Release.Name }}
  
  # Pod安全策略
  podSecurityStandard: baseline
  
  # 资源配额
  resourceQuota:
    enabled: true
    quota:
      requests.cpu: "10"
      requests.memory: 20Gi
      requests.storage: 100Gi
      persistentvolumeclaims: "20"
      services.loadbalancers: "5"
  
  # 限制范围
  limitRange:
    enabled: true
    default:
      cpu: "1"
      memory: 1Gi
    defaultRequest:
      cpu: "100m"
      memory: 128Mi

# 监控配置
monitoring:
  serviceMonitor:
    enabled: true
    labels:
      prometheus: kube-prometheus

# 备份配置
backup:
  enabled: true
  schedule: "0 2 * * *"
  retention: 7
```

**部署vcluster**

```bash
#!/bin/bash
# deploy-vcluster.sh
# 部署虚拟集群

set -e

VCLUSTER_NAME="tenant-alpha-vcluster"
NAMESPACE="vcluster-alpha"
KUBECONFIG_OUTPUT="./kubeconfig-${VCLUSTER_NAME}.yaml"

echo "=== 部署虚拟集群 ==="

# 1. 安装vcluster CLI
echo "1. 安装vcluster CLI..."
curl -L -o vcluster "https://github.com/loft-sh/vcluster/releases/latest/download/vcluster-linux-amd64"
chmod +x vcluster
sudo mv vcluster /usr/local/bin/

# 2. 创建Namespace
echo "2. 创建Namespace..."
kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

# 3. 部署vcluster
echo "3. 部署vcluster..."
vcluster create $VCLUSTER_NAME   --namespace $NAMESPACE   --values vcluster-values.yaml   --connect=false

# 4. 等待vcluster就绪
echo "4. 等待vcluster就绪..."
kubectl wait --for=condition=ready pod   -l app=$VCLUSTER_NAME   -n $NAMESPACE   --timeout=300s

# 5. 获取kubeconfig
echo "5. 获取kubeconfig..."
vcluster connect $VCLUSTER_NAME   --namespace $NAMESPACE   --update-current=false   --print > $KUBECONFIG_OUTPUT

echo "虚拟集群已部署！"
echo "Kubeconfig文件: $KUBECONFIG_OUTPUT"
echo ""
echo "使用以下命令访问虚拟集群:"
echo "export KUBECONFIG=$KUBECONFIG_OUTPUT"
echo "kubectl get nodes"

# 6. 验证虚拟集群
echo ""
echo "6. 验证虚拟集群..."
export KUBECONFIG=$KUBECONFIG_OUTPUT
kubectl cluster-info
kubectl get nodes
kubectl get namespaces

echo "=== 部署完成 ==="
```

**虚拟集群管理**

```go
// vcluster-manager.go
// 虚拟集群管理工具

package main

import (
	"context"
	"fmt"
	"log"
	"os/exec"
	"time"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
)

// VClusterConfig 虚拟集群配置
type VClusterConfig struct {
	Name           string
	Namespace      string
	K8sVersion     string
	Replicas       int32
	CPULimit       string
	MemoryLimit    string
	StorageSize    string
	StorageClass   string
	EnableBackup   bool
	EnableMonitoring bool
}

// VClusterManager 虚拟集群管理器
type VClusterManager struct {
	clientset *kubernetes.Clientset
}

// NewVClusterManager 创建虚拟集群管理器
func NewVClusterManager(kubeconfig string) (*VClusterManager, error) {
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create clientset: %v", err)
	}
	
	return &VClusterManager{clientset: clientset}, nil
}

// CreateVCluster 创建虚拟集群
func (vm *VClusterManager) CreateVCluster(ctx context.Context, config *VClusterConfig) error {
	log.Printf("Creating virtual cluster: %s", config.Name)
	
	// 1. 创建Namespace
	if err := vm.createNamespace(ctx, config); err != nil {
		return fmt.Errorf("failed to create namespace: %v", err)
	}
	
	// 2. 生成Helm values
	valuesFile, err := vm.generateHelmValues(config)
	if err != nil {
		return fmt.Errorf("failed to generate helm values: %v", err)
	}
	
	// 3. 使用vcluster CLI创建
	cmd := exec.Command("vcluster", "create", config.Name,
		"--namespace", config.Namespace,
		"--values", valuesFile,
		"--connect=false",
	)
	
	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("failed to create vcluster: %v, output: %s", err, output)
	}
	
	log.Printf("Virtual cluster created: %s", config.Name)
	
	// 4. 等待就绪
	if err := vm.waitForReady(ctx, config); err != nil {
		return fmt.Errorf("vcluster not ready: %v", err)
	}
	
	log.Printf("Virtual cluster is ready: %s", config.Name)
	return nil
}

// createNamespace 创建Namespace
func (vm *VClusterManager) createNamespace(ctx context.Context, config *VClusterConfig) error {
	ns := &corev1.Namespace{
		ObjectMeta: metav1.ObjectMeta{
			Name: config.Namespace,
			Labels: map[string]string{
				"vcluster": config.Name,
				"type":     "virtual-cluster",
			},
		},
	}
	
	_, err := vm.clientset.CoreV1().Namespaces().Create(ctx, ns, metav1.CreateOptions{})
	if err != nil {
		// 忽略已存在错误
		return nil
	}
	
	return nil
}

// generateHelmValues 生成Helm values文件
func (vm *VClusterManager) generateHelmValues(config *VClusterConfig) (string, error) {
	// 这里简化实现，实际应该生成完整的values.yaml
	valuesContent := fmt.Sprintf(`
vcluster:
  image: rancher/k3s:%s
  replicas: %d
  resources:
    limits:
      cpu: %s
      memory: %s
  storage:
    size: %s
    className: %s

isolation:
  enabled: true
  namespace: %s

monitoring:
  serviceMonitor:
    enabled: %t

backup:
  enabled: %t
`, config.K8sVersion, config.Replicas, config.CPULimit, config.MemoryLimit,
		config.StorageSize, config.StorageClass, config.Namespace,
		config.EnableMonitoring, config.EnableBackup)
	
	// 写入临时文件
	tmpFile := fmt.Sprintf("/tmp/vcluster-values-%s.yaml", config.Name)
	// 实际实现应该写入文件
	log.Printf("Generated values file: %s", tmpFile)
	
	return tmpFile, nil
}

// waitForReady 等待虚拟集群就绪
func (vm *VClusterManager) waitForReady(ctx context.Context, config *VClusterConfig) error {
	timeout := time.After(5 * time.Minute)
	ticker := time.NewTicker(10 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-timeout:
			return fmt.Errorf("timeout waiting for vcluster to be ready")
		case <-ticker.C:
			// 检查Pod状态
			pods, err := vm.clientset.CoreV1().Pods(config.Namespace).List(ctx, metav1.ListOptions{
				LabelSelector: fmt.Sprintf("app=%s", config.Name),
			})
			if err != nil {
				log.Printf("Error checking pod status: %v", err)
				continue
			}
			
			if len(pods.Items) == 0 {
				log.Printf("Waiting for pods to be created...")
				continue
			}
			
			allReady := true
			for _, pod := range pods.Items {
				if pod.Status.Phase != corev1.PodRunning {
					allReady = false
					break
				}
			}
			
			if allReady {
				return nil
			}
			
			log.Printf("Waiting for all pods to be ready...")
		}
	}
}

// DeleteVCluster 删除虚拟集群
func (vm *VClusterManager) DeleteVCluster(ctx context.Context, name, namespace string) error {
	log.Printf("Deleting virtual cluster: %s", name)
	
	cmd := exec.Command("vcluster", "delete", name,
		"--namespace", namespace,
	)
	
	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("failed to delete vcluster: %v, output: %s", err, output)
	}
	
	log.Printf("Virtual cluster deleted: %s", name)
	return nil
}

// GetVClusterKubeconfig 获取虚拟集群的kubeconfig
func (vm *VClusterManager) GetVClusterKubeconfig(name, namespace string) (string, error) {
	cmd := exec.Command("vcluster", "connect", name,
		"--namespace", namespace,
		"--update-current=false",
		"--print",
	)
	
	output, err := cmd.Output()
	if err != nil {
		return "", fmt.Errorf("failed to get kubeconfig: %v", err)
	}
	
	return string(output), nil
}

// ListVClusters 列出所有虚拟集群
func (vm *VClusterManager) ListVClusters(ctx context.Context) ([]VClusterInfo, error) {
	// 查找所有vcluster相关的Namespace
	namespaces, err := vm.clientset.CoreV1().Namespaces().List(ctx, metav1.ListOptions{
		LabelSelector: "type=virtual-cluster",
	})
	if err != nil {
		return nil, fmt.Errorf("failed to list namespaces: %v", err)
	}
	
	var vclusters []VClusterInfo
	for _, ns := range namespaces.Items {
		vclusterName := ns.Labels["vcluster"]
		if vclusterName == "" {
			continue
		}
		
		// 获取Pod状态
		pods, err := vm.clientset.CoreV1().Pods(ns.Name).List(ctx, metav1.ListOptions{
			LabelSelector: fmt.Sprintf("app=%s", vclusterName),
		})
		if err != nil {
			log.Printf("Error getting pods for vcluster %s: %v", vclusterName, err)
			continue
		}
		
		status := "Unknown"
		if len(pods.Items) > 0 {
			allRunning := true
			for _, pod := range pods.Items {
				if pod.Status.Phase != corev1.PodRunning {
					allRunning = false
					break
				}
			}
			if allRunning {
				status = "Running"
			} else {
				status = "Pending"
			}
		}
		
		vclusters = append(vclusters, VClusterInfo{
			Name:      vclusterName,
			Namespace: ns.Name,
			Status:    status,
			Age:       time.Since(ns.CreationTimestamp.Time).String(),
		})
	}
	
	return vclusters, nil
}

// VClusterInfo 虚拟集群信息
type VClusterInfo struct {
	Name      string
	Namespace string
	Status    string
	Age       string
}

// 使用示例
func main() {
	ctx := context.Background()
	
	// 创建管理器
	manager, err := NewVClusterManager("/path/to/kubeconfig")
	if err != nil {
		log.Fatalf("Failed to create manager: %v", err)
	}
	
	// 配置虚拟集群
	config := &VClusterConfig{
		Name:             "tenant-alpha-vcluster",
		Namespace:        "vcluster-alpha",
		K8sVersion:       "v1.28.2-k3s1",
		Replicas:         3,
		CPULimit:         "2",
		MemoryLimit:      "4Gi",
		StorageSize:      "10Gi",
		StorageClass:     "fast-ssd",
		EnableBackup:     true,
		EnableMonitoring: true,
	}
	
	// 创建虚拟集群
	if err := manager.CreateVCluster(ctx, config); err != nil {
		log.Fatalf("Failed to create vcluster: %v", err)
	}
	
	// 获取kubeconfig
	kubeconfig, err := manager.GetVClusterKubeconfig(config.Name, config.Namespace)
	if err != nil {
		log.Fatalf("Failed to get kubeconfig: %v", err)
	}
	fmt.Printf("Kubeconfig:\n%s\n", kubeconfig)
	
	// 列出所有虚拟集群
	vclusters, err := manager.ListVClusters(ctx)
	if err != nil {
		log.Fatalf("Failed to list vclusters: %v", err)
	}
	
	fmt.Println("\nVirtual Clusters:")
	for _, vc := range vclusters {
		fmt.Printf("- %s (namespace: %s, status: %s, age: %s)\n",
			vc.Name, vc.Namespace, vc.Status, vc.Age)
	}
}
```

**其他虚拟集群方案**

```yaml
# 虚拟集群技术对比

1. vcluster (Loft Labs):
   架构: 在Pod中运行完整的K3s/K8s
   优势:
     - 开源免费
     - 轻量级（基于K3s）
     - 易于部署和管理
     - 良好的资源隔离
   劣势:
     - 资源开销相对较大
     - 需要额外的同步机制
   适用场景:
     - 开发测试环境
     - 多租户SaaS平台
     - CI/CD流水线

2. Kamaji:
   架构: 托管控制平面，共享数据平面
   优势:
     - 控制平面完全隔离
     - 支持多种etcd后端
     - 更好的性能
     - 企业级特性
   劣势:
     - 实现复杂
     - 需要额外的基础设施
   适用场景:
     - 大规模多租户
     - 云服务提供商
     - 企业私有云

3. Cluster API vCluster Provider:
   架构: 基于Cluster API的虚拟集群
   优势:
     - 标准化的集群管理
     - 与Cluster API生态集成
     - 声明式管理
   劣势:
     - 学习曲线陡峭
     - 依赖Cluster API
   适用场景:
     - 已使用Cluster API的环境
     - 需要统一管理物理和虚拟集群

4. kcp (Kubernetes Control Plane):
   架构: 轻量级控制平面，无节点概念
   优势:
     - 极致轻量
     - 快速启动
     - 适合大规模场景
   劣势:
     - 仍在早期开发阶段
     - 生态不完善
   适用场景:
     - 实验性项目
     - 超大规模多租户
```

**虚拟集群网络配置**

```yaml
# vcluster-network-config.yaml
# 虚拟集群网络配置示例

apiVersion: v1
kind: ConfigMap
metadata:
  name: vcluster-network-config
  namespace: vcluster-alpha
data:
  # 网络模式配置
  network-mode: |
    # 模式1: 默认模式（推荐）
    # Pod使用host集群的网络，Service通过同步实现
    mode: default
    
    # 模式2: 独立网络
    # 使用CNI插件创建独立网络
    mode: isolated
    cni:
      plugin: calico
      podCIDR: 10.100.0.0/16
      serviceCIDR: 10.101.0.0/16
    
    # 模式3: 混合模式
    # 部分资源使用独立网络，部分共享
    mode: hybrid
    isolated:
      - namespace: production
      - namespace: staging
    shared:
      - namespace: development

  # Service同步配置
  service-sync: |
    # 从虚拟集群同步到host集群
    toHost:
      - from: "*/nginx-*"
        to: "vcluster-alpha-nginx-*"
      - from: "production/*"
        to: "vcluster-alpha-prod-*"
    
    # 从host集群同步到虚拟集群
    fromHost:
      - from: "shared-services/*"
        to: "external/*"

  # DNS配置
  dns-config: |
    # 虚拟集群DNS设置
    clusterDomain: vcluster.local
    
    # DNS转发规则
    forwarding:
      # 转发到host集群DNS
      - domain: "*.svc.cluster.local"
        nameserver: 10.96.0.10
      
      # 转发到外部DNS
      - domain: "*.example.com"
        nameserver: 8.8.8.8

  # Ingress配置
  ingress-config: |
    # Ingress同步策略
    sync:
      enabled: true
      # 自动添加前缀避免冲突
      hostnamePrefix: "vcluster-alpha-"
      
      # TLS证书同步
      tlsSecrets:
        enabled: true
        namespace: cert-manager
    
    # Ingress Controller配置
    controller:
      # 使用host集群的Ingress Controller
      useHost: true
      # 或部署独立的Ingress Controller
      deploy: false

---
# NetworkPolicy for vcluster
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: vcluster-network-policy
  namespace: vcluster-alpha
spec:
  podSelector:
    matchLabels:
      app: tenant-alpha-vcluster
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  # 允许来自同namespace的流量
  - from:
    - podSelector: {}
  
  # 允许来自host集群API Server的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 8443
  
  egress:
  # 允许访问host集群API Server
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 6443
  
  # 允许DNS查询
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  
  # 允许访问同步的Pod
  - to:
    - podSelector: {}
```

**虚拟集群监控和日志**

```yaml
# vcluster-monitoring.yaml
# 虚拟集群监控配置

---
# ServiceMonitor for Prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vcluster-metrics
  namespace: vcluster-alpha
  labels:
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: tenant-alpha-vcluster
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---
# PrometheusRule for alerting
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: vcluster-alerts
  namespace: vcluster-alpha
spec:
  groups:
  - name: vcluster
    interval: 30s
    rules:
    # vcluster Pod不健康告警
    - alert: VClusterPodNotReady
      expr: |
        kube_pod_status_phase{namespace="vcluster-alpha",phase!="Running"} > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "vcluster Pod不健康"
        description: "虚拟集群 {{ $labels.pod }} 状态异常"
    
    # vcluster API Server延迟告警
    - alert: VClusterAPIServerHighLatency
      expr: |
        histogram_quantile(0.99, 
          rate(apiserver_request_duration_seconds_bucket{namespace="vcluster-alpha"}[5m])
        ) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "vcluster API Server延迟过高"
        description: "P99延迟: {{ $value }}s"
    
    # vcluster etcd存储告警
    - alert: VClusterEtcdStorageHigh
      expr: |
        (etcd_mvcc_db_total_size_in_bytes{namespace="vcluster-alpha"} 
         / etcd_server_quota_backend_bytes{namespace="vcluster-alpha"}) > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "vcluster etcd存储使用率过高"
        description: "存储使用率: {{ $value | humanizePercentage }}"

---
# Grafana Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: vcluster-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  vcluster-dashboard.json: |
    {
      "dashboard": {
        "title": "Virtual Cluster Monitoring",
        "panels": [
          {
            "title": "vcluster Pod状态",
            "targets": [
              {
                "expr": "kube_pod_status_phase{namespace=~"vcluster-.*"}"
              }
            ]
          },
          {
            "title": "API Server请求率",
            "targets": [
              {
                "expr": "rate(apiserver_request_total{namespace=~"vcluster-.*"}[5m])"
              }
            ]
          },
          {
            "title": "资源使用情况",
            "targets": [
              {
                "expr": "container_memory_usage_bytes{namespace=~"vcluster-.*"}"
              },
              {
                "expr": "rate(container_cpu_usage_seconds_total{namespace=~"vcluster-.*"}[5m])"
              }
            ]
          }
        ]
      }
    }
```

**虚拟集群最佳实践**

```yaml
# 虚拟集群最佳实践总结

1. 资源规划:
   控制平面资源:
     - 小型vcluster（<50 nodes）: 1 CPU, 2Gi内存
     - 中型vcluster（50-200 nodes）: 2 CPU, 4Gi内存
     - 大型vcluster（>200 nodes）: 4 CPU, 8Gi内存
   
   存储规划:
     - etcd存储: 至少10Gi，建议使用SSD
     - 日志存储: 根据日志量规划
     - 备份存储: etcd大小的3-5倍
   
   网络规划:
     - 预留足够的IP地址空间
     - 避免CIDR冲突
     - 考虑跨集群通信需求

2. 高可用配置:
   控制平面:
     - 至少3个副本
     - 使用Pod反亲和性分散到不同节点
     - 配置PodDisruptionBudget
   
   etcd:
     - 使用持久化存储
     - 定期备份
     - 监控存储使用率
   
   网络:
     - 配置健康检查
     - 使用Service Mesh提高可靠性
     - 实施重试和超时策略

3. 安全加固:
   网络隔离:
     - 使用NetworkPolicy限制流量
     - 启用mTLS加密
     - 限制对host集群的访问
   
   访问控制:
     - 为每个vcluster配置独立的RBAC
     - 使用ServiceAccount而非用户凭证
     - 定期轮换证书和密钥
   
   审计:
     - 启用API Server审计日志
     - 监控异常访问模式
     - 定期安全扫描

4. 性能优化:
   API Server:
     - 调整--max-requests-inflight参数
     - 启用API优先级和公平性
     - 使用缓存减少etcd负载
   
   etcd:
     - 使用SSD存储
     - 调整--quota-backend-bytes
     - 定期压缩和碎片整理
   
   同步优化:
     - 只同步必要的资源
     - 使用标签选择器过滤
     - 调整同步间隔

5. 运维管理:
   监控:
     - 监控控制平面健康状态
     - 跟踪资源使用趋势
     - 设置合理的告警阈值
   
   备份恢复:
     - 定期备份etcd数据
     - 测试恢复流程
     - 保留多个备份版本
   
   升级策略:
     - 先在测试环境验证
     - 使用滚动升级
     - 准备回滚方案
   
   成本优化:
     - 合理设置资源限制
     - 使用节点亲和性优化调度
     - 定期清理未使用的vcluster

6. 故障排查:
   常见问题:
     - Pod无法启动: 检查资源配额和镜像
     - 网络不通: 验证NetworkPolicy和DNS
     - 性能下降: 检查etcd和API Server负载
   
   调试工具:
     - kubectl logs: 查看控制平面日志
     - kubectl exec: 进入容器调试
     - vcluster connect: 连接到虚拟集群
     - kubectl describe: 查看资源详情
   
   日志收集:
     - 收集控制平面日志
     - 收集同步器日志
     - 收集host集群相关日志
```

**虚拟集群使用场景**

```yaml
# 虚拟集群典型使用场景

场景1: 多租户SaaS平台
  需求:
    - 为每个客户提供独立的Kubernetes环境
    - 完全的资源和API隔离
    - 独立的版本和配置
  
  方案:
    - 每个客户一个vcluster
    - 使用ResourceQuota限制资源
    - 配置独立的Ingress和域名
    - 实施严格的网络隔离
  
  优势:
    - 强隔离保证安全性
    - 客户可以自由管理资源
    - 降低运维复杂度

场景2: 开发测试环境
  需求:
    - 快速创建和销毁环境
    - 与生产环境隔离
    - 支持多个并行测试
  
  方案:
    - 为每个分支/PR创建临时vcluster
    - 测试完成后自动清理
    - 使用较小的资源配额
    - 共享镜像仓库和存储
  
  优势:
    - 快速环境准备
    - 完全隔离避免干扰
    - 节省成本

场景3: CI/CD流水线
  需求:
    - 并行执行多个构建任务
    - 隔离不同的构建环境
    - 自动化管理
  
  方案:
    - 为每个构建任务创建临时vcluster
    - 使用GitOps管理配置
    - 集成到CI/CD工具链
    - 构建完成后自动清理
  
  优势:
    - 提高并行度
    - 避免环境污染
    - 可重复的构建环境

场景4: 培训和演示
  需求:
    - 为学员提供独立环境
    - 快速重置环境
    - 限制资源使用
  
  方案:
    - 为每个学员创建vcluster
    - 预配置常用工具和应用
    - 设置资源配额防止滥用
    - 定期清理和重置
  
  优势:
    - 学员有完整的管理权限
    - 互不干扰
    - 易于管理和重置
```



### 10.1.4 多租户最佳实践

**多租户架构选型决策树**

```yaml
# 多租户方案选择指南

决策因素:
  1. 租户信任级别
  2. 隔离强度要求
  3. 成本预算
  4. 运维复杂度
  5. 性能要求
  6. 合规性要求

决策流程:
  问题1: 租户是否来自不同组织？
    是 -> 考虑硬多租户或虚拟集群
    否 -> 继续问题2
  
  问题2: 是否需要独立的Kubernetes版本？
    是 -> 使用虚拟集群
    否 -> 继续问题3
  
  问题3: 是否需要隔离CRD和集群资源？
    是 -> 使用虚拟集群
    否 -> 继续问题4
  
  问题4: 预算是否充足？
    是 -> 使用虚拟集群或独立集群
    否 -> 使用Namespace隔离
  
  问题5: 是否有严格的合规要求？
    是 -> 使用独立集群
    否 -> 根据前面的答案选择方案

推荐方案:
  场景A - 企业内部团队:
    方案: Namespace + RBAC + ResourceQuota
    理由: 成本低，管理简单，隔离足够
  
  场景B - 多客户SaaS:
    方案: 虚拟集群（vcluster）
    理由: 强隔离，独立管理，成本可控
  
  场景C - 金融/医疗等高合规行业:
    方案: 独立物理集群
    理由: 最强隔离，满足合规要求
  
  场景D - 开发测试环境:
    方案: 虚拟集群（临时）
    理由: 快速创建销毁，完全隔离
```

**多租户安全加固**

```yaml
# multi-tenant-security.yaml
# 多租户安全加固配置

---
# PodSecurityPolicy（已废弃，使用Pod Security Standards）
# Pod Security Standards配置
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-alpha
  labels:
    # 设置Pod安全标准
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
spec: {}

---
# 使用Gatekeeper/OPA实施策略
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        openAPIV3Schema:
          type: object
          properties:
            labels:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels
        
        violation[{"msg": msg, "details": {"missing_labels": missing}}] {
          provided := {label | input.review.object.metadata.labels[label]}
          required := {label | label := input.parameters.labels[_]}
          missing := required - provided
          count(missing) > 0
          msg := sprintf("必须包含标签: %v", [missing])
        }

---
# 应用策略：要求所有Pod必须有租户标签
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: require-tenant-label
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - tenant-alpha
      - tenant-beta
  parameters:
    labels:
      - "tenant"
      - "environment"
      - "cost-center"

---
# 限制容器镜像来源
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8sallowedrepos
spec:
  crd:
    spec:
      names:
        kind: K8sAllowedRepos
      validation:
        openAPIV3Schema:
          type: object
          properties:
            repos:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sallowedrepos
        
        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          satisfied := [good | repo = input.parameters.repos[_]
                              good = startswith(container.image, repo)]
          not any(satisfied)
          msg := sprintf("容器镜像 %v 不在允许的仓库列表中", [container.image])
        }

---
# 应用策略：限制镜像仓库
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sAllowedRepos
metadata:
  name: allowed-repos
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - tenant-alpha
  parameters:
    repos:
      - "registry.example.com/"
      - "docker.io/library/"

---
# 禁止特权容器
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8spsprivilegedcontainer
spec:
  crd:
    spec:
      names:
        kind: K8sPSPrivilegedContainer
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8spsprivileged
        
        violation[{"msg": msg}] {
          c := input_containers[_]
          c.securityContext.privileged
          msg := sprintf("禁止使用特权容器: %v", [c.name])
        }
        
        input_containers[c] {
          c := input.review.object.spec.containers[_]
        }
        
        input_containers[c] {
          c := input.review.object.spec.initContainers[_]
        }

---
# 应用策略：禁止特权容器
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sPSPrivilegedContainer
metadata:
  name: deny-privileged-containers
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - tenant-alpha
      - tenant-beta
```

**多租户成本管理**

```go
// cost-manager.go
// 多租户成本管理和计费系统

package main

import (
	"context"
	"fmt"
	"log"
	"time"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
	metricsv "k8s.io/metrics/pkg/client/clientset/versioned"
)

// CostManager 成本管理器
type CostManager struct {
	clientset        *kubernetes.Clientset
	metricsClientset *metricsv.Clientset
	
	// 价格配置（每小时）
	cpuPrice     float64 // 每核CPU价格
	memoryPrice  float64 // 每GB内存价格
	storagePrice float64 // 每GB存储价格
}

// TenantCost 租户成本
type TenantCost struct {
	TenantName    string
	Namespace     string
	Period        string
	
	// 资源使用量
	CPUCoreHours    float64
	MemoryGBHours   float64
	StorageGBHours  float64
	
	// 成本
	CPUCost         float64
	MemoryCost      float64
	StorageCost     float64
	TotalCost       float64
	
	// 资源详情
	PodCount        int
	ServiceCount    int
	PVCCount        int
}

// NewCostManager 创建成本管理器
func NewCostManager(kubeconfig string) (*CostManager, error) {
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create clientset: %v", err)
	}
	
	metricsClientset, err := metricsv.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create metrics clientset: %v", err)
	}
	
	return &CostManager{
		clientset:        clientset,
		metricsClientset: metricsClientset,
		cpuPrice:         0.05,  // $0.05/核/小时
		memoryPrice:      0.01,  // $0.01/GB/小时
		storagePrice:     0.001, // $0.001/GB/小时
	}, nil
}

// CalculateTenantCost 计算租户成本
func (cm *CostManager) CalculateTenantCost(ctx context.Context, namespace string, hours float64) (*TenantCost, error) {
	cost := &TenantCost{
		Namespace: namespace,
		Period:    fmt.Sprintf("%.1f hours", hours),
	}
	
	// 1. 获取Pod指标
	podMetrics, err := cm.metricsClientset.MetricsV1beta1().PodMetricses(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get pod metrics: %v", err)
	}
	
	// 计算CPU和内存使用
	var totalCPU, totalMemory float64
	for _, pm := range podMetrics.Items {
		for _, container := range pm.Containers {
			// CPU（毫核转换为核）
			cpuMillis := float64(container.Usage.Cpu().MilliValue())
			totalCPU += cpuMillis / 1000.0
			
			// 内存（字节转换为GB）
			memoryBytes := float64(container.Usage.Memory().Value())
			totalMemory += memoryBytes / (1024 * 1024 * 1024)
		}
	}
	
	cost.CPUCoreHours = totalCPU * hours
	cost.MemoryGBHours = totalMemory * hours
	cost.CPUCost = cost.CPUCoreHours * cm.cpuPrice
	cost.MemoryCost = cost.MemoryGBHours * cm.memoryPrice
	
	// 2. 获取存储使用
	pvcs, err := cm.clientset.CoreV1().PersistentVolumeClaims(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get PVCs: %v", err)
	}
	
	var totalStorage float64
	for _, pvc := range pvcs.Items {
		if pvc.Status.Phase == corev1.ClaimBound {
			storageBytes := float64(pvc.Spec.Resources.Requests.Storage().Value())
			totalStorage += storageBytes / (1024 * 1024 * 1024)
		}
	}
	
	cost.StorageGBHours = totalStorage * hours
	cost.StorageCost = cost.StorageGBHours * cm.storagePrice
	cost.PVCCount = len(pvcs.Items)
	
	// 3. 获取资源数量
	pods, err := cm.clientset.CoreV1().Pods(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get pods: %v", err)
	}
	cost.PodCount = len(pods.Items)
	
	services, err := cm.clientset.CoreV1().Services(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get services: %v", err)
	}
	cost.ServiceCount = len(services.Items)
	
	// 4. 计算总成本
	cost.TotalCost = cost.CPUCost + cost.MemoryCost + cost.StorageCost
	
	// 从Namespace标签获取租户名称
	ns, err := cm.clientset.CoreV1().Namespaces().Get(ctx, namespace, metav1.GetOptions{})
	if err == nil {
		cost.TenantName = ns.Labels["tenant"]
	}
	
	return cost, nil
}

// GenerateCostReport 生成成本报告
func (cm *CostManager) GenerateCostReport(ctx context.Context, tenantLabel string) ([]*TenantCost, error) {
	// 获取所有租户Namespace
	namespaces, err := cm.clientset.CoreV1().Namespaces().List(ctx, metav1.ListOptions{
		LabelSelector: labels.SelectorFromSet(map[string]string{
			"tenant": tenantLabel,
		}).String(),
	})
	if err != nil {
		return nil, fmt.Errorf("failed to list namespaces: %v", err)
	}
	
	var costs []*TenantCost
	for _, ns := range namespaces.Items {
		// 计算过去24小时的成本
		cost, err := cm.CalculateTenantCost(ctx, ns.Name, 24.0)
		if err != nil {
			log.Printf("Failed to calculate cost for %s: %v", ns.Name, err)
			continue
		}
		costs = append(costs, cost)
	}
	
	return costs, nil
}

// ExportCostReport 导出成本报告
func (cm *CostManager) ExportCostReport(costs []*TenantCost) string {
	report := "租户成本报告\n"
	report += "=" + "\n\n"
	
	var totalCost float64
	for _, cost := range costs {
		report += fmt.Sprintf("租户: %s (Namespace: %s)\n", cost.TenantName, cost.Namespace)
		report += fmt.Sprintf("  周期: %s\n", cost.Period)
		report += fmt.Sprintf("  资源:\n")
		report += fmt.Sprintf("    - Pod数量: %d\n", cost.PodCount)
		report += fmt.Sprintf("    - Service数量: %d\n", cost.ServiceCount)
		report += fmt.Sprintf("    - PVC数量: %d\n", cost.PVCCount)
		report += fmt.Sprintf("  使用量:\n")
		report += fmt.Sprintf("    - CPU: %.2f 核·小时\n", cost.CPUCoreHours)
		report += fmt.Sprintf("    - 内存: %.2f GB·小时\n", cost.MemoryGBHours)
		report += fmt.Sprintf("    - 存储: %.2f GB·小时\n", cost.StorageGBHours)
		report += fmt.Sprintf("  成本:\n")
		report += fmt.Sprintf("    - CPU成本: $%.2f\n", cost.CPUCost)
		report += fmt.Sprintf("    - 内存成本: $%.2f\n", cost.MemoryCost)
		report += fmt.Sprintf("    - 存储成本: $%.2f\n", cost.StorageCost)
		report += fmt.Sprintf("    - 总成本: $%.2f\n", cost.TotalCost)
		report += "\n"
		
		totalCost += cost.TotalCost
	}
	
	report += fmt.Sprintf("总计成本: $%.2f\n", totalCost)
	
	return report
}

// SetupCostAlerts 设置成本告警
func (cm *CostManager) SetupCostAlerts(ctx context.Context, namespace string, threshold float64) error {
	// 这里简化实现，实际应该集成到告警系统
	cost, err := cm.CalculateTenantCost(ctx, namespace, 24.0)
	if err != nil {
		return err
	}
	
	if cost.TotalCost > threshold {
		log.Printf("⚠️  成本告警: 租户 %s 的日成本 $%.2f 超过阈值 $%.2f",
			cost.TenantName, cost.TotalCost, threshold)
		// 发送告警通知
		// sendAlert(cost)
	}
	
	return nil
}

// 使用示例
func main() {
	ctx := context.Background()
	
	// 创建成本管理器
	manager, err := NewCostManager("/path/to/kubeconfig")
	if err != nil {
		log.Fatalf("Failed to create cost manager: %v", err)
	}
	
	// 计算单个租户成本
	cost, err := manager.CalculateTenantCost(ctx, "tenant-alpha", 24.0)
	if err != nil {
		log.Fatalf("Failed to calculate cost: %v", err)
	}
	
	fmt.Printf("租户: %s\n", cost.TenantName)
	fmt.Printf("总成本: $%.2f\n", cost.TotalCost)
	
	// 生成所有租户的成本报告
	costs, err := manager.GenerateCostReport(ctx, "")
	if err != nil {
		log.Fatalf("Failed to generate report: %v", err)
	}
	
	report := manager.ExportCostReport(costs)
	fmt.Println(report)
	
	// 设置成本告警
	if err := manager.SetupCostAlerts(ctx, "tenant-alpha", 100.0); err != nil {
		log.Printf("Failed to setup alerts: %v", err)
	}
}
```

**多租户运维自动化**

```yaml
# tenant-operator.yaml
# 使用Operator模式自动化租户管理

apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: tenants.multitenancy.example.com
spec:
  group: multitenancy.example.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                # 租户基本信息
                displayName:
                  type: string
                contact:
                  type: string
                costCenter:
                  type: string
                environment:
                  type: string
                  enum: [development, staging, production]
                
                # 资源配额
                resourceQuota:
                  type: object
                  properties:
                    cpu:
                      type: string
                    memory:
                      type: string
                    storage:
                      type: string
                    pods:
                      type: integer
                
                # 网络配置
                networking:
                  type: object
                  properties:
                    isolation:
                      type: boolean
                    allowedNamespaces:
                      type: array
                      items:
                        type: string
                    externalAccess:
                      type: boolean
                
                # 安全配置
                security:
                  type: object
                  properties:
                    podSecurityStandard:
                      type: string
                      enum: [privileged, baseline, restricted]
                    allowPrivileged:
                      type: boolean
                    allowedRegistries:
                      type: array
                      items:
                        type: string
                
                # 用户和权限
                users:
                  type: array
                  items:
                    type: object
                    properties:
                      email:
                        type: string
                      role:
                        type: string
                        enum: [admin, developer, viewer]
            
            status:
              type: object
              properties:
                phase:
                  type: string
                  enum: [Pending, Active, Terminating, Failed]
                conditions:
                  type: array
                  items:
                    type: object
                    properties:
                      type:
                        type: string
                      status:
                        type: string
                      reason:
                        type: string
                      message:
                        type: string
                      lastTransitionTime:
                        type: string
                        format: date-time
                resourceUsage:
                  type: object
                  properties:
                    cpu:
                      type: string
                    memory:
                      type: string
                    storage:
                      type: string
                    pods:
                      type: integer
  scope: Cluster
  names:
    plural: tenants
    singular: tenant
    kind: Tenant
    shortNames:
    - tn

---
# 租户CR示例
apiVersion: multitenancy.example.com/v1
kind: Tenant
metadata:
  name: alpha-team
spec:
  displayName: "Alpha Team"
  contact: "alpha-team@example.com"
  costCenter: "engineering"
  environment: production
  
  resourceQuota:
    cpu: "100"
    memory: "200Gi"
    storage: "1Ti"
    pods: 500
  
  networking:
    isolation: true
    allowedNamespaces:
      - shared-services
      - monitoring
    externalAccess: true
  
  security:
    podSecurityStandard: baseline
    allowPrivileged: false
    allowedRegistries:
      - "registry.example.com"
      - "docker.io/library"
  
  users:
    - email: "alice@example.com"
      role: admin
    - email: "bob@example.com"
      role: developer
    - email: "charlie@example.com"
      role: viewer
```

**多租户监控和可观测性**

```yaml
# tenant-monitoring.yaml
# 租户级别的监控配置

---
# Prometheus规则：租户资源使用监控
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: tenant-resource-monitoring
  namespace: monitoring
spec:
  groups:
  - name: tenant-resources
    interval: 30s
    rules:
    # CPU配额使用率
    - record: tenant:cpu_quota_usage:ratio
      expr: |
        sum by (namespace) (
          rate(container_cpu_usage_seconds_total{container!=""}[5m])
        ) / on(namespace) group_left()
        kube_resourcequota{resource="requests.cpu", type="hard"}
    
    # 内存配额使用率
    - record: tenant:memory_quota_usage:ratio
      expr: |
        sum by (namespace) (
          container_memory_working_set_bytes{container!=""}
        ) / on(namespace) group_left()
        kube_resourcequota{resource="requests.memory", type="hard"}
    
    # Pod数量配额使用率
    - record: tenant:pod_quota_usage:ratio
      expr: |
        count by (namespace) (
          kube_pod_info
        ) / on(namespace) group_left()
        kube_resourcequota{resource="pods", type="hard"}
    
    # CPU配额使用率告警
    - alert: TenantCPUQuotaHigh
      expr: tenant:cpu_quota_usage:ratio > 0.8
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "租户CPU配额使用率过高"
        description: "租户 {{ $labels.namespace }} 的CPU使用率为 {{ $value | humanizePercentage }}"
    
    # 内存配额使用率告警
    - alert: TenantMemoryQuotaHigh
      expr: tenant:memory_quota_usage:ratio > 0.8
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "租户内存配额使用率过高"
        description: "租户 {{ $labels.namespace }} 的内存使用率为 {{ $value | humanizePercentage }}"
    
    # Pod配额即将耗尽
    - alert: TenantPodQuotaNearLimit
      expr: tenant:pod_quota_usage:ratio > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "租户Pod配额即将耗尽"
        description: "租户 {{ $labels.namespace }} 的Pod使用率为 {{ $value | humanizePercentage }}"

---
# Grafana Dashboard for Tenants
apiVersion: v1
kind: ConfigMap
metadata:
  name: tenant-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  tenant-overview.json: |
    {
      "dashboard": {
        "title": "Multi-Tenant Overview",
        "panels": [
          {
            "title": "租户资源配额使用率",
            "targets": [
              {
                "expr": "tenant:cpu_quota_usage:ratio",
                "legendFormat": "{{ namespace }} - CPU"
              },
              {
                "expr": "tenant:memory_quota_usage:ratio",
                "legendFormat": "{{ namespace }} - Memory"
              }
            ],
            "type": "graph"
          },
          {
            "title": "租户Pod数量",
            "targets": [
              {
                "expr": "count by (namespace) (kube_pod_info)"
              }
            ],
            "type": "graph"
          },
          {
            "title": "租户成本分布",
            "targets": [
              {
                "expr": "sum by (namespace) (tenant_cost_total)"
              }
            ],
            "type": "piechart"
          },
          {
            "title": "租户网络流量",
            "targets": [
              {
                "expr": "sum by (namespace) (rate(container_network_receive_bytes_total[5m]))"
              }
            ],
            "type": "graph"
          }
        ]
      }
    }
```

**多租户故障排查指南**

```bash
#!/bin/bash
# tenant-troubleshooting.sh
# 多租户故障排查工具

set -e

TENANT_NS="$1"

if [ -z "$TENANT_NS" ]; then
    echo "用法: $0 <tenant-namespace>"
    exit 1
fi

echo "=== 租户故障排查: $TENANT_NS ==="
echo ""

# 1. 检查Namespace状态
echo "1. Namespace状态:"
kubectl get ns $TENANT_NS -o yaml | grep -A 5 "status:"
echo ""

# 2. 检查ResourceQuota使用情况
echo "2. ResourceQuota使用情况:"
kubectl describe resourcequota -n $TENANT_NS
echo ""

# 3. 检查LimitRange配置
echo "3. LimitRange配置:"
kubectl describe limitrange -n $TENANT_NS
echo ""

# 4. 检查Pod状态
echo "4. Pod状态:"
kubectl get pods -n $TENANT_NS -o wide
echo ""

# 5. 检查失败的Pod
echo "5. 失败的Pod详情:"
kubectl get pods -n $TENANT_NS --field-selector=status.phase!=Running,status.phase!=Succeeded
for pod in $(kubectl get pods -n $TENANT_NS --field-selector=status.phase!=Running,status.phase!=Succeeded -o jsonpath='{.items[*].metadata.name}'); do
    echo "Pod: $pod"
    kubectl describe pod $pod -n $TENANT_NS | grep -A 10 "Events:"
    echo ""
done

# 6. 检查NetworkPolicy
echo "6. NetworkPolicy配置:"
kubectl get networkpolicy -n $TENANT_NS
echo ""

# 7. 检查RBAC权限
echo "7. RBAC权限:"
kubectl get role,rolebinding -n $TENANT_NS
echo ""

# 8. 检查事件
echo "8. 最近的事件:"
kubectl get events -n $TENANT_NS --sort-by='.lastTimestamp' | tail -20
echo ""

# 9. 检查资源使用情况
echo "9. 实际资源使用:"
kubectl top pods -n $TENANT_NS 2>/dev/null || echo "Metrics Server未安装"
echo ""

# 10. 检查PVC状态
echo "10. PVC状态:"
kubectl get pvc -n $TENANT_NS
echo ""

# 11. 检查Service和Endpoints
echo "11. Service和Endpoints:"
kubectl get svc,endpoints -n $TENANT_NS
echo ""

# 12. 生成诊断报告
echo "12. 生成诊断报告..."
REPORT_FILE="tenant-${TENANT_NS}-diagnostic-$(date +%Y%m%d-%H%M%S).txt"
{
    echo "租户诊断报告: $TENANT_NS"
    echo "生成时间: $(date)"
    echo "="
    echo ""
    
    echo "Namespace信息:"
    kubectl get ns $TENANT_NS -o yaml
    echo ""
    
    echo "所有资源:"
    kubectl get all -n $TENANT_NS
    echo ""
    
    echo "ResourceQuota:"
    kubectl get resourcequota -n $TENANT_NS -o yaml
    echo ""
    
    echo "NetworkPolicy:"
    kubectl get networkpolicy -n $TENANT_NS -o yaml
    echo ""
    
    echo "最近事件:"
    kubectl get events -n $TENANT_NS --sort-by='.lastTimestamp'
} > $REPORT_FILE

echo "诊断报告已保存到: $REPORT_FILE"
echo ""

echo "=== 故障排查完成 ==="
```

**多租户最佳实践总结**

```yaml
# 多租户实施路线图

阶段1: 规划设计（1-2周）
  任务:
    - 评估多租户需求和场景
    - 选择合适的多租户模型
    - 设计资源配额和隔离策略
    - 规划网络和安全架构
  
  交付物:
    - 多租户架构设计文档
    - 资源配额规划表
    - 安全策略文档
    - 实施计划

阶段2: 基础设施准备（1-2周）
  任务:
    - 部署和配置Kubernetes集群
    - 安装必要的插件（CNI、CSI、Ingress）
    - 配置监控和日志系统
    - 准备镜像仓库和存储
  
  交付物:
    - 就绪的Kubernetes集群
    - 监控和日志系统
    - 基础设施文档

阶段3: 多租户实施（2-3周）
  任务:
    - 创建租户Namespace和配额
    - 配置RBAC和网络策略
    - 部署策略引擎（OPA/Gatekeeper）
    - 实施虚拟集群（如需要）
    - 配置租户监控和告警
  
  交付物:
    - 租户环境
    - 策略配置
    - 监控仪表盘
    - 运维手册

阶段4: 测试验证（1-2周）
  任务:
    - 功能测试（资源隔离、网络隔离）
    - 性能测试（负载测试、压力测试）
    - 安全测试（渗透测试、合规检查）
    - 故障演练（混沌工程）
  
  交付物:
    - 测试报告
    - 性能基准
    - 安全评估报告
    - 改进建议

阶段5: 上线和优化（持续）
  任务:
    - 租户迁移和上线
    - 监控和调优
    - 成本优化
    - 持续改进
  
  交付物:
    - 上线报告
    - 优化建议
    - 运维文档更新
    - 最佳实践总结

# 关键成功因素

技术层面:
  1. 选择合适的多租户模型
  2. 实施多层次的隔离机制
  3. 建立完善的监控和告警
  4. 自动化租户管理流程
  5. 定期进行安全审计

管理层面:
  1. 明确的租户SLA
  2. 透明的成本计费
  3. 完善的文档和培训
  4. 快速的问题响应机制
  5. 持续的优化和改进

# 常见陷阱和避免方法

陷阱1: 过度隔离
  问题: 隔离过于严格导致资源浪费和管理复杂
  避免: 根据实际需求选择合适的隔离级别

陷阱2: 配额设置不合理
  问题: 配额过小影响业务，过大浪费资源
  避免: 基于历史数据和业务预测设置配额，定期调整

陷阱3: 忽视网络隔离
  问题: 租户之间可以互相访问，存在安全风险
  避免: 实施默认拒绝的NetworkPolicy

陷阱4: 缺乏监控和告警
  问题: 无法及时发现和解决问题
  避免: 建立完善的监控体系和告警机制

陷阱5: 手动管理租户
  问题: 效率低下，容易出错
  避免: 使用Operator或自动化工具管理租户

# 检查清单

部署前检查:
  ☐ 多租户架构设计已评审
  ☐ 资源配额已规划
  ☐ 网络策略已配置
  ☐ RBAC权限已设置
  ☐ 监控和日志已就绪
  ☐ 备份恢复已测试
  ☐ 文档已完善

运行时检查:
  ☐ 资源配额使用率正常
  ☐ 网络隔离有效
  ☐ 无权限泄露
  ☐ 监控告警正常
  ☐ 成本在预算内
  ☐ 租户满意度高

定期审查:
  ☐ 每月审查资源使用情况
  ☐ 每季度审查安全策略
  ☐ 每半年审查架构设计
  ☐ 每年进行全面评估
```

**本节总结**

在本节中，我们深入探讨了Kubernetes多租户与资源隔离的各个方面：

1. **多租户架构概述**：介绍了多租户的概念、价值和挑战，以及三种多租户模型（软多租户、硬多租户、混合多租户）的特点和适用场景。

2. **Namespace级别的资源隔离**：详细讲解了如何使用Namespace、ResourceQuota、LimitRange、NetworkPolicy和RBAC实现基础的多租户隔离，并提供了完整的配置示例和自动化管理工具。

3. **虚拟集群技术**：深入介绍了vcluster等虚拟集群解决方案，展示了如何在物理集群之上创建逻辑隔离的虚拟集群，提供更强的隔离和独立性。

4. **多租户最佳实践**：总结了多租户架构选型、安全加固、成本管理、运维自动化等方面的最佳实践，并提供了实施路线图和检查清单。

**关键要点**：

- 根据信任级别和隔离需求选择合适的多租户模型
- 实施多层次的隔离机制（资源、网络、安全）
- 使用自动化工具简化租户管理
- 建立完善的监控、告警和成本管理体系
- 定期审查和优化多租户配置

**下一节预告**：

在下一节中，我们将学习集群联邦与多集群管理，探讨如何管理跨地域、跨云的多个Kubernetes集群，实现统一的资源调度和故障转移。



## 10.2 集群联邦与多集群管理

### 10.2.1 多集群架构设计

**为什么需要多集群**

在企业级Kubernetes实践中，单一集群往往无法满足所有需求。多集群架构成为必然选择。

**多集群的驱动因素**：

```yaml
# 多集群需求场景

1. 高可用和灾难恢复:
   需求:
     - 避免单点故障
     - 跨地域容灾
     - 业务连续性保障
   
   方案:
     - 主备集群架构
     - 多活集群架构
     - 跨区域部署

2. 地理分布和就近访问:
   需求:
     - 降低网络延迟
     - 满足数据主权要求
     - 提升用户体验
   
   方案:
     - 按地域部署集群
     - 边缘计算集群
     - CDN集成

3. 环境隔离:
   需求:
     - 开发、测试、生产环境分离
     - 不同安全级别隔离
     - 合规性要求
   
   方案:
     - 按环境划分集群
     - 按安全级别划分
     - 按合规要求划分

4. 资源和成本优化:
   需求:
     - 利用不同云厂商优势
     - 避免厂商锁定
     - 成本优化
   
   方案:
     - 混合云架构
     - 多云架构
     - 按需弹性扩展

5. 组织和团队隔离:
   需求:
     - 不同业务线独立管理
     - 团队自主权
     - 爆炸半径控制
   
   方案:
     - 按业务线划分集群
     - 按团队划分集群
     - 层级化管理

6. 技术和版本多样性:
   需求:
     - 支持不同Kubernetes版本
     - 渐进式升级
     - 技术栈多样性
   
   方案:
     - 版本分离集群
     - 金丝雀集群
     - 实验性集群
```

**多集群架构模式**

```yaml
# 多集群架构的主要模式

模式1: 独立集群（Independent Clusters）
  描述: 每个集群完全独立运行，无统一管理
  
  特点:
    - 集群间无依赖
    - 独立的控制平面和数据平面
    - 手动管理和协调
  
  优势:
    - 实现简单
    - 故障隔离好
    - 灵活性高
  
  劣势:
    - 管理复杂度高
    - 资源利用率低
    - 缺乏统一视图
  
  适用场景:
    - 小规模部署（<5个集群）
    - 完全独立的业务
    - 临时或实验性集群

---

模式2: 主从集群（Hub-Spoke）
  描述: 一个主集群管理多个从集群
  
  架构:
    主集群（Hub）:
      - 统一的管理平面
      - 策略和配置中心
      - 监控和日志聚合
    
    从集群（Spoke）:
      - 执行工作负载
      - 接收主集群指令
      - 上报状态和指标
  
  优势:
    - 集中管理
    - 统一视图
    - 策略一致性
  
  劣势:
    - 主集群单点故障
    - 扩展性受限
    - 网络依赖强
  
  适用场景:
    - 中等规模部署（5-20个集群）
    - 需要集中管理
    - 地理分布的边缘集群

---

模式3: 网格集群（Mesh）
  描述: 集群间对等连接，无中心节点
  
  特点:
    - 去中心化
    - 集群间直接通信
    - 分布式决策
  
  优势:
    - 无单点故障
    - 扩展性好
    - 灵活性高
  
  劣势:
    - 实现复杂
    - 管理难度大
    - 网络开销大
  
  适用场景:
    - 大规模部署（>20个集群）
    - 需要高可用
    - 复杂的跨集群通信

---

模式4: 联邦集群（Federation）
  描述: 使用联邦控制平面统一管理多个集群
  
  架构:
    联邦控制平面:
      - 统一的API入口
      - 资源分发和调度
      - 跨集群协调
    
    成员集群:
      - 独立的Kubernetes集群
      - 接收联邦指令
      - 保持自主性
  
  优势:
    - 统一管理接口
    - 跨集群资源调度
    - 高可用和容灾
  
  劣势:
    - 实现复杂
    - 学习曲线陡峭
    - 额外的管理开销
  
  适用场景:
    - 需要跨集群资源调度
    - 多地域部署
    - 高可用要求高
```

**多集群网络架构**

```yaml
# 多集群网络连接方案

方案1: VPN隧道
  描述: 通过VPN建立集群间的安全连接
  
  实现:
    - IPSec VPN
    - WireGuard
    - OpenVPN
  
  特点:
    - 加密传输
    - 跨云支持
    - 配置相对简单
  
  性能:
    - 延迟: 中等（+5-20ms）
    - 吞吐: 受VPN网关限制
    - 开销: 加密解密开销
  
  适用场景:
    - 跨云连接
    - 安全要求高
    - 流量不大

---

方案2: 专线连接
  描述: 使用云厂商提供的专线服务
  
  实现:
    - AWS Direct Connect
    - Azure ExpressRoute
    - GCP Cloud Interconnect
    - 阿里云高速通道
  
  特点:
    - 低延迟
    - 高带宽
    - 稳定可靠
  
  性能:
    - 延迟: 低（<5ms）
    - 吞吐: 高（1-100Gbps）
    - 开销: 成本高
  
  适用场景:
    - 大流量场景
    - 延迟敏感应用
    - 混合云架构

---

方案3: Service Mesh
  描述: 使用服务网格实现跨集群通信
  
  实现:
    - Istio Multi-Cluster
    - Linkerd Multi-Cluster
    - Consul Connect
  
  特点:
    - 应用层连接
    - 流量管理
    - 可观测性
  
  性能:
    - 延迟: 中等（+2-10ms）
    - 吞吐: 受Sidecar限制
    - 开销: 额外的Proxy
  
  适用场景:
    - 微服务架构
    - 需要高级流量管理
    - 已使用Service Mesh

---

方案4: Submariner
  描述: 专为Kubernetes设计的跨集群网络方案
  
  实现:
    - Gateway节点
    - IPSec隧道
    - Service Discovery
  
  特点:
    - Kubernetes原生
    - Pod到Pod直连
    - Service跨集群访问
  
  性能:
    - 延迟: 低（<5ms）
    - 吞吐: 高
    - 开销: 较小
  
  适用场景:
    - Kubernetes专用
    - 需要Pod直连
    - 开源方案优先
```

**多集群存储架构**

```yaml
# 多集群存储策略

策略1: 独立存储
  描述: 每个集群使用独立的存储系统
  
  实现:
    - 本地存储（Local PV）
    - 云厂商存储（EBS、Azure Disk）
    - 分布式存储（Ceph、GlusterFS）
  
  优势:
    - 简单直接
    - 性能好
    - 故障隔离
  
  劣势:
    - 数据不共享
    - 迁移困难
    - 成本高
  
  适用场景:
    - 无状态应用为主
    - 数据本地化要求
    - 集群间无数据共享需求

---

策略2: 跨集群存储
  描述: 使用支持跨集群的存储系统
  
  实现:
    - Portworx
    - StorageOS
    - Rook/Ceph（跨集群配置）
  
  优势:
    - 数据共享
    - 统一管理
    - 支持迁移
  
  劣势:
    - 实现复杂
    - 性能开销
    - 网络依赖
  
  适用场景:
    - 有状态应用
    - 需要数据共享
    - 跨集群迁移

---

策略3: 对象存储
  描述: 使用对象存储作为共享存储
  
  实现:
    - S3（AWS、MinIO）
    - Azure Blob Storage
    - Google Cloud Storage
  
  优势:
    - 天然跨集群
    - 高可用
    - 成本低
  
  劣势:
    - 非POSIX接口
    - 延迟较高
    - 不适合所有场景
  
  适用场景:
    - 文件存储
    - 备份归档
    - 大数据处理

---

策略4: 数据库复制
  描述: 使用数据库的复制功能实现跨集群数据同步
  
  实现:
    - MySQL主从复制
    - PostgreSQL流复制
    - MongoDB副本集
    - Redis Cluster
  
  优势:
    - 数据库原生支持
    - 性能好
    - 成熟稳定
  
  劣势:
    - 应用层实现
    - 复杂度高
    - 一致性挑战
  
  适用场景:
    - 数据库应用
    - 读写分离
    - 多活架构
```

**多集群架构设计原则**

```yaml
# 多集群设计的核心原则

1. 松耦合原则:
   描述: 集群间应保持松耦合，减少依赖
   
   实践:
     - 避免跨集群的强依赖
     - 使用异步通信
     - 设计容错机制
     - 支持独立运行
   
   反模式:
     - 跨集群的同步调用
     - 共享状态
     - 紧密耦合的服务

2. 自治原则:
   描述: 每个集群应能独立运行和决策
   
   实践:
     - 本地化数据和配置
     - 独立的控制平面
     - 本地故障处理
     - 降级策略
   
   反模式:
     - 依赖中心化决策
     - 单点故障
     - 无降级方案

3. 一致性原则:
   描述: 保持配置和策略的一致性
   
   实践:
     - 统一的配置管理
     - 版本控制
     - 自动化部署
     - 配置验证
   
   工具:
     - GitOps（ArgoCD、Flux）
     - Helm
     - Kustomize

4. 可观测性原则:
   描述: 建立统一的监控和日志体系
   
   实践:
     - 集中式监控
     - 分布式追踪
     - 统一日志收集
     - 跨集群告警
   
   工具:
     - Prometheus Federation
     - Thanos
     - Grafana
     - Jaeger

5. 安全性原则:
   描述: 确保跨集群通信的安全性
   
   实践:
     - mTLS加密
     - 网络隔离
     - 身份认证
     - 访问控制
   
   工具:
     - Service Mesh
     - VPN
     - Certificate Manager

6. 成本优化原则:
   描述: 平衡功能和成本
   
   实践:
     - 合理规划集群数量
     - 资源共享
     - 按需扩展
     - 成本监控
   
   策略:
     - 混合云
     - Spot实例
     - 资源预留
```

**多集群架构决策树**

```yaml
# 多集群架构选择指南

决策流程:
  
  问题1: 是否需要跨集群资源调度？
    是 -> 考虑联邦集群（KubeFed）
    否 -> 继续问题2
  
  问题2: 集群数量是多少？
    <5个 -> 考虑独立集群或主从模式
    5-20个 -> 考虑主从模式
    >20个 -> 考虑网格模式或联邦
  
  问题3: 是否需要跨集群服务通信？
    是 -> 考虑Service Mesh或Submariner
    否 -> 继续问题4
  
  问题4: 是否需要统一管理？
    是 -> 考虑主从模式或联邦
    否 -> 考虑独立集群
  
  问题5: 预算是否充足？
    是 -> 选择功能完善的方案
    否 -> 选择开源轻量方案

推荐方案:
  
  场景A - 小型企业（<5个集群）:
    方案: 独立集群 + GitOps
    理由: 简单直接，成本低
    工具: ArgoCD、Helm
  
  场景B - 中型企业（5-20个集群）:
    方案: 主从模式 + Rancher/OCM
    理由: 集中管理，易于运维
    工具: Rancher、Open Cluster Management
  
  场景C - 大型企业（>20个集群）:
    方案: 联邦集群 + Service Mesh
    理由: 扩展性好，功能完善
    工具: KubeFed、Istio Multi-Cluster
  
  场景D - 多云架构:
    方案: 混合方案（主从+联邦）
    理由: 灵活性高，避免锁定
    工具: Rancher + KubeFed
  
  场景E - 边缘计算:
    方案: 主从模式 + KubeEdge
    理由: 适合边缘场景
    工具: KubeEdge、K3s
```



### 10.2.2 KubeFed集群联邦

**KubeFed概述**

KubeFed（Kubernetes Cluster Federation）是Kubernetes官方的多集群管理解决方案，它提供了跨集群的资源分发、调度和管理能力。

**KubeFed核心概念**：

```yaml
# KubeFed架构组件

1. Host Cluster（宿主集群）:
   角色: 运行KubeFed控制平面
   组件:
     - KubeFed Controller Manager
     - KubeFed API Server
     - KubeFed Webhook
   职责:
     - 管理联邦配置
     - 协调成员集群
     - 资源分发和调度

2. Member Cluster（成员集群）:
   角色: 接收和执行联邦指令
   要求:
     - 可被Host Cluster访问
     - 注册到联邦
     - 运行工作负载
   职责:
     - 执行联邦资源
     - 上报状态
     - 本地资源管理

3. Federated Resources（联邦资源）:
   类型:
     - FederatedDeployment
     - FederatedService
     - FederatedConfigMap
     - FederatedSecret
     - FederatedNamespace
   特点:
     - 跨集群分发
     - 统一管理
     - 差异化配置

4. Placement Policy（放置策略）:
   功能: 决定资源部署到哪些集群
   策略:
     - ClusterSelector: 基于标签选择
     - Weight: 权重分配
     - Affinity: 亲和性规则
     - Spread: 分散策略

5. Override Policy（覆盖策略）:
   功能: 为不同集群定制配置
   用途:
     - 资源规格调整
     - 环境变量覆盖
     - 镜像地址替换
     - 副本数调整
```

**KubeFed安装部署**

```bash
#!/bin/bash
# install-kubefed.sh
# 安装KubeFed v2

set -e

KUBEFED_VERSION="v0.10.0"
HOST_CLUSTER_CONTEXT="host-cluster"
KUBEFED_NAMESPACE="kube-federation-system"

echo "=== 安装KubeFed ==="

# 1. 安装kubefedctl CLI
echo "1. 安装kubefedctl CLI..."
curl -LO "https://github.com/kubernetes-sigs/kubefed/releases/download/${KUBEFED_VERSION}/kubefedctl-${KUBEFED_VERSION}-linux-amd64.tgz"
tar -xzf "kubefedctl-${KUBEFED_VERSION}-linux-amd64.tgz"
sudo mv kubefedctl /usr/local/bin/
rm "kubefedctl-${KUBEFED_VERSION}-linux-amd64.tgz"

# 2. 验证安装
echo "2. 验证kubefedctl安装..."
kubefedctl version

# 3. 在Host Cluster上安装KubeFed
echo "3. 在Host Cluster上安装KubeFed..."
kubectl config use-context $HOST_CLUSTER_CONTEXT

# 创建namespace
kubectl create namespace $KUBEFED_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

# 使用Helm安装KubeFed
helm repo add kubefed-charts https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts
helm repo update

helm install kubefed kubefed-charts/kubefed   --namespace $KUBEFED_NAMESPACE   --version $KUBEFED_VERSION   --set controllermanager.replicaCount=3   --set controllermanager.resources.limits.cpu=500m   --set controllermanager.resources.limits.memory=512Mi

# 4. 等待KubeFed就绪
echo "4. 等待KubeFed就绪..."
kubectl wait --for=condition=available --timeout=300s   deployment/kubefed-controller-manager   -n $KUBEFED_NAMESPACE

# 5. 验证安装
echo "5. 验证KubeFed安装..."
kubectl get pods -n $KUBEFED_NAMESPACE
kubectl get crd | grep kubefed

echo "=== KubeFed安装完成 ==="
```

**注册成员集群**

```bash
#!/bin/bash
# join-clusters.sh
# 将集群加入联邦

set -e

HOST_CLUSTER="host-cluster"
MEMBER_CLUSTERS=("cluster-1" "cluster-2" "cluster-3")
KUBEFED_NAMESPACE="kube-federation-system"

echo "=== 注册成员集群到联邦 ==="

# 切换到Host Cluster
kubectl config use-context $HOST_CLUSTER

for cluster in "${MEMBER_CLUSTERS[@]}"; do
    echo "注册集群: $cluster"
    
    # 使用kubefedctl join命令
    kubefedctl join $cluster         --cluster-context $cluster         --host-cluster-context $HOST_CLUSTER         --kubefed-namespace $KUBEFED_NAMESPACE         --v=2
    
    # 验证注册
    kubectl get kubefedcluster $cluster -n $KUBEFED_NAMESPACE
    
    echo "集群 $cluster 注册成功"
    echo ""
done

# 查看所有注册的集群
echo "所有联邦集群:"
kubectl get kubefedclusters -n $KUBEFED_NAMESPACE

echo "=== 集群注册完成 ==="
```

**联邦资源配置**

```yaml
# federated-deployment.yaml
# 联邦Deployment示例

---
# 1. 创建联邦Namespace
apiVersion: types.kubefed.io/v1beta1
kind: FederatedNamespace
metadata:
  name: demo-app
  namespace: demo-app
spec:
  # 放置策略：部署到所有集群
  placement:
    clusters:
    - name: cluster-1
    - name: cluster-2
    - name: cluster-3

---
# 2. 联邦Deployment
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: nginx
  namespace: demo-app
spec:
  # 模板：基础配置
  template:
    metadata:
      labels:
        app: nginx
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx:1.21
            ports:
            - containerPort: 80
            resources:
              requests:
                cpu: 100m
                memory: 128Mi
              limits:
                cpu: 500m
                memory: 512Mi
  
  # 放置策略：选择性部署
  placement:
    clusterSelector:
      matchLabels:
        environment: production
  
  # 覆盖策略：差异化配置
  overrides:
  # cluster-1: 增加副本数
  - clusterName: cluster-1
    clusterOverrides:
    - path: "/spec/replicas"
      value: 5
    - path: "/spec/template/spec/containers/0/resources/limits/cpu"
      value: "1"
  
  # cluster-2: 使用不同镜像
  - clusterName: cluster-2
    clusterOverrides:
    - path: "/spec/template/spec/containers/0/image"
      value: "nginx:1.22"
  
  # cluster-3: 添加环境变量
  - clusterName: cluster-3
    clusterOverrides:
    - path: "/spec/template/spec/containers/0/env"
      value:
      - name: CLUSTER_NAME
        value: cluster-3
      - name: REGION
        value: us-west

---
# 3. 联邦Service
apiVersion: types.kubefed.io/v1beta1
kind: FederatedService
metadata:
  name: nginx
  namespace: demo-app
spec:
  template:
    metadata:
      labels:
        app: nginx
    spec:
      type: LoadBalancer
      selector:
        app: nginx
      ports:
      - port: 80
        targetPort: 80
        protocol: TCP
  
  placement:
    clusters:
    - name: cluster-1
    - name: cluster-2
    - name: cluster-3
  
  # Service类型覆盖
  overrides:
  # cluster-1使用LoadBalancer
  - clusterName: cluster-1
    clusterOverrides:
    - path: "/spec/type"
      value: LoadBalancer
  
  # cluster-2和cluster-3使用NodePort
  - clusterName: cluster-2
    clusterOverrides:
    - path: "/spec/type"
      value: NodePort
  - clusterName: cluster-3
    clusterOverrides:
    - path: "/spec/type"
      value: NodePort

---
# 4. 联邦ConfigMap
apiVersion: types.kubefed.io/v1beta1
kind: FederatedConfigMap
metadata:
  name: app-config
  namespace: demo-app
spec:
  template:
    data:
      app.conf: |
        server {
          listen 80;
          server_name _;
          location / {
            root /usr/share/nginx/html;
            index index.html;
          }
        }
      log.level: "info"
  
  placement:
    clusters:
    - name: cluster-1
    - name: cluster-2
    - name: cluster-3
  
  # 不同集群使用不同配置
  overrides:
  - clusterName: cluster-1
    clusterOverrides:
    - path: "/data/log.level"
      value: "debug"
  - clusterName: cluster-3
    clusterOverrides:
    - path: "/data/log.level"
      value: "warn"

---
# 5. 联邦Secret
apiVersion: types.kubefed.io/v1beta1
kind: FederatedSecret
metadata:
  name: app-secret
  namespace: demo-app
spec:
  template:
    type: Opaque
    stringData:
      username: admin
      password: changeme
  
  placement:
    clusters:
    - name: cluster-1
    - name: cluster-2
    - name: cluster-3
```

**高级放置策略**

```yaml
# advanced-placement.yaml
# 高级放置和调度策略

---
# 1. 基于标签的集群选择
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: web-app
  namespace: demo-app
spec:
  template:
    # ... deployment spec ...
  
  placement:
    # 选择带有特定标签的集群
    clusterSelector:
      matchLabels:
        environment: production
        region: us-west
      matchExpressions:
      - key: tier
        operator: In
        values:
        - frontend
        - backend

---
# 2. 权重分配策略
apiVersion: types.kubefed.io/v1beta1
kind: ReplicaSchedulingPreference
metadata:
  name: web-app
  namespace: demo-app
spec:
  targetKind: FederatedDeployment
  totalReplicas: 100
  
  # 按权重分配副本
  clusters:
    cluster-1:
      weight: 50  # 50个副本
    cluster-2:
      weight: 30  # 30个副本
    cluster-3:
      weight: 20  # 20个副本
  
  # 最小副本数限制
  rebalance: true
  minReplicas:
    cluster-1: 10
    cluster-2: 5
    cluster-3: 5

---
# 3. 容量感知调度
apiVersion: types.kubefed.io/v1beta1
kind: ReplicaSchedulingPreference
metadata:
  name: capacity-aware
  namespace: demo-app
spec:
  targetKind: FederatedDeployment
  totalReplicas: 50
  
  # 基于集群容量自动分配
  rebalance: true
  
  # 集群容量限制
  clusters:
    cluster-1:
      maxReplicas: 30
      minReplicas: 5
    cluster-2:
      maxReplicas: 20
      minReplicas: 5
    cluster-3:
      maxReplicas: 15
      minReplicas: 5

---
# 4. 亲和性和反亲和性
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: affinity-app
  namespace: demo-app
spec:
  template:
    # ... deployment spec ...
  
  placement:
    clusterSelector:
      matchLabels:
        environment: production
    
    # 集群亲和性
    clusterAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        clusterSelectorTerms:
        - matchExpressions:
          - key: region
            operator: In
            values:
            - us-west
            - us-east
      
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: tier
            operator: In
            values:
            - premium

---
# 5. 分散策略
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: spread-app
  namespace: demo-app
spec:
  template:
    # ... deployment spec ...
  
  placement:
    # 尽可能分散到多个集群
    clusterSelector:
      matchLabels:
        environment: production
    
    # 分散约束
    spreadConstraints:
    - maxSkew: 2  # 集群间副本数差异不超过2
      topologyKey: region
      whenUnsatisfiable: DoNotSchedule
```


**KubeFed管理工具**

```go
// kubefed-manager.go
// KubeFed管理工具

package main

import (
	"context"
	"fmt"
	"log"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic"
	"k8s.io/client-go/tools/clientcmd"
)

// KubeFedManager KubeFed管理器
type KubeFedManager struct {
	dynamicClient dynamic.Interface
	namespace     string
}

// NewKubeFedManager 创建KubeFed管理器
func NewKubeFedManager(kubeconfig, namespace string) (*KubeFedManager, error) {
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	dynamicClient, err := dynamic.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create dynamic client: %v", err)
	}
	
	return &KubeFedManager{
		dynamicClient: dynamicClient,
		namespace:     namespace,
	}, nil
}

// ListClusters 列出所有联邦集群
func (km *KubeFedManager) ListClusters(ctx context.Context) ([]ClusterInfo, error) {
	gvr := schema.GroupVersionResource{
		Group:    "core.kubefed.io",
		Version:  "v1beta1",
		Resource: "kubefedclusters",
	}
	
	list, err := km.dynamicClient.Resource(gvr).Namespace(km.namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list clusters: %v", err)
	}
	
	var clusters []ClusterInfo
	for _, item := range list.Items {
		cluster := ClusterInfo{
			Name: item.GetName(),
		}
		
		// 获取状态
		status, found, err := unstructured.NestedMap(item.Object, "status")
		if err == nil && found {
			if conditions, ok := status["conditions"].([]interface{}); ok {
				for _, cond := range conditions {
					if condMap, ok := cond.(map[string]interface{}); ok {
						if condMap["type"] == "Ready" {
							cluster.Ready = condMap["status"] == "True"
						}
					}
				}
			}
		}
		
		// 获取标签
		cluster.Labels = item.GetLabels()
		
		clusters = append(clusters, cluster)
	}
	
	return clusters, nil
}

// ClusterInfo 集群信息
type ClusterInfo struct {
	Name   string
	Ready  bool
	Labels map[string]string
}

// CreateFederatedDeployment 创建联邦Deployment
func (km *KubeFedManager) CreateFederatedDeployment(ctx context.Context, config *FederatedDeploymentConfig) error {
	gvr := schema.GroupVersionResource{
		Group:    "types.kubefed.io",
		Version:  "v1beta1",
		Resource: "federateddeployments",
	}
	
	// 构建联邦Deployment对象
	fedDeploy := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"apiVersion": "types.kubefed.io/v1beta1",
			"kind":       "FederatedDeployment",
			"metadata": map[string]interface{}{
				"name":      config.Name,
				"namespace": config.Namespace,
			},
			"spec": map[string]interface{}{
				"template": config.Template,
				"placement": map[string]interface{}{
					"clusters": config.Clusters,
				},
				"overrides": config.Overrides,
			},
		},
	}
	
	_, err := km.dynamicClient.Resource(gvr).Namespace(config.Namespace).Create(ctx, fedDeploy, metav1.CreateOptions{})
	if err != nil {
		return fmt.Errorf("failed to create federated deployment: %v", err)
	}
	
	log.Printf("Created federated deployment: %s/%s", config.Namespace, config.Name)
	return nil
}

// FederatedDeploymentConfig 联邦Deployment配置
type FederatedDeploymentConfig struct {
	Name      string
	Namespace string
	Template  map[string]interface{}
	Clusters  []map[string]interface{}
	Overrides []map[string]interface{}
}

// GetFederatedDeploymentStatus 获取联邦Deployment状态
func (km *KubeFedManager) GetFederatedDeploymentStatus(ctx context.Context, namespace, name string) (*FederatedDeploymentStatus, error) {
	gvr := schema.GroupVersionResource{
		Group:    "types.kubefed.io",
		Version:  "v1beta1",
		Resource: "federateddeployments",
	}
	
	fedDeploy, err := km.dynamicClient.Resource(gvr).Namespace(namespace).Get(ctx, name, metav1.GetOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get federated deployment: %v", err)
	}
	
	status := &FederatedDeploymentStatus{
		Name:      name,
		Namespace: namespace,
		Clusters:  make(map[string]ClusterStatus),
	}
	
	// 解析状态
	statusMap, found, err := unstructured.NestedMap(fedDeploy.Object, "status")
	if err != nil || !found {
		return status, nil
	}
	
	// 获取集群状态
	if clusters, ok := statusMap["clusters"].([]interface{}); ok {
		for _, cluster := range clusters {
			if clusterMap, ok := cluster.(map[string]interface{}); ok {
				clusterName := clusterMap["name"].(string)
				status.Clusters[clusterName] = ClusterStatus{
					Name:  clusterName,
					Ready: clusterMap["status"] == "Ready",
				}
			}
		}
	}
	
	return status, nil
}

// FederatedDeploymentStatus 联邦Deployment状态
type FederatedDeploymentStatus struct {
	Name      string
	Namespace string
	Clusters  map[string]ClusterStatus
}

// ClusterStatus 集群状态
type ClusterStatus struct {
	Name  string
	Ready bool
}

// UpdateReplicaScheduling 更新副本调度策略
func (km *KubeFedManager) UpdateReplicaScheduling(ctx context.Context, config *ReplicaSchedulingConfig) error {
	gvr := schema.GroupVersionResource{
		Group:    "scheduling.kubefed.io",
		Version:  "v1alpha1",
		Resource: "replicaschedulingpreferences",
	}
	
	rsp := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"apiVersion": "scheduling.kubefed.io/v1alpha1",
			"kind":       "ReplicaSchedulingPreference",
			"metadata": map[string]interface{}{
				"name":      config.Name,
				"namespace": config.Namespace,
			},
			"spec": map[string]interface{}{
				"targetKind":   "FederatedDeployment",
				"totalReplicas": config.TotalReplicas,
				"rebalance":    config.Rebalance,
				"clusters":     config.Clusters,
			},
		},
	}
	
	_, err := km.dynamicClient.Resource(gvr).Namespace(config.Namespace).Update(ctx, rsp, metav1.UpdateOptions{})
	if err != nil {
		return fmt.Errorf("failed to update replica scheduling: %v", err)
	}
	
	log.Printf("Updated replica scheduling: %s/%s", config.Namespace, config.Name)
	return nil
}

// ReplicaSchedulingConfig 副本调度配置
type ReplicaSchedulingConfig struct {
	Name          string
	Namespace     string
	TotalReplicas int
	Rebalance     bool
	Clusters      map[string]interface{}
}

// PropagateResource 传播资源到所有集群
func (km *KubeFedManager) PropagateResource(ctx context.Context, resourceType, namespace, name string) error {
	log.Printf("Propagating %s %s/%s to all clusters", resourceType, namespace, name)
	
	// 这里简化实现，实际需要根据资源类型创建对应的联邦资源
	// 例如：Deployment -> FederatedDeployment
	
	return nil
}

// MigrateWorkload 迁移工作负载到其他集群
func (km *KubeFedManager) MigrateWorkload(ctx context.Context, namespace, name, fromCluster, toCluster string) error {
	log.Printf("Migrating workload %s/%s from %s to %s", namespace, name, fromCluster, toCluster)
	
	// 1. 获取当前联邦Deployment
	gvr := schema.GroupVersionResource{
		Group:    "types.kubefed.io",
		Version:  "v1beta1",
		Resource: "federateddeployments",
	}
	
	fedDeploy, err := km.dynamicClient.Resource(gvr).Namespace(namespace).Get(ctx, name, metav1.GetOptions{})
	if err != nil {
		return fmt.Errorf("failed to get federated deployment: %v", err)
	}
	
	// 2. 更新placement
	placement, found, err := unstructured.NestedSlice(fedDeploy.Object, "spec", "placement", "clusters")
	if err != nil || !found {
		return fmt.Errorf("failed to get placement: %v", err)
	}
	
	// 移除fromCluster，添加toCluster
	var newPlacement []interface{}
	for _, cluster := range placement {
		if clusterMap, ok := cluster.(map[string]interface{}); ok {
			if clusterMap["name"] != fromCluster {
				newPlacement = append(newPlacement, cluster)
			}
		}
	}
	newPlacement = append(newPlacement, map[string]interface{}{
		"name": toCluster,
	})
	
	// 3. 更新资源
	if err := unstructured.SetNestedSlice(fedDeploy.Object, newPlacement, "spec", "placement", "clusters"); err != nil {
		return fmt.Errorf("failed to set placement: %v", err)
	}
	
	_, err = km.dynamicClient.Resource(gvr).Namespace(namespace).Update(ctx, fedDeploy, metav1.UpdateOptions{})
	if err != nil {
		return fmt.Errorf("failed to update federated deployment: %v", err)
	}
	
	log.Printf("Workload migrated successfully")
	return nil
}

// 使用示例
func main() {
	ctx := context.Background()
	
	// 创建管理器
	manager, err := NewKubeFedManager("/path/to/kubeconfig", "kube-federation-system")
	if err != nil {
		log.Fatalf("Failed to create manager: %v", err)
	}
	
	// 列出所有集群
	clusters, err := manager.ListClusters(ctx)
	if err != nil {
		log.Fatalf("Failed to list clusters: %v", err)
	}
	
	fmt.Println("Federated Clusters:")
	for _, cluster := range clusters {
		status := "Not Ready"
		if cluster.Ready {
			status = "Ready"
		}
		fmt.Printf("- %s: %s\n", cluster.Name, status)
	}
	
	// 创建联邦Deployment
	config := &FederatedDeploymentConfig{
		Name:      "nginx",
		Namespace: "demo-app",
		Template: map[string]interface{}{
			"spec": map[string]interface{}{
				"replicas": 3,
				// ... 其他配置
			},
		},
		Clusters: []map[string]interface{}{
			{"name": "cluster-1"},
			{"name": "cluster-2"},
		},
	}
	
	if err := manager.CreateFederatedDeployment(ctx, config); err != nil {
		log.Fatalf("Failed to create federated deployment: %v", err)
	}
	
	// 获取状态
	status, err := manager.GetFederatedDeploymentStatus(ctx, "demo-app", "nginx")
	if err != nil {
		log.Fatalf("Failed to get status: %v", err)
	}
	
	fmt.Printf("\nDeployment Status:\n")
	for clusterName, clusterStatus := range status.Clusters {
		fmt.Printf("- %s: Ready=%v\n", clusterName, clusterStatus.Ready)
	}
	
	// 迁移工作负载
	if err := manager.MigrateWorkload(ctx, "demo-app", "nginx", "cluster-1", "cluster-3"); err != nil {
		log.Printf("Failed to migrate workload: %v", err)
	}
}
```

**KubeFed实战案例**

```yaml
# use-case-multi-region.yaml
# 案例1：多地域高可用部署

---
# 场景：在3个地域部署应用，实现高可用和就近访问

# 1. 标记集群地域
# 在每个集群上执行：
# kubectl label kubefedcluster cluster-us-west region=us-west -n kube-federation-system
# kubectl label kubefedcluster cluster-us-east region=us-east -n kube-federation-system
# kubectl label kubefedcluster cluster-eu-west region=eu-west -n kube-federation-system

---
# 2. 创建联邦应用
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: global-api
  namespace: production
spec:
  template:
    metadata:
      labels:
        app: global-api
    spec:
      replicas: 10
      selector:
        matchLabels:
          app: global-api
      template:
        metadata:
          labels:
            app: global-api
        spec:
          containers:
          - name: api
            image: myregistry/api:v1.0
            ports:
            - containerPort: 8080
            resources:
              requests:
                cpu: 500m
                memory: 512Mi
              limits:
                cpu: 1000m
                memory: 1Gi
            livenessProbe:
              httpGet:
                path: /health
                port: 8080
              initialDelaySeconds: 30
              periodSeconds: 10
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              initialDelaySeconds: 5
              periodSeconds: 5
  
  # 部署到所有地域
  placement:
    clusterSelector:
      matchLabels:
        environment: production
  
  # 每个地域使用不同配置
  overrides:
  # 美西：最多副本
  - clusterName: cluster-us-west
    clusterOverrides:
    - path: "/spec/replicas"
      value: 15
    - path: "/spec/template/spec/containers/0/env"
      value:
      - name: REGION
        value: us-west
      - name: DB_ENDPOINT
        value: db.us-west.example.com
  
  # 美东：中等副本
  - clusterName: cluster-us-east
    clusterOverrides:
    - path: "/spec/replicas"
      value: 10
    - path: "/spec/template/spec/containers/0/env"
      value:
      - name: REGION
        value: us-east
      - name: DB_ENDPOINT
        value: db.us-east.example.com
  
  # 欧洲：较少副本
  - clusterName: cluster-eu-west
    clusterOverrides:
    - path: "/spec/replicas"
      value: 5
    - path: "/spec/template/spec/containers/0/env"
      value:
      - name: REGION
        value: eu-west
      - name: DB_ENDPOINT
        value: db.eu-west.example.com

---
# 3. 副本调度策略
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: global-api
  namespace: production
spec:
  targetKind: FederatedDeployment
  totalReplicas: 30
  rebalance: true
  
  # 按地域流量分配副本
  clusters:
    cluster-us-west:
      weight: 50    # 15个副本
      minReplicas: 10
      maxReplicas: 20
    cluster-us-east:
      weight: 33    # 10个副本
      minReplicas: 5
      maxReplicas: 15
    cluster-eu-west:
      weight: 17    # 5个副本
      minReplicas: 3
      maxReplicas: 10

---
# 4. 全局负载均衡Service
apiVersion: types.kubefed.io/v1beta1
kind: FederatedService
metadata:
  name: global-api
  namespace: production
spec:
  template:
    metadata:
      labels:
        app: global-api
      annotations:
        # 使用外部DNS实现全局负载均衡
        external-dns.alpha.kubernetes.io/hostname: api.example.com
    spec:
      type: LoadBalancer
      selector:
        app: global-api
      ports:
      - port: 80
        targetPort: 8080
        protocol: TCP
  
  placement:
    clusters:
    - name: cluster-us-west
    - name: cluster-us-east
    - name: cluster-eu-west
```


**KubeFed故障转移**

```yaml
# failover-strategy.yaml
# 自动故障转移配置

---
# 案例2：自动故障转移

# 1. 健康检查配置
apiVersion: core.kubefed.io/v1beta1
kind: KubeFedCluster
metadata:
  name: cluster-1
  namespace: kube-federation-system
spec:
  apiEndpoint: https://cluster-1.example.com:6443
  secretRef:
    name: cluster-1-secret
  
  # 健康检查配置
  healthCheck:
    enabled: true
    period: 10s
    timeout: 5s
    failureThreshold: 3
    successThreshold: 1

---
# 2. 故障转移Deployment
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: critical-app
  namespace: production
spec:
  template:
    # ... deployment spec ...
  
  # 初始放置
  placement:
    clusters:
    - name: cluster-1  # 主集群
    - name: cluster-2  # 备集群
  
  # 副本分配
  overrides:
  - clusterName: cluster-1
    clusterOverrides:
    - path: "/spec/replicas"
      value: 10  # 主集群运行10个副本
  - clusterName: cluster-2
    clusterOverrides:
    - path: "/spec/replicas"
      value: 0   # 备集群初始为0

---
# 3. 副本调度策略（支持故障转移）
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: critical-app
  namespace: production
spec:
  targetKind: FederatedDeployment
  totalReplicas: 10
  rebalance: true
  
  clusters:
    cluster-1:
      weight: 100
      minReplicas: 0
      maxReplicas: 10
    cluster-2:
      weight: 100
      minReplicas: 0
      maxReplicas: 10
  
  # 当cluster-1不可用时，自动将副本调度到cluster-2
  intersectWithClusterSelector: true
```

**KubeFed监控和告警**

```yaml
# kubefed-monitoring.yaml
# KubeFed监控配置

---
# Prometheus规则
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubefed-alerts
  namespace: kube-federation-system
spec:
  groups:
  - name: kubefed
    interval: 30s
    rules:
    # 集群不健康告警
    - alert: KubeFedClusterUnhealthy
      expr: |
        kubefed_cluster_status{condition="Ready",status="False"} == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "联邦集群不健康"
        description: "集群 {{ $labels.cluster }} 已不健康超过5分钟"
    
    # 资源同步失败告警
    - alert: KubeFedResourceSyncFailed
      expr: |
        rate(kubefed_sync_errors_total[5m]) > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "联邦资源同步失败"
        description: "资源 {{ $labels.resource }} 同步失败率: {{ $value }}"
    
    # 集群连接延迟告警
    - alert: KubeFedClusterHighLatency
      expr: |
        kubefed_cluster_api_latency_seconds > 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "集群API延迟过高"
        description: "集群 {{ $labels.cluster }} API延迟: {{ $value }}s"
    
    # 副本分配不均告警
    - alert: KubeFedReplicaImbalance
      expr: |
        abs(
          kubefed_deployment_replicas{cluster="cluster-1"} - 
          kubefed_deployment_replicas{cluster="cluster-2"}
        ) > 5
      for: 15m
      labels:
        severity: info
      annotations:
        summary: "副本分配不均衡"
        description: "Deployment {{ $labels.deployment }} 副本分配差异过大"

---
# Grafana Dashboard
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubefed-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  kubefed-overview.json: |
    {
      "dashboard": {
        "title": "KubeFed Overview",
        "panels": [
          {
            "title": "集群健康状态",
            "targets": [
              {
                "expr": "kubefed_cluster_status{condition=\"Ready\"}"
              }
            ],
            "type": "stat"
          },
          {
            "title": "联邦资源数量",
            "targets": [
              {
                "expr": "count by (kind) (kubefed_resource_total)"
              }
            ],
            "type": "graph"
          },
          {
            "title": "同步错误率",
            "targets": [
              {
                "expr": "rate(kubefed_sync_errors_total[5m])"
              }
            ],
            "type": "graph"
          },
          {
            "title": "集群API延迟",
            "targets": [
              {
                "expr": "kubefed_cluster_api_latency_seconds"
              }
            ],
            "type": "graph"
          },
          {
            "title": "副本分布",
            "targets": [
              {
                "expr": "sum by (cluster) (kubefed_deployment_replicas)"
              }
            ],
            "type": "piechart"
          }
        ]
      }
    }
```

**KubeFed最佳实践**

```yaml
# KubeFed使用最佳实践

1. 集群管理:
   准备工作:
     - 确保所有集群版本兼容
     - 配置集群间网络连通性
     - 统一时钟同步
     - 准备足够的资源配额
   
   集群注册:
     - 使用有意义的集群名称
     - 添加描述性标签（region、environment、tier）
     - 配置合理的健康检查参数
     - 定期验证集群连接
   
   集群维护:
     - 定期更新集群证书
     - 监控集群健康状态
     - 及时处理不健康集群
     - 保持集群配置一致性

2. 资源管理:
   联邦资源设计:
     - 使用模板定义通用配置
     - 通过覆盖实现差异化
     - 避免过度使用覆盖
     - 保持配置简洁明了
   
   放置策略:
     - 使用标签选择器而非硬编码集群名
     - 设置合理的副本分配权重
     - 配置最小/最大副本限制
     - 启用自动重平衡
   
   版本管理:
     - 使用GitOps管理联邦资源
     - 版本控制所有配置
     - 实施变更审批流程
     - 支持快速回滚

3. 网络和服务:
   服务发现:
     - 使用MultiClusterService实现跨集群服务发现
     - 配置合理的DNS TTL
     - 实施健康检查
     - 考虑使用Service Mesh
   
   负载均衡:
     - 使用全局负载均衡器（GSLB）
     - 配置地理位置路由
     - 实施故障转移策略
     - 监控流量分布
   
   网络策略:
     - 配置跨集群NetworkPolicy
     - 限制不必要的跨集群通信
     - 使用加密传输
     - 定期审计网络规则

4. 安全性:
   认证授权:
     - 使用RBAC控制联邦资源访问
     - 最小权限原则
     - 定期审计权限
     - 使用ServiceAccount而非用户凭证
   
   密钥管理:
     - 使用FederatedSecret分发密钥
     - 定期轮换密钥
     - 避免在配置中硬编码密钥
     - 考虑使用外部密钥管理系统
   
   网络安全:
     - 使用mTLS加密集群间通信
     - 配置防火墙规则
     - 限制API Server访问
     - 启用审计日志

5. 监控和运维:
   监控指标:
     - 集群健康状态
     - 资源同步状态
     - API延迟和错误率
     - 副本分布情况
   
   告警配置:
     - 集群不可用告警
     - 同步失败告警
     - 性能下降告警
     - 配置漂移告警
   
   日志管理:
     - 收集KubeFed控制器日志
     - 收集集群事件
     - 集中存储和分析
     - 保留足够的历史记录
   
   故障处理:
     - 建立故障响应流程
     - 准备回滚方案
     - 定期演练故障场景
     - 记录故障和解决方案

6. 性能优化:
   控制器优化:
     - 调整并发数
     - 配置合理的同步间隔
     - 使用缓存减少API调用
     - 监控控制器资源使用
   
   网络优化:
     - 使用专线连接
     - 配置合理的超时时间
     - 启用连接池
     - 压缩大型资源
   
   资源优化:
     - 避免创建过多联邦资源
     - 合并相关资源
     - 定期清理未使用资源
     - 使用命名空间隔离

7. 成本管理:
   资源规划:
     - 合理分配集群资源
     - 避免过度冗余
     - 使用Spot实例
     - 实施自动扩缩容
   
   成本监控:
     - 跟踪每个集群的成本
     - 分析资源利用率
     - 识别浪费
     - 优化资源分配
   
   成本优化:
     - 使用混合云策略
     - 利用不同云厂商优势
     - 按需扩展集群
     - 定期审查和优化

8. 故障排查:
   常见问题:
     - 集群无法注册: 检查网络和证书
     - 资源同步失败: 检查RBAC和配额
     - 副本分配不均: 检查调度策略
     - 性能下降: 检查网络延迟和资源
   
   调试工具:
     - kubefedctl: 命令行工具
     - kubectl: 查看联邦资源
     - 日志分析: 控制器日志
     - 事件查看: kubectl get events
   
   诊断步骤:
     1. 检查集群健康状态
     2. 查看联邦资源状态
     3. 检查控制器日志
     4. 验证网络连通性
     5. 检查RBAC权限
     6. 查看资源配额
```

**KubeFed vs 其他方案对比**

```yaml
# 多集群管理方案对比

KubeFed:
  优势:
    - Kubernetes官方项目
    - 统一的API和资源模型
    - 支持跨集群资源调度
    - 灵活的放置和覆盖策略
  
  劣势:
    - 学习曲线陡峭
    - 实现复杂
    - 社区活跃度下降
    - 缺少企业级支持
  
  适用场景:
    - 需要跨集群资源调度
    - 多地域部署
    - 高可用要求

Rancher:
  优势:
    - 完整的管理界面
    - 易于使用
    - 企业级支持
    - 丰富的功能
  
  劣势:
    - 商业产品
    - 额外的管理开销
    - 可能的厂商锁定
  
  适用场景:
    - 需要图形界面
    - 企业级部署
    - 多云管理

Open Cluster Management (OCM):
  优势:
    - 红帽支持
    - 模块化设计
    - 扩展性好
    - 活跃的社区
  
  劣势:
    - 相对较新
    - 文档不够完善
    - 学习成本
  
  适用场景:
    - 大规模集群管理
    - 需要扩展性
    - 红帽生态

ArgoCD + ApplicationSet:
  优势:
    - GitOps原生
    - 声明式管理
    - 易于集成CI/CD
    - 活跃的社区
  
  劣势:
    - 不支持跨集群调度
    - 需要额外工具
    - 功能相对简单
  
  适用场景:
    - GitOps工作流
    - 应用部署管理
    - 中小规模集群
```



### 10.2.3 多集群服务网格

**多集群服务网格概述**

服务网格（Service Mesh）为微服务提供了流量管理、安全、可观测性等能力。在多集群环境中，服务网格可以实现跨集群的服务通信和统一管理。

**多集群服务网格的价值**：

```yaml
# 多集群服务网格的核心能力

1. 跨集群服务发现:
   功能:
     - 自动发现所有集群的服务
     - 统一的服务注册表
     - 动态更新服务端点
   
   实现:
     - DNS解析
     - 服务注册中心
     - 控制平面同步

2. 智能流量路由:
   功能:
     - 跨集群负载均衡
     - 地理位置路由
     - 故障转移
     - 金丝雀发布
   
   策略:
     - 基于延迟的路由
     - 基于地域的路由
     - 权重分配
     - 流量镜像

3. 安全通信:
   功能:
     - mTLS加密
     - 身份认证
     - 授权策略
     - 证书管理
   
   实现:
     - 自动证书轮换
     - 统一的身份体系
     - 细粒度访问控制

4. 可观测性:
   功能:
     - 分布式追踪
     - 指标收集
     - 日志聚合
     - 服务拓扑
   
   工具:
     - Jaeger/Zipkin
     - Prometheus
     - Grafana
     - Kiali

5. 弹性和容错:
   功能:
     - 超时控制
     - 重试策略
     - 熔断器
     - 限流
   
   策略:
     - 自动重试
     - 故障注入
     - 降级处理
```

**Istio多集群部署**

```yaml
# istio-multi-cluster.yaml
# Istio多集群配置

---
# 模式1: 单一控制平面（Primary-Remote）

# 架构：
#   - Primary集群：运行Istio控制平面
#   - Remote集群：只运行数据平面，连接到Primary

# Primary集群配置
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-primary
  namespace: istio-system
spec:
  profile: default
  
  # 多集群配置
  values:
    global:
      # 网络配置
      meshID: mesh1
      multiCluster:
        clusterName: cluster-1
      network: network1
      
      # 启用多集群
      meshNetworks:
        network1:
          endpoints:
          - fromRegistry: cluster-1
          gateways:
          - address: 0.0.0.0
            port: 15443
        network2:
          endpoints:
          - fromRegistry: cluster-2
          gateways:
          - registryServiceName: istio-eastwestgateway.istio-system.svc.cluster.local
            port: 15443
  
  components:
    # 东西向网关（用于跨集群通信）
    ingressGateways:
    - name: istio-eastwestgateway
      label:
        istio: eastwestgateway
        app: istio-eastwestgateway
        topology.istio.io/network: network1
      enabled: true
      k8s:
        env:
        - name: ISTIO_META_ROUTER_MODE
          value: "sni-dnat"
        - name: ISTIO_META_REQUESTED_NETWORK_VIEW
          value: network1
        service:
          type: LoadBalancer
          ports:
          - name: status-port
            port: 15021
            targetPort: 15021
          - name: tls
            port: 15443
            targetPort: 15443
          - name: tls-istiod
            port: 15012
            targetPort: 15012
          - name: tls-webhook
            port: 15017
            targetPort: 15017

---
# Remote集群配置
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-remote
  namespace: istio-system
spec:
  profile: remote
  
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster-2
      network: network2
      
      # 连接到Primary集群的控制平面
      remotePilotAddress: istiod.istio-system.svc.cluster-1.global
  
  components:
    # 东西向网关
    ingressGateways:
    - name: istio-eastwestgateway
      label:
        istio: eastwestgateway
        app: istio-eastwestgateway
        topology.istio.io/network: network2
      enabled: true
      k8s:
        env:
        - name: ISTIO_META_ROUTER_MODE
          value: "sni-dnat"
        - name: ISTIO_META_REQUESTED_NETWORK_VIEW
          value: network2
        service:
          type: LoadBalancer
          ports:
          - name: status-port
            port: 15021
          - name: tls
            port: 15443
          - name: tls-istiod
            port: 15012
          - name: tls-webhook
            port: 15017

---
# 模式2: 多控制平面（Multi-Primary）

# 架构：
#   - 每个集群都运行完整的Istio控制平面
#   - 控制平面之间同步服务发现信息

# Cluster-1配置
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-cluster1
  namespace: istio-system
spec:
  profile: default
  
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster-1
      network: network1
  
  components:
    ingressGateways:
    - name: istio-eastwestgateway
      enabled: true
      # ... 配置同上 ...

---
# Cluster-2配置
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-cluster2
  namespace: istio-system
spec:
  profile: default
  
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster-2
      network: network2
  
  components:
    ingressGateways:
    - name: istio-eastwestgateway
      enabled: true
      # ... 配置同上 ...
```

**Istio多集群部署脚本**

```bash
#!/bin/bash
# deploy-istio-multicluster.sh
# 部署Istio多集群

set -e

ISTIO_VERSION="1.20.0"
PRIMARY_CLUSTER="cluster-1"
REMOTE_CLUSTER="cluster-2"
MESH_ID="mesh1"
NETWORK1="network1"
NETWORK2="network2"

echo "=== 部署Istio多集群 ==="

# 1. 下载Istio
echo "1. 下载Istio ${ISTIO_VERSION}..."
curl -L https://istio.io/downloadIstio | ISTIO_VERSION=${ISTIO_VERSION} sh -
cd istio-${ISTIO_VERSION}
export PATH=$PWD/bin:$PATH

# 2. 在Primary集群安装Istio
echo "2. 在Primary集群安装Istio..."
kubectl config use-context $PRIMARY_CLUSTER

# 创建istio-system namespace
kubectl create namespace istio-system --dry-run=client -o yaml | kubectl apply -f -

# 创建CA证书（用于多集群mTLS）
kubectl create secret generic cacerts -n istio-system     --from-file=samples/certs/ca-cert.pem     --from-file=samples/certs/ca-key.pem     --from-file=samples/certs/root-cert.pem     --from-file=samples/certs/cert-chain.pem     --dry-run=client -o yaml | kubectl apply -f -

# 安装Istio（Primary配置）
istioctl install -y -f - <<EOF
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: default
  values:
    global:
      meshID: ${MESH_ID}
      multiCluster:
        clusterName: ${PRIMARY_CLUSTER}
      network: ${NETWORK1}
  components:
    ingressGateways:
    - name: istio-eastwestgateway
      label:
        istio: eastwestgateway
      enabled: true
      k8s:
        env:
        - name: ISTIO_META_ROUTER_MODE
          value: "sni-dnat"
        service:
          type: LoadBalancer
          ports:
          - port: 15021
            name: status-port
          - port: 15443
            name: tls
          - port: 15012
            name: tls-istiod
          - port: 15017
            name: tls-webhook
EOF

# 3. 暴露Primary集群的控制平面
echo "3. 暴露Primary集群的控制平面..."
kubectl apply -n istio-system -f samples/multicluster/expose-istiod.yaml

# 4. 暴露Primary集群的服务
echo "4. 暴露Primary集群的服务..."
kubectl apply -n istio-system -f samples/multicluster/expose-services.yaml

# 5. 获取东西向网关地址
echo "5. 获取东西向网关地址..."
export DISCOVERY_ADDRESS=$(kubectl -n istio-system get svc istio-eastwestgateway     -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo "Discovery Address: $DISCOVERY_ADDRESS"

# 6. 在Remote集群安装Istio
echo "6. 在Remote集群安装Istio..."
kubectl config use-context $REMOTE_CLUSTER

# 创建istio-system namespace
kubectl create namespace istio-system --dry-run=client -o yaml | kubectl apply -f -

# 复制CA证书到Remote集群
kubectl get secret cacerts -n istio-system --context=$PRIMARY_CLUSTER -o yaml |     kubectl apply --context=$REMOTE_CLUSTER -f -

# 安装Istio（Remote配置）
istioctl install -y --context=$REMOTE_CLUSTER -f - <<EOF
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: remote
  values:
    global:
      meshID: ${MESH_ID}
      multiCluster:
        clusterName: ${REMOTE_CLUSTER}
      network: ${NETWORK2}
      remotePilotAddress: ${DISCOVERY_ADDRESS}
  components:
    ingressGateways:
    - name: istio-eastwestgateway
      label:
        istio: eastwestgateway
      enabled: true
      k8s:
        env:
        - name: ISTIO_META_ROUTER_MODE
          value: "sni-dnat"
        service:
          type: LoadBalancer
          ports:
          - port: 15021
            name: status-port
          - port: 15443
            name: tls
EOF

# 7. 配置Remote集群访问
echo "7. 配置Remote集群访问..."
istioctl create-remote-secret     --context=$REMOTE_CLUSTER     --name=$REMOTE_CLUSTER |     kubectl apply -f - --context=$PRIMARY_CLUSTER

# 8. 验证安装
echo "8. 验证安装..."
kubectl config use-context $PRIMARY_CLUSTER
kubectl get pods -n istio-system

kubectl config use-context $REMOTE_CLUSTER
kubectl get pods -n istio-system

echo "=== Istio多集群部署完成 ==="
```

**跨集群服务通信**

```yaml
# cross-cluster-service.yaml
# 跨集群服务配置

---
# 1. 在Cluster-1部署服务A
apiVersion: v1
kind: Namespace
metadata:
  name: demo
  labels:
    istio-injection: enabled

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-a
  namespace: demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: service-a
  template:
    metadata:
      labels:
        app: service-a
        version: v1
    spec:
      containers:
      - name: service-a
        image: myregistry/service-a:v1
        ports:
        - containerPort: 8080
        env:
        - name: CLUSTER_NAME
          value: cluster-1

---
apiVersion: v1
kind: Service
metadata:
  name: service-a
  namespace: demo
spec:
  selector:
    app: service-a
  ports:
  - port: 8080
    targetPort: 8080

---
# 2. 在Cluster-2部署服务B
# （在cluster-2上执行）
apiVersion: v1
kind: Namespace
metadata:
  name: demo
  labels:
    istio-injection: enabled

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-b
  namespace: demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: service-b
  template:
    metadata:
      labels:
        app: service-b
        version: v1
    spec:
      containers:
      - name: service-b
        image: myregistry/service-b:v1
        ports:
        - containerPort: 8080
        env:
        - name: CLUSTER_NAME
          value: cluster-2

---
apiVersion: v1
kind: Service
metadata:
  name: service-b
  namespace: demo
spec:
  selector:
    app: service-b
  ports:
  - port: 8080
    targetPort: 8080

---
# 3. ServiceEntry（使服务在两个集群都可见）
# 在Primary集群创建
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: service-b-cluster2
  namespace: demo
spec:
  hosts:
  - service-b.demo.svc.cluster.local
  location: MESH_INTERNAL
  ports:
  - number: 8080
    name: http
    protocol: HTTP
  resolution: DNS
  endpoints:
  - address: service-b.demo.svc.cluster-2.global
    ports:
      http: 8080
    labels:
      cluster: cluster-2

---
# 4. DestinationRule（流量策略）
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: service-b
  namespace: demo
spec:
  host: service-b.demo.svc.cluster.local
  trafficPolicy:
    # 连接池配置
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100
    
    # 负载均衡
    loadBalancer:
      simple: LEAST_REQUEST
    
    # 异常检测
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
  
  # 子集定义
  subsets:
  - name: cluster-1
    labels:
      cluster: cluster-1
  - name: cluster-2
    labels:
      cluster: cluster-2

---
# 5. VirtualService（路由规则）
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: service-b
  namespace: demo
spec:
  hosts:
  - service-b.demo.svc.cluster.local
  http:
  - match:
    - headers:
        x-cluster:
          exact: cluster-2
    route:
    - destination:
        host: service-b.demo.svc.cluster.local
        subset: cluster-2
  
  # 默认路由：按权重分配
  - route:
    - destination:
        host: service-b.demo.svc.cluster.local
        subset: cluster-1
      weight: 70
    - destination:
        host: service-b.demo.svc.cluster.local
        subset: cluster-2
      weight: 30
    
    # 超时和重试
    timeout: 10s
    retries:
      attempts: 3
      perTryTimeout: 2s
      retryOn: 5xx,reset,connect-failure,refused-stream
```


**多集群流量管理**

```yaml
# advanced-traffic-management.yaml
# 高级流量管理策略

---
# 1. 地理位置路由
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: geo-routing
  namespace: demo
spec:
  hosts:
  - api.example.com
  gateways:
  - istio-system/global-gateway
  http:
  # 欧洲用户路由到EU集群
  - match:
    - headers:
        x-user-region:
          exact: eu
    route:
    - destination:
        host: api-service.demo.svc.cluster.local
        subset: eu-cluster
  
  # 亚洲用户路由到APAC集群
  - match:
    - headers:
        x-user-region:
          exact: apac
    route:
    - destination:
        host: api-service.demo.svc.cluster.local
        subset: apac-cluster
  
  # 默认路由到最近的集群
  - route:
    - destination:
        host: api-service.demo.svc.cluster.local
      weight: 100
    mirror:
      host: api-service.demo.svc.cluster.local
      subset: backup-cluster
    mirrorPercentage:
      value: 10.0

---
# 2. 故障转移
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: failover
  namespace: demo
spec:
  host: critical-service.demo.svc.cluster.local
  trafficPolicy:
    # 连接池
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 2
    
    # 负载均衡（优先本地）
    loadBalancer:
      localityLbSetting:
        enabled: true
        distribute:
        - from: us-west/*
          to:
            "us-west/*": 80
            "us-east/*": 20
        - from: us-east/*
          to:
            "us-east/*": 80
            "us-west/*": 20
        failover:
        - from: us-west/zone1
          to: us-west/zone2
        - from: us-west/zone2
          to: us-east/zone1
    
    # 异常检测和熔断
    outlierDetection:
      consecutiveGatewayErrors: 5
      consecutive5xxErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
      minHealthPercent: 40

---
# 3. 金丝雀发布（跨集群）
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: canary-release
  namespace: demo
spec:
  hosts:
  - app.example.com
  http:
  # 10%流量到新版本（cluster-2）
  - match:
    - headers:
        x-canary:
          exact: "true"
    route:
    - destination:
        host: app-service.demo.svc.cluster.local
        subset: v2-cluster2
      weight: 100
  
  # 普通流量：90% v1，10% v2
  - route:
    - destination:
        host: app-service.demo.svc.cluster.local
        subset: v1-cluster1
      weight: 90
    - destination:
        host: app-service.demo.svc.cluster.local
        subset: v2-cluster2
      weight: 10

---
# 4. 流量镜像（跨集群测试）
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: traffic-mirroring
  namespace: demo
spec:
  hosts:
  - test-service.demo.svc.cluster.local
  http:
  - route:
    - destination:
        host: test-service.demo.svc.cluster.local
        subset: production-cluster1
      weight: 100
    
    # 镜像20%流量到测试集群
    mirror:
      host: test-service.demo.svc.cluster.local
      subset: test-cluster2
    mirrorPercentage:
      value: 20.0

---
# 5. 超时和重试策略
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: resilience
  namespace: demo
spec:
  hosts:
  - resilient-service.demo.svc.cluster.local
  http:
  - route:
    - destination:
        host: resilient-service.demo.svc.cluster.local
    
    # 超时配置
    timeout: 10s
    
    # 重试策略
    retries:
      attempts: 3
      perTryTimeout: 2s
      retryOn: 5xx,reset,connect-failure,refused-stream
    
    # 故障注入（测试）
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
      abort:
        percentage:
          value: 0.01
        httpStatus: 503
```

**多集群安全配置**

```yaml
# multi-cluster-security.yaml
# 多集群安全配置

---
# 1. PeerAuthentication（mTLS）
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: istio-system
spec:
  # 全局启用严格mTLS
  mtls:
    mode: STRICT

---
# 2. AuthorizationPolicy（跨集群访问控制）
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: cross-cluster-authz
  namespace: demo
spec:
  selector:
    matchLabels:
      app: sensitive-service
  
  action: ALLOW
  
  rules:
  # 允许来自cluster-1的service-a访问
  - from:
    - source:
        principals:
        - "cluster.local/ns/demo/sa/service-a"
        namespaces:
        - demo
    to:
    - operation:
        methods: ["GET", "POST"]
        paths: ["/api/*"]
  
  # 允许来自cluster-2的service-b访问
  - from:
    - source:
        principals:
        - "cluster-2.local/ns/demo/sa/service-b"
        namespaces:
        - demo
    to:
    - operation:
        methods: ["GET"]
        paths: ["/api/read/*"]

---
# 3. RequestAuthentication（JWT验证）
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-auth
  namespace: demo
spec:
  selector:
    matchLabels:
      app: api-gateway
  
  jwtRules:
  - issuer: "https://auth.example.com"
    jwksUri: "https://auth.example.com/.well-known/jwks.json"
    audiences:
    - "api.example.com"
    forwardOriginalToken: true

---
# 4. 证书管理
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: istio-ca
  namespace: istio-system
spec:
  secretName: cacerts
  duration: 87600h # 10 years
  renewBefore: 8760h # 1 year
  commonName: istio-ca
  isCA: true
  issuerRef:
    name: selfsigned-issuer
    kind: ClusterIssuer
```

**Linkerd多集群**

```yaml
# linkerd-multi-cluster.yaml
# Linkerd多集群配置

---
# Linkerd多集群架构更简单，使用Service Mirror

# 1. 在源集群创建Link
apiVersion: multicluster.linkerd.io/v1alpha1
kind: Link
metadata:
  name: cluster-2
  namespace: linkerd-multicluster
spec:
  targetClusterName: cluster-2
  targetClusterDomain: cluster.local
  gatewayAddress: 10.0.2.100
  gatewayPort: 4143
  gatewayIdentity: linkerd-gateway.linkerd-multicluster.serviceaccount.identity.linkerd.cluster.local
  probeSpec:
    path: /ready
    port: 4191
    period: 3s

---
# 2. 导出服务（在目标集群）
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: demo
  annotations:
    # 标记为可导出
    mirror.linkerd.io/exported: "true"
spec:
  selector:
    app: backend
  ports:
  - port: 8080
    targetPort: 8080

---
# 3. 在源集群自动创建镜像服务
# Linkerd会自动创建：backend-cluster-2.demo.svc.cluster.local

---
# 4. 流量分割
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  name: backend-split
  namespace: demo
spec:
  service: backend
  backends:
  - service: backend
    weight: 70
  - service: backend-cluster-2
    weight: 30
```

**Consul Connect多集群**

```yaml
# consul-multi-cluster.yaml
# Consul Connect多集群配置

---
# 1. Consul配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: consul-config
  namespace: consul
data:
  config.json: |
    {
      "datacenter": "dc1",
      "primary_datacenter": "dc1",
      "connect": {
        "enabled": true
      },
      "ports": {
        "grpc": 8502
      },
      "enable_central_service_config": true,
      "config_entries": {
        "bootstrap": [
          {
            "kind": "proxy-defaults",
            "name": "global",
            "config": {
              "protocol": "http"
            },
            "mesh_gateway": {
              "mode": "local"
            }
          }
        ]
      }
    }

---
# 2. Mesh Gateway（用于跨集群通信）
apiVersion: v1
kind: Service
metadata:
  name: mesh-gateway
  namespace: consul
spec:
  type: LoadBalancer
  selector:
    app: mesh-gateway
  ports:
  - port: 443
    targetPort: 8443
    name: https

---
# 3. 服务导出
apiVersion: consul.hashicorp.com/v1alpha1
kind: ExportedServices
metadata:
  name: default
  namespace: consul
spec:
  services:
  - name: backend
    consumers:
    - peer: cluster-2

---
# 4. 服务意图（访问控制）
apiVersion: consul.hashicorp.com/v1alpha1
kind: ServiceIntentions
metadata:
  name: backend
  namespace: demo
spec:
  destination:
    name: backend
  sources:
  - name: frontend
    action: allow
    permissions:
    - http:
        pathPrefix: "/api"
        methods: ["GET", "POST"]
  - name: frontend
    peer: cluster-2
    action: allow
```

**多集群服务网格对比**

```yaml
# 服务网格方案对比

Istio:
  优势:
    - 功能最完善
    - 社区最活跃
    - 企业级支持
    - 丰富的流量管理
  
  劣势:
    - 复杂度高
    - 资源开销大
    - 学习曲线陡峭
  
  多集群支持:
    - Primary-Remote模式
    - Multi-Primary模式
    - 跨网络支持
    - 东西向网关
  
  适用场景:
    - 大型企业
    - 复杂微服务
    - 需要高级功能

Linkerd:
  优势:
    - 轻量级
    - 性能好
    - 易于使用
    - Rust实现
  
  劣势:
    - 功能相对简单
    - 社区较小
    - 扩展性有限
  
  多集群支持:
    - Service Mirror
    - Gateway模式
    - 简单配置
  
  适用场景:
    - 中小型企业
    - 性能敏感
    - 简单场景

Consul Connect:
  优势:
    - 与Consul集成
    - 多数据中心原生支持
    - 服务发现强大
    - 跨平台
  
  劣势:
    - Kubernetes集成不如Istio
    - 流量管理功能较少
    - 学习成本
  
  多集群支持:
    - Mesh Gateway
    - WAN Federation
    - Cluster Peering
  
  适用场景:
    - 已使用Consul
    - 多数据中心
    - 混合环境

Cilium Service Mesh:
  优势:
    - eBPF性能
    - 网络和服务网格一体
    - 无Sidecar
    - 低延迟
  
  劣势:
    - 相对较新
    - 功能还在完善
    - 需要较新内核
  
  多集群支持:
    - Cluster Mesh
    - 全局服务
    - 跨集群网络
  
  适用场景:
    - 性能要求极高
    - 已使用Cilium CNI
    - 新项目
```



### 10.2.4 多集群管理最佳实践

**多集群运维自动化**

```bash
#!/bin/bash
# multi-cluster-ops.sh
# 多集群运维自动化脚本

set -e

# 集群列表
CLUSTERS=("cluster-1" "cluster-2" "cluster-3")

# 颜色输出
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# 日志函数
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# 1. 健康检查所有集群
check_cluster_health() {
    log_info "检查所有集群健康状态..."
    
    for cluster in "${CLUSTERS[@]}"; do
        echo ""
        log_info "检查集群: $cluster"
        
        # 切换context
        kubectl config use-context $cluster > /dev/null 2>&1
        
        # 检查节点状态
        echo "节点状态:"
        kubectl get nodes --no-headers | while read line; do
            node=$(echo $line | awk '{print $1}')
            status=$(echo $line | awk '{print $2}')
            if [ "$status" != "Ready" ]; then
                log_error "节点 $node 状态异常: $status"
            else
                log_info "节点 $node: $status"
            fi
        done
        
        # 检查系统Pod
        echo "系统Pod状态:"
        kubectl get pods -n kube-system --no-headers | while read line; do
            pod=$(echo $line | awk '{print $1}')
            status=$(echo $line | awk '{print $3}')
            if [ "$status" != "Running" ] && [ "$status" != "Completed" ]; then
                log_error "Pod $pod 状态异常: $status"
            fi
        done
        
        # 检查API Server延迟
        start_time=$(date +%s%N)
        kubectl get --raw /healthz > /dev/null 2>&1
        end_time=$(date +%s%N)
        latency=$(( (end_time - start_time) / 1000000 ))
        
        if [ $latency -gt 1000 ]; then
            log_warn "API Server延迟过高: ${latency}ms"
        else
            log_info "API Server延迟: ${latency}ms"
        fi
    done
}

# 2. 同步配置到所有集群
sync_config() {
    local config_file=$1
    log_info "同步配置到所有集群: $config_file"
    
    for cluster in "${CLUSTERS[@]}"; do
        log_info "应用到集群: $cluster"
        kubectl --context=$cluster apply -f $config_file
    done
}

# 3. 批量执行命令
exec_on_all_clusters() {
    local command=$1
    log_info "在所有集群执行命令: $command"
    
    for cluster in "${CLUSTERS[@]}"; do
        echo ""
        log_info "集群: $cluster"
        kubectl --context=$cluster $command
    done
}

# 4. 收集所有集群的资源使用情况
collect_resource_usage() {
    log_info "收集资源使用情况..."
    
    for cluster in "${CLUSTERS[@]}"; do
        echo ""
        log_info "集群: $cluster"
        
        # CPU和内存使用
        kubectl --context=$cluster top nodes 2>/dev/null || log_warn "Metrics Server未安装"
        
        # Pod数量
        total_pods=$(kubectl --context=$cluster get pods --all-namespaces --no-headers | wc -l)
        running_pods=$(kubectl --context=$cluster get pods --all-namespaces --field-selector=status.phase=Running --no-headers | wc -l)
        log_info "Pod数量: $running_pods/$total_pods (Running/Total)"
        
        # PVC使用
        pvc_count=$(kubectl --context=$cluster get pvc --all-namespaces --no-headers | wc -l)
        log_info "PVC数量: $pvc_count"
    done
}

# 5. 备份所有集群的关键资源
backup_clusters() {
    local backup_dir="./backups/$(date +%Y%m%d-%H%M%S)"
    mkdir -p $backup_dir
    
    log_info "备份所有集群到: $backup_dir"
    
    for cluster in "${CLUSTERS[@]}"; do
        log_info "备份集群: $cluster"
        cluster_backup_dir="$backup_dir/$cluster"
        mkdir -p $cluster_backup_dir
        
        # 备份所有namespace的资源
        for ns in $(kubectl --context=$cluster get ns -o jsonpath='{.items[*].metadata.name}'); do
            log_info "备份namespace: $ns"
            kubectl --context=$cluster get all,cm,secret,pvc -n $ns -o yaml > "$cluster_backup_dir/$ns.yaml" 2>/dev/null || true
        done
        
        # 备份集群级资源
        kubectl --context=$cluster get clusterrole,clusterrolebinding,pv,sc -o yaml > "$cluster_backup_dir/cluster-resources.yaml" 2>/dev/null || true
    done
    
    log_info "备份完成: $backup_dir"
}

# 6. 验证跨集群连通性
test_connectivity() {
    log_info "测试跨集群连通性..."
    
    # 在每个集群部署测试Pod
    for cluster in "${CLUSTERS[@]}"; do
        kubectl --context=$cluster run test-pod-$cluster             --image=busybox             --restart=Never             --command -- sleep 3600 2>/dev/null || true
    done
    
    sleep 10
    
    # 测试连通性
    for src_cluster in "${CLUSTERS[@]}"; do
        for dst_cluster in "${CLUSTERS[@]}"; do
            if [ "$src_cluster" != "$dst_cluster" ]; then
                log_info "测试 $src_cluster -> $dst_cluster"
                # 这里简化，实际需要获取目标集群的Service IP并测试
                # kubectl --context=$src_cluster exec test-pod-$src_cluster -- ping -c 1 <target-ip>
            fi
        done
    done
    
    # 清理测试Pod
    for cluster in "${CLUSTERS[@]}"; do
        kubectl --context=$cluster delete pod test-pod-$cluster --ignore-not-found=true
    done
}

# 7. 生成多集群报告
generate_report() {
    local report_file="multi-cluster-report-$(date +%Y%m%d-%H%M%S).txt"
    
    log_info "生成多集群报告: $report_file"
    
    {
        echo "多集群状态报告"
        echo "生成时间: $(date)"
        echo "="
        echo ""
        
        for cluster in "${CLUSTERS[@]}"; do
            echo "集群: $cluster"
            echo "-"
            
            kubectl --context=$cluster cluster-info
            echo ""
            
            echo "节点数量:"
            kubectl --context=$cluster get nodes --no-headers | wc -l
            echo ""
            
            echo "Namespace数量:"
            kubectl --context=$cluster get ns --no-headers | wc -l
            echo ""
            
            echo "Pod数量:"
            kubectl --context=$cluster get pods --all-namespaces --no-headers | wc -l
            echo ""
            
            echo "Service数量:"
            kubectl --context=$cluster get svc --all-namespaces --no-headers | wc -l
            echo ""
            
            echo ""
        done
    } > $report_file
    
    log_info "报告已生成: $report_file"
}

# 主菜单
show_menu() {
    echo ""
    echo "多集群运维工具"
    echo "="
    echo "1. 健康检查"
    echo "2. 同步配置"
    echo "3. 批量执行命令"
    echo "4. 收集资源使用"
    echo "5. 备份集群"
    echo "6. 测试连通性"
    echo "7. 生成报告"
    echo "0. 退出"
    echo ""
    read -p "请选择操作: " choice
    
    case $choice in
        1) check_cluster_health ;;
        2) read -p "配置文件路径: " config_file; sync_config $config_file ;;
        3) read -p "kubectl命令: " command; exec_on_all_clusters "$command" ;;
        4) collect_resource_usage ;;
        5) backup_clusters ;;
        6) test_connectivity ;;
        7) generate_report ;;
        0) exit 0 ;;
        *) log_error "无效选择" ;;
    esac
    
    show_menu
}

# 如果有参数，直接执行对应功能
if [ $# -gt 0 ]; then
    case $1 in
        health) check_cluster_health ;;
        sync) sync_config $2 ;;
        exec) exec_on_all_clusters "$2" ;;
        usage) collect_resource_usage ;;
        backup) backup_clusters ;;
        test) test_connectivity ;;
        report) generate_report ;;
        *) log_error "未知命令: $1" ;;
    esac
else
    show_menu
fi
```

**多集群监控和可观测性**

```yaml
# multi-cluster-monitoring.yaml
# 多集群监控配置

---
# 1. Prometheus Federation（联邦）
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    # 联邦配置：从其他集群抓取指标
    scrape_configs:
    # 本地集群
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
    
    # 联邦：从cluster-1抓取
    - job_name: 'federate-cluster-1'
      honor_labels: true
      metrics_path: '/federate'
      params:
        'match[]':
        - '{job=~".+"}'
      static_configs:
      - targets:
        - 'prometheus.monitoring.svc.cluster-1.global:9090'
        labels:
          cluster: 'cluster-1'
    
    # 联邦：从cluster-2抓取
    - job_name: 'federate-cluster-2'
      honor_labels: true
      metrics_path: '/federate'
      params:
        'match[]':
        - '{job=~".+"}'
      static_configs:
      - targets:
        - 'prometheus.monitoring.svc.cluster-2.global:9090'
        labels:
          cluster: 'cluster-2'

---
# 2. Thanos（长期存储和全局查询）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: thanos-query
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: thanos-query
  template:
    metadata:
      labels:
        app: thanos-query
    spec:
      containers:
      - name: thanos-query
        image: thanosio/thanos:v0.32.0
        args:
        - query
        - --http-address=0.0.0.0:9090
        - --grpc-address=0.0.0.0:10901
        - --query.replica-label=replica
        # 连接到所有集群的Thanos Sidecar
        - --store=thanos-sidecar.monitoring.svc.cluster-1.global:10901
        - --store=thanos-sidecar.monitoring.svc.cluster-2.global:10901
        - --store=thanos-sidecar.monitoring.svc.cluster-3.global:10901
        # 连接到Thanos Store（长期存储）
        - --store=thanos-store.monitoring.svc.cluster.local:10901
        ports:
        - containerPort: 9090
          name: http
        - containerPort: 10901
          name: grpc

---
# 3. Grafana多集群Dashboard
apiVersion: v1
kind: ConfigMap
metadata:
  name: multi-cluster-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  multi-cluster-overview.json: |
    {
      "dashboard": {
        "title": "Multi-Cluster Overview",
        "panels": [
          {
            "title": "集群健康状态",
            "targets": [
              {
                "expr": "up{job=\"kubernetes-nodes\"}"
              }
            ],
            "type": "stat",
            "fieldConfig": {
              "defaults": {
                "mappings": [
                  {"value": 1, "text": "健康", "color": "green"},
                  {"value": 0, "text": "异常", "color": "red"}
                ]
              }
            }
          },
          {
            "title": "各集群Pod数量",
            "targets": [
              {
                "expr": "count by (cluster) (kube_pod_info)"
              }
            ],
            "type": "graph"
          },
          {
            "title": "各集群CPU使用率",
            "targets": [
              {
                "expr": "sum by (cluster) (rate(container_cpu_usage_seconds_total[5m]))"
              }
            ],
            "type": "graph"
          },
          {
            "title": "各集群内存使用率",
            "targets": [
              {
                "expr": "sum by (cluster) (container_memory_working_set_bytes) / sum by (cluster) (machine_memory_bytes)"
              }
            ],
            "type": "graph"
          },
          {
            "title": "跨集群流量",
            "targets": [
              {
                "expr": "sum by (source_cluster, destination_cluster) (rate(istio_requests_total[5m]))"
              }
            ],
            "type": "heatmap"
          },
          {
            "title": "集群API延迟",
            "targets": [
              {
                "expr": "histogram_quantile(0.99, sum by (cluster, le) (rate(apiserver_request_duration_seconds_bucket[5m])))"
              }
            ],
            "type": "graph"
          }
        ]
      }
    }

---
# 4. 告警规则
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: multi-cluster-alerts
  namespace: monitoring
spec:
  groups:
  - name: multi-cluster
    interval: 30s
    rules:
    # 集群不可达告警
    - alert: ClusterUnreachable
      expr: |
        up{job="federate-cluster-*"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "集群不可达"
        description: "集群 {{ $labels.cluster }} 已不可达超过5分钟"
    
    # 跨集群延迟过高
    - alert: CrossClusterHighLatency
      expr: |
        histogram_quantile(0.99, 
          sum by (source_cluster, destination_cluster, le) (
            rate(istio_request_duration_milliseconds_bucket{
              source_cluster!="",
              destination_cluster!="",
              source_cluster!=destination_cluster
            }[5m])
          )
        ) > 1000
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "跨集群延迟过高"
        description: "从 {{ $labels.source_cluster }} 到 {{ $labels.destination_cluster }} 的P99延迟: {{ $value }}ms"
    
    # 集群资源不均衡
    - alert: ClusterResourceImbalance
      expr: |
        abs(
          max by (cluster) (sum by (cluster) (kube_pod_info)) - 
          min by (cluster) (sum by (cluster) (kube_pod_info))
        ) > 100
      for: 30m
      labels:
        severity: info
      annotations:
        summary: "集群资源分配不均衡"
        description: "集群间Pod数量差异过大"
```


**多集群灾难恢复**

```yaml
# disaster-recovery.yaml
# 多集群灾难恢复策略

---
# 1. 备份策略

备份范围:
  应用数据:
    - 数据库备份
    - 对象存储备份
    - PV快照
  
  配置数据:
    - Kubernetes资源定义
    - ConfigMap和Secret
    - RBAC配置
  
  集群状态:
    - etcd备份
    - 证书和密钥
    - 自定义资源

备份频率:
  关键数据: 每小时
  应用配置: 每天
  集群状态: 每周

备份存储:
  - 跨地域对象存储
  - 多副本保存
  - 加密存储
  - 定期验证

---
# 2. Velero多集群备份配置

apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  name: default
  namespace: velero
spec:
  provider: aws
  objectStorage:
    bucket: multi-cluster-backups
    prefix: cluster-1
  config:
    region: us-west-2
    s3ForcePathStyle: "true"

---
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup
  namespace: velero
spec:
  schedule: "0 2 * * *"  # 每天凌晨2点
  template:
    includedNamespaces:
    - "*"
    excludedNamespaces:
    - kube-system
    - kube-public
    includedResources:
    - "*"
    snapshotVolumes: true
    ttl: 720h  # 保留30天

---
# 3. 故障转移流程

故障检测:
  自动检测:
    - 健康检查失败
    - API Server不可达
    - 节点大量失败
    - 网络分区
  
  手动触发:
    - 计划维护
    - 灾难演练
    - 紧急情况

故障转移步骤:
  1. 确认故障:
     - 验证集群状态
     - 评估影响范围
     - 确定恢复策略
  
  2. 流量切换:
     - 更新DNS记录
     - 调整负载均衡器
     - 修改路由规则
  
  3. 数据恢复:
     - 恢复最新备份
     - 验证数据完整性
     - 同步增量数据
  
  4. 服务验证:
     - 健康检查
     - 功能测试
     - 性能验证
  
  5. 监控和调优:
     - 监控关键指标
     - 调整资源配置
     - 优化性能

故障恢复:
  1. 修复原集群
  2. 数据同步
  3. 流量回切
  4. 验证和监控

---
# 4. 自动故障转移配置

apiVersion: v1
kind: ConfigMap
metadata:
  name: failover-config
  namespace: kube-system
data:
  config.yaml: |
    # 故障检测配置
    healthCheck:
      interval: 30s
      timeout: 10s
      failureThreshold: 3
      successThreshold: 1
    
    # 故障转移策略
    failover:
      # 自动故障转移
      automatic: true
      
      # 故障转移目标
      targets:
      - cluster: cluster-2
        priority: 1
        weight: 100
      - cluster: cluster-3
        priority: 2
        weight: 50
      
      # 回切策略
      failback:
        automatic: false
        cooldown: 1h
    
    # 通知配置
    notifications:
      slack:
        webhook: https://hooks.slack.com/services/xxx
      email:
        to: ops@example.com
```

**多集群成本优化**

```yaml
# cost-optimization.yaml
# 多集群成本优化策略

---
# 1. 成本分析维度

成本构成:
  计算成本:
    - 节点费用（EC2、GCE等）
    - CPU和内存使用
    - Spot/Preemptible实例
  
  存储成本:
    - 持久化存储（EBS、PD等）
    - 对象存储（S3、GCS等）
    - 备份存储
  
  网络成本:
    - 跨区域流量
    - 跨云流量
    - 负载均衡器
  
  管理成本:
    - 控制平面费用
    - 监控和日志
    - 备份和恢复

---
# 2. 成本优化策略

计算优化:
  策略:
    - 使用Spot/Preemptible实例
    - 自动扩缩容
    - 合理设置资源请求和限制
    - 使用节点亲和性优化调度
  
  实施:
    # Cluster Autoscaler配置
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: cluster-autoscaler-config
      namespace: kube-system
    data:
      config.yaml: |
        scaleDown:
          enabled: true
          delayAfterAdd: 10m
          delayAfterDelete: 10s
          delayAfterFailure: 3m
          unneededTime: 10m
          utilizationThreshold: 0.5
        
        nodeGroups:
        - name: spot-nodes
          minSize: 0
          maxSize: 100
          priority: 100
        - name: on-demand-nodes
          minSize: 3
          maxSize: 20
          priority: 50

存储优化:
  策略:
    - 使用合适的存储类型
    - 定期清理未使用的PV
    - 压缩和去重
    - 生命周期管理
  
  实施:
    # StorageClass配置
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: cost-optimized
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: gp3
      iops: "3000"
      throughput: "125"
    reclaimPolicy: Delete
    volumeBindingMode: WaitForFirstConsumer

网络优化:
  策略:
    - 减少跨区域流量
    - 使用CDN
    - 优化服务网格配置
    - 压缩数据传输
  
  实施:
    # 本地优先路由
    apiVersion: networking.istio.io/v1beta1
    kind: DestinationRule
    metadata:
      name: locality-lb
    spec:
      host: "*.svc.cluster.local"
      trafficPolicy:
        loadBalancer:
          localityLbSetting:
            enabled: true
            distribute:
            - from: "us-west/*"
              to:
                "us-west/*": 90
                "us-east/*": 10

---
# 3. 成本监控和告警

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cost-alerts
  namespace: monitoring
spec:
  groups:
  - name: cost
    interval: 1h
    rules:
    # 成本超预算告警
    - alert: CostOverBudget
      expr: |
        sum by (cluster) (cluster_cost_hourly) * 24 * 30 > cluster_budget_monthly
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "集群成本超预算"
        description: "集群 {{ $labels.cluster }} 月度成本预计: ${{ $value }}"
    
    # 资源浪费告警
    - alert: ResourceWaste
      expr: |
        (
          sum by (cluster) (kube_pod_container_resource_requests_cpu_cores) - 
          sum by (cluster) (rate(container_cpu_usage_seconds_total[5m]))
        ) / sum by (cluster) (kube_pod_container_resource_requests_cpu_cores) > 0.5
      for: 6h
      labels:
        severity: info
      annotations:
        summary: "资源利用率低"
        description: "集群 {{ $labels.cluster }} CPU利用率: {{ $value | humanizePercentage }}"
```

**多集群最佳实践总结**

```yaml
# 多集群管理最佳实践

1. 架构设计:
   原则:
     - 松耦合设计
     - 故障隔离
     - 自治能力
     - 可扩展性
   
   建议:
     - 根据业务需求选择合适的多集群模式
     - 设计清晰的集群边界和职责
     - 预留足够的扩展空间
     - 考虑未来的演进路径

2. 网络规划:
   原则:
     - 避免IP冲突
     - 低延迟连接
     - 安全隔离
     - 高可用性
   
   建议:
     - 提前规划IP地址空间
     - 使用专线或VPN连接
     - 实施网络分段和隔离
     - 配置冗余网络路径

3. 服务发现:
   原则:
     - 统一命名
     - 自动发现
     - 健康检查
     - 负载均衡
   
   建议:
     - 使用服务网格实现跨集群服务发现
     - 配置合理的DNS TTL
     - 实施健康检查和故障转移
     - 监控服务可用性

4. 数据管理:
   原则:
     - 数据一致性
     - 备份恢复
     - 跨集群同步
     - 合规性
   
   建议:
     - 选择合适的数据复制策略
     - 定期备份关键数据
     - 测试恢复流程
     - 遵守数据主权要求

5. 安全性:
   原则:
     - 零信任
     - 最小权限
     - 加密传输
     - 审计日志
   
   建议:
     - 使用mTLS加密集群间通信
     - 实施细粒度的访问控制
     - 定期轮换证书和密钥
     - 启用审计日志并定期审查

6. 监控和可观测性:
   原则:
     - 统一视图
     - 实时监控
     - 分布式追踪
     - 告警及时
   
   建议:
     - 使用Prometheus Federation或Thanos
     - 配置跨集群的分布式追踪
     - 建立统一的日志收集系统
     - 设置合理的告警阈值

7. 运维自动化:
   原则:
     - 基础设施即代码
     - GitOps工作流
     - 自动化部署
     - 自愈能力
   
   建议:
     - 使用Terraform管理基础设施
     - 使用ArgoCD或Flux实现GitOps
     - 自动化常见运维任务
     - 实施自动故障恢复

8. 成本优化:
   原则:
     - 资源利用率
     - 按需扩展
     - 成本可见性
     - 持续优化
   
   建议:
     - 使用Spot/Preemptible实例
     - 实施自动扩缩容
     - 监控和分析成本
     - 定期审查和优化

9. 灾难恢复:
   原则:
     - 定期备份
     - 快速恢复
     - 演练验证
     - 文档完善
   
   建议:
     - 制定详细的DR计划
     - 定期备份关键数据和配置
     - 定期进行DR演练
     - 保持DR文档更新

10. 团队协作:
    原则:
      - 清晰职责
      - 有效沟通
      - 知识共享
      - 持续改进
    
    建议:
      - 建立清晰的运维流程
      - 使用协作工具（Slack、Jira等）
      - 定期分享经验和教训
      - 建立知识库和文档

# 实施路线图

阶段1: 规划和设计（2-4周）
  任务:
    - 评估多集群需求
    - 设计架构方案
    - 规划网络和存储
    - 制定实施计划
  
  交付物:
    - 架构设计文档
    - 网络规划文档
    - 实施计划
    - 风险评估

阶段2: 基础设施准备（2-4周）
  任务:
    - 部署Kubernetes集群
    - 配置网络连接
    - 安装基础组件
    - 配置监控和日志
  
  交付物:
    - 就绪的集群环境
    - 网络连通性验证
    - 监控系统
    - 运维文档

阶段3: 多集群管理实施（3-6周）
  任务:
    - 部署联邦或服务网格
    - 配置跨集群服务发现
    - 实施安全策略
    - 配置备份和恢复
  
  交付物:
    - 多集群管理平台
    - 服务网格配置
    - 安全策略
    - 备份系统

阶段4: 应用迁移（4-8周）
  任务:
    - 迁移应用到多集群
    - 配置跨集群路由
    - 优化性能
    - 验证功能
  
  交付物:
    - 迁移后的应用
    - 性能测试报告
    - 功能验证报告
    - 用户文档

阶段5: 优化和完善（持续）
  任务:
    - 监控和调优
    - 成本优化
    - 安全加固
    - 持续改进
  
  交付物:
    - 优化报告
    - 成本分析
    - 安全审计报告
    - 改进建议

# 常见陷阱和避免方法

陷阱1: 过度复杂化
  问题: 引入不必要的复杂性
  避免: 从简单开始，按需扩展

陷阱2: 忽视网络延迟
  问题: 跨集群通信延迟高
  避免: 优化网络连接，使用本地优先路由

陷阱3: 数据一致性问题
  问题: 跨集群数据不一致
  避免: 选择合适的一致性模型，实施数据验证

陷阱4: 安全配置不当
  问题: 集群间通信不安全
  避免: 使用mTLS，实施严格的访问控制

陷阱5: 缺乏监控
  问题: 无法及时发现问题
  避免: 建立完善的监控和告警体系

陷阱6: 成本失控
  问题: 多集群成本过高
  避免: 监控成本，实施优化策略

陷阱7: 缺乏灾难恢复计划
  问题: 灾难发生时无法快速恢复
  避免: 制定DR计划，定期演练

陷阱8: 文档不完善
  问题: 运维困难，知识流失
  避免: 建立完善的文档体系
```

**本节总结**

在本节中，我们全面探讨了集群联邦与多集群管理的各个方面：

1. **多集群架构设计**：介绍了多集群的驱动因素、架构模式（独立集群、主从集群、网格集群、联邦集群）、网络架构、存储架构和设计原则，并提供了详细的决策树帮助选择合适的方案。

2. **KubeFed集群联邦**：深入讲解了KubeFed的核心概念、安装部署、联邦资源配置、高级放置策略、管理工具、实战案例、故障转移和最佳实践，提供了完整的Go管理工具和多地域高可用部署案例。

3. **多集群服务网格**：详细介绍了Istio、Linkerd、Consul Connect等服务网格的多集群部署方案，包括跨集群服务通信、高级流量管理（地理位置路由、故障转移、金丝雀发布）、安全配置和方案对比。

4. **多集群管理最佳实践**：总结了运维自动化、监控和可观测性、灾难恢复、成本优化等方面的最佳实践，提供了完整的运维脚本、监控配置和实施路线图。

**关键要点**：

- 根据业务需求选择合适的多集群架构模式
- 重视网络规划和连通性
- 使用服务网格实现跨集群服务通信
- 建立完善的监控和可观测性体系
- 制定灾难恢复计划并定期演练
- 持续优化成本和性能
- 自动化运维流程

**下一节预告**：

在下一节中，我们将学习灾难恢复与业务连续性，探讨如何设计和实施完善的备份恢复策略，确保业务在各种故障场景下的连续性。

