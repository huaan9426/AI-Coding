# 第10章 Kubernetes高级特性与企业级实践

## 本章概述

在前面的章节中，我们系统学习了Kubernetes的核心概念、资源对象、网络存储、安全机制和可观测性等内容。本章将深入探讨Kubernetes的高级特性和企业级实践，帮助你构建生产级的、可扩展的、安全的Kubernetes平台。

**本章主要内容**：

1. **多租户与资源隔离**：如何在单个集群中安全地支持多个团队或应用
2. **集群联邦与多集群管理**：跨地域、跨云的多集群架构设计
3. **灾难恢复与业务连续性**：备份恢复策略和高可用架构
4. **安全加固与合规性**：企业级安全最佳实践和合规要求
5. **大规模集群运维**：管理数千节点的超大规模集群
6. **Kubernetes扩展开发**：自定义资源、Operator和准入控制器
7. **云原生最佳实践**：从传统应用到云原生的迁移路径
8. **本章总结**：高级特性的综合应用和未来展望

**学习目标**：

- 掌握多租户架构设计和资源隔离技术
- 理解多集群管理的挑战和解决方案
- 建立完善的灾难恢复和业务连续性体系
- 实施企业级安全加固和合规性管理
- 掌握大规模集群的运维技巧和性能优化
- 学会开发Kubernetes扩展和自定义控制器
- 了解云原生最佳实践和迁移策略

**前置知识**：

- 熟悉Kubernetes核心概念和资源对象（第1-4章）
- 理解网络、存储和安全机制（第5-7章）
- 掌握应用部署和可观测性（第8-9章）

让我们开始探索Kubernetes的高级特性和企业级实践！

---

## 10.1 多租户与资源隔离

### 10.1.1 多租户架构概述

**什么是多租户**

多租户（Multi-Tenancy）是指在单个Kubernetes集群中安全地支持多个用户、团队或应用的能力。每个租户拥有独立的资源配额、网络隔离和访问控制，互不干扰。

**多租户的价值**：

1. **成本优化**：共享集群基础设施，降低运维成本
2. **资源利用率**：通过资源共享提高整体利用率
3. **简化管理**：统一的集群管理和升级
4. **快速交付**：租户可以快速获得资源和环境

**多租户的挑战**：

1. **资源隔离**：防止租户之间的资源争抢
2. **网络隔离**：确保租户之间的网络安全
3. **安全隔离**：防止权限泄露和越权访问
4. **性能隔离**：避免"吵闹邻居"问题
5. **可见性隔离**：租户只能看到自己的资源

**多租户模型**

Kubernetes支持三种多租户模型：

```yaml
# 1. 软多租户（Soft Multi-Tenancy）
# 适用场景：同一组织内的不同团队
# 隔离级别：中等
# 信任级别：高
特点:
  - 使用Namespace进行逻辑隔离
  - 通过RBAC控制访问权限
  - ResourceQuota限制资源使用
  - NetworkPolicy实现网络隔离
  
优势:
  - 实现简单，运维成本低
  - 资源利用率高
  - 适合可信环境
  
劣势:
  - 隔离强度有限
  - 存在安全风险
  - 不适合多客户场景

---

# 2. 硬多租户（Hard Multi-Tenancy）
# 适用场景：不同组织或客户
# 隔离级别：高
# 信任级别：低
特点:
  - 每个租户独立集群
  - 完全的资源和网络隔离
  - 独立的控制平面
  - 强安全边界
  
优势:
  - 安全性最高
  - 完全隔离
  - 适合多客户SaaS
  
劣势:
  - 成本高
  - 运维复杂
  - 资源利用率低

---

# 3. 混合多租户（Hybrid Multi-Tenancy）
# 适用场景：大型企业或云服务商
# 隔离级别：可配置
# 信任级别：中等
特点:
  - 结合软硬多租户优势
  - 使用虚拟集群技术
  - 共享控制平面，隔离数据平面
  - 灵活的隔离策略
  
优势:
  - 平衡成本和安全
  - 灵活性高
  - 适合复杂场景
  
劣势:
  - 实现复杂
  - 需要额外工具支持
```

**多租户架构设计原则**

```yaml
# 多租户架构设计的核心原则

1. 最小权限原则（Principle of Least Privilege）:
   描述: 租户只能访问其必需的资源
   实现:
     - 使用RBAC精细化权限控制
     - 默认拒绝，显式授权
     - 定期审计权限使用情况
   
2. 纵深防御（Defense in Depth）:
   描述: 多层次的安全防护
   实现:
     - Namespace隔离
     - NetworkPolicy网络隔离
     - PodSecurityPolicy安全策略
     - ResourceQuota资源限制
     - 审计日志记录
   
3. 故障隔离（Fault Isolation）:
   描述: 一个租户的故障不影响其他租户
   实现:
     - 资源配额限制
     - PriorityClass优先级
     - Pod反亲和性
     - 独立的存储和网络
   
4. 公平性（Fairness）:
   描述: 资源分配公平合理
   实现:
     - ResourceQuota配额管理
     - LimitRange默认限制
     - 资源预留和超卖策略
     - 动态资源调整
   
5. 可观测性（Observability）:
   描述: 租户可以监控自己的资源
   实现:
     - 租户级别的监控指标
     - 独立的日志收集
     - 资源使用报表
     - 告警和通知机制
```

**多租户实现技术栈**

```yaml
# Kubernetes原生能力
核心组件:
  - Namespace: 逻辑隔离
  - RBAC: 访问控制
  - ResourceQuota: 资源配额
  - LimitRange: 资源限制
  - NetworkPolicy: 网络隔离
  - PodSecurityPolicy: 安全策略

# 增强工具
虚拟集群:
  - vcluster: 轻量级虚拟集群
  - Kamaji: 托管控制平面
  - Cluster API: 集群生命周期管理

策略引擎:
  - OPA/Gatekeeper: 策略即代码
  - Kyverno: Kubernetes原生策略引擎
  - Polaris: 最佳实践验证

网络隔离:
  - Calico: 高级网络策略
  - Cilium: eBPF网络和安全
  - Istio: 服务网格隔离

资源管理:
  - Hierarchical Namespace Controller: 层级命名空间
  - Capsule: 多租户Operator
  - Loft: 虚拟集群平台
```


### 10.1.2 Namespace级别的资源隔离

**Namespace隔离基础**

Namespace是Kubernetes中最基本的多租户隔离机制，它提供了逻辑上的资源分组和隔离。

**Namespace隔离的资源类型**：

```yaml
# Namespace级别的资源（受Namespace隔离）
namespace_scoped_resources:
  工作负载:
    - Pod
    - Deployment
    - StatefulSet
    - DaemonSet
    - Job
    - CronJob
  
  配置和存储:
    - ConfigMap
    - Secret
    - PersistentVolumeClaim
  
  服务发现:
    - Service
    - Endpoints
    - Ingress
  
  访问控制:
    - Role
    - RoleBinding
    - ServiceAccount
  
  资源管理:
    - ResourceQuota
    - LimitRange

# 集群级别的资源（不受Namespace隔离）
cluster_scoped_resources:
  - Node
  - PersistentVolume
  - StorageClass
  - ClusterRole
  - ClusterRoleBinding
  - Namespace
  - CustomResourceDefinition
```

**创建租户Namespace**

```yaml
# tenant-namespace.yaml
# 为租户创建完整的Namespace配置

apiVersion: v1
kind: Namespace
metadata:
  name: tenant-alpha
  labels:
    tenant: alpha
    environment: production
    cost-center: "engineering"
  annotations:
    description: "Alpha团队的生产环境"
    contact: "alpha-team@example.com"
    created-by: "platform-team"

---
# ResourceQuota - 限制租户的资源使用
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-alpha-quota
  namespace: tenant-alpha
spec:
  hard:
    # 计算资源限制
    requests.cpu: "100"           # 最多请求100核CPU
    requests.memory: 200Gi        # 最多请求200GB内存
    limits.cpu: "200"             # 最多限制200核CPU
    limits.memory: 400Gi          # 最多限制400GB内存
    
    # 存储资源限制
    requests.storage: 1Ti         # 最多请求1TB存储
    persistentvolumeclaims: "50"  # 最多50个PVC
    
    # 对象数量限制
    pods: "500"                   # 最多500个Pod
    services: "100"               # 最多100个Service
    services.loadbalancers: "10"  # 最多10个LoadBalancer
    services.nodeports: "20"      # 最多20个NodePort
    
    # 其他资源限制
    configmaps: "200"             # 最多200个ConfigMap
    secrets: "200"                # 最多200个Secret
    replicationcontrollers: "50"  # 最多50个RC
    
  # 作用域限制
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["high", "medium"]

---
# LimitRange - 设置默认资源限制
apiVersion: v1
kind: LimitRange
metadata:
  name: tenant-alpha-limits
  namespace: tenant-alpha
spec:
  limits:
  # Pod级别限制
  - type: Pod
    max:
      cpu: "16"
      memory: 32Gi
    min:
      cpu: "100m"
      memory: 128Mi
  
  # Container级别限制
  - type: Container
    default:              # 默认限制
      cpu: "1"
      memory: 1Gi
    defaultRequest:       # 默认请求
      cpu: "500m"
      memory: 512Mi
    max:                  # 最大限制
      cpu: "8"
      memory: 16Gi
    min:                  # 最小请求
      cpu: "100m"
      memory: 128Mi
    maxLimitRequestRatio: # 限制/请求比率
      cpu: "4"
      memory: "4"
  
  # PVC级别限制
  - type: PersistentVolumeClaim
    max:
      storage: 100Gi
    min:
      storage: 1Gi

---
# NetworkPolicy - 网络隔离
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tenant-alpha-network-policy
  namespace: tenant-alpha
spec:
  podSelector: {}  # 应用到所有Pod
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  # 允许同Namespace内的Pod互相访问
  - from:
    - podSelector: {}
  
  # 允许来自Ingress Controller的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
  
  egress:
  # 允许访问同Namespace内的Pod
  - to:
    - podSelector: {}
  
  # 允许DNS查询
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  
  # 允许访问外部服务（示例：数据库）
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432
  
  # 允许访问互联网（可选）
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
```

**RBAC权限配置**

```yaml
# tenant-rbac.yaml
# 为租户配置细粒度的访问控制

---
# ServiceAccount - 租户的服务账号
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tenant-alpha-admin
  namespace: tenant-alpha

---
# Role - 租户管理员角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: tenant-admin
  namespace: tenant-alpha
rules:
# 完全控制工作负载
- apiGroups: ["", "apps", "batch"]
  resources:
    - pods
    - pods/log
    - pods/exec
    - deployments
    - statefulsets
    - daemonsets
    - jobs
    - cronjobs
    - replicasets
  verbs: ["*"]

# 完全控制配置和存储
- apiGroups: [""]
  resources:
    - configmaps
    - secrets
    - persistentvolumeclaims
  verbs: ["*"]

# 完全控制服务发现
- apiGroups: ["", "networking.k8s.io"]
  resources:
    - services
    - endpoints
    - ingresses
  verbs: ["*"]

# 只读访问资源配额
- apiGroups: [""]
  resources:
    - resourcequotas
    - limitranges
  verbs: ["get", "list", "watch"]

# 只读访问事件
- apiGroups: [""]
  resources:
    - events
  verbs: ["get", "list", "watch"]

---
# Role - 租户开发者角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: tenant-developer
  namespace: tenant-alpha
rules:
# 管理工作负载（不包括删除）
- apiGroups: ["", "apps", "batch"]
  resources:
    - pods
    - deployments
    - statefulsets
    - jobs
    - cronjobs
  verbs: ["get", "list", "watch", "create", "update", "patch"]

# 查看日志和执行命令
- apiGroups: [""]
  resources:
    - pods/log
    - pods/exec
  verbs: ["get", "create"]

# 管理配置（不包括Secret）
- apiGroups: [""]
  resources:
    - configmaps
  verbs: ["get", "list", "watch", "create", "update", "patch"]

# 只读访问Secret
- apiGroups: [""]
  resources:
    - secrets
  verbs: ["get", "list"]

# 只读访问服务
- apiGroups: [""]
  resources:
    - services
    - endpoints
  verbs: ["get", "list", "watch"]

---
# Role - 租户只读角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: tenant-viewer
  namespace: tenant-alpha
rules:
# 只读访问所有资源
- apiGroups: ["", "apps", "batch", "networking.k8s.io"]
  resources: ["*"]
  verbs: ["get", "list", "watch"]

# 查看日志
- apiGroups: [""]
  resources:
    - pods/log
  verbs: ["get"]

---
# RoleBinding - 绑定管理员角色
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-alpha-admin-binding
  namespace: tenant-alpha
subjects:
- kind: User
  name: alice@example.com
  apiGroup: rbac.authorization.k8s.io
- kind: ServiceAccount
  name: tenant-alpha-admin
  namespace: tenant-alpha
roleRef:
  kind: Role
  name: tenant-admin
  apiGroup: rbac.authorization.k8s.io

---
# RoleBinding - 绑定开发者角色
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-alpha-developer-binding
  namespace: tenant-alpha
subjects:
- kind: Group
  name: alpha-developers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: tenant-developer
  apiGroup: rbac.authorization.k8s.io

---
# RoleBinding - 绑定只读角色
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: tenant-alpha-viewer-binding
  namespace: tenant-alpha
subjects:
- kind: Group
  name: alpha-viewers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: tenant-viewer
  apiGroup: rbac.authorization.k8s.io
```

**租户管理自动化脚本**

```go
// tenant-manager.go
// 自动化租户创建和管理的Go程序

package main

import (
	"context"
	"fmt"
	"log"

	corev1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
)

// TenantConfig 租户配置
type TenantConfig struct {
	Name        string
	Environment string
	CostCenter  string
	Contact     string
	
	// 资源配额
	CPURequest    string
	MemoryRequest string
	CPULimit      string
	MemoryLimit   string
	StorageQuota  string
	
	// 对象数量限制
	MaxPods     int64
	MaxServices int64
	MaxPVCs     int64
	
	// 管理员用户
	Admins []string
	// 开发者组
	Developers []string
	// 只读用户组
	Viewers []string
}

// TenantManager 租户管理器
type TenantManager struct {
	clientset *kubernetes.Clientset
}

// NewTenantManager 创建租户管理器
func NewTenantManager(kubeconfig string) (*TenantManager, error) {
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create clientset: %v", err)
	}
	
	return &TenantManager{clientset: clientset}, nil
}

// CreateTenant 创建租户
func (tm *TenantManager) CreateTenant(ctx context.Context, config *TenantConfig) error {
	// 1. 创建Namespace
	if err := tm.createNamespace(ctx, config); err != nil {
		return fmt.Errorf("failed to create namespace: %v", err)
	}
	log.Printf("Created namespace: %s", config.Name)
	
	// 2. 创建ResourceQuota
	if err := tm.createResourceQuota(ctx, config); err != nil {
		return fmt.Errorf("failed to create resource quota: %v", err)
	}
	log.Printf("Created resource quota for: %s", config.Name)
	
	// 3. 创建LimitRange
	if err := tm.createLimitRange(ctx, config); err != nil {
		return fmt.Errorf("failed to create limit range: %v", err)
	}
	log.Printf("Created limit range for: %s", config.Name)
	
	// 4. 创建NetworkPolicy
	if err := tm.createNetworkPolicy(ctx, config); err != nil {
		return fmt.Errorf("failed to create network policy: %v", err)
	}
	log.Printf("Created network policy for: %s", config.Name)
	
	// 5. 创建RBAC
	if err := tm.createRBAC(ctx, config); err != nil {
		return fmt.Errorf("failed to create RBAC: %v", err)
	}
	log.Printf("Created RBAC for: %s", config.Name)
	
	log.Printf("Successfully created tenant: %s", config.Name)
	return nil
}

// createNamespace 创建Namespace
func (tm *TenantManager) createNamespace(ctx context.Context, config *TenantConfig) error {
	ns := &corev1.Namespace{
		ObjectMeta: metav1.ObjectMeta{
			Name: config.Name,
			Labels: map[string]string{
				"tenant":      config.Name,
				"environment": config.Environment,
				"cost-center": config.CostCenter,
			},
			Annotations: map[string]string{
				"contact":    config.Contact,
				"created-by": "tenant-manager",
			},
		},
	}
	
	_, err := tm.clientset.CoreV1().Namespaces().Create(ctx, ns, metav1.CreateOptions{})
	return err
}

// createResourceQuota 创建ResourceQuota
func (tm *TenantManager) createResourceQuota(ctx context.Context, config *TenantConfig) error {
	quota := &corev1.ResourceQuota{
		ObjectMeta: metav1.ObjectMeta{
			Name:      fmt.Sprintf("%s-quota", config.Name),
			Namespace: config.Name,
		},
		Spec: corev1.ResourceQuotaSpec{
			Hard: corev1.ResourceList{
				"requests.cpu":           resource.MustParse(config.CPURequest),
				"requests.memory":        resource.MustParse(config.MemoryRequest),
				"limits.cpu":             resource.MustParse(config.CPULimit),
				"limits.memory":          resource.MustParse(config.MemoryLimit),
				"requests.storage":       resource.MustParse(config.StorageQuota),
				"pods":                   *resource.NewQuantity(config.MaxPods, resource.DecimalSI),
				"services":               *resource.NewQuantity(config.MaxServices, resource.DecimalSI),
				"persistentvolumeclaims": *resource.NewQuantity(config.MaxPVCs, resource.DecimalSI),
			},
		},
	}
	
	_, err := tm.clientset.CoreV1().ResourceQuotas(config.Name).Create(ctx, quota, metav1.CreateOptions{})
	return err
}

// createLimitRange 创建LimitRange
func (tm *TenantManager) createLimitRange(ctx context.Context, config *TenantConfig) error {
	limitRange := &corev1.LimitRange{
		ObjectMeta: metav1.ObjectMeta{
			Name:      fmt.Sprintf("%s-limits", config.Name),
			Namespace: config.Name,
		},
		Spec: corev1.LimitRangeSpec{
			Limits: []corev1.LimitRangeItem{
				{
					Type: corev1.LimitTypeContainer,
					Default: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("1"),
						corev1.ResourceMemory: resource.MustParse("1Gi"),
					},
					DefaultRequest: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("500m"),
						corev1.ResourceMemory: resource.MustParse("512Mi"),
					},
					Max: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("8"),
						corev1.ResourceMemory: resource.MustParse("16Gi"),
					},
					Min: corev1.ResourceList{
						corev1.ResourceCPU:    resource.MustParse("100m"),
						corev1.ResourceMemory: resource.MustParse("128Mi"),
					},
				},
			},
		},
	}
	
	_, err := tm.clientset.CoreV1().LimitRanges(config.Name).Create(ctx, limitRange, metav1.CreateOptions{})
	return err
}

// createNetworkPolicy 创建NetworkPolicy（简化版）
func (tm *TenantManager) createNetworkPolicy(ctx context.Context, config *TenantConfig) error {
	// 这里简化实现，实际应该使用networking.k8s.io/v1的NetworkPolicy
	// 由于篇幅限制，省略详细实现
	log.Printf("NetworkPolicy creation skipped in this example")
	return nil
}

// createRBAC 创建RBAC
func (tm *TenantManager) createRBAC(ctx context.Context, config *TenantConfig) error {
	// 创建管理员Role
	adminRole := &rbacv1.Role{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "tenant-admin",
			Namespace: config.Name,
		},
		Rules: []rbacv1.PolicyRule{
			{
				APIGroups: []string{"", "apps", "batch"},
				Resources: []string{"*"},
				Verbs:     []string{"*"},
			},
		},
	}
	
	if _, err := tm.clientset.RbacV1().Roles(config.Name).Create(ctx, adminRole, metav1.CreateOptions{}); err != nil {
		return err
	}
	
	// 创建管理员RoleBinding
	for _, admin := range config.Admins {
		binding := &rbacv1.RoleBinding{
			ObjectMeta: metav1.ObjectMeta{
				Name:      fmt.Sprintf("%s-admin-binding", admin),
				Namespace: config.Name,
			},
			Subjects: []rbacv1.Subject{
				{
					Kind:     "User",
					Name:     admin,
					APIGroup: "rbac.authorization.k8s.io",
				},
			},
			RoleRef: rbacv1.RoleRef{
				Kind:     "Role",
				Name:     "tenant-admin",
				APIGroup: "rbac.authorization.k8s.io",
			},
		}
		
		if _, err := tm.clientset.RbacV1().RoleBindings(config.Name).Create(ctx, binding, metav1.CreateOptions{}); err != nil {
			return err
		}
	}
	
	return nil
}

// DeleteTenant 删除租户
func (tm *TenantManager) DeleteTenant(ctx context.Context, tenantName string) error {
	// 删除Namespace会级联删除所有资源
	err := tm.clientset.CoreV1().Namespaces().Delete(ctx, tenantName, metav1.DeleteOptions{})
	if err != nil {
		return fmt.Errorf("failed to delete namespace: %v", err)
	}
	
	log.Printf("Successfully deleted tenant: %s", tenantName)
	return nil
}

// GetTenantResourceUsage 获取租户资源使用情况
func (tm *TenantManager) GetTenantResourceUsage(ctx context.Context, tenantName string) (*ResourceUsage, error) {
	// 获取ResourceQuota
	quota, err := tm.clientset.CoreV1().ResourceQuotas(tenantName).Get(ctx, fmt.Sprintf("%s-quota", tenantName), metav1.GetOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get resource quota: %v", err)
	}
	
	usage := &ResourceUsage{
		TenantName: tenantName,
		CPU: ResourceMetric{
			Used:  quota.Status.Used[corev1.ResourceRequestsCPU],
			Limit: quota.Status.Hard[corev1.ResourceRequestsCPU],
		},
		Memory: ResourceMetric{
			Used:  quota.Status.Used[corev1.ResourceRequestsMemory],
			Limit: quota.Status.Hard[corev1.ResourceRequestsMemory],
		},
		Storage: ResourceMetric{
			Used:  quota.Status.Used[corev1.ResourceRequestsStorage],
			Limit: quota.Status.Hard[corev1.ResourceRequestsStorage],
		},
		Pods: ObjectMetric{
			Used:  int(quota.Status.Used.Pods().Value()),
			Limit: int(quota.Status.Hard.Pods().Value()),
		},
	}
	
	return usage, nil
}

// ResourceUsage 资源使用情况
type ResourceUsage struct {
	TenantName string
	CPU        ResourceMetric
	Memory     ResourceMetric
	Storage    ResourceMetric
	Pods       ObjectMetric
}

// ResourceMetric 资源指标
type ResourceMetric struct {
	Used  resource.Quantity
	Limit resource.Quantity
}

// ObjectMetric 对象指标
type ObjectMetric struct {
	Used  int
	Limit int
}

// 使用示例
func main() {
	ctx := context.Background()
	
	// 创建租户管理器
	manager, err := NewTenantManager("/path/to/kubeconfig")
	if err != nil {
		log.Fatalf("Failed to create tenant manager: %v", err)
	}
	
	// 配置租户
	config := &TenantConfig{
		Name:          "tenant-alpha",
		Environment:   "production",
		CostCenter:    "engineering",
		Contact:       "alpha-team@example.com",
		CPURequest:    "100",
		MemoryRequest: "200Gi",
		CPULimit:      "200",
		MemoryLimit:   "400Gi",
		StorageQuota:  "1Ti",
		MaxPods:       500,
		MaxServices:   100,
		MaxPVCs:       50,
		Admins:        []string{"alice@example.com"},
		Developers:    []string{"alpha-developers"},
		Viewers:       []string{"alpha-viewers"},
	}
	
	// 创建租户
	if err := manager.CreateTenant(ctx, config); err != nil {
		log.Fatalf("Failed to create tenant: %v", err)
	}
	
	// 获取资源使用情况
	usage, err := manager.GetTenantResourceUsage(ctx, "tenant-alpha")
	if err != nil {
		log.Fatalf("Failed to get resource usage: %v", err)
	}
	
	fmt.Printf("Tenant: %s\n", usage.TenantName)
	fmt.Printf("CPU: %s / %s\n", usage.CPU.Used.String(), usage.CPU.Limit.String())
	fmt.Printf("Memory: %s / %s\n", usage.Memory.Used.String(), usage.Memory.Limit.String())
	fmt.Printf("Pods: %d / %d\n", usage.Pods.Used, usage.Pods.Limit)
}
```

**租户隔离验证**

```bash
#!/bin/bash
# tenant-isolation-test.sh
# 验证租户隔离的有效性

set -e

TENANT_NAME="tenant-alpha"
TEST_USER="alice@example.com"

echo "=== 租户隔离验证测试 ==="

# 1. 验证Namespace隔离
echo "1. 验证Namespace隔离..."
kubectl get ns $TENANT_NAME
kubectl describe ns $TENANT_NAME

# 2. 验证ResourceQuota
echo "2. 验证ResourceQuota..."
kubectl get resourcequota -n $TENANT_NAME
kubectl describe resourcequota -n $TENANT_NAME

# 3. 验证LimitRange
echo "3. 验证LimitRange..."
kubectl get limitrange -n $TENANT_NAME
kubectl describe limitrange -n $TENANT_NAME

# 4. 验证NetworkPolicy
echo "4. 验证NetworkPolicy..."
kubectl get networkpolicy -n $TENANT_NAME
kubectl describe networkpolicy -n $TENANT_NAME

# 5. 验证RBAC权限
echo "5. 验证RBAC权限..."
kubectl get role,rolebinding -n $TENANT_NAME

# 6. 测试跨Namespace访问（应该被拒绝）
echo "6. 测试跨Namespace访问..."
kubectl auth can-i get pods -n default --as=$TEST_USER
kubectl auth can-i get pods -n $TENANT_NAME --as=$TEST_USER

# 7. 测试资源配额限制
echo "7. 测试资源配额限制..."
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: quota-test
  namespace: $TENANT_NAME
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    resources:
      requests:
        cpu: "10000"  # 超过配额
        memory: "10000Gi"
EOF

# 8. 测试网络隔离
echo "8. 测试网络隔离..."
kubectl run test-pod -n $TENANT_NAME --image=busybox --rm -it -- sh -c "
  # 测试同Namespace内访问
  wget -O- http://service-in-same-ns:8080
  
  # 测试跨Namespace访问（应该被拒绝）
  wget -O- http://service-in-other-ns.other-namespace:8080
"

echo "=== 验证完成 ==="
```

**最佳实践总结**

```yaml
# Namespace级别资源隔离的最佳实践

1. Namespace命名规范:
   格式: <team>-<environment>-<purpose>
   示例:
     - alpha-prod-web
     - beta-dev-api
     - platform-staging-db

2. ResourceQuota设置原则:
   - 根据团队规模和应用需求设置合理配额
   - 预留20-30%的缓冲空间
   - 定期审查和调整配额
   - 使用PriorityClass区分关键和非关键工作负载

3. LimitRange配置建议:
   - 设置合理的默认值，避免资源浪费
   - 限制单个容器的最大资源，防止资源垄断
   - 设置合理的limit/request比率（建议2-4倍）

4. NetworkPolicy策略:
   - 默认拒绝所有流量
   - 显式允许必要的通信
   - 使用标签选择器而非IP地址
   - 定期审查和更新策略

5. RBAC权限管理:
   - 遵循最小权限原则
   - 使用Role而非ClusterRole
   - 定期审计权限使用情况
   - 使用ServiceAccount而非用户凭证

6. 监控和告警:
   - 监控资源配额使用率
   - 设置配额使用率告警（如80%、90%）
   - 监控NetworkPolicy拒绝的连接
   - 审计RBAC权限变更

7. 自动化管理:
   - 使用GitOps管理租户配置
   - 自动化租户创建和删除流程
   - 定期清理未使用的资源
   - 使用Admission Webhook强制策略
```



### 10.1.3 虚拟集群技术

**虚拟集群概述**

虚拟集群（Virtual Cluster）是一种在物理Kubernetes集群之上创建逻辑隔离集群的技术。每个虚拟集群拥有独立的控制平面（API Server、Controller Manager、Scheduler），但共享底层的计算、存储和网络资源。

**虚拟集群的优势**：

```yaml
# 虚拟集群 vs 传统Namespace隔离

传统Namespace隔离:
  优点:
    - 实现简单
    - 资源开销小
    - 原生Kubernetes支持
  
  缺点:
    - 隔离强度有限
    - 共享控制平面
    - 无法隔离CRD
    - 集群级资源冲突
    - 版本升级影响所有租户

虚拟集群:
  优点:
    - 强隔离（独立控制平面）
    - 完全的API兼容性
    - 独立的CRD和集群资源
    - 独立升级和配置
    - 更好的安全性
  
  缺点:
    - 资源开销较大
    - 实现复杂
    - 需要额外工具支持
```

**虚拟集群架构**

```yaml
# 虚拟集群架构设计

物理集群（Host Cluster）:
  角色: 提供底层基础设施
  组件:
    - 物理节点
    - 存储系统
    - 网络系统
    - 基础控制平面

虚拟集群（Virtual Cluster）:
  角色: 为租户提供独立的Kubernetes环境
  组件:
    - 虚拟API Server
    - 虚拟Controller Manager
    - 虚拟Scheduler
    - 虚拟etcd（可选）
  
  资源映射:
    虚拟资源 -> 物理资源:
      - vPod -> Pod（在host namespace中）
      - vService -> Service（带前缀）
      - vPVC -> PVC（带前缀）
      - vNode -> 虚拟节点（映射到物理节点）

同步机制:
  下行同步（Virtual -> Host）:
    - Pod规格
    - Service定义
    - PVC请求
    - ConfigMap/Secret
  
  上行同步（Host -> Virtual）:
    - Pod状态
    - Service端点
    - PVC绑定状态
    - 事件信息
```

**vcluster实战**

vcluster是最流行的开源虚拟集群解决方案，由Loft Labs开发。

```yaml
# vcluster-values.yaml
# vcluster配置文件

# 虚拟集群基本配置
vcluster:
  # 镜像配置
  image: rancher/k3s:v1.28.2-k3s1
  
  # 资源限制
  resources:
    limits:
      cpu: "2"
      memory: 4Gi
    requests:
      cpu: "1"
      memory: 2Gi
  
  # 持久化存储
  storage:
    size: 10Gi
    className: fast-ssd
  
  # 高可用配置
  replicas: 3
  
  # 额外参数
  extraArgs:
    - --service-cidr=10.96.0.0/16
    - --cluster-cidr=10.244.0.0/16

# 同步配置
sync:
  # 同步的资源类型
  services:
    enabled: true
  configmaps:
    enabled: true
    all: false  # 只同步被Pod使用的ConfigMap
  secrets:
    enabled: true
    all: false  # 只同步被Pod使用的Secret
  endpoints:
    enabled: true
  pods:
    enabled: true
    ephemeralContainers: true
    status: true
  events:
    enabled: true
  persistentvolumeclaims:
    enabled: true
  ingresses:
    enabled: true
  storageclasses:
    enabled: true
  priorityclasses:
    enabled: false
  networkpolicies:
    enabled: true
  volumesnapshots:
    enabled: false
  poddisruptionbudgets:
    enabled: false
  serviceaccounts:
    enabled: true

# 网络配置
networking:
  # 使用host网络
  replicateServices:
    toHost:
      - from: default/*
        to: vcluster-{{ .Release.Name }}-*
  
  # DNS配置
  advanced:
    clusterDomain: "cluster.local"
    fallbackHostCluster: true

# RBAC配置
rbac:
  clusterRole:
    create: true
  role:
    create: true

# 隔离配置
isolation:
  enabled: true
  namespace: vcluster-{{ .Release.Name }}
  
  # Pod安全策略
  podSecurityStandard: baseline
  
  # 资源配额
  resourceQuota:
    enabled: true
    quota:
      requests.cpu: "10"
      requests.memory: 20Gi
      requests.storage: 100Gi
      persistentvolumeclaims: "20"
      services.loadbalancers: "5"
  
  # 限制范围
  limitRange:
    enabled: true
    default:
      cpu: "1"
      memory: 1Gi
    defaultRequest:
      cpu: "100m"
      memory: 128Mi

# 监控配置
monitoring:
  serviceMonitor:
    enabled: true
    labels:
      prometheus: kube-prometheus

# 备份配置
backup:
  enabled: true
  schedule: "0 2 * * *"
  retention: 7
```

**部署vcluster**

```bash
#!/bin/bash
# deploy-vcluster.sh
# 部署虚拟集群

set -e

VCLUSTER_NAME="tenant-alpha-vcluster"
NAMESPACE="vcluster-alpha"
KUBECONFIG_OUTPUT="./kubeconfig-${VCLUSTER_NAME}.yaml"

echo "=== 部署虚拟集群 ==="

# 1. 安装vcluster CLI
echo "1. 安装vcluster CLI..."
curl -L -o vcluster "https://github.com/loft-sh/vcluster/releases/latest/download/vcluster-linux-amd64"
chmod +x vcluster
sudo mv vcluster /usr/local/bin/

# 2. 创建Namespace
echo "2. 创建Namespace..."
kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

# 3. 部署vcluster
echo "3. 部署vcluster..."
vcluster create $VCLUSTER_NAME   --namespace $NAMESPACE   --values vcluster-values.yaml   --connect=false

# 4. 等待vcluster就绪
echo "4. 等待vcluster就绪..."
kubectl wait --for=condition=ready pod   -l app=$VCLUSTER_NAME   -n $NAMESPACE   --timeout=300s

# 5. 获取kubeconfig
echo "5. 获取kubeconfig..."
vcluster connect $VCLUSTER_NAME   --namespace $NAMESPACE   --update-current=false   --print > $KUBECONFIG_OUTPUT

echo "虚拟集群已部署！"
echo "Kubeconfig文件: $KUBECONFIG_OUTPUT"
echo ""
echo "使用以下命令访问虚拟集群:"
echo "export KUBECONFIG=$KUBECONFIG_OUTPUT"
echo "kubectl get nodes"

# 6. 验证虚拟集群
echo ""
echo "6. 验证虚拟集群..."
export KUBECONFIG=$KUBECONFIG_OUTPUT
kubectl cluster-info
kubectl get nodes
kubectl get namespaces

echo "=== 部署完成 ==="
```

**虚拟集群管理**

```go
// vcluster-manager.go
// 虚拟集群管理工具

package main

import (
	"context"
	"fmt"
	"log"
	"os/exec"
	"time"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
)

// VClusterConfig 虚拟集群配置
type VClusterConfig struct {
	Name           string
	Namespace      string
	K8sVersion     string
	Replicas       int32
	CPULimit       string
	MemoryLimit    string
	StorageSize    string
	StorageClass   string
	EnableBackup   bool
	EnableMonitoring bool
}

// VClusterManager 虚拟集群管理器
type VClusterManager struct {
	clientset *kubernetes.Clientset
}

// NewVClusterManager 创建虚拟集群管理器
func NewVClusterManager(kubeconfig string) (*VClusterManager, error) {
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create clientset: %v", err)
	}
	
	return &VClusterManager{clientset: clientset}, nil
}

// CreateVCluster 创建虚拟集群
func (vm *VClusterManager) CreateVCluster(ctx context.Context, config *VClusterConfig) error {
	log.Printf("Creating virtual cluster: %s", config.Name)
	
	// 1. 创建Namespace
	if err := vm.createNamespace(ctx, config); err != nil {
		return fmt.Errorf("failed to create namespace: %v", err)
	}
	
	// 2. 生成Helm values
	valuesFile, err := vm.generateHelmValues(config)
	if err != nil {
		return fmt.Errorf("failed to generate helm values: %v", err)
	}
	
	// 3. 使用vcluster CLI创建
	cmd := exec.Command("vcluster", "create", config.Name,
		"--namespace", config.Namespace,
		"--values", valuesFile,
		"--connect=false",
	)
	
	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("failed to create vcluster: %v, output: %s", err, output)
	}
	
	log.Printf("Virtual cluster created: %s", config.Name)
	
	// 4. 等待就绪
	if err := vm.waitForReady(ctx, config); err != nil {
		return fmt.Errorf("vcluster not ready: %v", err)
	}
	
	log.Printf("Virtual cluster is ready: %s", config.Name)
	return nil
}

// createNamespace 创建Namespace
func (vm *VClusterManager) createNamespace(ctx context.Context, config *VClusterConfig) error {
	ns := &corev1.Namespace{
		ObjectMeta: metav1.ObjectMeta{
			Name: config.Namespace,
			Labels: map[string]string{
				"vcluster": config.Name,
				"type":     "virtual-cluster",
			},
		},
	}
	
	_, err := vm.clientset.CoreV1().Namespaces().Create(ctx, ns, metav1.CreateOptions{})
	if err != nil {
		// 忽略已存在错误
		return nil
	}
	
	return nil
}

// generateHelmValues 生成Helm values文件
func (vm *VClusterManager) generateHelmValues(config *VClusterConfig) (string, error) {
	// 这里简化实现，实际应该生成完整的values.yaml
	valuesContent := fmt.Sprintf(`
vcluster:
  image: rancher/k3s:%s
  replicas: %d
  resources:
    limits:
      cpu: %s
      memory: %s
  storage:
    size: %s
    className: %s

isolation:
  enabled: true
  namespace: %s

monitoring:
  serviceMonitor:
    enabled: %t

backup:
  enabled: %t
`, config.K8sVersion, config.Replicas, config.CPULimit, config.MemoryLimit,
		config.StorageSize, config.StorageClass, config.Namespace,
		config.EnableMonitoring, config.EnableBackup)
	
	// 写入临时文件
	tmpFile := fmt.Sprintf("/tmp/vcluster-values-%s.yaml", config.Name)
	// 实际实现应该写入文件
	log.Printf("Generated values file: %s", tmpFile)
	
	return tmpFile, nil
}

// waitForReady 等待虚拟集群就绪
func (vm *VClusterManager) waitForReady(ctx context.Context, config *VClusterConfig) error {
	timeout := time.After(5 * time.Minute)
	ticker := time.NewTicker(10 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-timeout:
			return fmt.Errorf("timeout waiting for vcluster to be ready")
		case <-ticker.C:
			// 检查Pod状态
			pods, err := vm.clientset.CoreV1().Pods(config.Namespace).List(ctx, metav1.ListOptions{
				LabelSelector: fmt.Sprintf("app=%s", config.Name),
			})
			if err != nil {
				log.Printf("Error checking pod status: %v", err)
				continue
			}
			
			if len(pods.Items) == 0 {
				log.Printf("Waiting for pods to be created...")
				continue
			}
			
			allReady := true
			for _, pod := range pods.Items {
				if pod.Status.Phase != corev1.PodRunning {
					allReady = false
					break
				}
			}
			
			if allReady {
				return nil
			}
			
			log.Printf("Waiting for all pods to be ready...")
		}
	}
}

// DeleteVCluster 删除虚拟集群
func (vm *VClusterManager) DeleteVCluster(ctx context.Context, name, namespace string) error {
	log.Printf("Deleting virtual cluster: %s", name)
	
	cmd := exec.Command("vcluster", "delete", name,
		"--namespace", namespace,
	)
	
	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("failed to delete vcluster: %v, output: %s", err, output)
	}
	
	log.Printf("Virtual cluster deleted: %s", name)
	return nil
}

// GetVClusterKubeconfig 获取虚拟集群的kubeconfig
func (vm *VClusterManager) GetVClusterKubeconfig(name, namespace string) (string, error) {
	cmd := exec.Command("vcluster", "connect", name,
		"--namespace", namespace,
		"--update-current=false",
		"--print",
	)
	
	output, err := cmd.Output()
	if err != nil {
		return "", fmt.Errorf("failed to get kubeconfig: %v", err)
	}
	
	return string(output), nil
}

// ListVClusters 列出所有虚拟集群
func (vm *VClusterManager) ListVClusters(ctx context.Context) ([]VClusterInfo, error) {
	// 查找所有vcluster相关的Namespace
	namespaces, err := vm.clientset.CoreV1().Namespaces().List(ctx, metav1.ListOptions{
		LabelSelector: "type=virtual-cluster",
	})
	if err != nil {
		return nil, fmt.Errorf("failed to list namespaces: %v", err)
	}
	
	var vclusters []VClusterInfo
	for _, ns := range namespaces.Items {
		vclusterName := ns.Labels["vcluster"]
		if vclusterName == "" {
			continue
		}
		
		// 获取Pod状态
		pods, err := vm.clientset.CoreV1().Pods(ns.Name).List(ctx, metav1.ListOptions{
			LabelSelector: fmt.Sprintf("app=%s", vclusterName),
		})
		if err != nil {
			log.Printf("Error getting pods for vcluster %s: %v", vclusterName, err)
			continue
		}
		
		status := "Unknown"
		if len(pods.Items) > 0 {
			allRunning := true
			for _, pod := range pods.Items {
				if pod.Status.Phase != corev1.PodRunning {
					allRunning = false
					break
				}
			}
			if allRunning {
				status = "Running"
			} else {
				status = "Pending"
			}
		}
		
		vclusters = append(vclusters, VClusterInfo{
			Name:      vclusterName,
			Namespace: ns.Name,
			Status:    status,
			Age:       time.Since(ns.CreationTimestamp.Time).String(),
		})
	}
	
	return vclusters, nil
}

// VClusterInfo 虚拟集群信息
type VClusterInfo struct {
	Name      string
	Namespace string
	Status    string
	Age       string
}

// 使用示例
func main() {
	ctx := context.Background()
	
	// 创建管理器
	manager, err := NewVClusterManager("/path/to/kubeconfig")
	if err != nil {
		log.Fatalf("Failed to create manager: %v", err)
	}
	
	// 配置虚拟集群
	config := &VClusterConfig{
		Name:             "tenant-alpha-vcluster",
		Namespace:        "vcluster-alpha",
		K8sVersion:       "v1.28.2-k3s1",
		Replicas:         3,
		CPULimit:         "2",
		MemoryLimit:      "4Gi",
		StorageSize:      "10Gi",
		StorageClass:     "fast-ssd",
		EnableBackup:     true,
		EnableMonitoring: true,
	}
	
	// 创建虚拟集群
	if err := manager.CreateVCluster(ctx, config); err != nil {
		log.Fatalf("Failed to create vcluster: %v", err)
	}
	
	// 获取kubeconfig
	kubeconfig, err := manager.GetVClusterKubeconfig(config.Name, config.Namespace)
	if err != nil {
		log.Fatalf("Failed to get kubeconfig: %v", err)
	}
	fmt.Printf("Kubeconfig:\n%s\n", kubeconfig)
	
	// 列出所有虚拟集群
	vclusters, err := manager.ListVClusters(ctx)
	if err != nil {
		log.Fatalf("Failed to list vclusters: %v", err)
	}
	
	fmt.Println("\nVirtual Clusters:")
	for _, vc := range vclusters {
		fmt.Printf("- %s (namespace: %s, status: %s, age: %s)\n",
			vc.Name, vc.Namespace, vc.Status, vc.Age)
	}
}
```

**其他虚拟集群方案**

```yaml
# 虚拟集群技术对比

1. vcluster (Loft Labs):
   架构: 在Pod中运行完整的K3s/K8s
   优势:
     - 开源免费
     - 轻量级（基于K3s）
     - 易于部署和管理
     - 良好的资源隔离
   劣势:
     - 资源开销相对较大
     - 需要额外的同步机制
   适用场景:
     - 开发测试环境
     - 多租户SaaS平台
     - CI/CD流水线

2. Kamaji:
   架构: 托管控制平面，共享数据平面
   优势:
     - 控制平面完全隔离
     - 支持多种etcd后端
     - 更好的性能
     - 企业级特性
   劣势:
     - 实现复杂
     - 需要额外的基础设施
   适用场景:
     - 大规模多租户
     - 云服务提供商
     - 企业私有云

3. Cluster API vCluster Provider:
   架构: 基于Cluster API的虚拟集群
   优势:
     - 标准化的集群管理
     - 与Cluster API生态集成
     - 声明式管理
   劣势:
     - 学习曲线陡峭
     - 依赖Cluster API
   适用场景:
     - 已使用Cluster API的环境
     - 需要统一管理物理和虚拟集群

4. kcp (Kubernetes Control Plane):
   架构: 轻量级控制平面，无节点概念
   优势:
     - 极致轻量
     - 快速启动
     - 适合大规模场景
   劣势:
     - 仍在早期开发阶段
     - 生态不完善
   适用场景:
     - 实验性项目
     - 超大规模多租户
```

**虚拟集群网络配置**

```yaml
# vcluster-network-config.yaml
# 虚拟集群网络配置示例

apiVersion: v1
kind: ConfigMap
metadata:
  name: vcluster-network-config
  namespace: vcluster-alpha
data:
  # 网络模式配置
  network-mode: |
    # 模式1: 默认模式（推荐）
    # Pod使用host集群的网络，Service通过同步实现
    mode: default
    
    # 模式2: 独立网络
    # 使用CNI插件创建独立网络
    mode: isolated
    cni:
      plugin: calico
      podCIDR: 10.100.0.0/16
      serviceCIDR: 10.101.0.0/16
    
    # 模式3: 混合模式
    # 部分资源使用独立网络，部分共享
    mode: hybrid
    isolated:
      - namespace: production
      - namespace: staging
    shared:
      - namespace: development

  # Service同步配置
  service-sync: |
    # 从虚拟集群同步到host集群
    toHost:
      - from: "*/nginx-*"
        to: "vcluster-alpha-nginx-*"
      - from: "production/*"
        to: "vcluster-alpha-prod-*"
    
    # 从host集群同步到虚拟集群
    fromHost:
      - from: "shared-services/*"
        to: "external/*"

  # DNS配置
  dns-config: |
    # 虚拟集群DNS设置
    clusterDomain: vcluster.local
    
    # DNS转发规则
    forwarding:
      # 转发到host集群DNS
      - domain: "*.svc.cluster.local"
        nameserver: 10.96.0.10
      
      # 转发到外部DNS
      - domain: "*.example.com"
        nameserver: 8.8.8.8

  # Ingress配置
  ingress-config: |
    # Ingress同步策略
    sync:
      enabled: true
      # 自动添加前缀避免冲突
      hostnamePrefix: "vcluster-alpha-"
      
      # TLS证书同步
      tlsSecrets:
        enabled: true
        namespace: cert-manager
    
    # Ingress Controller配置
    controller:
      # 使用host集群的Ingress Controller
      useHost: true
      # 或部署独立的Ingress Controller
      deploy: false

---
# NetworkPolicy for vcluster
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: vcluster-network-policy
  namespace: vcluster-alpha
spec:
  podSelector:
    matchLabels:
      app: tenant-alpha-vcluster
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  # 允许来自同namespace的流量
  - from:
    - podSelector: {}
  
  # 允许来自host集群API Server的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 8443
  
  egress:
  # 允许访问host集群API Server
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 6443
  
  # 允许DNS查询
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  
  # 允许访问同步的Pod
  - to:
    - podSelector: {}
```

**虚拟集群监控和日志**

```yaml
# vcluster-monitoring.yaml
# 虚拟集群监控配置

---
# ServiceMonitor for Prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vcluster-metrics
  namespace: vcluster-alpha
  labels:
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: tenant-alpha-vcluster
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---
# PrometheusRule for alerting
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: vcluster-alerts
  namespace: vcluster-alpha
spec:
  groups:
  - name: vcluster
    interval: 30s
    rules:
    # vcluster Pod不健康告警
    - alert: VClusterPodNotReady
      expr: |
        kube_pod_status_phase{namespace="vcluster-alpha",phase!="Running"} > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "vcluster Pod不健康"
        description: "虚拟集群 {{ $labels.pod }} 状态异常"
    
    # vcluster API Server延迟告警
    - alert: VClusterAPIServerHighLatency
      expr: |
        histogram_quantile(0.99, 
          rate(apiserver_request_duration_seconds_bucket{namespace="vcluster-alpha"}[5m])
        ) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "vcluster API Server延迟过高"
        description: "P99延迟: {{ $value }}s"
    
    # vcluster etcd存储告警
    - alert: VClusterEtcdStorageHigh
      expr: |
        (etcd_mvcc_db_total_size_in_bytes{namespace="vcluster-alpha"} 
         / etcd_server_quota_backend_bytes{namespace="vcluster-alpha"}) > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "vcluster etcd存储使用率过高"
        description: "存储使用率: {{ $value | humanizePercentage }}"

---
# Grafana Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: vcluster-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  vcluster-dashboard.json: |
    {
      "dashboard": {
        "title": "Virtual Cluster Monitoring",
        "panels": [
          {
            "title": "vcluster Pod状态",
            "targets": [
              {
                "expr": "kube_pod_status_phase{namespace=~"vcluster-.*"}"
              }
            ]
          },
          {
            "title": "API Server请求率",
            "targets": [
              {
                "expr": "rate(apiserver_request_total{namespace=~"vcluster-.*"}[5m])"
              }
            ]
          },
          {
            "title": "资源使用情况",
            "targets": [
              {
                "expr": "container_memory_usage_bytes{namespace=~"vcluster-.*"}"
              },
              {
                "expr": "rate(container_cpu_usage_seconds_total{namespace=~"vcluster-.*"}[5m])"
              }
            ]
          }
        ]
      }
    }
```

**虚拟集群最佳实践**

```yaml
# 虚拟集群最佳实践总结

1. 资源规划:
   控制平面资源:
     - 小型vcluster（<50 nodes）: 1 CPU, 2Gi内存
     - 中型vcluster（50-200 nodes）: 2 CPU, 4Gi内存
     - 大型vcluster（>200 nodes）: 4 CPU, 8Gi内存
   
   存储规划:
     - etcd存储: 至少10Gi，建议使用SSD
     - 日志存储: 根据日志量规划
     - 备份存储: etcd大小的3-5倍
   
   网络规划:
     - 预留足够的IP地址空间
     - 避免CIDR冲突
     - 考虑跨集群通信需求

2. 高可用配置:
   控制平面:
     - 至少3个副本
     - 使用Pod反亲和性分散到不同节点
     - 配置PodDisruptionBudget
   
   etcd:
     - 使用持久化存储
     - 定期备份
     - 监控存储使用率
   
   网络:
     - 配置健康检查
     - 使用Service Mesh提高可靠性
     - 实施重试和超时策略

3. 安全加固:
   网络隔离:
     - 使用NetworkPolicy限制流量
     - 启用mTLS加密
     - 限制对host集群的访问
   
   访问控制:
     - 为每个vcluster配置独立的RBAC
     - 使用ServiceAccount而非用户凭证
     - 定期轮换证书和密钥
   
   审计:
     - 启用API Server审计日志
     - 监控异常访问模式
     - 定期安全扫描

4. 性能优化:
   API Server:
     - 调整--max-requests-inflight参数
     - 启用API优先级和公平性
     - 使用缓存减少etcd负载
   
   etcd:
     - 使用SSD存储
     - 调整--quota-backend-bytes
     - 定期压缩和碎片整理
   
   同步优化:
     - 只同步必要的资源
     - 使用标签选择器过滤
     - 调整同步间隔

5. 运维管理:
   监控:
     - 监控控制平面健康状态
     - 跟踪资源使用趋势
     - 设置合理的告警阈值
   
   备份恢复:
     - 定期备份etcd数据
     - 测试恢复流程
     - 保留多个备份版本
   
   升级策略:
     - 先在测试环境验证
     - 使用滚动升级
     - 准备回滚方案
   
   成本优化:
     - 合理设置资源限制
     - 使用节点亲和性优化调度
     - 定期清理未使用的vcluster

6. 故障排查:
   常见问题:
     - Pod无法启动: 检查资源配额和镜像
     - 网络不通: 验证NetworkPolicy和DNS
     - 性能下降: 检查etcd和API Server负载
   
   调试工具:
     - kubectl logs: 查看控制平面日志
     - kubectl exec: 进入容器调试
     - vcluster connect: 连接到虚拟集群
     - kubectl describe: 查看资源详情
   
   日志收集:
     - 收集控制平面日志
     - 收集同步器日志
     - 收集host集群相关日志
```

**虚拟集群使用场景**

```yaml
# 虚拟集群典型使用场景

场景1: 多租户SaaS平台
  需求:
    - 为每个客户提供独立的Kubernetes环境
    - 完全的资源和API隔离
    - 独立的版本和配置
  
  方案:
    - 每个客户一个vcluster
    - 使用ResourceQuota限制资源
    - 配置独立的Ingress和域名
    - 实施严格的网络隔离
  
  优势:
    - 强隔离保证安全性
    - 客户可以自由管理资源
    - 降低运维复杂度

场景2: 开发测试环境
  需求:
    - 快速创建和销毁环境
    - 与生产环境隔离
    - 支持多个并行测试
  
  方案:
    - 为每个分支/PR创建临时vcluster
    - 测试完成后自动清理
    - 使用较小的资源配额
    - 共享镜像仓库和存储
  
  优势:
    - 快速环境准备
    - 完全隔离避免干扰
    - 节省成本

场景3: CI/CD流水线
  需求:
    - 并行执行多个构建任务
    - 隔离不同的构建环境
    - 自动化管理
  
  方案:
    - 为每个构建任务创建临时vcluster
    - 使用GitOps管理配置
    - 集成到CI/CD工具链
    - 构建完成后自动清理
  
  优势:
    - 提高并行度
    - 避免环境污染
    - 可重复的构建环境

场景4: 培训和演示
  需求:
    - 为学员提供独立环境
    - 快速重置环境
    - 限制资源使用
  
  方案:
    - 为每个学员创建vcluster
    - 预配置常用工具和应用
    - 设置资源配额防止滥用
    - 定期清理和重置
  
  优势:
    - 学员有完整的管理权限
    - 互不干扰
    - 易于管理和重置
```



### 10.1.4 多租户最佳实践

**多租户架构选型决策树**

```yaml
# 多租户方案选择指南

决策因素:
  1. 租户信任级别
  2. 隔离强度要求
  3. 成本预算
  4. 运维复杂度
  5. 性能要求
  6. 合规性要求

决策流程:
  问题1: 租户是否来自不同组织？
    是 -> 考虑硬多租户或虚拟集群
    否 -> 继续问题2
  
  问题2: 是否需要独立的Kubernetes版本？
    是 -> 使用虚拟集群
    否 -> 继续问题3
  
  问题3: 是否需要隔离CRD和集群资源？
    是 -> 使用虚拟集群
    否 -> 继续问题4
  
  问题4: 预算是否充足？
    是 -> 使用虚拟集群或独立集群
    否 -> 使用Namespace隔离
  
  问题5: 是否有严格的合规要求？
    是 -> 使用独立集群
    否 -> 根据前面的答案选择方案

推荐方案:
  场景A - 企业内部团队:
    方案: Namespace + RBAC + ResourceQuota
    理由: 成本低，管理简单，隔离足够
  
  场景B - 多客户SaaS:
    方案: 虚拟集群（vcluster）
    理由: 强隔离，独立管理，成本可控
  
  场景C - 金融/医疗等高合规行业:
    方案: 独立物理集群
    理由: 最强隔离，满足合规要求
  
  场景D - 开发测试环境:
    方案: 虚拟集群（临时）
    理由: 快速创建销毁，完全隔离
```

**多租户安全加固**

```yaml
# multi-tenant-security.yaml
# 多租户安全加固配置

---
# PodSecurityPolicy（已废弃，使用Pod Security Standards）
# Pod Security Standards配置
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-alpha
  labels:
    # 设置Pod安全标准
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
spec: {}

---
# 使用Gatekeeper/OPA实施策略
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        openAPIV3Schema:
          type: object
          properties:
            labels:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels
        
        violation[{"msg": msg, "details": {"missing_labels": missing}}] {
          provided := {label | input.review.object.metadata.labels[label]}
          required := {label | label := input.parameters.labels[_]}
          missing := required - provided
          count(missing) > 0
          msg := sprintf("必须包含标签: %v", [missing])
        }

---
# 应用策略：要求所有Pod必须有租户标签
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: require-tenant-label
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - tenant-alpha
      - tenant-beta
  parameters:
    labels:
      - "tenant"
      - "environment"
      - "cost-center"

---
# 限制容器镜像来源
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8sallowedrepos
spec:
  crd:
    spec:
      names:
        kind: K8sAllowedRepos
      validation:
        openAPIV3Schema:
          type: object
          properties:
            repos:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sallowedrepos
        
        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          satisfied := [good | repo = input.parameters.repos[_]
                              good = startswith(container.image, repo)]
          not any(satisfied)
          msg := sprintf("容器镜像 %v 不在允许的仓库列表中", [container.image])
        }

---
# 应用策略：限制镜像仓库
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sAllowedRepos
metadata:
  name: allowed-repos
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - tenant-alpha
  parameters:
    repos:
      - "registry.example.com/"
      - "docker.io/library/"

---
# 禁止特权容器
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8spsprivilegedcontainer
spec:
  crd:
    spec:
      names:
        kind: K8sPSPrivilegedContainer
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8spsprivileged
        
        violation[{"msg": msg}] {
          c := input_containers[_]
          c.securityContext.privileged
          msg := sprintf("禁止使用特权容器: %v", [c.name])
        }
        
        input_containers[c] {
          c := input.review.object.spec.containers[_]
        }
        
        input_containers[c] {
          c := input.review.object.spec.initContainers[_]
        }

---
# 应用策略：禁止特权容器
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sPSPrivilegedContainer
metadata:
  name: deny-privileged-containers
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - tenant-alpha
      - tenant-beta
```

**多租户成本管理**

```go
// cost-manager.go
// 多租户成本管理和计费系统

package main

import (
	"context"
	"fmt"
	"log"
	"time"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
	metricsv "k8s.io/metrics/pkg/client/clientset/versioned"
)

// CostManager 成本管理器
type CostManager struct {
	clientset        *kubernetes.Clientset
	metricsClientset *metricsv.Clientset
	
	// 价格配置（每小时）
	cpuPrice     float64 // 每核CPU价格
	memoryPrice  float64 // 每GB内存价格
	storagePrice float64 // 每GB存储价格
}

// TenantCost 租户成本
type TenantCost struct {
	TenantName    string
	Namespace     string
	Period        string
	
	// 资源使用量
	CPUCoreHours    float64
	MemoryGBHours   float64
	StorageGBHours  float64
	
	// 成本
	CPUCost         float64
	MemoryCost      float64
	StorageCost     float64
	TotalCost       float64
	
	// 资源详情
	PodCount        int
	ServiceCount    int
	PVCCount        int
}

// NewCostManager 创建成本管理器
func NewCostManager(kubeconfig string) (*CostManager, error) {
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		return nil, fmt.Errorf("failed to build config: %v", err)
	}
	
	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create clientset: %v", err)
	}
	
	metricsClientset, err := metricsv.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create metrics clientset: %v", err)
	}
	
	return &CostManager{
		clientset:        clientset,
		metricsClientset: metricsClientset,
		cpuPrice:         0.05,  // $0.05/核/小时
		memoryPrice:      0.01,  // $0.01/GB/小时
		storagePrice:     0.001, // $0.001/GB/小时
	}, nil
}

// CalculateTenantCost 计算租户成本
func (cm *CostManager) CalculateTenantCost(ctx context.Context, namespace string, hours float64) (*TenantCost, error) {
	cost := &TenantCost{
		Namespace: namespace,
		Period:    fmt.Sprintf("%.1f hours", hours),
	}
	
	// 1. 获取Pod指标
	podMetrics, err := cm.metricsClientset.MetricsV1beta1().PodMetricses(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get pod metrics: %v", err)
	}
	
	// 计算CPU和内存使用
	var totalCPU, totalMemory float64
	for _, pm := range podMetrics.Items {
		for _, container := range pm.Containers {
			// CPU（毫核转换为核）
			cpuMillis := float64(container.Usage.Cpu().MilliValue())
			totalCPU += cpuMillis / 1000.0
			
			// 内存（字节转换为GB）
			memoryBytes := float64(container.Usage.Memory().Value())
			totalMemory += memoryBytes / (1024 * 1024 * 1024)
		}
	}
	
	cost.CPUCoreHours = totalCPU * hours
	cost.MemoryGBHours = totalMemory * hours
	cost.CPUCost = cost.CPUCoreHours * cm.cpuPrice
	cost.MemoryCost = cost.MemoryGBHours * cm.memoryPrice
	
	// 2. 获取存储使用
	pvcs, err := cm.clientset.CoreV1().PersistentVolumeClaims(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get PVCs: %v", err)
	}
	
	var totalStorage float64
	for _, pvc := range pvcs.Items {
		if pvc.Status.Phase == corev1.ClaimBound {
			storageBytes := float64(pvc.Spec.Resources.Requests.Storage().Value())
			totalStorage += storageBytes / (1024 * 1024 * 1024)
		}
	}
	
	cost.StorageGBHours = totalStorage * hours
	cost.StorageCost = cost.StorageGBHours * cm.storagePrice
	cost.PVCCount = len(pvcs.Items)
	
	// 3. 获取资源数量
	pods, err := cm.clientset.CoreV1().Pods(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get pods: %v", err)
	}
	cost.PodCount = len(pods.Items)
	
	services, err := cm.clientset.CoreV1().Services(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get services: %v", err)
	}
	cost.ServiceCount = len(services.Items)
	
	// 4. 计算总成本
	cost.TotalCost = cost.CPUCost + cost.MemoryCost + cost.StorageCost
	
	// 从Namespace标签获取租户名称
	ns, err := cm.clientset.CoreV1().Namespaces().Get(ctx, namespace, metav1.GetOptions{})
	if err == nil {
		cost.TenantName = ns.Labels["tenant"]
	}
	
	return cost, nil
}

// GenerateCostReport 生成成本报告
func (cm *CostManager) GenerateCostReport(ctx context.Context, tenantLabel string) ([]*TenantCost, error) {
	// 获取所有租户Namespace
	namespaces, err := cm.clientset.CoreV1().Namespaces().List(ctx, metav1.ListOptions{
		LabelSelector: labels.SelectorFromSet(map[string]string{
			"tenant": tenantLabel,
		}).String(),
	})
	if err != nil {
		return nil, fmt.Errorf("failed to list namespaces: %v", err)
	}
	
	var costs []*TenantCost
	for _, ns := range namespaces.Items {
		// 计算过去24小时的成本
		cost, err := cm.CalculateTenantCost(ctx, ns.Name, 24.0)
		if err != nil {
			log.Printf("Failed to calculate cost for %s: %v", ns.Name, err)
			continue
		}
		costs = append(costs, cost)
	}
	
	return costs, nil
}

// ExportCostReport 导出成本报告
func (cm *CostManager) ExportCostReport(costs []*TenantCost) string {
	report := "租户成本报告\n"
	report += "=" + "\n\n"
	
	var totalCost float64
	for _, cost := range costs {
		report += fmt.Sprintf("租户: %s (Namespace: %s)\n", cost.TenantName, cost.Namespace)
		report += fmt.Sprintf("  周期: %s\n", cost.Period)
		report += fmt.Sprintf("  资源:\n")
		report += fmt.Sprintf("    - Pod数量: %d\n", cost.PodCount)
		report += fmt.Sprintf("    - Service数量: %d\n", cost.ServiceCount)
		report += fmt.Sprintf("    - PVC数量: %d\n", cost.PVCCount)
		report += fmt.Sprintf("  使用量:\n")
		report += fmt.Sprintf("    - CPU: %.2f 核·小时\n", cost.CPUCoreHours)
		report += fmt.Sprintf("    - 内存: %.2f GB·小时\n", cost.MemoryGBHours)
		report += fmt.Sprintf("    - 存储: %.2f GB·小时\n", cost.StorageGBHours)
		report += fmt.Sprintf("  成本:\n")
		report += fmt.Sprintf("    - CPU成本: $%.2f\n", cost.CPUCost)
		report += fmt.Sprintf("    - 内存成本: $%.2f\n", cost.MemoryCost)
		report += fmt.Sprintf("    - 存储成本: $%.2f\n", cost.StorageCost)
		report += fmt.Sprintf("    - 总成本: $%.2f\n", cost.TotalCost)
		report += "\n"
		
		totalCost += cost.TotalCost
	}
	
	report += fmt.Sprintf("总计成本: $%.2f\n", totalCost)
	
	return report
}

// SetupCostAlerts 设置成本告警
func (cm *CostManager) SetupCostAlerts(ctx context.Context, namespace string, threshold float64) error {
	// 这里简化实现，实际应该集成到告警系统
	cost, err := cm.CalculateTenantCost(ctx, namespace, 24.0)
	if err != nil {
		return err
	}
	
	if cost.TotalCost > threshold {
		log.Printf("⚠️  成本告警: 租户 %s 的日成本 $%.2f 超过阈值 $%.2f",
			cost.TenantName, cost.TotalCost, threshold)
		// 发送告警通知
		// sendAlert(cost)
	}
	
	return nil
}

// 使用示例
func main() {
	ctx := context.Background()
	
	// 创建成本管理器
	manager, err := NewCostManager("/path/to/kubeconfig")
	if err != nil {
		log.Fatalf("Failed to create cost manager: %v", err)
	}
	
	// 计算单个租户成本
	cost, err := manager.CalculateTenantCost(ctx, "tenant-alpha", 24.0)
	if err != nil {
		log.Fatalf("Failed to calculate cost: %v", err)
	}
	
	fmt.Printf("租户: %s\n", cost.TenantName)
	fmt.Printf("总成本: $%.2f\n", cost.TotalCost)
	
	// 生成所有租户的成本报告
	costs, err := manager.GenerateCostReport(ctx, "")
	if err != nil {
		log.Fatalf("Failed to generate report: %v", err)
	}
	
	report := manager.ExportCostReport(costs)
	fmt.Println(report)
	
	// 设置成本告警
	if err := manager.SetupCostAlerts(ctx, "tenant-alpha", 100.0); err != nil {
		log.Printf("Failed to setup alerts: %v", err)
	}
}
```

**多租户运维自动化**

```yaml
# tenant-operator.yaml
# 使用Operator模式自动化租户管理

apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: tenants.multitenancy.example.com
spec:
  group: multitenancy.example.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                # 租户基本信息
                displayName:
                  type: string
                contact:
                  type: string
                costCenter:
                  type: string
                environment:
                  type: string
                  enum: [development, staging, production]
                
                # 资源配额
                resourceQuota:
                  type: object
                  properties:
                    cpu:
                      type: string
                    memory:
                      type: string
                    storage:
                      type: string
                    pods:
                      type: integer
                
                # 网络配置
                networking:
                  type: object
                  properties:
                    isolation:
                      type: boolean
                    allowedNamespaces:
                      type: array
                      items:
                        type: string
                    externalAccess:
                      type: boolean
                
                # 安全配置
                security:
                  type: object
                  properties:
                    podSecurityStandard:
                      type: string
                      enum: [privileged, baseline, restricted]
                    allowPrivileged:
                      type: boolean
                    allowedRegistries:
                      type: array
                      items:
                        type: string
                
                # 用户和权限
                users:
                  type: array
                  items:
                    type: object
                    properties:
                      email:
                        type: string
                      role:
                        type: string
                        enum: [admin, developer, viewer]
            
            status:
              type: object
              properties:
                phase:
                  type: string
                  enum: [Pending, Active, Terminating, Failed]
                conditions:
                  type: array
                  items:
                    type: object
                    properties:
                      type:
                        type: string
                      status:
                        type: string
                      reason:
                        type: string
                      message:
                        type: string
                      lastTransitionTime:
                        type: string
                        format: date-time
                resourceUsage:
                  type: object
                  properties:
                    cpu:
                      type: string
                    memory:
                      type: string
                    storage:
                      type: string
                    pods:
                      type: integer
  scope: Cluster
  names:
    plural: tenants
    singular: tenant
    kind: Tenant
    shortNames:
    - tn

---
# 租户CR示例
apiVersion: multitenancy.example.com/v1
kind: Tenant
metadata:
  name: alpha-team
spec:
  displayName: "Alpha Team"
  contact: "alpha-team@example.com"
  costCenter: "engineering"
  environment: production
  
  resourceQuota:
    cpu: "100"
    memory: "200Gi"
    storage: "1Ti"
    pods: 500
  
  networking:
    isolation: true
    allowedNamespaces:
      - shared-services
      - monitoring
    externalAccess: true
  
  security:
    podSecurityStandard: baseline
    allowPrivileged: false
    allowedRegistries:
      - "registry.example.com"
      - "docker.io/library"
  
  users:
    - email: "alice@example.com"
      role: admin
    - email: "bob@example.com"
      role: developer
    - email: "charlie@example.com"
      role: viewer
```

**多租户监控和可观测性**

```yaml
# tenant-monitoring.yaml
# 租户级别的监控配置

---
# Prometheus规则：租户资源使用监控
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: tenant-resource-monitoring
  namespace: monitoring
spec:
  groups:
  - name: tenant-resources
    interval: 30s
    rules:
    # CPU配额使用率
    - record: tenant:cpu_quota_usage:ratio
      expr: |
        sum by (namespace) (
          rate(container_cpu_usage_seconds_total{container!=""}[5m])
        ) / on(namespace) group_left()
        kube_resourcequota{resource="requests.cpu", type="hard"}
    
    # 内存配额使用率
    - record: tenant:memory_quota_usage:ratio
      expr: |
        sum by (namespace) (
          container_memory_working_set_bytes{container!=""}
        ) / on(namespace) group_left()
        kube_resourcequota{resource="requests.memory", type="hard"}
    
    # Pod数量配额使用率
    - record: tenant:pod_quota_usage:ratio
      expr: |
        count by (namespace) (
          kube_pod_info
        ) / on(namespace) group_left()
        kube_resourcequota{resource="pods", type="hard"}
    
    # CPU配额使用率告警
    - alert: TenantCPUQuotaHigh
      expr: tenant:cpu_quota_usage:ratio > 0.8
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "租户CPU配额使用率过高"
        description: "租户 {{ $labels.namespace }} 的CPU使用率为 {{ $value | humanizePercentage }}"
    
    # 内存配额使用率告警
    - alert: TenantMemoryQuotaHigh
      expr: tenant:memory_quota_usage:ratio > 0.8
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "租户内存配额使用率过高"
        description: "租户 {{ $labels.namespace }} 的内存使用率为 {{ $value | humanizePercentage }}"
    
    # Pod配额即将耗尽
    - alert: TenantPodQuotaNearLimit
      expr: tenant:pod_quota_usage:ratio > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "租户Pod配额即将耗尽"
        description: "租户 {{ $labels.namespace }} 的Pod使用率为 {{ $value | humanizePercentage }}"

---
# Grafana Dashboard for Tenants
apiVersion: v1
kind: ConfigMap
metadata:
  name: tenant-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  tenant-overview.json: |
    {
      "dashboard": {
        "title": "Multi-Tenant Overview",
        "panels": [
          {
            "title": "租户资源配额使用率",
            "targets": [
              {
                "expr": "tenant:cpu_quota_usage:ratio",
                "legendFormat": "{{ namespace }} - CPU"
              },
              {
                "expr": "tenant:memory_quota_usage:ratio",
                "legendFormat": "{{ namespace }} - Memory"
              }
            ],
            "type": "graph"
          },
          {
            "title": "租户Pod数量",
            "targets": [
              {
                "expr": "count by (namespace) (kube_pod_info)"
              }
            ],
            "type": "graph"
          },
          {
            "title": "租户成本分布",
            "targets": [
              {
                "expr": "sum by (namespace) (tenant_cost_total)"
              }
            ],
            "type": "piechart"
          },
          {
            "title": "租户网络流量",
            "targets": [
              {
                "expr": "sum by (namespace) (rate(container_network_receive_bytes_total[5m]))"
              }
            ],
            "type": "graph"
          }
        ]
      }
    }
```

**多租户故障排查指南**

```bash
#!/bin/bash
# tenant-troubleshooting.sh
# 多租户故障排查工具

set -e

TENANT_NS="$1"

if [ -z "$TENANT_NS" ]; then
    echo "用法: $0 <tenant-namespace>"
    exit 1
fi

echo "=== 租户故障排查: $TENANT_NS ==="
echo ""

# 1. 检查Namespace状态
echo "1. Namespace状态:"
kubectl get ns $TENANT_NS -o yaml | grep -A 5 "status:"
echo ""

# 2. 检查ResourceQuota使用情况
echo "2. ResourceQuota使用情况:"
kubectl describe resourcequota -n $TENANT_NS
echo ""

# 3. 检查LimitRange配置
echo "3. LimitRange配置:"
kubectl describe limitrange -n $TENANT_NS
echo ""

# 4. 检查Pod状态
echo "4. Pod状态:"
kubectl get pods -n $TENANT_NS -o wide
echo ""

# 5. 检查失败的Pod
echo "5. 失败的Pod详情:"
kubectl get pods -n $TENANT_NS --field-selector=status.phase!=Running,status.phase!=Succeeded
for pod in $(kubectl get pods -n $TENANT_NS --field-selector=status.phase!=Running,status.phase!=Succeeded -o jsonpath='{.items[*].metadata.name}'); do
    echo "Pod: $pod"
    kubectl describe pod $pod -n $TENANT_NS | grep -A 10 "Events:"
    echo ""
done

# 6. 检查NetworkPolicy
echo "6. NetworkPolicy配置:"
kubectl get networkpolicy -n $TENANT_NS
echo ""

# 7. 检查RBAC权限
echo "7. RBAC权限:"
kubectl get role,rolebinding -n $TENANT_NS
echo ""

# 8. 检查事件
echo "8. 最近的事件:"
kubectl get events -n $TENANT_NS --sort-by='.lastTimestamp' | tail -20
echo ""

# 9. 检查资源使用情况
echo "9. 实际资源使用:"
kubectl top pods -n $TENANT_NS 2>/dev/null || echo "Metrics Server未安装"
echo ""

# 10. 检查PVC状态
echo "10. PVC状态:"
kubectl get pvc -n $TENANT_NS
echo ""

# 11. 检查Service和Endpoints
echo "11. Service和Endpoints:"
kubectl get svc,endpoints -n $TENANT_NS
echo ""

# 12. 生成诊断报告
echo "12. 生成诊断报告..."
REPORT_FILE="tenant-${TENANT_NS}-diagnostic-$(date +%Y%m%d-%H%M%S).txt"
{
    echo "租户诊断报告: $TENANT_NS"
    echo "生成时间: $(date)"
    echo "="
    echo ""
    
    echo "Namespace信息:"
    kubectl get ns $TENANT_NS -o yaml
    echo ""
    
    echo "所有资源:"
    kubectl get all -n $TENANT_NS
    echo ""
    
    echo "ResourceQuota:"
    kubectl get resourcequota -n $TENANT_NS -o yaml
    echo ""
    
    echo "NetworkPolicy:"
    kubectl get networkpolicy -n $TENANT_NS -o yaml
    echo ""
    
    echo "最近事件:"
    kubectl get events -n $TENANT_NS --sort-by='.lastTimestamp'
} > $REPORT_FILE

echo "诊断报告已保存到: $REPORT_FILE"
echo ""

echo "=== 故障排查完成 ==="
```

**多租户最佳实践总结**

```yaml
# 多租户实施路线图

阶段1: 规划设计（1-2周）
  任务:
    - 评估多租户需求和场景
    - 选择合适的多租户模型
    - 设计资源配额和隔离策略
    - 规划网络和安全架构
  
  交付物:
    - 多租户架构设计文档
    - 资源配额规划表
    - 安全策略文档
    - 实施计划

阶段2: 基础设施准备（1-2周）
  任务:
    - 部署和配置Kubernetes集群
    - 安装必要的插件（CNI、CSI、Ingress）
    - 配置监控和日志系统
    - 准备镜像仓库和存储
  
  交付物:
    - 就绪的Kubernetes集群
    - 监控和日志系统
    - 基础设施文档

阶段3: 多租户实施（2-3周）
  任务:
    - 创建租户Namespace和配额
    - 配置RBAC和网络策略
    - 部署策略引擎（OPA/Gatekeeper）
    - 实施虚拟集群（如需要）
    - 配置租户监控和告警
  
  交付物:
    - 租户环境
    - 策略配置
    - 监控仪表盘
    - 运维手册

阶段4: 测试验证（1-2周）
  任务:
    - 功能测试（资源隔离、网络隔离）
    - 性能测试（负载测试、压力测试）
    - 安全测试（渗透测试、合规检查）
    - 故障演练（混沌工程）
  
  交付物:
    - 测试报告
    - 性能基准
    - 安全评估报告
    - 改进建议

阶段5: 上线和优化（持续）
  任务:
    - 租户迁移和上线
    - 监控和调优
    - 成本优化
    - 持续改进
  
  交付物:
    - 上线报告
    - 优化建议
    - 运维文档更新
    - 最佳实践总结

# 关键成功因素

技术层面:
  1. 选择合适的多租户模型
  2. 实施多层次的隔离机制
  3. 建立完善的监控和告警
  4. 自动化租户管理流程
  5. 定期进行安全审计

管理层面:
  1. 明确的租户SLA
  2. 透明的成本计费
  3. 完善的文档和培训
  4. 快速的问题响应机制
  5. 持续的优化和改进

# 常见陷阱和避免方法

陷阱1: 过度隔离
  问题: 隔离过于严格导致资源浪费和管理复杂
  避免: 根据实际需求选择合适的隔离级别

陷阱2: 配额设置不合理
  问题: 配额过小影响业务，过大浪费资源
  避免: 基于历史数据和业务预测设置配额，定期调整

陷阱3: 忽视网络隔离
  问题: 租户之间可以互相访问，存在安全风险
  避免: 实施默认拒绝的NetworkPolicy

陷阱4: 缺乏监控和告警
  问题: 无法及时发现和解决问题
  避免: 建立完善的监控体系和告警机制

陷阱5: 手动管理租户
  问题: 效率低下，容易出错
  避免: 使用Operator或自动化工具管理租户

# 检查清单

部署前检查:
  ☐ 多租户架构设计已评审
  ☐ 资源配额已规划
  ☐ 网络策略已配置
  ☐ RBAC权限已设置
  ☐ 监控和日志已就绪
  ☐ 备份恢复已测试
  ☐ 文档已完善

运行时检查:
  ☐ 资源配额使用率正常
  ☐ 网络隔离有效
  ☐ 无权限泄露
  ☐ 监控告警正常
  ☐ 成本在预算内
  ☐ 租户满意度高

定期审查:
  ☐ 每月审查资源使用情况
  ☐ 每季度审查安全策略
  ☐ 每半年审查架构设计
  ☐ 每年进行全面评估
```

**本节总结**

在本节中，我们深入探讨了Kubernetes多租户与资源隔离的各个方面：

1. **多租户架构概述**：介绍了多租户的概念、价值和挑战，以及三种多租户模型（软多租户、硬多租户、混合多租户）的特点和适用场景。

2. **Namespace级别的资源隔离**：详细讲解了如何使用Namespace、ResourceQuota、LimitRange、NetworkPolicy和RBAC实现基础的多租户隔离，并提供了完整的配置示例和自动化管理工具。

3. **虚拟集群技术**：深入介绍了vcluster等虚拟集群解决方案，展示了如何在物理集群之上创建逻辑隔离的虚拟集群，提供更强的隔离和独立性。

4. **多租户最佳实践**：总结了多租户架构选型、安全加固、成本管理、运维自动化等方面的最佳实践，并提供了实施路线图和检查清单。

**关键要点**：

- 根据信任级别和隔离需求选择合适的多租户模型
- 实施多层次的隔离机制（资源、网络、安全）
- 使用自动化工具简化租户管理
- 建立完善的监控、告警和成本管理体系
- 定期审查和优化多租户配置

**下一节预告**：

在下一节中，我们将学习集群联邦与多集群管理，探讨如何管理跨地域、跨云的多个Kubernetes集群，实现统一的资源调度和故障转移。

