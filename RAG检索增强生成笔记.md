# RAG æ£€ç´¢å¢å¼ºç”Ÿæˆå­¦ä¹ ç¬”è®°

> è®© AI åŸºäºä½ çš„æ–‡æ¡£å›ç­”é—®é¢˜ - ä¸çç¼–ï¼Œæœ‰ä¾æ®

---

## ğŸ“‹ ç›®å½•

1. [RAG æ˜¯ä»€ä¹ˆ](#1-rag-æ˜¯ä»€ä¹ˆ)
2. [ä¸ºä»€ä¹ˆéœ€è¦ RAG](#2-ä¸ºä»€ä¹ˆéœ€è¦-rag)
3. [RAG å·¥ä½œæµç¨‹](#3-rag-å·¥ä½œæµç¨‹)
4. [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#4-æ ¸å¿ƒç»„ä»¶è¯¦è§£)
5. [å‘é‡åŒ–ï¼ˆEmbeddingï¼‰](#5-å‘é‡åŒ–embedding)
6. [å‘é‡æ•°æ®åº“](#6-å‘é‡æ•°æ®åº“)
7. [æ£€ç´¢ç­–ç•¥](#7-æ£€ç´¢ç­–ç•¥)
8. [å®Œæ•´å®ç°æ¡ˆä¾‹](#8-å®Œæ•´å®ç°æ¡ˆä¾‹)
9. [ä¼˜åŒ–æŠ€å·§](#9-ä¼˜åŒ–æŠ€å·§)
10. [å¸¸è§é—®é¢˜](#10-å¸¸è§é—®é¢˜)

---

## 1. RAG æ˜¯ä»€ä¹ˆ

### ä¸€å¥è¯

**RAG = å…ˆæœç´¢ç›¸å…³å†…å®¹ï¼Œå†è®© AI åŸºäºæœç´¢ç»“æœå›ç­”ã€‚**

### å…¨ç§°

**RAG = Retrieval-Augmented Generationï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰**

### å½¢è±¡æ¯”å–»

**ä¼ ç»Ÿ LLMï¼ˆè£¸ GPTï¼‰ï¼š**
```
ä½ ï¼š2023å¹´å…¬å¸è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ
AIï¼šæˆ‘çŒœå¤§æ¦‚æ˜¯... [èƒ¡è¯´å…«é“ï¼Œå› ä¸ºæ²¡è®­ç»ƒè¿‡ä½ å…¬å¸çš„æ•°æ®]
```

**RAGï¼š**
```
ä½ ï¼š2023å¹´å…¬å¸è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ

ç³»ç»Ÿï¼š
1. æœç´¢ç›¸å…³æ–‡æ¡£ â†’ æ‰¾åˆ°ã€Š2023å¹´è´¢æŠ¥.pdfã€‹ç¬¬3é¡µ
2. æå–å†…å®¹ï¼š"2023å¹´è¥æ”¶1.2äº¿å…ƒ"
3. æŠŠå†…å®¹ç»™ AI â†’ AIè¯´ï¼š"æ ¹æ®æ–‡æ¡£ï¼Œ2023å¹´è¥æ”¶1.2äº¿å…ƒ"

AIï¼šæ ¹æ®è´¢æŠ¥ï¼Œ2023å¹´å…¬å¸è¥æ”¶ä¸º1.2äº¿å…ƒã€‚[æœ‰ä¾æ®ï¼Œä¸çç¼–]
```

### æ ¸å¿ƒä»·å€¼

- âœ… **ä¸çç¼–** - AI åŸºäºä½ çš„æ–‡æ¡£å›ç­”
- âœ… **å®æ—¶æ›´æ–°** - æ–‡æ¡£æ›´æ–°ï¼Œç­”æ¡ˆå°±æ›´æ–°
- âœ… **å¯è¿½æº¯** - çŸ¥é“ç­”æ¡ˆæ¥æºäºå“ªä»½æ–‡æ¡£
- âœ… **ç§æœ‰æ•°æ®** - å¤„ç†å…¬å¸å†…éƒ¨æ–‡æ¡£ã€ä¸ªäººç¬”è®°

---

## 2. ä¸ºä»€ä¹ˆéœ€è¦ RAG

### é—®é¢˜1ï¼šLLM çš„çŸ¥è¯†æœ‰æˆªæ­¢æ—¥æœŸ

```python
# GPT-4 çš„è®­ç»ƒæ•°æ®æˆªæ­¢åˆ° 2023å¹´4æœˆ
ä½ é—®ï¼š"2024å¹´å¥¥è¿ä¼šåœ¨å“ªé‡Œä¸¾åŠï¼Ÿ"
GPT-4ï¼š"æˆ‘çš„çŸ¥è¯†æˆªæ­¢åˆ°2023å¹´4æœˆï¼Œæ— æ³•å›ç­”ã€‚"
```

**RAG è§£å†³æ–¹æ¡ˆï¼š**
```python
# ä½ ä¸Šä¼ æœ€æ–°æ–°é—»æ–‡æ¡£
æ–‡æ¡£ï¼š"2024å¹´å¥¥è¿ä¼šå°†åœ¨å·´é»ä¸¾åŠ"

# RAG ç³»ç»Ÿ
1. æœç´¢æ–‡æ¡£ â†’ æ‰¾åˆ°ç›¸å…³å†…å®¹
2. AI åŸºäºæ–‡æ¡£å›ç­”ï¼š"2024å¹´å¥¥è¿ä¼šåœ¨å·´é»ä¸¾åŠ"
```

---

### é—®é¢˜2ï¼šLLM ä¸çŸ¥é“ä½ çš„ç§æœ‰æ•°æ®

```python
ä½ é—®ï¼š"æˆ‘ä»¬å…¬å¸çš„è¯·å‡æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ"
GPT-4ï¼š"æˆ‘ä¸çŸ¥é“è´µå…¬å¸çš„å…·ä½“æµç¨‹..."
```

**RAG è§£å†³æ–¹æ¡ˆï¼š**
```python
# ä½ ä¸Šä¼ ã€Šå‘˜å·¥æ‰‹å†Œ.pdfã€‹
æ–‡æ¡£ï¼š"è¯·å‡æµç¨‹ï¼š1. å¡«å†™ç”³è¯·å• 2. ä¸»ç®¡å®¡æ‰¹ 3. HRå¤‡æ¡ˆ"

# RAG ç³»ç»Ÿ
AIï¼š"æ ¹æ®å‘˜å·¥æ‰‹å†Œï¼Œè¯·å‡æµç¨‹ä¸ºï¼š1. å¡«å†™ç”³è¯·å•..."
```

---

### é—®é¢˜3ï¼šLLM ä¼š"å¹»è§‰"ï¼ˆç¼–é€ äº‹å®ï¼‰

```python
ä½ é—®ï¼š"å¼ ä¸‰çš„ç”µè¯æ˜¯å¤šå°‘ï¼Ÿ"
GPT-4ï¼š"138-xxxx-xxxx"  [éšä¾¿ç¼–çš„ï¼Œå®Œå…¨é”™è¯¯]
```

**RAG è§£å†³æ–¹æ¡ˆï¼š**
```python
# ä½ ä¸Šä¼ é€šè®¯å½•
æ–‡æ¡£ï¼š"å¼ ä¸‰ - 139-1234-5678"

# RAG ç³»ç»Ÿ
AIï¼š"æ ¹æ®é€šè®¯å½•ï¼Œå¼ ä¸‰çš„ç”µè¯æ˜¯ 139-1234-5678"
```

---

## 3. RAG å·¥ä½œæµç¨‹

### 3.1 å®Œæ•´æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç¦»çº¿å¤„ç†ï¼ˆå‡†å¤‡é˜¶æ®µï¼‰                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä½ çš„æ–‡æ¡£ï¼ˆPDFã€Wordã€ç½‘é¡µ...ï¼‰
    â†“
ã€1. æ–‡æ¡£åŠ è½½ã€‘
    â†“
å®Œæ•´æ–‡æ¡£å†…å®¹
    â†“
ã€2. æ–‡æœ¬åˆ‡åˆ†ã€‘
    â†“
æ–‡æ¡£å—1  æ–‡æ¡£å—2  æ–‡æ¡£å—3  æ–‡æ¡£å—4 ...
    â†“
ã€3. å‘é‡åŒ–ï¼ˆEmbeddingï¼‰ã€‘
    â†“
å‘é‡1   å‘é‡2   å‘é‡3   å‘é‡4 ...
    â†“
ã€4. å­˜å…¥å‘é‡æ•°æ®åº“ã€‘
    â†“
å‘é‡æ•°æ®åº“ï¼ˆChroma / FAISS / Pineconeï¼‰


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åœ¨çº¿å¤„ç†ï¼ˆæŸ¥è¯¢é˜¶æ®µï¼‰                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç”¨æˆ·æé—®ï¼š"2023å¹´è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ"
    â†“
ã€5. é—®é¢˜å‘é‡åŒ–ã€‘
    â†“
é—®é¢˜å‘é‡
    â†“
ã€6. å‘é‡ç›¸ä¼¼åº¦æœç´¢ã€‘åœ¨å‘é‡æ•°æ®åº“ä¸­æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„æ–‡æ¡£å—
    â†“
æ‰¾åˆ°æœ€ç›¸å…³çš„ 3 ä¸ªæ–‡æ¡£å—
    â†“
ã€7. æ„å»º Promptã€‘
    â†“
"æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ï¼š
 å†…å®¹1ï¼š...
 å†…å®¹2ï¼š...
 å†…å®¹3ï¼š...
 é—®é¢˜ï¼š2023å¹´è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ"
    â†“
ã€8. è°ƒç”¨ LLMã€‘
    â†“
AI å›ç­”ï¼š"æ ¹æ®è´¢æŠ¥ï¼Œ2023å¹´è¥æ”¶1.2äº¿å…ƒ"
    â†“
è¿”å›ç»™ç”¨æˆ·
```

---

### 3.2 ä¸¤ä¸ªå…³é”®é˜¶æ®µ

#### é˜¶æ®µ1ï¼šç¦»çº¿å¤„ç†ï¼ˆä¸€æ¬¡æ€§ï¼Œå‡†å¤‡çŸ¥è¯†åº“ï¼‰

```python
# æ­¥éª¤1ï¼šåŠ è½½æ–‡æ¡£
from langchain.document_loaders import PyPDFLoader
loader = PyPDFLoader("company_report.pdf")
documents = loader.load()

# æ­¥éª¤2ï¼šåˆ‡åˆ†æ–‡æ¡£
from langchain.text_splitter import RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(documents)

# æ­¥éª¤3ï¼šå‘é‡åŒ–å¹¶å­˜å‚¨
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)
```

#### é˜¶æ®µ2ï¼šåœ¨çº¿æŸ¥è¯¢ï¼ˆæ¯æ¬¡é—®ç­”æ—¶æ‰§è¡Œï¼‰

```python
# æ­¥éª¤1ï¼šç”¨æˆ·æé—®
question = "2023å¹´è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ"

# æ­¥éª¤2ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£
relevant_docs = vectorstore.similarity_search(question, k=3)

# æ­¥éª¤3ï¼šæ„å»º Prompt å¹¶è°ƒç”¨ LLM
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(temperature=0),
    retriever=vectorstore.as_retriever()
)

answer = qa_chain.run(question)
print(answer)
```

---

## 4. æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 4.1 Document Loadersï¼ˆæ–‡æ¡£åŠ è½½å™¨ï¼‰

**ä½œç”¨ï¼š** æŠŠå„ç§æ ¼å¼çš„æ–‡ä»¶è¯»æˆç»Ÿä¸€çš„æ–‡æœ¬æ ¼å¼

#### PDF æ–‡ä»¶
```python
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("report.pdf")
documents = loader.load()

# æ¯ä¸ª document åŒ…å«ï¼š
# - page_content: æ–‡æœ¬å†…å®¹
# - metadata: {'source': 'report.pdf', 'page': 1}
```

#### Word æ–‡æ¡£
```python
from langchain.document_loaders import UnstructuredWordDocumentLoader

loader = UnstructuredWordDocumentLoader("doc.docx")
documents = loader.load()
```

#### ç½‘é¡µ
```python
from langchain.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://example.com")
documents = loader.load()
```

#### çº¯æ–‡æœ¬
```python
from langchain.document_loaders import TextLoader

loader = TextLoader("notes.txt")
documents = loader.load()
```

#### å¤šä¸ªæ–‡ä»¶ï¼ˆæ–‡ä»¶å¤¹ï¼‰
```python
from langchain.document_loaders import DirectoryLoader

loader = DirectoryLoader("./docs", glob="**/*.pdf")
documents = loader.load()
```

---

### 4.2 Text Splittersï¼ˆæ–‡æœ¬åˆ‡åˆ†å™¨ï¼‰

**ä½œç”¨ï¼š** æŠŠé•¿æ–‡æ¡£åˆ‡æˆå°å—ï¼ˆå› ä¸º LLM æœ‰ token é™åˆ¶ï¼‰

#### ä¸ºä»€ä¹ˆè¦åˆ‡åˆ†ï¼Ÿ

```
åŸæ–‡æ¡£ï¼š20é¡µPDFï¼Œçº¦30000å­—
â†’ GPT-3.5 æœ€å¤šå¤„ç† 4096 tokensï¼ˆçº¦3000å­—ï¼‰
â†’ å¿…é¡»åˆ‡åˆ†æˆå°å—
```

#### RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,       # æ¯å—æœ€å¤š 1000 å­—ç¬¦
    chunk_overlap=200,     # å—ä¹‹é—´é‡å  200 å­—ç¬¦
    length_function=len,   # ç”¨ä»€ä¹ˆå‡½æ•°è®¡ç®—é•¿åº¦
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]  # ä¼˜å…ˆåœ¨è¿™äº›åœ°æ–¹åˆ‡åˆ†
)

chunks = splitter.split_documents(documents)
```

**å‚æ•°è§£é‡Šï¼š**
- `chunk_size=1000` - æ¯å—å¤§å°
- `chunk_overlap=200` - é‡å éƒ¨åˆ†ï¼ˆé¿å…åˆ‡æ–­å®Œæ•´ä¿¡æ¯ï¼‰

**ä¸ºä»€ä¹ˆè¦é‡å ï¼Ÿ**
```
æ–‡æ¡£ï¼š...å…¬å¸2023å¹´è¥æ”¶1.2äº¿å…ƒï¼Œæ¯”å»å¹´å¢é•¿20%...

ä¸é‡å åˆ‡åˆ†ï¼š
å—1ï¼š"...å…¬å¸2023å¹´è¥æ”¶1.2"
å—2ï¼š"äº¿å…ƒï¼Œæ¯”å»å¹´å¢é•¿20%..."
â†’ "1.2äº¿å…ƒ"è¢«åˆ‡æ–­äº†ï¼

é‡å åˆ‡åˆ†ï¼š
å—1ï¼š"...å…¬å¸2023å¹´è¥æ”¶1.2äº¿å…ƒï¼Œæ¯”å»"
å—2ï¼š"è¥æ”¶1.2äº¿å…ƒï¼Œæ¯”å»å¹´å¢é•¿20%..."
â†’ å®Œæ•´ä¿¡æ¯è¢«ä¿ç•™åœ¨ä¸¤ä¸ªå—ä¸­
```

#### CharacterTextSplitterï¼ˆç®€å•åˆ‡åˆ†ï¼‰

```python
from langchain.text_splitter import CharacterTextSplitter

splitter = CharacterTextSplitter(
    separator="\n",       # æŒ‰æ¢è¡Œåˆ‡åˆ†
    chunk_size=1000,
    chunk_overlap=200
)
```

#### æŒ‰ Token åˆ‡åˆ†ï¼ˆç²¾ç¡®æ§åˆ¶ï¼‰

```python
from langchain.text_splitter import TokenTextSplitter

splitter = TokenTextSplitter(
    chunk_size=500,       # æ¯å— 500 tokens
    chunk_overlap=50
)
```

---

## 5. å‘é‡åŒ–ï¼ˆEmbeddingï¼‰

### 5.1 ä»€ä¹ˆæ˜¯å‘é‡åŒ–

**ä¸€å¥è¯ï¼š** æŠŠæ–‡å­—å˜æˆä¸€ä¸²æ•°å­—ï¼ˆå‘é‡ï¼‰ï¼Œè®©è®¡ç®—æœºèƒ½æ¯”è¾ƒç›¸ä¼¼åº¦ã€‚

**å½¢è±¡ç†è§£ï¼š**
```
æ–‡å­—ï¼š"è‹¹æœå¾ˆå¥½åƒ"
å‘é‡ï¼š[0.2, -0.5, 0.8, 0.1, ..., 0.3]ï¼ˆ1536ä¸ªæ•°å­—ï¼‰

æ–‡å­—ï¼š"è‹¹æœå‘³é“ä¸é”™"
å‘é‡ï¼š[0.19, -0.48, 0.82, 0.09, ..., 0.28]

æ¯”è¾ƒä¸¤ä¸ªå‘é‡ â†’ å‘ç°å¾ˆç›¸ä¼¼ï¼ˆå› ä¸ºæ„æ€æ¥è¿‘ï¼‰
```

**ä¸ºä»€ä¹ˆéœ€è¦å‘é‡ï¼Ÿ**
```
é—®é¢˜ï¼š"å…¬å¸å»å¹´èµšäº†å¤šå°‘é’±ï¼Ÿ"
æ–‡æ¡£1ï¼š"2023å¹´å…¬å¸è¥æ”¶1.2äº¿å…ƒ"
æ–‡æ¡£2ï¼š"å…¬å¸äº§å“ä»‹ç»..."

æ€ä¹ˆçŸ¥é“æ–‡æ¡£1æ›´ç›¸å…³ï¼Ÿ
â†’ æŠŠé—®é¢˜å’Œæ–‡æ¡£éƒ½å˜æˆå‘é‡
â†’ è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
â†’ æ–‡æ¡£1çš„å‘é‡å’Œé—®é¢˜æ›´æ¥è¿‘
```

---

### 5.2 å¸¸ç”¨ Embedding æ¨¡å‹

#### OpenAI Embeddingsï¼ˆæ¨èï¼‰

```python
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"  # æ–°æ¨¡å‹ï¼Œä¾¿å®œå¿«é€Ÿ
)

# æ–‡å­— â†’ å‘é‡
text = "è‹¹æœå¾ˆå¥½åƒ"
vector = embeddings.embed_query(text)
print(len(vector))  # 1536 ä¸ªæ•°å­—
```

**OpenAI Embedding æ¨¡å‹å¯¹æ¯”ï¼š**
| æ¨¡å‹ | å‘é‡ç»´åº¦ | ä»·æ ¼ | é€‚ç”¨åœºæ™¯ |
|------|---------|------|---------|
| text-embedding-3-small | 1536 | $0.02 / 1M tokens | æ—¥å¸¸ä½¿ç”¨ï¼ˆæ¨èï¼‰ |
| text-embedding-3-large | 3072 | $0.13 / 1M tokens | é«˜ç²¾åº¦è¦æ±‚ |
| text-embedding-ada-002 | 1536 | $0.10 / 1M tokens | æ—§æ¨¡å‹ |

#### æœ¬åœ° Embeddingsï¼ˆå…è´¹ï¼‰

```python
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# ä¼˜ç‚¹ï¼šå…è´¹ï¼Œç¦»çº¿å¯ç”¨
# ç¼ºç‚¹ï¼šæ•ˆæœç•¥å·®ï¼Œéœ€è¦ä¸‹è½½æ¨¡å‹
```

#### ä¸­æ–‡ Embeddings

```python
# ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„æ¨¡å‹
embeddings = HuggingFaceEmbeddings(
    model_name="shibing624/text2vec-base-chinese"
)
```

---

### 5.3 å‘é‡ç›¸ä¼¼åº¦è®¡ç®—

**æ ¸å¿ƒåŸç†ï¼š** è®¡ç®—ä¸¤ä¸ªå‘é‡çš„å¤¹è§’ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰

```python
import numpy as np

# ä¸¤ä¸ªå‘é‡
vec1 = np.array([0.2, 0.5, 0.8])
vec2 = np.array([0.19, 0.48, 0.82])

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
print(similarity)  # 0.999ï¼ˆéå¸¸ç›¸ä¼¼ï¼‰

# ç›¸ä¼¼åº¦èŒƒå›´ï¼š-1 åˆ° 1
# 1 = å®Œå…¨ç›¸åŒ
# 0 = æ— å…³
# -1 = å®Œå…¨ç›¸å
```

**å®é™…åº”ç”¨ï¼š**
```python
# é—®é¢˜å‘é‡
question_vec = embeddings.embed_query("2023å¹´è¥æ”¶å¤šå°‘ï¼Ÿ")

# æ–‡æ¡£å‘é‡
doc1_vec = embeddings.embed_query("2023å¹´è¥æ”¶1.2äº¿å…ƒ")
doc2_vec = embeddings.embed_query("å…¬å¸åœ°å€åœ¨åŒ—äº¬")

# è®¡ç®—ç›¸ä¼¼åº¦
similarity1 = cosine_similarity(question_vec, doc1_vec)  # 0.85ï¼ˆé«˜ï¼‰
similarity2 = cosine_similarity(question_vec, doc2_vec)  # 0.12ï¼ˆä½ï¼‰

# ç»“è®ºï¼šdoc1 æ›´ç›¸å…³
```

---

## 6. å‘é‡æ•°æ®åº“

### 6.1 ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“

**ä¸€å¥è¯ï¼š** ä¸“é—¨å­˜å‚¨å‘é‡å¹¶å¿«é€Ÿæœç´¢ç›¸ä¼¼å‘é‡çš„æ•°æ®åº“ã€‚

**å’Œæ™®é€šæ•°æ®åº“çš„åŒºåˆ«ï¼š**
| å¯¹æ¯” | æ™®é€šæ•°æ®åº“ | å‘é‡æ•°æ®åº“ |
|------|-----------|-----------|
| å­˜å‚¨ | æ–‡å­—ã€æ•°å­— | å‘é‡ï¼ˆæ•°ç»„ï¼‰ |
| æŸ¥è¯¢ | ç²¾ç¡®åŒ¹é… | ç›¸ä¼¼åº¦æœç´¢ |
| ä¾‹å­ | `SELECT * WHERE name='å¼ ä¸‰'` | `æ‰¾å‡ºä¸[0.2, 0.5, ...]æœ€ç›¸ä¼¼çš„10ä¸ªå‘é‡` |

---

### 6.2 Chromaï¼ˆæ¨èï¼šæœ¬åœ°ä½¿ç”¨ï¼‰

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# åˆ›å»ºå‘é‡æ•°æ®åº“
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=chunks,                  # æ–‡æ¡£å—åˆ—è¡¨
    embedding=embeddings,              # å‘é‡åŒ–æ¨¡å‹
    persist_directory="./chroma_db"   # ä¿å­˜åˆ°ç£ç›˜ï¼ˆå¯é€‰ï¼‰
)

# æŒä¹…åŒ–ä¿å­˜
vectorstore.persist()

# ä¸‹æ¬¡åŠ è½½
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)
```

**ä¼˜ç‚¹ï¼š**
- âœ… ç®€å•æ˜“ç”¨
- âœ… æœ¬åœ°å­˜å‚¨ï¼ˆä¸éœ€è¦æœåŠ¡å™¨ï¼‰
- âœ… å…è´¹å¼€æº
- âœ… æ”¯æŒæŒä¹…åŒ–

**ç¼ºç‚¹ï¼š**
- âš ï¸ å•æœºä½¿ç”¨ï¼Œä¸æ”¯æŒåˆ†å¸ƒå¼
- âš ï¸ æ•°æ®é‡å¤§ï¼ˆç™¾ä¸‡çº§ï¼‰æ—¶æ€§èƒ½ä¸‹é™

---

### 6.3 FAISSï¼ˆæ¨èï¼šé«˜æ€§èƒ½ï¼‰

```python
from langchain.vectorstores import FAISS

# åˆ›å»ºå‘é‡æ•°æ®åº“
vectorstore = FAISS.from_documents(chunks, embeddings)

# ä¿å­˜åˆ°æœ¬åœ°
vectorstore.save_local("faiss_index")

# åŠ è½½
vectorstore = FAISS.load_local("faiss_index", embeddings)
```

**ä¼˜ç‚¹ï¼š**
- âœ… Meta å¼€å‘ï¼Œæ€§èƒ½æé«˜
- âœ… æ”¯æŒè¶…å¤§è§„æ¨¡ï¼ˆäº¿çº§ï¼‰
- âœ… å…è´¹å¼€æº

**ç¼ºç‚¹ï¼š**
- âš ï¸ åªåœ¨å†…å­˜ä¸­ï¼ˆéœ€è¦æ‰‹åŠ¨æŒä¹…åŒ–ï¼‰
- âš ï¸ é…ç½®ç¨å¤æ‚

---

### 6.4 Pineconeï¼ˆæ¨èï¼šäº‘æœåŠ¡ï¼‰

```python
from langchain.vectorstores import Pinecone
import pinecone

# åˆå§‹åŒ–
pinecone.init(api_key="your-api-key", environment="us-west1-gcp")

# åˆ›å»ºç´¢å¼•
index_name = "my-index"
vectorstore = Pinecone.from_documents(chunks, embeddings, index_name=index_name)
```

**ä¼˜ç‚¹ï¼š**
- âœ… å…¨æ‰˜ç®¡ï¼Œæ— éœ€ç»´æŠ¤
- âœ… æ”¯æŒè¶…å¤§è§„æ¨¡
- âœ… åˆ†å¸ƒå¼ï¼Œé«˜å¯ç”¨

**ç¼ºç‚¹ï¼š**
- âŒ æ”¶è´¹ï¼ˆå…è´¹ç‰ˆæœ‰é™åˆ¶ï¼‰
- âŒ éœ€è¦è”ç½‘

---

### 6.5 å¯¹æ¯”æ€»ç»“

| æ•°æ®åº“ | é€‚ç”¨åœºæ™¯ | ä»·æ ¼ | æ•°æ®è§„æ¨¡ |
|--------|---------|------|---------|
| **Chroma** | ä¸ªäººé¡¹ç›®ã€åŸå‹å¼€å‘ | å…è´¹ | å°åˆ°ä¸­ï¼ˆ10ä¸‡çº§ï¼‰ |
| **FAISS** | é«˜æ€§èƒ½éœ€æ±‚ã€ç¦»çº¿ä½¿ç”¨ | å…è´¹ | å¤§ï¼ˆç™¾ä¸‡åˆ°äº¿çº§ï¼‰ |
| **Pinecone** | ç”Ÿäº§ç¯å¢ƒã€éœ€è¦é«˜å¯ç”¨ | æ”¶è´¹ | ä»»æ„è§„æ¨¡ |
| **Qdrant** | éœ€è¦ç»†ç²’åº¦æ§åˆ¶ | å…è´¹/æ”¶è´¹ | ä»»æ„è§„æ¨¡ |
| **Weaviate** | å¤æ‚æŸ¥è¯¢ã€å›¾è°±é›†æˆ | å…è´¹/æ”¶è´¹ | ä»»æ„è§„æ¨¡ |

---

## 7. æ£€ç´¢ç­–ç•¥

### 7.1 åŸºç¡€æ£€ç´¢ï¼ˆSimilarity Searchï¼‰

```python
# æœ€ç®€å•ï¼šæ‰¾æœ€ç›¸ä¼¼çš„ k ä¸ªæ–‡æ¡£
results = vectorstore.similarity_search(
    query="2023å¹´è¥æ”¶å¤šå°‘ï¼Ÿ",
    k=3  # è¿”å›æœ€ç›¸å…³çš„ 3 ä¸ªæ–‡æ¡£å—
)

for doc in results:
    print(doc.page_content)
    print(doc.metadata)
```

---

### 7.2 å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„æ£€ç´¢

```python
# è¿”å›æ–‡æ¡£å’Œç›¸ä¼¼åº¦åˆ†æ•°
results = vectorstore.similarity_search_with_score(
    query="2023å¹´è¥æ”¶å¤šå°‘ï¼Ÿ",
    k=5
)

for doc, score in results:
    print(f"ç›¸ä¼¼åº¦ï¼š{score}")
    print(f"å†…å®¹ï¼š{doc.page_content[:100]}")
    print("---")

# å¯ä»¥æ ¹æ®åˆ†æ•°è¿‡æ»¤
filtered = [(doc, score) for doc, score in results if score > 0.8]
```

---

### 7.3 MMRï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰

**é—®é¢˜ï¼š** ç›¸ä¼¼åº¦æœç´¢å¯èƒ½è¿”å›é‡å¤å†…å®¹

```python
# é—®é¢˜ï¼š"ä»‹ç»ä¸€ä¸‹Python"
# ç›¸ä¼¼åº¦æœç´¢è¿”å›ï¼š
æ–‡æ¡£1ï¼š"Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€..."
æ–‡æ¡£2ï¼š"Pythonæ˜¯ç¼–ç¨‹è¯­è¨€..."
æ–‡æ¡£3ï¼š"Pythonæ˜¯ä¸€é—¨ç¼–ç¨‹è¯­è¨€..."
# ä¸‰ä¸ªæ–‡æ¡£å†…å®¹å‡ ä¹ä¸€æ ·ï¼
```

**MMR è§£å†³æ–¹æ¡ˆï¼š** å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§

```python
results = vectorstore.max_marginal_relevance_search(
    query="ä»‹ç»Python",
    k=3,
    fetch_k=10,  # å…ˆæ‰¾ 10 ä¸ªç›¸å…³çš„
    lambda_mult=0.5  # å¤šæ ·æ€§æƒé‡ï¼ˆ0=åªè¦å¤šæ ·æ€§ï¼Œ1=åªè¦ç›¸å…³æ€§ï¼‰
)

# è¿”å›çš„ 3 ä¸ªæ–‡æ¡£ï¼š
æ–‡æ¡£1ï¼š"Pythonæ˜¯ç¼–ç¨‹è¯­è¨€..."
æ–‡æ¡£2ï¼š"Pythonçš„å†å²..."ï¼ˆä¸åŒè§’åº¦ï¼‰
æ–‡æ¡£3ï¼š"Pythonçš„åº”ç”¨åœºæ™¯..."ï¼ˆæ›´å¤šæ ·åŒ–ï¼‰
```

---

### 7.4 æ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰

**ç»“åˆå…³é”®è¯æœç´¢å’Œå‘é‡æœç´¢**

```python
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# å‘é‡æ£€ç´¢å™¨
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# å…³é”®è¯æ£€ç´¢å™¨ï¼ˆBM25ï¼‰
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 3

# æ··åˆæ£€ç´¢å™¨
ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.5, 0.5]  # å„å  50%
)

results = ensemble_retriever.get_relevant_documents("2023å¹´è¥æ”¶")
```

---

### 7.5 å…ƒæ•°æ®è¿‡æ»¤

```python
# åœºæ™¯ï¼šåªæœç´¢ç‰¹å®šä½œè€…çš„æ–‡æ¡£
results = vectorstore.similarity_search(
    query="Pythonæ•™ç¨‹",
    k=3,
    filter={"author": "å¼ ä¸‰"}  # åªæœç´¢å¼ ä¸‰å†™çš„æ–‡æ¡£
)

# åœºæ™¯ï¼šåªæœç´¢ç‰¹å®šæ—¥æœŸèŒƒå›´
results = vectorstore.similarity_search(
    query="å­£åº¦æŠ¥å‘Š",
    k=3,
    filter={"date": {"$gte": "2023-01-01", "$lt": "2024-01-01"}}
)
```

---

### 7.6 Self-Queryï¼ˆæ™ºèƒ½è¿‡æ»¤ï¼‰

**è®© AI è‡ªåŠ¨æå–è¿‡æ»¤æ¡ä»¶**

```python
from langchain.retrievers import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

# å®šä¹‰å…ƒæ•°æ®å­—æ®µ
metadata_field_info = [
    AttributeInfo(
        name="author",
        description="æ–‡æ¡£ä½œè€…",
        type="string"
    ),
    AttributeInfo(
        name="year",
        description="æ–‡æ¡£å¹´ä»½",
        type="integer"
    )
]

retriever = SelfQueryRetriever.from_llm(
    llm=ChatOpenAI(temperature=0),
    vectorstore=vectorstore,
    document_contents="å…¬å¸æ–‡æ¡£",
    metadata_field_info=metadata_field_info
)

# ç”¨æˆ·é—®ï¼š"å¼ ä¸‰åœ¨2023å¹´å†™çš„æ–‡æ¡£"
# AI è‡ªåŠ¨æå–ï¼šauthor="å¼ ä¸‰", year=2023
results = retriever.get_relevant_documents("å¼ ä¸‰åœ¨2023å¹´å†™çš„æ–‡æ¡£")
```

---

## 8. å®Œæ•´å®ç°æ¡ˆä¾‹

### 8.1 æœ€ç®€ RAGï¼ˆ50è¡Œä»£ç ï¼‰

```python
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# 1. åŠ è½½æ–‡æ¡£
loader = PyPDFLoader("company_report.pdf")
documents = loader.load()

# 2. åˆ‡åˆ†
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(documents)

# 3. å‘é‡åŒ–å¹¶å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)

# 4. åˆ›å»ºé—®ç­”é“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(temperature=0),
    retriever=vectorstore.as_retriever()
)

# 5. æé—®
question = "2023å¹´å…¬å¸è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ"
answer = qa_chain.run(question)
print(answer)
```

---

### 8.2 å¸¦å¼•ç”¨æ¥æºçš„ RAG

```python
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(temperature=0),
    retriever=vectorstore.as_retriever(),
    return_source_documents=True  # è¿”å›æ¥æºæ–‡æ¡£
)

result = qa_chain({"query": "2023å¹´è¥æ”¶å¤šå°‘ï¼Ÿ"})

print("ç­”æ¡ˆï¼š", result["result"])
print("\næ¥æºæ–‡æ¡£ï¼š")
for doc in result["source_documents"]:
    print(f"- {doc.metadata['source']} ç¬¬{doc.metadata.get('page', '?')}é¡µ")
    print(f"  å†…å®¹ï¼š{doc.page_content[:100]}...\n")
```

**è¾“å‡ºï¼š**
```
ç­”æ¡ˆï¼š2023å¹´å…¬å¸è¥æ”¶ä¸º1.2äº¿å…ƒã€‚

æ¥æºæ–‡æ¡£ï¼š
- company_report.pdf ç¬¬3é¡µ
  å†…å®¹ï¼šæ ¹æ®è´¢åŠ¡æŠ¥è¡¨ï¼Œ2023å¹´åº¦å…¬å¸å®ç°è¥ä¸šæ”¶å…¥1.2äº¿å…ƒ...

- company_report.pdf ç¬¬15é¡µ
  å†…å®¹ï¼šå¹´åº¦æ€»ç»“ï¼šè¥æ”¶åŒæ¯”å¢é•¿20%ï¼Œè¾¾åˆ°1.2äº¿å…ƒ...
```

---

### 8.3 è‡ªå®šä¹‰ Prompt çš„ RAG

```python
from langchain.prompts import PromptTemplate

# è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢åŠ¡åˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

è¦æ±‚ï¼š
1. åªåŸºäºæ–‡æ¡£å†…å®¹å›ç­”
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯´"æ–‡æ¡£ä¸­æœªæåŠ"
3. å¼•ç”¨å…·ä½“æ•°å­—æ—¶æ³¨æ˜å‡ºå¤„

ç­”æ¡ˆï¼š
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(temperature=0),
    retriever=vectorstore.as_retriever(),
    chain_type_kwargs={"prompt": prompt}
)

answer = qa_chain.run("2023å¹´è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ")
```

---

### 8.4 å¯¹è¯å¼ RAGï¼ˆå¸¦å†å²è®°å¿†ï¼‰

```python
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# åˆ›å»ºè®°å¿†
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# åˆ›å»ºå¯¹è¯é“¾
conversation_chain = ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(temperature=0),
    retriever=vectorstore.as_retriever(),
    memory=memory
)

# å¤šè½®å¯¹è¯
response1 = conversation_chain({"question": "2023å¹´è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ"})
print(response1["answer"])
# è¾“å‡ºï¼š"1.2äº¿å…ƒ"

response2 = conversation_chain({"question": "æ¯”å»å¹´å¢é•¿äº†å¤šå°‘ï¼Ÿ"})
print(response2["answer"])
# è¾“å‡ºï¼š"æ¯”2022å¹´å¢é•¿20%"ï¼ˆAI è®°å¾—ä¸Šä¸€è½®åœ¨è¯´è¥æ”¶ï¼‰
```

---

### 8.5 å¤šæ–‡æ¡£ RAG

```python
from langchain.document_loaders import DirectoryLoader

# åŠ è½½æ•´ä¸ªæ–‡ä»¶å¤¹
loader = DirectoryLoader(
    "./documents",
    glob="**/*.pdf",  # æ‰€æœ‰ PDF
    show_progress=True
)

documents = loader.load()
print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

# åˆ‡åˆ†
chunks = splitter.split_documents(documents)

# å‘é‡åŒ–ï¼ˆè‡ªåŠ¨å¤„ç†æ‰€æœ‰æ–‡æ¡£ï¼‰
vectorstore = Chroma.from_documents(chunks, embeddings)

# é—®ç­”ï¼ˆä¼šåœ¨æ‰€æœ‰æ–‡æ¡£ä¸­æœç´¢ï¼‰
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(temperature=0),
    retriever=vectorstore.as_retriever()
)

answer = qa_chain.run("æ€»ç»“æ‰€æœ‰æ–‡æ¡£çš„ä¸»è¦å†…å®¹")
```

---

## 9. ä¼˜åŒ–æŠ€å·§

### 9.1 åˆ‡åˆ†ä¼˜åŒ–

#### é—®é¢˜ï¼šåˆ‡åˆ†å¤ªå¤§æˆ–å¤ªå°

```python
# å¤ªå¤§ï¼ˆ2000å­—ç¬¦ï¼‰
chunk_size=2000
# é—®é¢˜ï¼šåŒ…å«å¤ªå¤šæ— å…³ä¿¡æ¯ï¼ŒAI æ‰¾ä¸åˆ°é‡ç‚¹

# å¤ªå°ï¼ˆ200å­—ç¬¦ï¼‰
chunk_size=200
# é—®é¢˜ï¼šä¸Šä¸‹æ–‡ä¸å®Œæ•´ï¼Œç†è§£å›°éš¾
```

#### æœ€ä½³å®è·µ

```python
# æ¨èèŒƒå›´ï¼š500-1500å­—ç¬¦
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # ä¸€èˆ¬æ–‡æ¡£
    chunk_overlap=200,      # 20% é‡å 
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
)

# æŠ€æœ¯æ–‡æ¡£ï¼ˆä»£ç å¤šï¼‰
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,        # ç¨å¤§ï¼Œä¿æŒä»£ç å®Œæ•´
    chunk_overlap=300
)

# å¯¹è¯è®°å½•ï¼ˆçŸ­å¥å¤šï¼‰
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,         # ç¨å°
    chunk_overlap=100
)
```

---

### 9.2 æ£€ç´¢ä¼˜åŒ–

#### è°ƒæ•´æ£€ç´¢æ•°é‡

```python
# æ£€ç´¢å¤ªå°‘ï¼ˆk=1ï¼‰
retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
# é—®é¢˜ï¼šå¯èƒ½æ¼æ‰é‡è¦ä¿¡æ¯

# æ£€ç´¢å¤ªå¤šï¼ˆk=10ï¼‰
retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
# é—®é¢˜ï¼šå¤ªå¤šå™ªéŸ³ï¼ŒAI å®¹æ˜“æ··æ·†ï¼Œä¸”æµªè´¹ token

# æ¨èï¼š3-5 ä¸ª
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
```

#### ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤

```python
# åªè¿”å›ç›¸ä¼¼åº¦ > 0.7 çš„æ–‡æ¡£
results = vectorstore.similarity_search_with_score(query, k=5)
filtered = [doc for doc, score in results if score > 0.7]

if not filtered:
    print("æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„æ–‡æ¡£")
else:
    # ä½¿ç”¨ filtered æ„å»ºç­”æ¡ˆ
    ...
```

---

### 9.3 Prompt ä¼˜åŒ–

```python
# åŸºç¡€ Promptï¼ˆæ•ˆæœä¸€èˆ¬ï¼‰
template = "æ ¹æ®ï¼š{context}\nå›ç­”ï¼š{question}"

# ä¼˜åŒ– Promptï¼ˆæ•ˆæœæ›´å¥½ï¼‰
template = """
ä½ æ˜¯ä¸“ä¸šåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ–‡æ¡£ç‰‡æ®µï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. ä»…åŸºäºæ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ 
2. å¦‚æœæ–‡æ¡£æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
3. å¼•ç”¨å…³é”®ä¿¡æ¯æ—¶æ³¨æ˜æ¥æº
4. ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”

ä½ çš„å›ç­”ï¼š
"""

prompt = PromptTemplate(template=template, input_variables=["context", "question"])
```

---

### 9.4 å…ƒæ•°æ®å¢å¼º

```python
# åˆ‡åˆ†æ—¶æ·»åŠ å…ƒæ•°æ®
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = []

for doc in documents:
    # ä¸ºæ¯ä¸ªæ–‡æ¡£å—æ·»åŠ ä¸°å¯Œçš„å…ƒæ•°æ®
    doc_chunks = splitter.split_documents([doc])
    for chunk in doc_chunks:
        chunk.metadata.update({
            "source": doc.metadata.get("source", "unknown"),
            "page": doc.metadata.get("page", 0),
            "doc_type": "report",  # è‡ªå®šä¹‰
            "year": 2023,          # è‡ªå®šä¹‰
            "department": "è´¢åŠ¡éƒ¨"  # è‡ªå®šä¹‰
        })
        chunks.append(chunk)

# åˆ›å»ºå‘é‡åº“æ—¶å…ƒæ•°æ®ä¼šä¸€èµ·å­˜å‚¨
vectorstore = Chroma.from_documents(chunks, embeddings)

# æ£€ç´¢æ—¶å¯ä»¥æ ¹æ®å…ƒæ•°æ®è¿‡æ»¤
results = vectorstore.similarity_search(
    "è¥æ”¶",
    filter={"year": 2023, "department": "è´¢åŠ¡éƒ¨"}
)
```

---

### 9.5 é‡æ’åºï¼ˆRerankingï¼‰

**é—®é¢˜ï¼š** å‘é‡æœç´¢å¯èƒ½ä¸å¤Ÿç²¾å‡†

**è§£å†³æ–¹æ¡ˆï¼š** å…ˆç”¨å‘é‡æœç´¢æ‰¾ top 20ï¼Œå†ç”¨æ›´ç²¾ç¡®çš„æ¨¡å‹é‡æ–°æ’åº

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank

# åŸºç¡€æ£€ç´¢å™¨
base_retriever = vectorstore.as_retriever(search_kwargs={"k": 20})

# é‡æ’åºå™¨ï¼ˆä½¿ç”¨ Cohere çš„ rerank æ¨¡å‹ï¼‰
compressor = CohereRerank(cohere_api_key="your-api-key")

# ç»„åˆ
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

# ä½¿ç”¨ï¼ˆä¼šè‡ªåŠ¨é‡æ’åºå¹¶è¿”å›æœ€ç›¸å…³çš„ top 3ï¼‰
results = compression_retriever.get_relevant_documents("2023å¹´è¥æ”¶")
```

---

## 10. å¸¸è§é—®é¢˜

### Q1: å‘é‡æ•°æ®åº“å­˜åœ¨å“ªé‡Œï¼Ÿ

**Chromaï¼š**
```python
# å†…å­˜æ¨¡å¼ï¼ˆç¨‹åºå…³é—­æ•°æ®ä¸¢å¤±ï¼‰
vectorstore = Chroma.from_documents(chunks, embeddings)

# æŒä¹…åŒ–æ¨¡å¼ï¼ˆä¿å­˜åˆ°ç£ç›˜ï¼‰
vectorstore = Chroma.from_documents(
    chunks,
    embeddings,
    persist_directory="./chroma_db"  # ä¿å­˜ä½ç½®
)
vectorstore.persist()  # ç«‹å³ä¿å­˜

# ä¸‹æ¬¡åŠ è½½
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)
```

---

### Q2: å¦‚ä½•æ›´æ–°æ–‡æ¡£ï¼Ÿ

**æ–¹æ³•1ï¼šåˆ é™¤æ—§çš„ï¼Œæ·»åŠ æ–°çš„**
```python
# åˆ é™¤æ‰€æœ‰æ–‡æ¡£
vectorstore.delete_collection()

# é‡æ–°åŠ è½½å’Œå‘é‡åŒ–
documents = loader.load()
chunks = splitter.split_documents(documents)
vectorstore = Chroma.from_documents(chunks, embeddings)
```

**æ–¹æ³•2ï¼šå¢é‡æ›´æ–°**
```python
# åªæ·»åŠ æ–°æ–‡æ¡£
new_docs = loader.load()
new_chunks = splitter.split_documents(new_docs)
vectorstore.add_documents(new_chunks)
```

**æ–¹æ³•3ï¼šæ ¹æ® ID æ›´æ–°**
```python
# åˆ é™¤ç‰¹å®šæ–‡æ¡£
vectorstore.delete(ids=["doc_id_1", "doc_id_2"])

# æ·»åŠ æ›´æ–°åçš„æ–‡æ¡£
vectorstore.add_documents(updated_chunks)
```

---

### Q3: å¦‚ä½•å¤„ç†è¶…å¤§æ–‡æ¡£ï¼Ÿ

**é—®é¢˜ï¼š** 1000é¡µ PDFï¼Œåˆ‡åˆ†åæœ‰5000ä¸ªå—ï¼Œå…¨éƒ¨å‘é‡åŒ–å¾ˆæ…¢

**è§£å†³æ–¹æ¡ˆ1ï¼šæ‰¹é‡å¤„ç†**
```python
# åˆ†æ‰¹å‘é‡åŒ–
batch_size = 100
for i in range(0, len(chunks), batch_size):
    batch = chunks[i:i+batch_size]
    vectorstore.add_documents(batch)
    print(f"å¤„ç†äº† {i+batch_size}/{len(chunks)}")
```

**è§£å†³æ–¹æ¡ˆ2ï¼šå¹¶è¡Œå¤„ç†**
```python
from concurrent.futures import ThreadPoolExecutor

def process_batch(batch):
    return embeddings.embed_documents([doc.page_content for doc in batch])

with ThreadPoolExecutor(max_workers=4) as executor:
    # å¹¶è¡Œå‘é‡åŒ–
    results = executor.map(process_batch, batches)
```

**è§£å†³æ–¹æ¡ˆ3ï¼šä½¿ç”¨æ›´å¿«çš„ Embedding æ¨¡å‹**
```python
# text-embedding-3-smallï¼ˆOpenAI æœ€æ–°ï¼Œé€Ÿåº¦å¿«ï¼‰
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
```

---

### Q4: RAG å›ç­”ä¸å‡†ç¡®æ€ä¹ˆåŠï¼Ÿ

**é—®é¢˜è¯Šæ–­æµç¨‹ï¼š**

#### æ­¥éª¤1ï¼šæ£€æŸ¥æ£€ç´¢ç»“æœ
```python
# çœ‹çœ‹æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ç›¸å…³
results = vectorstore.similarity_search("ä½ çš„é—®é¢˜", k=3)
for i, doc in enumerate(results):
    print(f"\næ–‡æ¡£ {i+1}:")
    print(doc.page_content)
    print(doc.metadata)

# å¦‚æœæ£€ç´¢ç»“æœä¸ç›¸å…³ â†’ é—®é¢˜åœ¨æ£€ç´¢é˜¶æ®µ
# å¦‚æœæ£€ç´¢ç»“æœç›¸å…³ â†’ é—®é¢˜åœ¨ç”Ÿæˆé˜¶æ®µ
```

#### æ­¥éª¤2ï¼šè°ƒæ•´æ£€ç´¢å‚æ•°
```python
# å¢åŠ æ£€ç´¢æ•°é‡
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# ä½¿ç”¨ MMRï¼ˆæé«˜å¤šæ ·æ€§ï¼‰
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5, "fetch_k": 20}
)
```

#### æ­¥éª¤3ï¼šä¼˜åŒ– Prompt
```python
template = """
ä¸¥æ ¼åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚

æ–‡æ¡£ï¼š
{context}

é—®é¢˜ï¼š{question}

æ³¨æ„ï¼š
1. åªä½¿ç”¨æ–‡æ¡£ä¸­çš„ä¿¡æ¯ï¼Œä¸è¦æ¨æµ‹
2. å¦‚æœæ–‡æ¡£æ²¡æœ‰ç­”æ¡ˆï¼Œè¯´"æ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. å¼•ç”¨åŸæ–‡æ—¶åŠ å¼•å·

ç­”æ¡ˆï¼š
"""
```

#### æ­¥éª¤4ï¼šè°ƒæ•´åˆ‡åˆ†ç­–ç•¥
```python
# å¦‚æœç­”æ¡ˆè¢«åˆ‡æ–­ï¼Œå¢åŠ  chunk_size
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,  # å¢å¤§
    chunk_overlap=300  # å¢åŠ é‡å 
)
```

---

### Q5: å¦‚ä½•è®© RAG æ”¯æŒå¤šè¯­è¨€ï¼Ÿ

```python
# ä½¿ç”¨å¤šè¯­è¨€ Embedding æ¨¡å‹
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# æˆ–ä½¿ç”¨ OpenAIï¼ˆå¤©ç„¶æ”¯æŒå¤šè¯­è¨€ï¼‰
embeddings = OpenAIEmbeddings()

# åˆ›å»ºå‘é‡åº“ï¼ˆæ”¯æŒä¸­è‹±æ··åˆï¼‰
vectorstore = Chroma.from_documents(
    documents=[
        Document(page_content="This is English", metadata={"lang": "en"}),
        Document(page_content="è¿™æ˜¯ä¸­æ–‡", metadata={"lang": "zh"})
    ],
    embedding=embeddings
)

# æŸ¥è¯¢ï¼ˆä»»æ„è¯­è¨€ï¼‰
vectorstore.similarity_search("ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ")  # ä¸­æ–‡æŸ¥è¯¢
vectorstore.similarity_search("What is Python?")  # è‹±æ–‡æŸ¥è¯¢
```

---

## æ€»ç»“

### RAG æ ¸å¿ƒæµç¨‹

```
æ–‡æ¡£ â†’ åˆ‡åˆ† â†’ å‘é‡åŒ– â†’ å­˜å‚¨åˆ°å‘é‡åº“ï¼ˆç¦»çº¿ï¼‰
           â†“
é—®é¢˜ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£ â†’ æ„å»º Prompt â†’ LLM ç”Ÿæˆç­”æ¡ˆï¼ˆåœ¨çº¿ï¼‰
```

### å¿…é¡»æŒæ¡çš„æ¦‚å¿µ

- âœ… **RAG æ˜¯ä»€ä¹ˆ** - æ£€ç´¢ + ç”Ÿæˆ
- âœ… **ä¸ºä»€ä¹ˆéœ€è¦ RAG** - è§£å†³ LLM å¹»è§‰ã€çŸ¥è¯†è¿‡æœŸã€ç§æœ‰æ•°æ®é—®é¢˜
- âœ… **æ ¸å¿ƒç»„ä»¶** - Loaderã€Splitterã€Embeddingã€VectorStoreã€Retriever
- âœ… **å‘é‡åŒ–åŸç†** - æ–‡å­—å˜æ•°å­—ï¼Œè®¡ç®—ç›¸ä¼¼åº¦
- âœ… **å‘é‡æ•°æ®åº“** - Chromaï¼ˆæœ¬åœ°ï¼‰ã€FAISSï¼ˆé«˜æ€§èƒ½ï¼‰ã€Pineconeï¼ˆäº‘æœåŠ¡ï¼‰
- âœ… **æ£€ç´¢ç­–ç•¥** - Similarityã€MMRã€æ··åˆæ£€ç´¢

### æœ€ä½³å®è·µ

1. **åˆ‡åˆ†å¤§å°** - 500-1500å­—ç¬¦ï¼Œ20%é‡å 
2. **æ£€ç´¢æ•°é‡** - 3-5ä¸ªæ–‡æ¡£å—
3. **Prompt ä¼˜åŒ–** - æ˜ç¡®è¦æ±‚ã€é™åˆ¶å¹»è§‰
4. **å…ƒæ•°æ®ä¸°å¯Œ** - æ·»åŠ æ¥æºã€æ—¥æœŸç­‰ä¿¡æ¯
5. **æŒä¹…åŒ–** - Chroma ä½¿ç”¨ persist_directory

### æ€§èƒ½ä¼˜åŒ–

- æ‰¹é‡å¤„ç†å¤§æ–‡æ¡£
- ä½¿ç”¨æ›´å¿«çš„ Embedding æ¨¡å‹ï¼ˆtext-embedding-3-smallï¼‰
- åˆç†è®¾ç½®ç¼“å­˜
- è€ƒè™‘ä½¿ç”¨é‡æ’åºï¼ˆRerankingï¼‰

---

**ç¬”è®°åˆ›å»ºæ—¶é—´ï¼š** 2025-11-20
**ç”¨é€”ï¼š** AI åº”ç”¨å¼€å‘ - RAG å®æˆ˜
**å»ºè®®ï¼š** é…åˆ LangChain å’Œ LLM ç¬”è®°ä¸€èµ·å­¦ä¹ 
