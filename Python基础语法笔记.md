# Python åŸºç¡€è¯­æ³•ç¬”è®°

> AI åº”ç”¨å¼€å‘å¿…å¤‡ - åªè®²æœ€å¸¸ç”¨çš„ï¼Œç”¨æœ€ç®€å•çš„è¯

---

## ğŸ“‹ ç›®å½•

1. [å˜é‡å’Œèµ‹å€¼](#1-å˜é‡å’Œèµ‹å€¼)
2. [æ•°æ®ç±»å‹](#2-æ•°æ®ç±»å‹)
3. [å­—ç¬¦ä¸²æ“ä½œ](#3-å­—ç¬¦ä¸²æ“ä½œ)
4. [åˆ—è¡¨](#4-åˆ—è¡¨)
5. [å­—å…¸](#5-å­—å…¸)
6. [å¯¼å…¥æ¨¡å—](#6-å¯¼å…¥æ¨¡å—)
7. [å‡½æ•°](#7-å‡½æ•°)
8. [ç±»å’Œå¯¹è±¡](#8-ç±»å’Œå¯¹è±¡)
9. [æ–¹æ³•è°ƒç”¨](#9-æ–¹æ³•è°ƒç”¨)
10. [æ¡ä»¶åˆ¤æ–­](#10-æ¡ä»¶åˆ¤æ–­)
11. [å¾ªç¯](#11-å¾ªç¯)
12. [æ–‡ä»¶æ“ä½œ](#12-æ–‡ä»¶æ“ä½œ)

---

## 1. å˜é‡å’Œèµ‹å€¼

### ä¸€å¥è¯

**æŠŠå€¼æ”¾è¿›ç›’å­é‡Œï¼Œç›’å­å°±æ˜¯å˜é‡ã€‚**

### è¯­æ³•

```python
å˜é‡å = å€¼
```

### è§£é‡Š

- `=` æ˜¯èµ‹å€¼ç¬¦å·ï¼ˆä¸æ˜¯"ç­‰äº"ï¼Œæ˜¯"æŠŠå³è¾¹çš„å€¼æ”¾è¿›å·¦è¾¹çš„å˜é‡"ï¼‰
- å˜é‡åè‡ªå·±èµ·ï¼Œè§åçŸ¥æ„
- Python ä¸éœ€è¦å£°æ˜ç±»å‹ï¼Œç›´æ¥èµ‹å€¼å°±è¡Œ

### ä¾‹å­

```python
# åŸºæœ¬èµ‹å€¼
name = "å¼ ä¸‰"
age = 25
price = 99.5

# AI å¼€å‘ä¸­å¸¸è§
model_name = "gpt-4"
temperature = 0.7
max_tokens = 1000

# ä¸€æ¬¡èµ‹å€¼å¤šä¸ª
x, y, z = 1, 2, 3

# äº¤æ¢å˜é‡
a, b = b, a
```

---

## 2. æ•°æ®ç±»å‹

### ä¸€å¥è¯

**ä¸åŒç±»å‹çš„å€¼ï¼Œç”¨æ³•ä¸åŒã€‚**

### å¸¸ç”¨ç±»å‹

#### 2.1 å­—ç¬¦ä¸²ï¼ˆstrï¼‰- æ–‡æœ¬

```python
# è¯­æ³•ï¼šç”¨å¼•å·åŒ…èµ·æ¥
text = "è¿™æ˜¯å­—ç¬¦ä¸²"
text2 = 'å•å¼•å·ä¹Ÿå¯ä»¥'
text3 = """ä¸‰å¼•å·å¯ä»¥
æ¢è¡Œå†™å¾ˆé•¿çš„æ–‡æœ¬"""

# AI ä¸­çš„ä½¿ç”¨
prompt = "è¯·ç”¨ä¸­æ–‡è§£é‡Šé‡å­åŠ›å­¦"
api_key = "sk-xxxxxxxxxxxxx"
```

#### 2.2 æ•´æ•°ï¼ˆintï¼‰- æ•°å­—

```python
# è¯­æ³•ï¼šç›´æ¥å†™æ•°å­—
count = 100
age = 25

# AI ä¸­çš„ä½¿ç”¨
max_tokens = 2000
chunk_size = 1000
```

#### 2.3 æµ®ç‚¹æ•°ï¼ˆfloatï¼‰- å°æ•°

```python
# è¯­æ³•ï¼šå¸¦å°æ•°ç‚¹çš„æ•°å­—
temperature = 0.7
price = 99.99

# AI ä¸­çš„ä½¿ç”¨
similarity_score = 0.85
confidence = 0.92
```

#### 2.4 å¸ƒå°”å€¼ï¼ˆboolï¼‰- çœŸ/å‡

```python
# è¯­æ³•ï¼šTrue æˆ– Falseï¼ˆé¦–å­—æ¯å¤§å†™ï¼‰
is_active = True
has_error = False

# AI ä¸­çš„ä½¿ç”¨
stream = True  # æ˜¯å¦æµå¼è¾“å‡º
verbose = False  # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
```

#### 2.5 None - ç©ºå€¼

```python
# è¯­æ³•ï¼šNone è¡¨ç¤º"æ²¡æœ‰å€¼"
result = None

# AI ä¸­çš„ä½¿ç”¨
error_message = None  # æ²¡æœ‰é”™è¯¯æ—¶ä¸º None
```

---

## 3. å­—ç¬¦ä¸²æ“ä½œ

### ä¸€å¥è¯

**æ–‡æœ¬çš„å„ç§ç©æ³•ã€‚**

### 3.1 å­—ç¬¦ä¸²æ‹¼æ¥

```python
# æ–¹æ³•1ï¼šç”¨ +
first_name = "å¼ "
last_name = "ä¸‰"
full_name = first_name + last_name  # "å¼ ä¸‰"

# æ–¹æ³•2ï¼šç”¨ f-stringï¼ˆæ¨èï¼ŒAI å¼€å‘æœ€å¸¸ç”¨ï¼‰
name = "å¼ ä¸‰"
age = 25
message = f"æˆ‘å«{name}ï¼Œä»Šå¹´{age}å²"
# è¾“å‡ºï¼š"æˆ‘å«å¼ ä¸‰ï¼Œä»Šå¹´25å²"

# AI ä¸­çš„ä½¿ç”¨
model = "gpt-4"
temp = 0.7
prompt = f"ä½¿ç”¨æ¨¡å‹ {model}ï¼Œæ¸©åº¦ {temp}"
```

### 3.2 å­—ç¬¦ä¸²æ ¼å¼åŒ–ï¼ˆå ä½ç¬¦ï¼‰

```python
# æ–¹æ³•1ï¼šformat() æ–¹æ³•
template = "è¯·ç”¨{language}è§£é‡Š{concept}"
result = template.format(language="ä¸­æ–‡", concept="AI")
# è¾“å‡ºï¼š"è¯·ç”¨ä¸­æ–‡è§£é‡ŠAI"

# æ–¹æ³•2ï¼šf-stringï¼ˆæ›´ç®€æ´ï¼‰
language = "ä¸­æ–‡"
concept = "AI"
result = f"è¯·ç”¨{language}è§£é‡Š{concept}"
```

### 3.3 å¸¸ç”¨å­—ç¬¦ä¸²æ–¹æ³•

```python
text = "Hello World"

# è½¬å¤§å†™
text.upper()  # "HELLO WORLD"

# è½¬å°å†™
text.lower()  # "hello world"

# å»æ‰é¦–å°¾ç©ºæ ¼
"  hello  ".strip()  # "hello"

# æ›¿æ¢
text.replace("World", "Python")  # "Hello Python"

# åˆ†å‰²æˆåˆ—è¡¨
"a,b,c".split(",")  # ["a", "b", "c"]

# æ£€æŸ¥æ˜¯å¦åŒ…å«
"World" in text  # True
```

### AI ä¸­çš„å®é™…ä½¿ç”¨

```python
# æ¸…ç†ç”¨æˆ·è¾“å…¥
user_input = "  ä»€ä¹ˆæ˜¯ AIï¼Ÿ  "
clean_input = user_input.strip()

# æ„å»ºæç¤ºè¯
role = "Pythonè€å¸ˆ"
question = "ä»€ä¹ˆæ˜¯å˜é‡"
prompt = f"ä½ æ˜¯{role}ï¼Œå›ç­”é—®é¢˜ï¼š{question}"

# å¤„ç†æ–‡ä»¶è·¯å¾„
file_path = "data/documents/report.pdf"
file_name = file_path.split("/")[-1]  # "report.pdf"
```

---

## 4. åˆ—è¡¨ï¼ˆListï¼‰

### ä¸€å¥è¯

**ä¸€ä¸²æœ‰åºçš„å€¼ï¼Œç”¨æ–¹æ‹¬å·åŒ…èµ·æ¥ã€‚**

### è¯­æ³•

```python
åˆ—è¡¨å = [å€¼1, å€¼2, å€¼3]
```

### åŸºæœ¬æ“ä½œ

```python
# åˆ›å»ºåˆ—è¡¨
numbers = [1, 2, 3, 4, 5]
names = ["å¼ ä¸‰", "æå››", "ç‹äº”"]
mixed = [1, "hello", 3.14, True]  # å¯ä»¥æ··åˆç±»å‹

# è®¿é—®å…ƒç´ ï¼ˆç´¢å¼•ä» 0 å¼€å§‹ï¼‰
numbers[0]  # 1ï¼ˆç¬¬ä¸€ä¸ªï¼‰
numbers[2]  # 3ï¼ˆç¬¬ä¸‰ä¸ªï¼‰
numbers[-1]  # 5ï¼ˆæœ€åä¸€ä¸ªï¼‰

# ä¿®æ”¹å…ƒç´ 
numbers[0] = 10
# numbers å˜æˆ [10, 2, 3, 4, 5]

# æ·»åŠ å…ƒç´ 
numbers.append(6)  # åœ¨æœ«å°¾æ·»åŠ 
# numbers å˜æˆ [10, 2, 3, 4, 5, 6]

# åˆ é™¤å…ƒç´ 
numbers.remove(10)  # åˆ é™¤å€¼ä¸º 10 çš„å…ƒç´ 

# åˆ—è¡¨é•¿åº¦
len(numbers)  # 5

# åˆ‡ç‰‡ï¼ˆå–ä¸€éƒ¨åˆ†ï¼‰
numbers[1:3]  # [2, 3]ï¼ˆä»ç´¢å¼•1åˆ°3ï¼Œä¸åŒ…æ‹¬3ï¼‰
numbers[:2]  # [10, 2]ï¼ˆå‰ä¸¤ä¸ªï¼‰
numbers[2:]  # [3, 4, 5]ï¼ˆä»ç¬¬ä¸‰ä¸ªåˆ°æœ€åï¼‰
```

### AI ä¸­çš„å®é™…ä½¿ç”¨

```python
# å­˜å‚¨å¤šä¸ªæ–‡æ¡£
documents = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]

# å­˜å‚¨å¯¹è¯å†å²
chat_history = [
    "ç”¨æˆ·: ä»€ä¹ˆæ˜¯AIï¼Ÿ",
    "åŠ©æ‰‹: AIæ˜¯äººå·¥æ™ºèƒ½...",
    "ç”¨æˆ·: èƒ½ä¸¾ä¸ªä¾‹å­å—ï¼Ÿ"
]

# å­˜å‚¨æœç´¢ç»“æœ
search_results = [
    "ç»“æœ1ï¼š...",
    "ç»“æœ2ï¼š...",
    "ç»“æœ3ï¼š..."
]

# éå†åˆ—è¡¨å¤„ç†æ¯ä¸ªæ–‡æ¡£
for doc in documents:
    print(f"å¤„ç†æ–‡æ¡£ï¼š{doc}")

# è·å–å‰3ä¸ªç»“æœ
top_3 = search_results[:3]
```

---

## 5. å­—å…¸ï¼ˆDictionaryï¼‰

### ä¸€å¥è¯

**é”®å€¼å¯¹çš„é›†åˆï¼Œå°±åƒé€šè®¯å½•ï¼ˆåå­—å¯¹åº”ç”µè¯ï¼‰ã€‚**

### è¯­æ³•

```python
å­—å…¸å = {"é”®1": å€¼1, "é”®2": å€¼2}
```

### åŸºæœ¬æ“ä½œ

```python
# åˆ›å»ºå­—å…¸
person = {
    "name": "å¼ ä¸‰",
    "age": 25,
    "city": "åŒ—äº¬"
}

# è®¿é—®å€¼
person["name"]  # "å¼ ä¸‰"
person["age"]  # 25

# æ·»åŠ /ä¿®æ”¹
person["email"] = "zhangsan@example.com"  # æ·»åŠ 
person["age"] = 26  # ä¿®æ”¹

# åˆ é™¤
del person["city"]

# æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨
"name" in person  # True

# è·å–æ‰€æœ‰é”®
person.keys()  # ["name", "age", "email"]

# è·å–æ‰€æœ‰å€¼
person.values()  # ["å¼ ä¸‰", 26, "zhangsan@example.com"]

# å®‰å…¨è·å–ï¼ˆä¸å­˜åœ¨è¿”å›é»˜è®¤å€¼ï¼‰
person.get("phone", "æœªæä¾›")  # "æœªæä¾›"
```

### AI ä¸­çš„å®é™…ä½¿ç”¨

```python
# API é…ç½®
config = {
    "model": "gpt-4",
    "temperature": 0.7,
    "max_tokens": 2000,
    "stream": True
}

# è®¿é—®é…ç½®
model_name = config["model"]
temp = config["temperature"]

# å¯¹è¯æ¶ˆæ¯
message = {
    "role": "user",
    "content": "ä»€ä¹ˆæ˜¯é‡å­åŠ›å­¦ï¼Ÿ"
}

# æ–‡æ¡£å…ƒæ•°æ®
doc_metadata = {
    "file_name": "report.pdf",
    "page_count": 50,
    "author": "å¼ ä¸‰",
    "created_at": "2025-01-01"
}

# å‘é‡æ•°æ®åº“æŸ¥è¯¢ç»“æœ
result = {
    "document": "...",
    "score": 0.85,
    "metadata": {"page": 3}
}
```

---

## 6. å¯¼å…¥æ¨¡å—

### ä¸€å¥è¯

**æŠŠåˆ«äººå†™å¥½çš„ä»£ç æ‹¿æ¥ç”¨ã€‚**

### è¯­æ³•

```python
# æ–¹å¼1ï¼šå¯¼å…¥æ•´ä¸ªæ¨¡å—
import æ¨¡å—å

# æ–¹å¼2ï¼šå¯¼å…¥æ¨¡å—ä¸­çš„ç‰¹å®šå†…å®¹
from æ¨¡å—å import ç±»å/å‡½æ•°å

# æ–¹å¼3ï¼šå¯¼å…¥å¹¶æ”¹å
import æ¨¡å—å as åˆ«å
from æ¨¡å—å import ç±»å as åˆ«å
```

### ä¾‹å­

```python
# æ–¹å¼1ï¼šå¯¼å…¥æ•´ä¸ªæ¨¡å—
import os
os.path.exists("file.txt")  # ä½¿ç”¨æ—¶è¦åŠ æ¨¡å—å

# æ–¹å¼2ï¼šå¯¼å…¥ç‰¹å®šå†…å®¹ï¼ˆæ¨èï¼‰
from os.path import exists
exists("file.txt")  # ç›´æ¥ç”¨

# æ–¹å¼3ï¼šæ”¹åï¼ˆç®€åŒ–é•¿åå­—ï¼‰
import numpy as np
np.array([1, 2, 3])
```

### AI ä¸­çš„å®é™…ä½¿ç”¨

```python
# LangChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# æ–‡æ¡£å¤„ç†
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# å‘é‡æ•°æ®åº“
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# æ ‡å‡†åº“
import os  # æ“ä½œç³»ç»ŸåŠŸèƒ½
import json  # JSON å¤„ç†
from dotenv import load_dotenv  # åŠ è½½ç¯å¢ƒå˜é‡
```

---

## 7. å‡½æ•°

### ä¸€å¥è¯

**æŠŠé‡å¤çš„ä»£ç æ‰“åŒ…èµ·æ¥ï¼Œèµ·ä¸ªåå­—ï¼Œéœ€è¦æ—¶è°ƒç”¨ã€‚**

### è¯­æ³•

```python
# å®šä¹‰å‡½æ•°
def å‡½æ•°å(å‚æ•°1, å‚æ•°2):
    # å‡½æ•°ä½“
    return è¿”å›å€¼

# è°ƒç”¨å‡½æ•°
ç»“æœ = å‡½æ•°å(å€¼1, å€¼2)
```

### åŸºæœ¬ä¾‹å­

```python
# æ— å‚æ•°æ— è¿”å›å€¼
def say_hello():
    print("ä½ å¥½ï¼")

say_hello()  # è°ƒç”¨

# æœ‰å‚æ•°æœ‰è¿”å›å€¼
def add(a, b):
    result = a + b
    return result

sum_result = add(3, 5)  # 8

# é»˜è®¤å‚æ•°
def greet(name, greeting="ä½ å¥½"):
    return f"{greeting}, {name}!"

greet("å¼ ä¸‰")  # "ä½ å¥½, å¼ ä¸‰!"
greet("å¼ ä¸‰", "æ—©ä¸Šå¥½")  # "æ—©ä¸Šå¥½, å¼ ä¸‰!"

# å…³é”®å­—å‚æ•°
def create_user(name, age, city):
    return {"name": name, "age": age, "city": city}

user = create_user(name="å¼ ä¸‰", age=25, city="åŒ—äº¬")
```

### AI ä¸­çš„å®é™…ä½¿ç”¨

```python
# åŠ è½½æ–‡æ¡£
def load_pdf(file_path):
    from langchain.document_loaders import PyPDFLoader
    loader = PyPDFLoader(file_path)
    documents = loader.load()
    return documents

# åˆ‡åˆ†æ–‡æ¡£
def split_documents(documents, chunk_size=1000):
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size)
    chunks = splitter.split_documents(documents)
    return chunks

# è°ƒç”¨ GPT
def ask_gpt(question, temperature=0.7):
    from langchain.llms import OpenAI
    llm = OpenAI(temperature=temperature)
    answer = llm(question)
    return answer

# ä½¿ç”¨
docs = load_pdf("report.pdf")
chunks = split_documents(docs, chunk_size=500)
answer = ask_gpt("ä»€ä¹ˆæ˜¯AIï¼Ÿ", temperature=0.5)
```

---

## 8. ç±»å’Œå¯¹è±¡

### ä¸€å¥è¯

**ç±»æ˜¯è“å›¾ï¼Œå¯¹è±¡æ˜¯æ ¹æ®è“å›¾é€ å‡ºæ¥çš„å®ç‰©ã€‚**

### æ ¸å¿ƒæ¦‚å¿µ

```python
# ç±»å®šä¹‰ï¼ˆè“å›¾ï¼‰
class Car:
    def __init__(self, brand, color):  # æ„é€ å‡½æ•°
        self.brand = brand  # å±æ€§
        self.color = color

    def drive(self):  # æ–¹æ³•
        print(f"{self.color}çš„{self.brand}å¼€åŠ¨äº†")

# åˆ›å»ºå¯¹è±¡ï¼ˆé€ è½¦ï¼‰
my_car = Car("ä¸°ç”°", "çº¢è‰²")
your_car = Car("æœ¬ç”°", "è“è‰²")

# è®¿é—®å±æ€§
print(my_car.brand)  # "ä¸°ç”°"
print(my_car.color)  # "çº¢è‰²"

# è°ƒç”¨æ–¹æ³•
my_car.drive()  # "çº¢è‰²çš„ä¸°ç”°å¼€åŠ¨äº†"
```

### AI ä¸­çš„ç†è§£ï¼ˆä½ ä¸éœ€è¦å†™ç±»ï¼Œåªéœ€è¦ä¼šç”¨ï¼‰

```python
# LangChain å·²ç»å®šä¹‰å¥½äº†ç±»ï¼Œä½ åªéœ€è¦ç”¨

# ä¾‹å­1ï¼šåˆ›å»º LLM å¯¹è±¡
from langchain.llms import OpenAI
llm = OpenAI(temperature=0.7)  # åˆ›å»ºå¯¹è±¡
response = llm("ä½ å¥½")  # è°ƒç”¨å¯¹è±¡

# ä¾‹å­2ï¼šåˆ›å»º PromptTemplate å¯¹è±¡
from langchain.prompts import PromptTemplate
template = "è¯·ç”¨{language}è§£é‡Š{concept}"
prompt = PromptTemplate(
    template=template,
    input_variables=["language", "concept"]
)
result = prompt.format(language="ä¸­æ–‡", concept="AI")

# ä¾‹å­3ï¼šåˆ›å»ºå‘é‡æ•°æ®åº“å¯¹è±¡
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=OpenAIEmbeddings()
)
```

**ä½ éœ€è¦ç†è§£çš„ï¼š**
- `OpenAI(...)` = åˆ›å»ºå¯¹è±¡
- `llm(...)` = è°ƒç”¨å¯¹è±¡ï¼ˆå¯¹è±¡å¯ä»¥åƒå‡½æ•°ä¸€æ ·è¢«è°ƒç”¨ï¼‰
- `prompt.format(...)` = è°ƒç”¨å¯¹è±¡çš„æ–¹æ³•

---

## 9. æ–¹æ³•è°ƒç”¨

### ä¸€å¥è¯

**å¯¹è±¡è‡ªå¸¦çš„åŠŸèƒ½ï¼Œç”¨ `.` è°ƒç”¨ã€‚**

### è¯­æ³•

```python
å¯¹è±¡.æ–¹æ³•å(å‚æ•°)
```

### å­—ç¬¦ä¸²æ–¹æ³•

```python
text = "hello world"
text.upper()  # "HELLO WORLD"
text.split(" ")  # ["hello", "world"]
```

### åˆ—è¡¨æ–¹æ³•

```python
numbers = [1, 2, 3]
numbers.append(4)  # æ·»åŠ å…ƒç´ 
numbers.remove(2)  # åˆ é™¤å…ƒç´ 
```

### é“¾å¼è°ƒç”¨ï¼ˆæ–¹æ³•è¿”å›å¯¹è±¡ï¼Œç»§ç»­è°ƒç”¨ï¼‰

```python
text = "  Hello World  "
result = text.strip().lower().replace("world", "python")
# "hello python"

# æ‹†è§£ï¼š
# 1. text.strip() â†’ "Hello World"
# 2. .lower() â†’ "hello world"
# 3. .replace(...) â†’ "hello python"
```

### AI ä¸­çš„å®é™…ä½¿ç”¨

```python
# æ–‡æ¡£åŠ è½½å’Œå¤„ç†ï¼ˆé“¾å¼è°ƒç”¨ï¼‰
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

loader = PyPDFLoader("report.pdf")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=1000)
chunks = splitter.split_documents(documents)

# å‘é‡æ•°æ®åº“æ“ä½œ
vectorstore.similarity_search("æŸ¥è¯¢å†…å®¹", k=3)  # æœç´¢ç›¸ä¼¼æ–‡æ¡£
vectorstore.add_documents(new_docs)  # æ·»åŠ æ–‡æ¡£

# LangChain é“¾å¼æ“ä½œ
from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run(language="ä¸­æ–‡", concept="AI")
```

---

## 10. æ¡ä»¶åˆ¤æ–­

### ä¸€å¥è¯

**æ ¹æ®æ¡ä»¶å†³å®šåšä»€ä¹ˆã€‚**

### è¯­æ³•

```python
if æ¡ä»¶:
    # æ¡ä»¶ä¸ºçœŸæ—¶æ‰§è¡Œ
elif å¦ä¸€ä¸ªæ¡ä»¶:
    # ç¬¬äºŒä¸ªæ¡ä»¶ä¸ºçœŸæ—¶æ‰§è¡Œ
else:
    # éƒ½ä¸æ»¡è¶³æ—¶æ‰§è¡Œ
```

### åŸºæœ¬ä¾‹å­

```python
age = 18

if age >= 18:
    print("æˆå¹´äºº")
else:
    print("æœªæˆå¹´")

# å¤šæ¡ä»¶
score = 85

if score >= 90:
    print("ä¼˜ç§€")
elif score >= 80:
    print("è‰¯å¥½")
elif score >= 60:
    print("åŠæ ¼")
else:
    print("ä¸åŠæ ¼")
```

### å¸¸ç”¨æ¡ä»¶è¡¨è¾¾å¼

```python
# æ¯”è¾ƒ
x == y  # ç­‰äº
x != y  # ä¸ç­‰äº
x > y   # å¤§äº
x < y   # å°äº
x >= y  # å¤§äºç­‰äº
x <= y  # å°äºç­‰äº

# é€»è¾‘è¿ç®—
a and b  # å¹¶ä¸”ï¼ˆéƒ½ä¸ºçœŸæ‰ä¸ºçœŸï¼‰
a or b   # æˆ–è€…ï¼ˆæœ‰ä¸€ä¸ªä¸ºçœŸå°±ä¸ºçœŸï¼‰
not a    # éï¼ˆå–åï¼‰

# æˆå‘˜åˆ¤æ–­
"a" in "abc"  # Trueï¼ˆå­—ç¬¦ä¸²åŒ…å«ï¼‰
"d" in "abc"  # False
3 in [1, 2, 3]  # Trueï¼ˆåˆ—è¡¨åŒ…å«ï¼‰

# åˆ¤æ–­ç©ºå€¼
x is None
x is not None
```

### AI ä¸­çš„å®é™…ä½¿ç”¨

```python
# æ£€æŸ¥ API å¯†é’¥
api_key = os.getenv("OPENAI_API_KEY")
if api_key is None:
    print("è¯·è®¾ç½® API å¯†é’¥")
else:
    llm = OpenAI(api_key=api_key)

# æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
if file_path.endswith(".pdf"):
    loader = PyPDFLoader(file_path)
elif file_path.endswith(".txt"):
    loader = TextLoader(file_path)
else:
    print("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹")

# æ£€æŸ¥æœç´¢ç»“æœ
results = vectorstore.similarity_search(query, k=3)
if len(results) == 0:
    print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
else:
    print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")

# æ ¹æ®ç›¸ä¼¼åº¦ç­›é€‰
for result in results:
    if result.metadata["score"] > 0.8:
        print("é«˜ç›¸å…³åº¦æ–‡æ¡£")
```

---

## 11. å¾ªç¯

### ä¸€å¥è¯

**é‡å¤åšåŒä¸€ä»¶äº‹ã€‚**

### 11.1 for å¾ªç¯ï¼ˆéå†åˆ—è¡¨ï¼‰

```python
# è¯­æ³•
for å˜é‡ in åˆ—è¡¨:
    # å¯¹æ¯ä¸ªå…ƒç´ æ‰§è¡Œçš„ä»£ç 
```

```python
# åŸºæœ¬ä¾‹å­
names = ["å¼ ä¸‰", "æå››", "ç‹äº”"]
for name in names:
    print(name)
# è¾“å‡ºï¼š
# å¼ ä¸‰
# æå››
# ç‹äº”

# éå†æ•°å­—
for i in range(5):  # range(5) ç”Ÿæˆ [0, 1, 2, 3, 4]
    print(i)

# éå†å­—å…¸
person = {"name": "å¼ ä¸‰", "age": 25}
for key, value in person.items():
    print(f"{key}: {value}")
# è¾“å‡ºï¼š
# name: å¼ ä¸‰
# age: 25
```

### 11.2 while å¾ªç¯ï¼ˆæ»¡è¶³æ¡ä»¶å°±ç»§ç»­ï¼‰

```python
# è¯­æ³•
while æ¡ä»¶:
    # æ¡ä»¶ä¸ºçœŸæ—¶æ‰§è¡Œ
```

```python
# åŸºæœ¬ä¾‹å­
count = 0
while count < 5:
    print(count)
    count += 1  # count = count + 1
# è¾“å‡ºï¼š0 1 2 3 4
```

### AI ä¸­çš„å®é™…ä½¿ç”¨

```python
# å¤„ç†å¤šä¸ªæ–‡æ¡£
pdf_files = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
all_documents = []

for file in pdf_files:
    loader = PyPDFLoader(file)
    docs = loader.load()
    all_documents.extend(docs)  # æ·»åŠ åˆ°æ€»åˆ—è¡¨
    print(f"å·²å¤„ç†ï¼š{file}")

# åˆ‡åˆ†æ‰€æœ‰æ–‡æ¡£
splitter = RecursiveCharacterTextSplitter(chunk_size=1000)
all_chunks = []

for doc in all_documents:
    chunks = splitter.split_documents([doc])
    all_chunks.extend(chunks)

# æ‰¹é‡å‘é‡åŒ–
for i in range(0, len(all_chunks), 100):  # æ¯æ¬¡å¤„ç†100ä¸ª
    batch = all_chunks[i:i+100]
    vectorstore.add_documents(batch)
    print(f"å·²å‘é‡åŒ– {i+100} ä¸ªæ–‡æ¡£å—")

# æœç´¢ç»“æœå¤„ç†
results = vectorstore.similarity_search(query, k=5)
for i, result in enumerate(results):  # enumerate è·å–ç´¢å¼•å’Œå€¼
    print(f"ç»“æœ {i+1}:")
    print(result.page_content[:100])  # æ‰“å°å‰100å­—ç¬¦
```

---

## 12. æ–‡ä»¶æ“ä½œ

### ä¸€å¥è¯

**è¯»å†™ç”µè„‘ä¸Šçš„æ–‡ä»¶ã€‚**

### 12.1 è¯»å–æ–‡ä»¶

```python
# è¯­æ³•
with open("æ–‡ä»¶è·¯å¾„", "r", encoding="utf-8") as f:
    content = f.read()
```

```python
# è¯»å–æ•´ä¸ªæ–‡ä»¶
with open("data.txt", "r", encoding="utf-8") as f:
    content = f.read()
    print(content)

# æŒ‰è¡Œè¯»å–
with open("data.txt", "r", encoding="utf-8") as f:
    for line in f:
        print(line.strip())  # strip() å»æ‰æ¢è¡Œç¬¦

# è¯»å–æ‰€æœ‰è¡Œåˆ°åˆ—è¡¨
with open("data.txt", "r", encoding="utf-8") as f:
    lines = f.readlines()  # è¿”å›åˆ—è¡¨
```

### 12.2 å†™å…¥æ–‡ä»¶

```python
# è¯­æ³•
with open("æ–‡ä»¶è·¯å¾„", "w", encoding="utf-8") as f:
    f.write("å†…å®¹")
```

```python
# å†™å…¥ï¼ˆè¦†ç›–åŸå†…å®¹ï¼‰
with open("output.txt", "w", encoding="utf-8") as f:
    f.write("è¿™æ˜¯ç¬¬ä¸€è¡Œ\n")
    f.write("è¿™æ˜¯ç¬¬äºŒè¡Œ\n")

# è¿½åŠ ï¼ˆä¿ç•™åŸå†…å®¹ï¼‰
with open("output.txt", "a", encoding="utf-8") as f:
    f.write("è¿½åŠ ä¸€è¡Œ\n")
```

### 12.3 JSON æ–‡ä»¶

```python
import json

# å†™å…¥ JSON
data = {"name": "å¼ ä¸‰", "age": 25}
with open("data.json", "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

# è¯»å– JSON
with open("data.json", "r", encoding="utf-8") as f:
    data = json.load(f)
    print(data["name"])  # "å¼ ä¸‰"
```

### AI ä¸­çš„å®é™…ä½¿ç”¨

```python
# åŠ è½½ API å¯†é’¥
from dotenv import load_dotenv
import os
load_dotenv()  # ä» .env æ–‡ä»¶åŠ è½½
api_key = os.getenv("OPENAI_API_KEY")

# è¯»å–æç¤ºè¯æ¨¡æ¿
with open("prompts/qa_template.txt", "r", encoding="utf-8") as f:
    template = f.read()

# ä¿å­˜å¯¹è¯å†å²
chat_history = [
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯AIï¼Ÿ"},
    {"role": "assistant", "content": "AIæ˜¯äººå·¥æ™ºèƒ½..."}
]
with open("history.json", "w", encoding="utf-8") as f:
    json.dump(chat_history, f, ensure_ascii=False, indent=2)

# æ‰¹é‡å¤„ç†æ–‡æœ¬æ–‡ä»¶
import os
txt_files = [f for f in os.listdir("documents") if f.endswith(".txt")]
for file in txt_files:
    with open(f"documents/{file}", "r", encoding="utf-8") as f:
        content = f.read()
        # å¤„ç† content...
```

---

## 13. å¸¸è§æ¨¡å¼é€ŸæŸ¥

### æ¨¡å¼1ï¼šé…ç½®å­—å…¸

```python
# AI åº”ç”¨é…ç½®
config = {
    "model": "gpt-4",
    "temperature": 0.7,
    "max_tokens": 2000,
    "stream": False
}

# ä½¿ç”¨
llm = OpenAI(**config)  # ** è§£åŒ…å­—å…¸ä¸ºå‚æ•°
```

### æ¨¡å¼2ï¼šåˆ—è¡¨æ¨å¯¼å¼ï¼ˆå¿«é€Ÿåˆ›å»ºåˆ—è¡¨ï¼‰

```python
# åŸºæœ¬è¯­æ³•
æ–°åˆ—è¡¨ = [è¡¨è¾¾å¼ for å˜é‡ in æ—§åˆ—è¡¨]

# ä¾‹å­
numbers = [1, 2, 3, 4, 5]
squares = [x**2 for x in numbers]  # [1, 4, 9, 16, 25]

# å¸¦æ¡ä»¶
evens = [x for x in numbers if x % 2 == 0]  # [2, 4]

# AI ä¸­çš„ä½¿ç”¨
pdf_files = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
file_paths = [f"documents/{file}" for file in pdf_files]
# ["documents/doc1.pdf", "documents/doc2.pdf", ...]
```

### æ¨¡å¼3ï¼šå¼‚å¸¸å¤„ç†

```python
# è¯­æ³•
try:
    # å¯èƒ½å‡ºé”™çš„ä»£ç 
except é”™è¯¯ç±»å‹:
    # å‡ºé”™åçš„å¤„ç†
```

```python
# åŸºæœ¬ä¾‹å­
try:
    result = 10 / 0
except ZeroDivisionError:
    print("ä¸èƒ½é™¤ä»¥0")

# AI ä¸­çš„ä½¿ç”¨
try:
    llm = OpenAI(api_key=api_key)
    response = llm("ä½ å¥½")
except Exception as e:
    print(f"è°ƒç”¨å¤±è´¥ï¼š{e}")

# æ–‡ä»¶æ“ä½œ
try:
    with open("config.json", "r") as f:
        config = json.load(f)
except FileNotFoundError:
    print("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
    config = {}  # ä½¿ç”¨é»˜è®¤é…ç½®
```

### æ¨¡å¼4ï¼šç¯å¢ƒå˜é‡

```python
import os
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# è¯»å–ç¯å¢ƒå˜é‡
api_key = os.getenv("OPENAI_API_KEY")
model_name = os.getenv("MODEL", "gpt-3.5-turbo")  # æä¾›é»˜è®¤å€¼

# æ£€æŸ¥
if not api_key:
    raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY")
```

---

## 14. AI å¼€å‘å¸¸ç”¨ä»£ç ç‰‡æ®µ

### ç‰‡æ®µ1ï¼šåˆå§‹åŒ– LangChain

```python
import os
from dotenv import load_dotenv
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

# åŠ è½½ API å¯†é’¥
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# åˆ›å»º LLM
llm = OpenAI(temperature=0.7)
# æˆ–ä½¿ç”¨èŠå¤©æ¨¡å‹
chat = ChatOpenAI(model="gpt-4", temperature=0.7)
```

### ç‰‡æ®µ2ï¼šåŠ è½½å’Œå¤„ç† PDF

```python
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åŠ è½½ PDF
loader = PyPDFLoader("document.pdf")
documents = loader.load()

# åˆ‡åˆ†æ–‡æ¡£
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = splitter.split_documents(documents)

print(f"åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªæ–‡æ¡£å—")
```

### ç‰‡æ®µ3ï¼šåˆ›å»ºå‘é‡æ•°æ®åº“

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# åˆ›å»ºå‘é‡æ•°æ®åº“
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"  # æŒä¹…åŒ–ä¿å­˜
)

# æœç´¢
results = vectorstore.similarity_search("æŸ¥è¯¢å†…å®¹", k=3)
for result in results:
    print(result.page_content)
```

### ç‰‡æ®µ4ï¼šé—®ç­”é“¾

```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# åˆ›å»ºé—®ç­”é“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(temperature=0),
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# æé—®
question = "è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
result = qa_chain({"query": question})

print("ç­”æ¡ˆï¼š", result["result"])
print("æ¥æºï¼š", result["source_documents"])
```

### ç‰‡æ®µ5ï¼šå¯¹è¯è®°å¿†

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# åˆ›å»ºè®°å¿†
memory = ConversationBufferMemory()

# åˆ›å»ºå¯¹è¯é“¾
conversation = ConversationChain(
    llm=llm,
    memory=memory
)

# å¤šè½®å¯¹è¯
response1 = conversation.run("æˆ‘å«å¼ ä¸‰")
response2 = conversation.run("æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")  # ä¼šè®°å¾—"å¼ ä¸‰"
```

---

## 15. è°ƒè¯•æŠ€å·§

### æŠ€å·§1ï¼šæ‰“å°å˜é‡

```python
# æŸ¥çœ‹å˜é‡å€¼
name = "å¼ ä¸‰"
print(name)

# æŸ¥çœ‹ç±»å‹
print(type(name))  # <class 'str'>

# æŸ¥çœ‹å¯¹è±¡æ‰€æœ‰å±æ€§å’Œæ–¹æ³•
print(dir(name))

# f-string è°ƒè¯•
print(f"name = {name}")
print(f"len(name) = {len(name)}")
```

### æŠ€å·§2ï¼šæ£€æŸ¥æ­¥éª¤

```python
# å¤„ç†æµç¨‹ä¸­åŠ æ‰“å°
print("1. å¼€å§‹åŠ è½½æ–‡æ¡£...")
documents = loader.load()
print(f"   åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

print("2. å¼€å§‹åˆ‡åˆ†...")
chunks = splitter.split_documents(documents)
print(f"   åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—")

print("3. å¼€å§‹å‘é‡åŒ–...")
vectorstore = Chroma.from_documents(chunks, embeddings)
print("   å‘é‡åŒ–å®Œæˆ")
```

### æŠ€å·§3ï¼šæŸ¥çœ‹å¯¹è±¡å†…å®¹

```python
# æŸ¥çœ‹åˆ—è¡¨
print(f"åˆ—è¡¨é•¿åº¦: {len(my_list)}")
print(f"å‰3ä¸ª: {my_list[:3]}")

# æŸ¥çœ‹å­—å…¸
print(f"å­—å…¸é”®: {config.keys()}")
print(f"å­—å…¸å†…å®¹: {config}")

# æŸ¥çœ‹æ–‡æ¡£å†…å®¹
print(f"æ–‡æ¡£å†…å®¹å‰100å­—: {document.page_content[:100]}")
print(f"æ–‡æ¡£å…ƒæ•°æ®: {document.metadata}")
```

---

## æ€»ç»“

### å¿…é¡»æŒæ¡ï¼ˆé©¬ä¸Šå°±ä¼šç”¨åˆ°ï¼‰

- âœ… **å˜é‡èµ‹å€¼**ï¼š`x = 10`
- âœ… **å­—ç¬¦ä¸²**ï¼š`"æ–‡æœ¬"` å’Œ `f"å˜é‡{x}"`
- âœ… **å­—å…¸**ï¼š`{"key": "value"}`
- âœ… **åˆ—è¡¨**ï¼š`[1, 2, 3]`
- âœ… **å¯¼å…¥**ï¼š`from xxx import yyy`
- âœ… **æ–¹æ³•è°ƒç”¨**ï¼š`å¯¹è±¡.æ–¹æ³•()`
- âœ… **for å¾ªç¯**ï¼š`for x in list:`

### æ…¢æ…¢å­¦ï¼ˆä»¥åä¼šç”¨åˆ°ï¼‰

- â³ **å‡½æ•°å®šä¹‰**ï¼š`def function():`
- â³ **æ¡ä»¶åˆ¤æ–­**ï¼š`if/elif/else`
- â³ **ç±»å’Œå¯¹è±¡**ï¼šç†è§£æ¦‚å¿µå³å¯
- â³ **å¼‚å¸¸å¤„ç†**ï¼š`try/except`

### å­¦ä¹ æ–¹æ³•

1. **å…ˆçœ‹ä¾‹å­ï¼Œå†ç†è§£è¯­æ³•**
2. **ä¸è¦è®°è¯­æ³•ï¼Œå¤šå†™å‡ éè‡ªç„¶å°±ä¼š**
3. **é‡åˆ°ä¸æ‡‚çš„å°±æœç´¢æˆ–é—®æˆ‘**
4. **ä»ç®€å•æ”¹èµ·ï¼Œæ…¢æ…¢å­¦ä¼šå†™æ–°ä»£ç **

---

**ç¬”è®°åˆ›å»ºæ—¶é—´ï¼š** 2025-11-19
**ç”¨é€”ï¼š** AI åº”ç”¨å¼€å‘ Python åŸºç¡€
**å»ºè®®ï¼š** è¾¹åšé¡¹ç›®è¾¹æŸ¥è¿™ä»½ç¬”è®°ï¼Œç”¨åˆ°å“ªä¸ªæŸ¥å“ªä¸ª
